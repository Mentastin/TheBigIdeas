\chapter{Normed and Banach spaces}
In this chapter we will always use either $\mathbb{F} = \R$ or $\mathbb{F} = \C$.

\begin{definition}
\begin{itemize}
\item A \udef{normed space} is a vector space equipped with a norm.
\item A \udef{Banach space} is a normed vector space that is complete as a metric space.
\end{itemize}
\end{definition}

Note that Hausdorff is not just for technical simplification. It is important for the lattice structure of subspaces, see \ref{subBanachSpaceLattice}.

A finite-dimensional normed space is automatically a Banach by proposition \ref{finiteDimComplete}.

Every proper subspace $U$ of a normed vector space $V$ has empty interior.
A nice consequence of this is that any closed proper subspace is necessarily nowhere dense. So if V is a Banach space, the Baire category theorem implies that V cannot be a countable union of closed proper subspaces. In particular, an infinite dimensional Banach space cannot be a countable union of finite dimensional subspaces. This means, for example, that a vector space of countable dimension (e.g\ the space of polynomials) cannot be equipped with a complete norm.

The space $\Bounded(V,W)$ is a Banach space.


Complemented subspace problem: \url{https://arxiv.org/pdf/math/0501048v1.pdf}


TODO: \url{https://math.stackexchange.com/questions/2151779/normed-vector-spaces-over-finite-fields/2568231}

\section{Normed spaces}

\begin{lemma}
A subspace of a normed vector space is a normed space, with the norm given by the restriction of the norm in the larger space.
\end{lemma}

\begin{definition}
A vector with norm 1 is called a \udef{unit vector}. Unit vectors are often written with a hat:
\[ \norm{\vhat{v}} = 1. \]
\end{definition}

\subsection{Uniform norm}
\begin{lemma} \label{vectorSpaceUniformNorm}
Let $\sSet{V,\norm{\cdot}}$ be a normed vector space and $\sSet{X,d}$ a metric space. The uniform norm on $(X\to V)$ is a norm.
\end{lemma}
By \ref{groupUniformNorm}, the norm convergence of this norm is the uniform convergence. Note that this norm is \emph{not} the operator norm.
\begin{proof}
It is a group norm by \ref{groupUniformNorm}. We just need to show that it is positively homogeneous:
\[ \norm{\lambda f}_u = \sup_{x\in X}\norm{\lambda f(x)} = \sup_{x\in X}|\lambda|\norm{f(x)} = |\lambda|\sup_{x\in X}\norm{f(x)} = |\lambda|\norm{f}_u. \]
\end{proof}

\subsection{TODO from TVS theory}
\begin{proposition} \label{dualNormTopologyStrong}
Let $\sSet{V, \norm{\cdot}}$ be a normed space. The norm topology on $\dual{V}$ is equal to the strong topology $\beta(\dual{V}, V)$.
\end{proposition}
\begin{proof}
TODO!
\end{proof}

\begin{lemma} \label{polarOfBall}
Let $\sSet{V, \norm{\cdot}}$ be a normed space and $\epsilon >0$. Then $\cball_V(0,\epsilon)^\pol = \cball_{\dual{V}}(0,\epsilon)$.
\end{lemma}
\begin{proof}
We have
\begin{align*}
f\in \cball_V(0,\epsilon)^\pol &\iff \forall x\in \cball_V(0,\epsilon): \; \abspair{f,x} \leq 1 \\
&\iff \forall x\in \cball_V(0,1): \; \abspair{f,\epsilon^{-1}x} \leq 1 \\
&\iff \forall x\in \cball_V(0,1): \; \abspair{f,x} \leq \epsilon \\
&\iff \norm{f} \leq \epsilon \\
&\iff f\in \cball_{\dual{V}}(0,\epsilon).
\end{align*}
\end{proof}

\begin{proposition}
Let $\sSet{V, \norm{\cdot}}$ be a normed space. Then $\cball_{\dual{V}}(0, 1)$ is pointwise compact.
\end{proposition}
\begin{proof}
By \ref{polarOfBall} and \ref{alaogluTheorem}.
\end{proof}

\subsection{The topology of a normed space}
\begin{definition}
Let $\sSet{V,\norm{\cdot}}$ be a normed space. The initial vector space convergence w.r.t. the norm is called the \udef{norm convergence}.
\end{definition}
The norm convergence is topological TODO ref(!). Its topology is called the \udef{norm topology}.

\begin{example}
The norm convergence is \emph{not} the initial convergence w.r.t. to the norm.

In the initial convergence w.r.t. to the norm, all vectors of the same norm are indistinguishable, so this convergence space is not $T_0$.

On the other hand, $\{0\}$ is closed in $\R$ and thus its preimage $\{0\} \subset V$ is closed in the norm topology (TODO ref preimage closed is closed). By \ref{HausdorffCriterionConvergenceGroup}, we have that the norm convergence must be Hausdorff, or $T_2$.
\end{example}


\begin{proposition}
The norm convergence is topological and metric.

Every normed space can be viewed as a metric space with the metric $d:V\times V \to \interval[co]{0,\infty}$ given by
\[ d(x,y) = \norm{x-y}. \]
This metric has the properties of
\begin{itemize}[leftmargin=6cm]
\item[\textbf{Translation invariance}] $d(x+a, y+a) = d(x,y)$;
\item[\textbf{Scaling}] $d(\lambda x, \lambda y) = |\lambda|d(x,y)$.
\end{itemize}
Conversely, any metric with translation invariance and scaling determines a norm:
\[ \norm{x} = d(x,\vec{0}). \]
Passing from norm to metric back to norm, we recover the original norm.
\end{proposition}
\begin{lemma}
A linear map $L:V\to W$ between normed spaces is an isometry for the metric \textup{if and only if} it preserves the norm, i.e.\
\[ \forall v\in V: \quad \norm{v}_V = \norm{L(v)}_W. \]
\end{lemma}
\begin{proof}
Assume $L$ is an isometry, then
\[ \norm{v} = d(v,\vec{0}) = d(L(v),L(\vec{0})) = \norm{L(v) - L(\vec{0})} = \norm{L(v) - \vec{0}} = \norm{L(v)}. \]
Assume $L$ preserves the norm, then
\[ d(L(v_1), d(v_2)) = \norm{L(v_1)-L(v_2)} = \norm{L(v_1-v_2)} = \norm{v_1-v_2} = d(v_1,v_2). \]
\end{proof}

\begin{proposition}
Let $V$ be a normed vector space, then the norm $\norm{\cdot}:V\to \R$ is a continuous map.
\end{proposition}
\begin{proof}
The reverse triangle inequality, $|\norm{v}-\norm{w}| \leq \norm{v-w}$, implies that the norm is Lipschitz continuous with Lipschitz constant $1$, so we can use \ref{LipschitzcontinuousContinuous}.
\end{proof}

\subsubsection{Subsets of normed spaces}

\begin{lemma}
Every proper subspace $U$ of a normed vector space $V$ has empty interior.
\end{lemma}
\begin{proof}
Suppose $U$ has a non-empty interior. Then it contains some ball $B(u,\epsilon)$. Now every vector in $V$ can be translated and rescaled to fit inside the ball $B(u,\epsilon)$. Indeed let $v\in V$ and set $u' = u+ \frac{\epsilon}{2\norm{v}}v \in B(u,\epsilon)$. Then, since $U$ is a subspace $v = \frac{2\norm{v}}{\epsilon}(u'-u)\in U$. So $U=V$.
\end{proof}

\begin{lemma}[Riesz's lemma] \label{RieszsLemma}
Let $V$ be a normed vector space. Given a non-dense subspace $X$ and a number $\theta<1$, there exists a unit vector $v\in V$ such that
\[ \theta \leq d(X,v) = \inf_{x\in X}\norm{x-v}. \]
\end{lemma}
\begin{proof}
Take a vector $v_1$ not in the closure of $X$ and put $a = \inf_{x\in X}\norm{x-v_1}$. Then $a>0$ by lemma \ref{sequencesSupInf}. For $\epsilon > 0$, let $x_1\in X$ be such that $\norm{x_1+v_1}<a+\epsilon$. Then take
\[ v = \frac{v_1 - x_1}{\norm{v_1-x_1}} \qquad \text{so} \qquad \norm{v}=1. \]
And
\[ \inf_{x\in X}\norm{x-v} = \inf_{x\in X}\norm{x-\frac{v_1 - x_1}{\norm{v_1-x_1}}} = \inf_{x\in X}\norm{\frac{x-v_1 + x_1}{\norm{v_1-x_1}}} = \frac{\inf_{x\in X}\norm{x-v_1}}{\norm{v_1-x_1}} \geq \frac{a}{a+\epsilon}. \]
By choosing $\epsilon >0$ small, $a/(a+\epsilon)$ can be made arbitrarily close to $1$.
\end{proof}
For finite-dimensional spaces we can even take $\theta=1$.

\begin{lemma} \label{boundedSetNormedSpace}
Let $V$ be a normed space and $B\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $B$ is bounded;
\item $B$ is vN bounded;
\item $B\subseteq \cball(0, M)$.
\end{enumerate}
\end{lemma}
\begin{proof}
Points (1) and (2) are equivalent by \ref{boundednessTVS}. To show the equivalence of (2) and (3) using \ref{vonNeumannBoundednessAbsorption}.

First assume $B$ is vN bounded. Then $B$ is absorbed by $\cball(0,1)$, so there exists $\epsilon > 0$ such that $\cball(0,\epsilon)\cdot B \subseteq \cball{0,1}$. This implies $B\subseteq \cball(0,1)\cdot B \subseteq \cball(0,\epsilon^{-1})$. Setting $M\defeq \epsilon^{-1}$ gives point (3).

Finally assume $B\subseteq \cball(0, M)$, so $M^{-1}B \subseteq \cball(0,1)$. Take an arbitrary neighbourhood $U$ of $0$. Then $\cball(0,\epsilon) \subseteq U$ for some $\epsilon > 0$. We have
\[ \cball(0, \epsilon M^{-1})\cdot B = \cball(0, \epsilon)\cdot M^{-1}B \subseteq \cball(0, \epsilon)\cdot \cball(0, 1) = \cball(0, \epsilon) \subseteq U. \]
Thus $B$ is absorbed by $U$.
\end{proof}

\subsubsection{Continuous operators}
\begin{theorem} \label{boundedLinearMaps}
Let $L$ be a linear operator between normed spaces $V,W$. The following are equivalent:
\begin{enumerate}
\item $L$ is continuous;
\item $L$ is continuous at $0$;
\item $L$ is uniformly continuous;
\item $L$ is Lipschitz continuous.
\end{enumerate}
\end{theorem}
\begin{proof}
The equivalences $(1) \Leftrightarrow (2) \Leftrightarrow (3)$ are given by \ref{uniformContinuityGroupHomomorphism}. The implication $(4)\Rightarrow (3)$ is given by \ref{LipschitzcontinuousContinuous}.

Finally we prove $(2)\Rightarrow (4)$. From continuity at zero, there exists a $\delta>0$ such that $\norm{L(h)} = \norm{L(h)-L(0)} \leq 1$ for all $h\in \dom(L)$ with $\norm{h}\leq \delta$. Thus for all nonzero $v\in \dom(L)$
\[ \norm{L(v)} = \norm{\frac{\norm{v}}{\delta}L(\delta \frac{v}{\norm{v}})} = \frac{\norm{v}}{\delta}\norm{L(\delta \frac{v}{\norm{v}})}\leq \frac{\norm{v}}{\delta}. \]
\end{proof}
\begin{corollary}
A linear operator $L:V\to W$ between normed spaces is a homeomorphism \textup{if and only if} there exists $C_1,C_2> 0$ such that
\[ \forall x\in V:\qquad C_1\norm{x}\leq \norm{L(x)} \leq C_2\norm{x}. \]
\end{corollary}

\begin{definition}
An operator $L$ between normed vector spaces is called \udef{bounded} if it is (Lipschitz) continuous.

The set of bounded operators from $V$ to $W$ is denoted $\Bounded(V,W)$. If $V=W$, we write $\Bounded(V)$.
\end{definition}
In other words, bounded means there exists an $M>0$ such that $\forall v\in \dom(L)$
\[ \norm{L(v)} \leq M \norm{v}. \]

\begin{proposition} \label{boundedAntiLinearMaps}
An antilinear map between complex vector spaces is continuous \textup{if and only if} it is bounded.
\end{proposition}
\begin{proof}
An antilinear map $A:V\to W$ is an $\R$-linear map $A:V_\R\to W_\R$. Now $V_\R, W_\R$ have the same norms as $V,W$ and thus the same topology. So $A:V\to W$ is continuous if and only if $A:V_\R\to W_\R$ is continuous.
\end{proof}


\subsubsection{Equicontinuity}
\begin{proposition} \label{equicontinuityBoundedOperators}
Let $X,Y$ be normed space and $K\subseteq \contLin(X,Y)$. Then $K$ is equicontinuous \textup{if and only if} $\sup_{T\in K}\norm{T} < \infty$.
\end{proposition}
TODO: This follows more easily from \ref{metricEquicontinuity}. Alternatively, it can also follow from \ref{vonNeumannBoundedImpliesEquicontinuous}.
\begin{proof}
By \ref{equicontinuityGroupHomomorphisms} we have that the equicontinuity of $K$ is equivalent to
\begin{align*}
\upset \evalMap^{\imf\imf}\big(\{K\}\otimes \neighbourhoods_{X}(0)\big) \to 0 &\iff \upset \evalMap^{\imf\imf}\big(\{K\}\otimes \{\cball_X(0,\epsilon)\}_{\epsilon > 0}\big) \to 0 \\
&\iff \upset \evalMap^{\imf\imf}\big(\{K\}\otimes \{\cball_X(0,1)\}\big)\cdot \{\cball_{\R}(0,\epsilon)\}_{\epsilon > 0} \to 0 \\
&\iff \text{$\evalMap^{\imf}\big(K\times \cball_X(0,1)\big)$ is vN bounded} \\
&\iff \exists M\geq 0: \; \sup_{\substack{T\in K \\ x\in \cball_X(0,1)}} \norm{Tx} \leq M \\
&\iff \exists M\geq 0: \; \sup_{T\in K} \norm{T} \leq M \\
&\iff \sup_{T\in K} \norm{T} < \infty,
\end{align*}
where we have used \ref{boundedSetNormedSpace}.
\end{proof}


\subsubsection{Comparison of norm topologies}
\begin{definition}
Let $V$ be a vector space and $\norm{\cdot}_1$, $\norm{\cdot}_2$ two norms on $V$. We say
\begin{itemize}
\item $\norm{\cdot}_1$ is \udef{bounded} by $\norm{\cdot}_2$ if there exists $C\in \R$ such that $\forall v\in V: \norm{v}_1 \leq C\norm{v}_2$;
\item $\norm{\cdot}_1$ and $\norm{\cdot}_2$ are \udef{equivalent} if each is bounded by the other.
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a vector space and $\norm{\cdot}_1$, $\norm{\cdot}_2$ two norms on $V$. These norms are equivalent \textup{if and only if} there exists a $C>0$ such that
\[ \frac{1}{C}\norm{\cdot}_1 \leq \norm{\cdot}_2 \leq C\norm{\cdot}_1. \]
\end{lemma}

\begin{lemma}
Let $V$ be a finite dimensional vector space and $\norm{\cdot}_1, \norm{\cdot}_2$ norms on $V$. 
\end{lemma}

\begin{proposition} \label{normComparison}
Let $V$ be a vector space and $\norm{\cdot}_1$, $\norm{\cdot}_2$ two norms on $V$. Then the following are equivalent:
\begin{enumerate}
\item $\norm{\cdot}_2$ is bounded by $\norm{\cdot}_1$;
\item $\id_V: \sSet{V,\norm{\cdot}_1} \to \sSet{V,\norm{\cdot}_2}$ is uniformly continuous;
\item the norm topology of $\norm{\cdot}_1$ is finer than the norm topology of $\norm{\cdot}_2$.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Leftrightarrow (2)$ By \ref{boundedLinearMaps} both are equivalent to
\[ \exists C>0: \forall v\in V: \qquad \norm{v}_2 = \norm{\id_V((v))}_2 \leq C\norm{v}_1. \]

$(2) \Leftrightarrow (3)$ Follows straight from \ref{identityContinuity}.
\end{proof}
\begin{corollary}
Equivalent norms induce the same topology.
\end{corollary}

\subsection{Properties of balls}
\begin{lemma} \label{ballAdherenceInherence}
Let $V$ be a normed vector space over $\F$. Then
\begin{enumerate}
\item $\ball(0,1)$ is $\mathfrak{a}$-open;
\item $\cball(0,1)$ is $\mathfrak{a}$-closed;
\item $\inh_\mathfrak{a}\big(\cball(0,1)\big) = \ball(0,1)$;
\item $\adh_\mathfrak{a}\big(\ball(0,1)\big) = \cball(0,1)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1,2) Since $\ball(0,1)$ is open and $\cball(0,1)$ is closed, \ref{metricConvergenceNeighbourhood}, and $\mathfrak{a}$ is stronger than the metric convergence, by \ref{algebraicConvergenceStrongerThanVectorConvergence}, the results follow from \ref{openClosedConvergenceInclusions}.

(3) First note that
\[ \ball(0,1) = \interior_{\norm{\cdot}}\big(\ball(0,1)) \subseteq \interior_{\norm{\cdot}}\big(\cball(0,1)) \subseteq \inh_{\mathfrak{a}}\big(\cball(0,1)), \]
by \ref{interleavedAdherenceInherenceInclusion} and \ref{principalInherenceAdherenceProperties} (since $\mathfrak{a}$ is stronger than the norm convergence).

For the converse, take $x\in \inh_{\mathfrak{a}}\big(\cball(0,1))$. Then, by \ref{constructionsInAlgebraicConvergence} (and since $x\in V$), there exists $\Gamma\in \neighbourhoods_{\F}(0)$ such that $x+ \Gamma\cdot x \subseteq \cball(0,1)$. Then there exists $\epsilon >0$ such that $\ball_\F(0,\epsilon) \subseteq \Gamma$. This implies that
\[ (1+ \epsilon/2)x = x + \frac{\epsilon}{2} x \in x+ \Gamma\cdot x \subseteq \cball(0,1), \]
so $(1+\epsilon/2)\norm{x} \leq 1$ and thus $\norm{x} \leq \frac{1}{1+\epsilon / 2} < 1$, which implies $x\in \ball(0,1)$.

(4) As before, we have
\[ \cball(0,1) = \closure_{\norm{\cdot}}\big(\cball(0,1)\big) \supseteq \closure_{\norm{\cdot}}\big(\ball(0,1)\big) \supseteq \adh_{\mathfrak{a}}\big(\ball(0,1)\big), \]
by \ref{interleavedAdherenceInherenceInclusion} and \ref{principalInherenceAdherenceProperties}.

For the converse, take $x\in \cball(0,1)$. We aim to show $x\in \adh_\mathfrak{a}\big(\ball(0,1)\big)$ using \ref{constructionsInAlgebraicConvergence}. To that end, set $v = x$ and take an arbitrary $\Gamma\in \neighbourhoods_\F(0)$. Then there exists $0<\epsilon <2$ such that $\ball_\F(0,\epsilon) \subseteq \Gamma$. Then $(1-\epsilon /2)x \in x + \Gamma\cdot x$ and
\[ \norm{\Big(1-\frac{\epsilon}{2}\Big)x} = \Big(1-\frac{\epsilon}{2}\Big)\norm{x} \leq 1-\frac{\epsilon}{2} < 1, \]
so $(1-\epsilon /2)x\in \ball(0,1)$. Thus $\big(x + \Gamma\cdot x\big)\mesh \ball(0,1)$.
\end{proof}

\begin{lemma} \label{extremePointsUnitBall}
Let $V$ be a real or complex normed vector space. Then
\begin{enumerate}
\item $\ext\big(\ball(0,1)\big) = \emptyset$;
\item $\ext\big(\cball(0,1)\big) \subseteq \cball(0,1)\setminus \ball(0,1)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Take arbitrary $x\in \ball(0,1)$. Then $\norm{x}< 1$, so we have $0 < \frac{1}{2} + \frac{\norm{x}}{2} < 1$ and $-\frac{1}{2} \leq \frac{3}{2}\norm{x} - \frac{1}{2} < \frac{3}{2} - \frac{1}{2} = 1$. Set $a \defeq \big(\frac{1}{2} + \frac{\norm{x}}{2}\big) \frac{x}{\norm{x}}$ and $b \defeq \big(\frac{3}{2}\norm{x} - \frac{1}{2}\big)\frac{x}{\norm{x}}$. Then $a,b\in\ball(0,1)$, $a\neq x$ and
\[ \frac{1}{2}(a+b) = \frac{1}{2}\bigg(\Big(\frac{1}{2} + \frac{1}{2}\norm{x}\Big)\frac{x}{\norm{x}} + \Big(\frac{3}{2}\norm{x} - \frac{1}{2}\Big)\frac{x}{\norm{x}}\bigg) = \frac{1}{2}\Big(\frac{4}{2}\norm{x}\frac{x}{\norm{x}}\Big) = x, \]
so $x\notin \ext\big(\ball(0,1)\big)$ by \ref{notExtremePointLemma}.

(2) This follows immediately from (1) and \ref{extremePointsSubset}.
\end{proof}

\begin{lemma} \label{strictConvexityUnitBall}
Let $V$ be a real normed vector space. Then the following are equivalent:
\begin{enumerate}
\item $\cball(0,1)$ is strictly convex;
\item $\cball(0,1)\setminus\ball(0,1) \subseteq \ext\big(\cball(0,1)\big)$;
\item $\cball(0,1)\setminus\ball(0,1) = \ext\big(\cball(0,1)\big)$;
\item $\norm{x+y} = \norm{x}+\norm{y}$ implies $\exists \lambda \geq 0: x = \lambda y$ for all $x,y \in V$;
\item $\norm{rx+(1-r)y}< 1$ for all $x\neq y\in \cball(0,1)$ and $0< r < 1$;
\item $\norm{x+y}< 2$ for all $x\neq y\in \cball(0,1)$.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Leftrightarrow (2)$ Immediate from \ref{strictConvexityEquivalentsConvexSubset} and \ref{ballAdherenceInherence}.

$(2) \Rightarrow (3)$ Immediate from \ref{extremePointsUnitBall}.

$(3) \Rightarrow (4)$ Suppose $x,y\in V$ are such that $\norm{x+y} = \norm{x} + \norm{y}$. WLOG we may assume $\norm{x}\leq \norm{y}$. Then
\begin{align*}
2 &\geq \norm{\frac{x}{\norm{x}} + \frac{y}{\norm{y}}} \\
&= \frac{1}{\norm{x}}\norm{x + \frac{\norm{x}}{\norm{y}}y} \\
&= \frac{1}{\norm{x}}\norm{x + y - \Big(1-\frac{\norm{x}}{\norm{y}}\Big)y} \\
&\geq \frac{1}{\norm{x}}\bigg|\norm{x + y} - \Big(1-\frac{\norm{x}}{\norm{y}}\Big)\norm{y}\bigg| \\
&= \frac{1}{\norm{x}}\bigg|\norm{x} + \norm{y} - \big(\norm{y}- \norm{x}\big)\bigg| \\
&= \frac{1}{\norm{x}}2\norm{x} = 2,
\end{align*}
so $\norm{\frac{1}{2}\big(\frac{x}{\norm{x}} + \frac{y}{\norm{y}}\big)} = 1$, which means that $\frac{1}{2}\big(\frac{x}{\norm{x}} + \frac{y}{\norm{y}}\big) \in \cball(0,1)\setminus\ball(0,1) = \ext\big(\cball(0,1)\big)$. Since $\frac{x}{\norm{x}}, \frac{y}{\norm{y}}\in\cball(0,1)$, we must have $\frac{x}{\norm{x}} = \frac{y}{\norm{y}}$ by \ref{notExtremePointLemma}. Thus $x = \frac{\norm{x}}{\norm{y}}y$.

$(4) \Rightarrow (5)$ Take arbitrary $x,y\in \cball(0,1)$ and $0<r<1$. By the triangle inequality we have
\[ \norm{rx + (1-r)y} \leq r\norm{x} + (1-r)\norm{y} \leq 1. \]
If either $\norm{x}<1$ or $\norm{y}<1$ we are done.

Now suppose $\norm{x} = 1 \norm{y}$. Suppose, towards a contradiction, that $\norm{rx+(1-r)y} = 1 = \norm{rx} + \norm{(1-r)y}$. Then, by assumption, $rx = \lambda (1-r) y$. Taking norms gives $r = r\norm{x} = \lambda (1-r)\norm{y} = \lambda (1-r)$, so $\lambda = \frac{r}{1-r}$ and $x = \lambda \frac{1-r}{r}y = y$, which contradicts the assumption $x\neq y$.

$(5) \Rightarrow (6)$ Set $r = \frac{1}{2}$.

$(6) \Rightarrow (2)$ Take $x\in \cball(0,1)\setminus \ball(0,1)$. Suppose, towards a contradiction, that $x\notin \ext\big(\cball(0,1)\big)$. Then $x = frac{a+b}{2}$ for some $a\neq b\in \cball(0,1)$, by \ref{notExtremePointLemma}. By assumption $\norm{x} = \frac{1}{2}\norm{a+b} < 1$, so $x\notin \cball(0,1)$. This is a contradiction.
\end{proof}

\begin{example}
The unit ball of an inner product space is strictly convex, since, by the parallelogram law, we have
\[ \norm{v+w}^2 = 2\big(\norm{v}^2 + \norm{w}^2\big) - \norm{v-w}^2 \leq 4 - \norm{v-w}^2 < 4 \]
and so $\norm{v+w} <2$ for all $v\neq w \in \cball(0,1)$.
\end{example}

\subsection{Linear independence and bases in normed spaces}
\url{https://math.stackexchange.com/questions/1518029/are-uncountable-schauder-like-bases-studied-used}

\subsection{Finite-dimensional normed (sub)spaces}

\begin{lemma} \label{coordinateContinuity}
Let $V$ be a normed vector space and $\{x_1, \ldots, x_n\}$ a linearly independent set of vectors. There exists a $c>0$ such that $\forall \alpha_1,\ldots, \alpha_n \in \mathbb{F}$:
\[ c(|\alpha_1|+\ldots+|\alpha_n|) \leq \norm{\alpha_1x_1 + \ldots + \alpha_nx_n}. \]
\end{lemma}
\begin{proof}
First note that the function
\[ (+)\circ \big(|\cdot|\circ \proj_1 \big| \ldots \big| |\cdot|\circ \proj_n\big): \F^n \to \F: (\alpha_1,\ldots, \alpha_n)\mapsto |\alpha_1|+\ldots + |\alpha_n| \]
is continuous by construction and \ref{normUniformlyContinuous}. Thus
\[ S \defeq \setbuilder{(\alpha_1, \ldots, \alpha_n)\in \F^n}{|\alpha_1|+\ldots + |\alpha_n| = 1} = \Big((+)\circ \big(|\cdot|\circ \proj_1 \big| \ldots \big| |\cdot|\circ \proj_n\big)\Big)^{\preimf}\big(\{1\}\big) \]
is closed by \ref{preimageOpenClosed} and \ref{FrechetCharacterisation}.

Now $S \subseteq \cball(0,1)^n$, which is compact by (TODO ref $\cball(0,1)$ compact) and \ref{TychonoffsTheorem}. So $S$ is compact by \ref{compactClosedSets}.

Now consider the function
\[ f = \norm{\cdot}\circ (+) \circ \big((\cdot)\circ (\proj_1,\constant{x_1})\big|\ldots\big|(\cdot)\circ (\proj_n,\constant{x_n})\big): \F^n \to \R^+: (\alpha_1,\ldots, \alpha_n)\mapsto \norm{\alpha_1 x_1 + \ldots + \alpha_n x_n}, \]
which is continuous by construction and \ref{normUniformlyContinuous}.

Thus $f^{\imf}(S)$ attains a minimum $c$ by the extreme value theorem \ref{extremeValueTheorem}. This minimum is not $0$ since $\norm{\alpha_1 x_1 + \ldots + \alpha_n x_n} = 0$ implies $\alpha_1 x_1 + \ldots + \alpha_n x_n = 0$ and thus that $\{x_1, \ldots, x_n\}$ is linearly dependent.

Now take arbitrary $\alpha_1,\ldots, \alpha_n \in \mathbb{F}$. Set $\alpha_1' \defeq \frac{\alpha_1}{|\alpha_1|+\ldots + |\alpha_n|}, \ldots, \alpha_n' \defeq \frac{\alpha_n}{|\alpha_1|+\ldots + |\alpha_n|}$. Then
\begin{align*}
c &\leq \norm{\alpha'_1 x_1 + \ldots + \alpha'_n x_n} \\
&= \norm{\frac{\alpha_1}{|\alpha_1|+\ldots + |\alpha_n|} x_1 + \ldots + \frac{\alpha_n}{|\alpha_1|+\ldots + |\alpha_n|} x_n} \\
&= \frac{1}{|\alpha_1|+\ldots + |\alpha_n|}\norm{\alpha_1 x_1 + \ldots + \alpha_n x_n},
\end{align*}
which implies the result.
\end{proof}

\begin{proposition} \label{finiteDimComplete}
Every finite-dimensional subspace of a normed vector space is complete.
\end{proposition}
\begin{proof}
Take a basis $\{e_i\}_{i=1}^n$ and let $c$ be as in lemma \ref{coordinateContinuity}. Consider an arbitrary Cauchy sequence $(v_k)_{k\in\N}$. We can write
\[ v_k = \alpha_{k,1}e_1 + \ldots + \alpha_{k,n}e_n. \]
We claim that $(\alpha_{k,i})_{k\in\N}$ is Cauchy in $\mathbb{F}$ for all $1\leq i\leq n$. Indeed, take an $\epsilon>0$. By the Cauchy nature of $(v_k)_{k\in\N}$ we can find a $k_0$ such that $\forall k', k''>k_0:$
\[ c\epsilon > \norm{v_{k'} - v_{k''}} \geq \norm{\sum_{i=1}^n (\alpha_{k',i}-\alpha_{k'',i})e_i}\geq c\sum_{i=1}^n |\alpha_{k',i}-\alpha_{k'',i}| \geq c |\alpha_{k',i}-\alpha_{k'',i}|. \]
Dividing left and right by $c$ gives exactly the Cauchy condition for each $1\leq i\leq n$. By the completeness of $\R$ or $\C$, each of these sequences has a limit $\alpha_i$.
Then $v= \sum_{i=1}^n\alpha_ie_i$ is an element of the subspace. The sequence $(v_k)$ converges to $v$ because
\[ \norm{v_k-v} = \norm{\sum_{i=1}^n (\alpha_{k,i}-\alpha_i)e_i} \leq \sum_{i=1}^n |\alpha_{k,i}-\alpha_i|\norm{e_i} \]
and the right-hand side goes to zero as $k\to \infty$.
\end{proof}
\begin{corollary} \label{finiteDimClosed}
Every finite-dimensional subspace of a normed vector space is closed.
\end{corollary}
TODO ref for proof.

\begin{proposition}
On a finite-dimensional vector space all norms are equivalent.
\end{proposition}
\begin{proof}
Let $\{e_i\}_{i=1}^n$ be a basis and take an arbitrary vector $v = \sum_{i=1}^nv_ie_i$. Let $\norm{\cdot}_1$ and $\norm{\cdot}_2$ be two norms.
We calculate
\[ \norm{v}_1 \leq \sum_{i=1}^n|v_i|\norm{e_i}_1 \leq k\sum_{i=1}^n|v_i| \leq \frac{k}{c_2}\norm{v}_2 \]
where the first inequality is the triangle inequality, the second comes from $k=\max\norm{e_i}_1$ and the third is lemma \ref{coordinateContinuity}. A similar calculation gives the other necessary inequality.
\end{proof}

\begin{proposition}
In a finite-dimensional normed space $V$, any subset $M \subseteq V$ is compact if and only if $M$ is closed and bounded.
\end{proposition}
\begin{proof}
TODO + ref Heine Borel property
\end{proof}


TODO: move up?
\begin{proposition} \label{compactnessUnitBall}
The closed unit ball of a vector space is compact \textup{if and only if} the vector space is finite-dimensional.
\end{proposition}
\begin{proof}
One direction is given by the previous proposition. For the other direction, we show the contrapositive: let the vector space be infinite-dimensional.
We define a sequence of unit vectors $(e_i)_{i\in\N}$ recursively as follows:
\begin{itemize}
\item $e_1$ is just a unit vector;
\item for $e_{n+1}$ apply Riesz's lemma \ref{RieszsLemma} to the subspace $\Span\{e_i\}_{i=1}^n$ and $\theta = 1/2$. This subspace cannot be dense, because it is a closed (by corollary \ref{finiteDimClosed}) finite-dimensional subspace of an infinite-dimensional vector space.
\end{itemize}
This yields a sequence such that for all $m,n$
\[ \norm{e_m - e_n}\geq \frac{1}{2}. \]
This sequence is not Cauchy and thus not convergent.
\end{proof}





\section{Bounded operators}
Bounded operators are Lipschitz continuous operators.

An operator between normed spaces is bounded iff it is continuous, see \ref{boundedLinearMaps}.

\begin{lemma}
Let $V,W$ be normed spaces and $T:V\to W$ a linear operator. Then $T$ is bounded \textup{if and only if} $T^{\preimf}[\ball(\vec{0},1)]$ has non-empty interior.
\end{lemma}
\begin{proof}
Assume $T$ is bounded, then $\ball(\vec{0}, \norm{T}^{-1}) \subseteq T^{\preimf}[\ball(\vec{0},1)]$, indeed for all $v\in \ball(\vec{0}, \norm{T}^{-1})$ we have
\[ \norm{Tv} \leq \norm{T}\;\norm{v} < \norm{T}\;\norm{T}^{-1} = 1.  \]
As $\ball(\vec{0}, \norm{T}^{-1})$ is open, it is contained in the interior, which is thus non-empty.

Now assume $T^{\preimf}[\ball(\vec{0},1)]$ has non-empty interior, then we can pick some $x$ in the interior and $T^{-\preimf}[\ball(\vec{0},1)]$ is a neighbourhood of $x$. Then we have that
\[ T^{\preimf}[\ball(\vec{0},1)] - x = T^{\preimf}[\ball(\vec{0},1)- T(x)] \]
is a neighbourhood of $0$. Now $T$ is bounded on $T^{\preimf}[\ball(\vec{0},1)- T(x)]$, so it is continuous by \ref{boundedOnVicinityImpliesContinuous}.
\end{proof}


\subsection{The algebra of bounded operators}
\begin{lemma} \label{existenceOperatorNorm}
Let $(V,\norm{\cdot}_V)$ and $(W,\norm{\cdot}_W)$ be normed spaces and $L\in\Lin(V, W)$. Then $L$ is bounded \textup{if and only if}
\[ \sup\setbuilder{\frac{\norm{Lx}_W}{\norm{x}_V}}{x\in V\setminus\{0\}} \] 
is finite.
\end{lemma}
\begin{definition}
Let $(V,\norm{\cdot}_V)$ and $(W,\norm{\cdot}_W)$ be normed spaces and $L\in\Lin(V, W)$ bounded. Then
\[ \norm{L} \defeq \sup\setbuilder{\frac{\norm{Lx}_W}{\norm{x}_V}}{x\in V\setminus\{0\}} \]
is called the \udef{operator norm} of $L$.
\end{definition}

\begin{proposition} \label{operatorNorm}
Let $L\in\Bounded(V,W)$ be a bounded operator and let $B(\vec{0},\epsilon)$ be an open ball centered at $\vec{0}$. Then
\begin{align*}
\norm{L} &= \frac{\sup L[B(\vec{0},\epsilon)]}{\epsilon} \\
&= \frac{\sup L[\overline{B}(\vec{0},\epsilon)]}{\epsilon} \\
&= \sup\setbuilder{\norm{Lx}}{\norm{x} = 1}.
\end{align*}
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition} \label{operatorNormIsNorm}
Let $S,T$ be compatible bounded operators. Then
\begin{enumerate}
\item $\norm{\lambda S} = |\lambda|\;\norm{S}$ for all $\lambda\in \F$;
\item $\norm{S+T} \leq \norm{S}+\norm{T}$;
\item $\norm{ST} \leq \norm{S}\norm{T}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We calculate
\[ \norm{\lambda S} = \sup_{\norm{x}=1}\norm{\lambda Sx} = \sup_{\norm{x}=1}|\lambda|\; \norm{Sx} = |\lambda| \sup_{\norm{x}=1}\norm{Sx} = |\lambda| \norm{S}. \]

(2) We calculate
\[ \norm{S+T} = \sup_{\norm{x}=1}\norm{Sx+Tx} \leq \sup_{\norm{x}=1}\big(\norm{Sx}+\norm{Tx}\big)\leq \sup_{\norm{x}=1}\big(\norm{S} + \norm{Tx}\big) = \norm{S} + \norm{T}. \]

(3) We calculate
\[ \norm{ST} = \sup_{\norm{x}=1}\norm{STx} \leq \sup_{\norm{x}=1}\big(\norm{S}\;\norm{Tx}\big) = \norm{S}\;\norm{T}. \]
\end{proof}
\begin{corollary} \label{BoundedSpace}
Let $(V,\norm{\cdot}_V)$ and $(W,\norm{\cdot}_W)$ be normed spaces. Then the set $\Bounded(V,W)$ of bounded linear maps is a normed subspace of $\Lin(V,W)$ when equipped with the operator norm.
\end{corollary}
\begin{proof}
By \ref{linearMapsVectorSpace}, $\Lin(V,W)$ is a vector space. Closure under addition and scalar multiplication follows from the proposition and \ref{operatorNorm}.

The proposition also immediately gives that the operator norm is a seminorm.

To show that it is in fact a norm, we just need to show that it is point-separating. Assume $S\in\Bounded(V,W)$ is such that $\norm{S} = 0$. Then $\sup_{\norm{x}=1}\norm{Sx} = 0$, which implies that $\norm{Sx} = 0$ for all unit vectors. Because the norm on $W$ is point-separating, this means that $Sx = 0$ for all unit vectors. Now for arbitrary $v\in V$, we have $Sv = \norm{v}\cdot S\left(\frac{v}{\norm{v}}\right) =\norm{v} \cdot 0 = 0$. Thus $S = 0$.
\end{proof}

\begin{lemma} \label{normCauchyPointwiseConvergentImpliesNormConvergent}
Let $V,W$ be normed spaces and $\seq{T_n}\in\Bounded(V,W)^\N$. If $\seq{T_n}$ is norm-Cauchy and pointwise convergent, then $\seq{T_n}$ is norm-convergent.
\end{lemma}
\begin{proof}
Let $T_n\overset{\text{ptwise}}{\longrightarrow} T$. Fix $\epsilon > 0$. Then there exists $N$ such that $\norm{T_m-T_n}\leq \epsilon$ for all $m,n\geq N$. Take arbitrary $v\in \cball_V(0,1)$. Then
\begin{align*}
\norm{(T-T_n)(v)} &= \norm{Tv - T_nv} \\
&= \lim_{m\to\infty}\norm{T_mv-T_nv} \\
&= \lim_{m\to\infty}\norm{(T_m-T_n)v} \\
&= \limsup_{m\to\infty}\norm{(T_m-T_n)v} \\
&\leq \sup_{m\geq N}\norm{(T_m-T_n)v} \\
&\leq \sup_{m\geq N}\norm{T_m-T_n} \\
&\leq \epsilon.
\end{align*}
Since $v\in \cball_V(0,1)$ was taken arbitrarily, we have $\norm{T-T_n} \leq \epsilon$. We conclude that $\seq{T_n}$ converges to $T$ is norm.
\end{proof}

\begin{proposition} \label{boundedOperatorsFormBanachSpace}
Let $X$ be a Banach space and $Y$ a normed space. Then
\begin{enumerate}
\item if $Y$ is complete, then $\Bounded(X,Y)$ is complete;
\item if $X\neq \{0\}$ and $\Bounded(X,Y)$ is complete, then $Y$ is complete.
\end{enumerate}
\end{proposition}
Thus, supposing $V$ is non-trivial, $\Bounded(V,W)$ is a Banach space \textup{if and only if} $W$ is a Banach space.
\begin{proof}
(1) Since $(X,Y)$ is a Banach-Steinhaus pair and $Y$ is complete (and thus quasi-complete), we can use \ref{quasiCompletenessFunctionSpaces} to conclude that $\contLin_p(X,Y)$ is quasi-complete and thus sequentially compelete by \ref{quasiCompleteImpliesSequentiallyComplete}.

Since $\Bounded(X,Y)$ is sequential, it is enough to show that it is sequentially complete, by (TODO ref!).

Now let $\seq{T_n}$ be a Cauchy sequence in $\Bounded(X,Y)$. Then it is a Cauchy sequence in  $\contLin_p(X,Y)$, which means it has a pointwise limit by sequential completeness. By \ref{normCauchyPointwiseConvergentImpliesNormConvergent} this pointwise limit is also a norm-limit.

(2) TODO ???
\end{proof}



\begin{proposition}[Bounded linear extension] \label{BLT}
Let $T:\dom(T)\subseteq X\to Y$ be a bounded operator between normed spaces. Then $T$ has a unique extension
\[ \widetilde{T}:\overline{\dom(T)}\to Y \]
where $\widetilde{T}$ is a bounded operator with $\norm*{\widetilde{T}} = \norm{T}$.
\end{proposition}
\begin{proof}
Normed vector spaces have the unique extension property because they are Hausdorff, \ref{uniqueExtensionHausdorff}. We just need to show the norm stays the same:

Clearly $\norm*{\tilde{T}} \geq \norm{T}$. For the converse take any $x\in X$. As $\overline{\dom(T)} = X$, there exists a sequence $\seq{x_i}\subset \dom(T)$ that converges to $x$. Then
\[ \norm*{\tilde{T}(x)}_Y = \norm{T\left(\lim_{i\to\infty}x_i\right)}_Y = \lim_{i\to\infty}\norm{T(x_i)}_Y \leq \lim_{i\to\infty}\norm{T}\;\norm{x_i}_X = \norm{T}\;\norm{x}_X. \]
\end{proof}


\subsubsection{The dual space}
\begin{lemma}
Let $X$ be a normed space and $Z\subset X$ a subspace. Any bounded linear functional in $\dual{Z}$ can be extended to a bounded linear functional in $\dual{X}$ with the same norm.
\end{lemma}
\begin{proof}
Let $f:Z\to \mathbb{F}$ be such a functional. Extend $f$ with the Hahn-Banach theorem \ref{seminormHahnBanach}, using $p(x) = \norm{f}_Z\norm{x}$.
\end{proof}
\begin{corollary} \label{existenceBoundedFunctionalOfSameNorm}
Let $X$ be a normed space and $x_0\neq 0$ an element of $X$. Then there exists a bounded linear functional $\omega_{x_0}$ such that
\[ \norm{\omega_{x_0}} = 1 \qquad \text{and} \qquad \omega_{x_0}(x_0)=\norm{x_0}. \]
\end{corollary}
\begin{proof}
Extend the functional $f: \Span\{x_0\}\to \mathbb{F}$ defined by
\[ f(x) = f(ax_0) = a\norm{x_0}. \]
\end{proof}
\begin{corollary} \label{normFromContinuousFunctionals}
Let $X$ be a normed space. Then $\forall x\in X:$
\[ \norm{x} = \sup_{\substack{f\in \dual{X} \\ f\neq 0}}\frac{|f(x)|}{\norm{f}} = \sup_{\substack{f\in \dual{X} \\ \norm{f} = 1}}|f(x)| = \sup_{\substack{f\in \dual{X} \\ 0< \norm{f} \leq 1}}|f(x)|. \]
\end{corollary}
\begin{proof}
We calculate
\[ \norm{x} \geq \sup_{\substack{f\in X' \\ f\neq 0}}\frac{|f(x)|}{\norm{f}} \geq \frac{|\omega_{x}(x)|}{\norm{\omega_{x}}} = \frac{\norm{x}}{1} = \norm{x} \]
where the first inequality follows from $|f(x)|\leq \norm{f}\norm{x}$ for all $f\in \dual{X}, x\in X$. The other two equalities follow because a maximum is reached by a norm-one functional.
\end{proof}
\begin{corollary} \label{uniformConvergenceWeakTopologyGivesNormTopology}
Let $X$ be a normed space, $\seq{x_i}_{i\in I} \in X^I$ and $x \in X$. Then $f(x_i) \to f(x)$ uniformly for $f\in\dual{X}$ with $\norm{f} = 1$ \textup{if and only if} $x_i \to x$ in the norm topology.
\end{corollary}
\begin{proof}
We have that
\[ \forall \epsilon > 0: \exists I_0\in I: \forall i\geq I_0: \forall \norm{f} = 1: \; |f(x_i-x)| \leq \epsilon \]
is equivalent to
\[ \forall \epsilon > 0: \exists I_0\in I: \forall i\geq I_0: \; \norm{x_i-x} = \sup_{\norm{f}=1} |f(x_i-x)| \leq \epsilon. \]
\end{proof}

\begin{lemma} \label{dualNormedSpaceBanach}
Let $X$ be a normed space. Then $\dual{X}$ is a Banach space.
\end{lemma}
\begin{proof}
The completeness of $\dual{X}$ follows from the completeness of $\F$ by \ref{boundedOperatorsFormBanachSpace}.
\end{proof}

\subsubsection{The bidual}

\begin{proposition} \label{canonicalIsometryIsometry}
Let $X$ be a normed space. Then function
\[ \evalMap_-: X\mapsto \bidual{X}: x\mapsto \evalMap_x|_{\dual{X}} \]
is an isometry.
\end{proposition}
\begin{proof}
We calculate
\[ \norm{\evalMap_x} = \sup_{f\in \dual{X}\setminus\{0\}} \frac{\norm{\evalMap_x(f)}}{\norm{f}} = \sup_{f\in \dual{X}\setminus\{0\}} \frac{\norm{f(x)}}{\norm{f}} = \norm{x}, \]
where we have used the definition of the norm in $\bidual{X}$ and \ref{normFromContinuousFunctionals}.
\end{proof}
\begin{corollary} \label{normedReflexiveIFFSemireflexive}
A normed space is reflexive \textup{if and only if} it is semi-reflexive.
\end{corollary}
\begin{proof}
The direction $\Rightarrow$ is immediate, for the direction $\Leftarrow$, we just need to observe that $\evalMap_-: X\mapsto \bidual{X}$ is an embedding, since it is an isometry (\ref{isometryLemma}).
\end{proof}

\subsubsection{Reflexive normed space}
\begin{proposition}
Let $X$ be a reflexive normed space. Then $X$ is a Banach space.
\end{proposition}
\begin{proof}
This follows from $X\cong \bidual{X}$ and \ref{dualNormedSpaceBanach}.
\end{proof}

\begin{lemma}
Let $X$ be a reflexive Banach space. Then $\evalMap_-: \sSet{X,\sigma} \to \contLin(\dual{X}, \F)_p$ is a homeomorphism.
\end{lemma}
\begin{proof}
TODO immediate from $f = \evalMap_f\circ \evalMap_-$ for all $f\in \dual{X}$ and the fact that $\evalMap_-$ is a linear homeomorphism.
\end{proof}
\begin{corollary} \label{ReflexiveBanachSpaceWeaklyQuasiComplete}
Let $X$ be a reflexive Banach space. Then $\sSet{X,\sigma}$ is quasi-complete.
\end{corollary}
\begin{proof}
Since $\evalMap_-$ is linear, it is also uniformly continuous. TODO
\end{proof}

\begin{example}
Banach space that is not weakly sequentially complete: \url{https://math.stackexchange.com/questions/866650/c0-1-is-not-weakly-sequentially-complete}. See also \url{https://arxiv.org/pdf/1602.04718.pdf}.
\end{example}

\subsubsection{Adjoints}
\begin{proposition} \label{adjointMapIsometric}
Let $X,Y$ be normed spaces and $T\in \Bounded(X,Y)$ a bounded operator. Then $T$ has a unique adjoint $T^*: \dual{Y}\to \dual{X}$ and $\norm{T} = \norm{T^*}$.
\end{proposition}
\begin{proof}
The existence and uniqueness of the adjoint is given by \ref{existenceAdjointWeaklyContinuousFunction}. We just need to show that $\norm{T} = \norm{T^*}$. We calculate
\begin{align*}
\norm{T^*} &= \sup_{f\in \dual{Y}\setminus\{0\}}\frac{\norm{T^*(f)}}{\norm{f}} \\
&= \sup_{f\in \dual{Y}\setminus\{0\}}\frac{\norm{f\circ T}}{\norm{f}} \\
&= \sup_{f\in \dual{Y}\setminus\{0\}}\sup_{x\in X\setminus\{0\}}\frac{\norm{(f\circ T)(x)}}{\norm{f}\,\norm{x}} \\
&= \sup_{x\in X\setminus\{0\}}\sup_{f\in \dual{Y}\setminus\{0\}}\frac{\norm{f(Tx)}}{\norm{f}\,\norm{x}} \\
&= \sup_{x\in X\setminus\{0\}}\frac{\norm{Tx}}{\norm{x}} \\
&= \norm{T},
\end{align*}
where we have repeatedly used the definition of the operator norm, as well as \ref{normFromContinuousFunctionals}.
\end{proof}

\subsection{The uniform boundedness principle}
TODO: \url{https://arxiv.org/pdf/1005.1585.pdf}

\begin{theorem}[Uniform boundedness principle] \label{uniformBoundednessPrinciple}
Let $\mathcal{F}\subset \Bounded(X,Y)$ be a family of bounded operators where $X$ is a Banach space and $Y$ a normed space, such that
\[ \sup\setbuilder{\norm{Tx}}{T\in\mathcal{F}} < \infty \qquad \text{for all $x\in X$}. \]
Then $\sup\setbuilder{\norm{T}}{T\in\mathcal{F}} < \infty$.
\end{theorem}
\begin{proof}
The proof is an application of the Baire category theorem. Define the closed subsets $K_n$ as
\[ K_n = \setbuilder{x\in X}{\forall T\in\mathcal{F}: \norm{Tx}\leq n}. \]
These are closed because the functional $f_T: X\to \R: x\mapsto \norm{Tx}$ is bounded and
\[ K_n = \bigcap_{T\in\mathcal{F}}f_T^{-1}[\,[0,n]\,]. \]
By assumption, $X=\bigcup_{n\in\N} K_n$. As $X$ is a Banach space, and thus a complete metric space, we can apply the Baire category theorem, \ref{BaireCategory}, to conclude that there is a $K_n$ with non-empty interior (by contraposition of the Baire condition). Take $x_0\in K_n^\circ$, then $-x_0+K_n^\circ \subset K_{n2}$. So $\vec{0}\in (K_{2n})^\circ$ and we can find a $\rho$ such that $B(\vec{0},\rho)\subset K_{2n}$. By proposition \ref{existenceOperatorNorm} we have $\norm{T}\leq 2n/\rho$ for all $T\in\mathcal{F}$.
\end{proof}
\begin{corollary}[Banach-Steinhaus] \label{BanachSteinhaus}
Let $X$ be a Banach space and $Y$ a normed space. Let $T_n: X\to Y$ be a sequence of bounded operators that converges pointwise to $T$. Then
\begin{enumerate}
\item $T$ is bounded;
\item $\seq{T_n}$ converges in the continuous convergence.
\end{enumerate}
\end{corollary}
\begin{proof}
For all $x\in X$ we have $\norm{T_nx} \to \norm{Tx}$, so $\seq{\norm{T_nx}}$ is a bounded sequence and $\sup_{n}\norm{T_nx} <\infty$. By the uniform boundedness principle, $\sup_{n}\norm{T_n}$ is bounded by some constant $M$.

Now for all $x\in X$ we have $\norm{T_nx} \leq M$ for all $n\in\N$ and thus $\norm{Tx}\leq M$. This means that $T$ is bounded and $\norm{T} \leq M$.
\end{proof}
This does not imply that $\seq{T_n}$ converges to $T$ in norm!

\begin{proposition} \label{weaklyConvergentSequenceNormBounded}
Let $X$ be a normed space and $\seq{x_n}$ a weakly convergent sequence. Then $\{x_n\}_{n\in \N}$ is norm-bounded.
\end{proposition}
Note that not every weakly convergent \emph{net} is norm-bounded. TODO: compare \ref{weaklyBoundedIffBounded}
\begin{proof}
Take arbitrary $f\in \dual{X}$. Since $\seq{x_n}$ is weakly convergent to some $x\in X$, we have
\[ |\evalMap_{x_n}(f)| = |f(x_n)| \to |f(x)| = |\evalMap_x(f)|, \]
so the sequence $\seq{\evalMap_{x_n}}$ is pointwise convergent in $\bidual{X}$. By \ref{imageCauchySequenceTotallyBounded}, this implies that the set $\{\evalMap_{x_n}\}$ is pointwise bounded. Since $\bidual{X}$ is a Banach space (by \ref{dualNormedSpaceBanach}), $(\bidual{X}, \dual{X})$ is a Banach-Steinhaus pair and thus $\{\evalMap_{x_n}\}$ is uniformly bounded, so
\[ \sup_{n\in \N}\norm{\evalMap_{x_n}} = \sup_{n\in \N}\norm{x_n} < \infty, \]
using the fact that $\evalMap_-$ is an isometry \ref{canonicalIsometryIsometry}. Thus $\seq{x_n}$ is a bounded sequence.
\end{proof}

\begin{proposition}
Let $X,Y$ be normed spaces and $T\in \Lin(X,Y)$. Then the following are equivalent:
\begin{enumerate}
\item $T$ is norm continuous;
\item $T$ is weakly continuous;
\item $T$ is weakly sequentially continuous.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ Immediate by \ref{continuityImpliesWeakContinuity}.

$(2) \Rightarrow (3)$ Immediate by \ref{continuityImpliesSequentialContinuity}.

$(3) \Rightarrow (1)$ The proof is by contraposition. Suppose $T$ is not bounded. Then for all $M\geq 0$ there exists a unit vector $x\in X$ such that $\norm{Tx} \geq M$. In particular, for all $n^2$, there exists a unit vector $x_n$ such that $\norm{Tx_n} \geq n^2$.

Set $y_n \defeq n^{-1}x_n$, so $\norm{y_n} = n^{-1}$. This implies that $\seq{y_n}$ converges to $0$ in norm, and thus also weakly. This implies that $\seq{Ty_n} \to 0$ weakly. Now $\norm{Ty_n} = n^{-1}Tx_n \leq n^{-1}n^2 = n$, so $\{Ty_n\}_{n\in\N}$ is not bounded, which implies $T$ is not weakly sequentially continuous by \ref{weaklyConvergentSequenceNormBounded}.
\end{proof}

\subsection{Open mapping and closed graph theorems}

\begin{proposition} \label{openUnitBall}
Let $X,Y$ be Banach spaces and $T:X\to Y$ a surjective bounded operator.  Then the image of the open unit ball $B(\vec{0},1)\subset X$ contains an open ball about $\vec{0}\in Y$.
\end{proposition}
\begin{proof}
We first prove $0\in \overline{T[B(\vec{0},r)]}^\circ$ for every $r>0$: (TODO: make computations lemma.)
\begin{itemize}
\item Using $X = \bigcup_{n=1}^\infty B(\vec{0},n)$, we see by surjectivity
\[ Y = T[X] = T\left[\bigcup_{n=1}^\infty B(\vec{0},n)\right] = \bigcup_{n=1}^\infty T[B(\vec{0},n)]. \]
Because $Y$ has the Baire property (theorem \ref{BaireCategory}) and $Y$ is both open and non-empty, it may not be meagre, by lemma \ref{BaireEquivalents}. So for some $n\in\N$, $T[B(\vec{0},n)]$ is non-rare, meaning that $\overline{T[B(\vec{0},n)]}$ has non-empty interior.
\item Because
\[ \overline{T[B(\vec{0},n)]} = \overline{2nT[B(\vec{0},1/2)]} = 2n\overline{T[B(\vec{0},1/2)]}, \]
$\overline{T[B(\vec{0},1/2)]}$ must have non-empty interior. Let $B(y_0,\epsilon)\subset \overline{T[B(\vec{0},1/2)]}$.
\item Note $B(0,\epsilon) = y_0 - B(y_0,\epsilon) \subset \overline{T[B(\vec{0},1)]}$ and thus $B(0,r\epsilon) \subset \overline{T[B(\vec{0},r)]}$.
\end{itemize}
We then prove $\overline{T[B(\vec{0},1/2)]} \subset T[B(\vec{0}, 1)]$, proving the proposition.
\begin{itemize}
\item Choose some $y_0\in \overline{T[B(\vec{0},1/2)]}$. Then every neighbourhood $B(y_0,\epsilon/4)$ intersects $T[B(\vec{0},1/2)]$.
\item Then
\[ B(y_0,\epsilon/4) = y_0 - B(\vec{0},\epsilon/4) \subset y_0 - \overline{T[B(\vec{0},1/4)]}, \]
so $y_0 - \overline{T[B(\vec{0},1/4)]}$ intersects $T[B(\vec{0},1/2)]$. Take a $y_1 \in \overline{T[B(\vec{0},1/4)]}$ such that $y_0-y_1$ is in this intersection. Then we have an $x_0\in B(\vec{0},1/2)$ such that $T(x_0) = y_0-y_1$.
\item We can continue recursively choosing $y_{n+1}\in \overline{T[B(\vec{0}, 2^{-(n+1)})]}$ and $x_n \in B(\vec{0}, 2^{-n})$ such that $y_n-y_{n+1} = T(x_n)$.
\item Consider the sequence $\sum_{k=0}^nx_k$. It is a Cauchy sequence in $X$. Call its limit $x$. Then $x\in B(\vec{0},1)$.
\item Because $\norm{y_n}\leq 2^{-n}\norm{T}$, $(y_n)$ converges to zero. Then
\[ \left( T\left(\sum^n_{k=1}x_k\right) \right)_{n\in\N} = \left( y_0-y_{n+1} \right)_{n\in\N} \]
converges to $y_0$. Thus $T(x) = y_0 \in T[B(\vec{0},1)]$.
\end{itemize}
\end{proof}

\begin{proposition} \label{zeroInInteriorOfImageImpliesOpen}
Let $X,Y$ be normed spaces and $T: X\to Y$ a linear map. If $\vec{0}$ lies in the interior of $T[B(\vec{0},r)]$ for some $r>0$, then $T$ is open.
\end{proposition}
\begin{proof}
TODO: make computations lemma.
Given the assumption, $0$ lies in the interior of $T[B(\vec{0},\epsilon)]$ for all $\epsilon>0$.
Because $T[B(x,\epsilon)] = T(x) + T[B(\vec{0},\epsilon)]$, $T(x)$ lies in the interior of $T[B(x,\epsilon)]$, for all $x\in X$.
Thus for all neighbourhoods $U(x)\subset X$, $T(x)\subset T[U]^\circ$ and so $T[U] \subset T[U]^\circ$, so $T[U]$ is open.
\end{proof}

\begin{theorem}[Open mapping]
Let $X,Y$ be Banach spaces and $T:X\to Y$ a surjective bounded operator. Then $T$ is an open map.
\end{theorem}
\begin{proof}
This is the consequence of propositions \ref{openUnitBall} and \ref{zeroInInteriorOfImageImpliesOpen}.
\end{proof}
\begin{corollary}[Bounded inverse theorem] \label{boundedInverse}
Let $X,Y$ be Banach spaces. If $T:X\to Y$ is is continuous, linear and bijective, then $T$ is a homeomorphism.
\end{corollary}


\begin{proposition}
Let $T: \dom(T)\subset X\to Y$ be a bounded linear operator. Then
\begin{enumerate}
\item if $\dom(T)$ is a closed subset of $X$, then $T$ has closed graph;
\item if $T$ has closed graph and $Y$ is complete, then $\dom(T)$ is a closed subset of $X$.
\end{enumerate}
\end{proposition}
\begin{proof}
We use proposition \ref{closedGraphEquivalence} twice: First assume $(x_n)$ and $(Tx_n)$ converge to $x$ and $y$, respectively. Then $x\in\dom(T)$ by closure and $y = Tx$ by continuity.

Now assume $T$ has closed graph and $Y$ is complete. Take $x\in\overline{\dom(T)}$ and $(x_n)\subset \dom(T)$ converging to $x$. Since $T$ is bounded:
\[ \norm{Tx_n - Tx_m} = \norm{T(x_n-x_m)} \leq \norm{T}\norm{x_n-x_m}, \]
so $(Tx_n)$ is Cauchy by \ref{CauchyCriterion} and thus by completeness has a limit, say $y$. Then $Tx=y$ by continuity. Since $T$ has closed graph, $x\in\dom(T)$. So $\overline{\dom(T)}\subseteq \dom(T)$ and $\dom(T)$ is closed. 
\end{proof}

\begin{theorem}[Closed graph theorem] \label{BanachClosedGraphTheorem}
Let $X,Y$ be Banach spaces and $T:X\to Y$ a linear operator with $\dom(T) = X$. Then $T$ is continuous \textup{if and only if} $T$ is a closed operator.
\end{theorem}
\begin{proof}
TODO
\end{proof}

\subsection{Convergences on sets of bounded operators}
\begin{example}
Let $X, Y$ be normed spaces. The norm function $\norm{\cdot}: \Bounded(X,Y)\to \R^+: T\mapsto \norm{T}$ may be discontinuous when $\Bounded(X,Y)$ is equipped with the strong operator topology (and thus also when it is equipped with the weak operator topology).

TODO: consider $\Bounded(H)$, where $H$ is an infinite-dimensional Hilbert space. Let $\{e_n\}_{n\in\N}$ be an orthonormal set of vectors. Let $P_n$ be the orthogonal projector on $\overline{\Span}\{e_k\}_{k\geq n}$. Now $\seq{P_n}$ converges strongly to $0$:
take arbitrary $v = x+y \in \overline{\Span}\{e_k\}_{k\geq 0} \oplus \overline{\Span}\{e_k\}_{k\geq 0}^\perp$.

Then, by \ref{sumExpansionOrthogonalProjector}, $P_nv = P_nx = \sum_{k\geq n}\inner{e_k, x}e_k = x - \sum_{k\leq n}\inner{e_k, x}e_k$, so, by the Plancherel formula \ref{plancherel},
\[ \norm{P_n} = \norm{x - \sum_{k\leq n}\inner{e_k, x}e_k} = \norm{x - \sum_{k=0}^n\inner{e_k, x}e_k} \to 0. \]

Since $\norm{P_n} = 1$ for all $n\in\N$, the sequence $\seq{\norm{P_n}}$ does not converge to $\norm{0} = 0$.
\end{example}

\begin{example}
Multiplication is not continuous in the weak or strong operator topology.

Counterexample: nilpotent operators of order 2 are strongly dense (TODO?). Take some operator $T$ that is not nilpotent and a net $\seq{T_i}$ of of nilpotent operators that converges strongly to $T$. Then $T_i\cdot T_i = T_i^2 = 0$, so $T_i^2 \to 0 \neq T^2$.
\end{example}

\begin{lemma}
Let $X, Y$ be normed spaces, $\seq{T_i}_{i\in I} \in \Bounded(X, Y)^I$ and $T \in \Bounded(X, Y)$. Then
\begin{enumerate}
\item $T_i \to T$ in the strong operator topology \textup{if and only if} $f(T_ix) \to f(Tx)$ uniformly for $f\in\dual{X}$ with $\norm{f} = 1$;
\item $T_i \to T$ in the norm topology \textup{if and only if} $T_ix\to Tx$ uniformly for $x\in X$ with $\norm{x} = 1$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Immediate from \ref{uniformConvergenceWeakTopologyGivesNormTopology}.

(2) By a similar argument to \ref{uniformConvergenceWeakTopologyGivesNormTopology}.
\end{proof}

\begin{proposition}
Let $X,Y$ be normed spaces, then
\begin{enumerate}
\item the adjoint map $*: \Bounded(X, Y) \to \Bounded(\dual{Y}, \dual{X})$ is norm-norm continuous;
\item if $X$ is reflexive, then the adjoint map $*: \Bounded(X, Y) \to \Bounded(\dual{Y}, \dual{X})$ is WOT-WOT continuous.
\end{enumerate}
\end{proposition}
In general the adjoint map is not SOT-SOT continuous, even for Hilbert spaces. See \ref{adjointMapNotSOTContinuous}.
\begin{proof}
(1) Continuity in norm is follows from the fact that the adjoint map is isometric \ref{adjointMapIsometric}.

(2) Since $X$ is reflexive, we have $\dual{X}= \{\evalMap_x\}_{x\in X}$. Now we calculate, for $x\in X, f\in \dual{Y}, T\in \Bounded(X,Y)$,
\[ \pair{\evalMap_x\otimes f, T^*} = \evalMap_x\big(T^*f\big) = \evalMap_x(f\circ T) = f\big(Tx\big) = \pair{f\otimes v, T}. \]
Thus $\pair{\evalMap_x\otimes f, -}\circ (-)^* = \pair{\evalMap_x\otimes f, (-)^*} = \pair{f\otimes x, -}$, which means that $(-)^*$ is continuous by \ref{characteristicPropertyInitialFinalConvergence}.
\end{proof}

\begin{example}
If $X$ is not reflexive, then the adjoint map may not be continuous in the weak operator topology.

Consider the Banach space $c_0$. Then $\dual{(c_0)} = \ell^1$ and $\dual{\dual{(c_0)}} = \ell^\infty$. Consider $S_l^n: c_0 \to c_0$, the left-shift by $n$. Then $(S_l^n: c_0 \to c_0)^* = S_r^n: \ell^1 \to \ell^1$. Then, for all $\seq{x_k} \in c_0$ and $\seq{y_k}\in \ell^1$, we have
\[ |\pair{S_l^n\seq{x_k}, \seq{y_k}}| = \left|\sum_{k=1}^\infty x_{k+n}y_k \right| \leq \sup_{k\geq n}|x_k|\left|\sum_{k=1}^\infty y_k \right| = \sup_{k\geq n}|x_k|\,\norm{\seq{y_n}}_1 \to 0, \]
so $S_l^n \overset{WOT}{\longrightarrow} 0$.

But $(S_l^n)^* = S_r^n: \ell^1\to \ell^1$ does not converge to $0$ in the WOT for $\Bounded(\ell^1)$. Indeed, take non-zero $\seq{y_k}\in \ell^1$ and $\seq{z_k} = \underline{1} \in \ell^\infty$.
Then
\[ |\pair{S_r^n\seq{y_k}, \seq{z_k}}| = \left|\sum_{k=1+n}^\infty y_{k+n}\cdot z_k \right| = \left|\sum_{k=1+n}^\infty y_{k+n}\cdot 1 \right| = \left|\sum_{k=1}^\infty y_{k} \right| = \norm{\seq{y_k}}_1 \neq 0. \]
\end{example}

\begin{proposition}
Let $X, Y$ be a Banach spaces. Then
\begin{enumerate}
\item $\Bounded_{\text{SOT}}(X,Y)$ is quasi-complete;
\item if $Y$ is reflexive, then $\Bounded_{\text{WOT}}(X,Y)$ is quasi-complete.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Since $Y$ is complete, it is also quasi-complete. By (TODO ref), $(X,Y)$ is a Banach-Steinhaus pair. By \ref{quasiCompletenessFunctionSpaces}, $\Bounded(X,Y)_\text{SOT} = \contLin_p(X,Y)$ is quasi-complete.

(2) We have $\Bounded(X,Y)_\text{SOT} = \contLin_p(X,\sSet{Y,\sigma})$ and $\sSet{Y,\sigma}$ is quasi-complete by \ref{ReflexiveBanachSpaceWeaklyQuasiComplete}. Then $\Bounded(X,Y)_\text{SOT}$ is quasi-complete by \ref{quasiCompletenessFunctionSpaces}.
\end{proof}



\subsection{Compact operators}
\begin{definition}
A linear map $L:V\to W$ between normed spaces is called \udef{compact} if $L^\imf\big(\cball(\vec{0}, 1)\big)$ is relatively compact.

The space of compact maps from $V$ to $W$ is denoted $\Compact(V,W)$.
\end{definition}
In other words, an operator is compact if the image of the closed unit ball has compact closure.

These operators were introduced to study equations of the form
\[ (T-\lambda I)x(t) = p(t). \]

\begin{proposition}
Let $L\in\Lin(V,W)$. The following are equivalent:
\begin{enumerate}
\item $L$ is compact;
\item the image of any bounded subset of $V$ is relatively compact in $W$;
\item there exists a neighbourhood $U$ of $0$ in $V$ such that the image of $U$ is a subset of a compact set in $W$;
\item for any bounded sequence $(x_n)_{n\in\N} \subseteq V$, then sequence $(Lx_n)_{n\in\N}$ contains a converging subsequence.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
All maps of finite rank are compact.
\end{corollary}
\begin{proof}
Closed balls in $\C^n$ are compact.
\end{proof}

\begin{proposition}
Let $V$ be a normed space. Then $\mathcal{K}(V)$ is a closed two-sided ideal in $\Bounded(V)$.
\end{proposition}

\begin{lemma}
The identity map on $X$ is compact \textup{if and only if} $X$ is finite-dimensional.
\end{lemma}
\begin{proof}
The unit ball is compact iff $X$ is finite-dimensional, by \ref{compactnessUnitBall}.
\end{proof}
\begin{corollary}
Let $T\in\Compact(X,Y)$. If $T$ is injective and $T^{-1}$ bounded, then $X$ is finite-dimensional.
\end{corollary}
\begin{proof}
In this case $\id_X = T^{-1}T$ is compact by TODO ref.
\end{proof}

\subsubsection{Compact operators on Banach spaces}
\begin{proposition}
Let $L\in\Hom(V,W)$ with $V,W$ Banach spaces. Then $L$ is compact \textup{if and only if} the image of any bounded subset of $V$ under $L$ is totally bounded in $W$.
\end{proposition}
TODO proof

\begin{lemma} \label{identityCompactFiniteDimensional}
Let $X$ be a Banach space. Then $\id_X$ is compact \textup{if and only if} $X$ is finite dimensional.
\end{lemma}

\begin{theorem}[Schauder's theorem] \label{SchaudersTheorem}
Let $X,Y$ be Banach spaces and $T\in\Bounded(X,Y)$. Then the following are equivalent:
\begin{enumerate}
\item $T: X\to Y$ is compact;
\item $T^*: Y^* \to X^*$ is compact.
\end{enumerate}
\end{theorem}
TODO: more general setting: $X,Y$ normed for $(1) \Rightarrow (2)$ and $X$ normed, $Y$ Banach for $(2)\implies (1)$?
\begin{proof}
TODO \ref{https://arxiv.org/pdf/1010.1298.pdf} and \url{https://math.stackexchange.com/questions/41432/easy-proof-adjointcompact-compact}
\end{proof}

\subsubsection{Calkin algebra}
Calkin algebra: study of properties invariant under compact pertubation.

\begin{proposition}
Let $X$ be a Banach space. Then $\Compact(X)$ is a closed two-sided ideal in $\Bounded(X)$.
\end{proposition}
\begin{proof}
TODO + $*$-ideal for Hilbert spaces.
\end{proof}
\begin{corollary}
An invertible operator $T$ on $X$ is compact \textup{if and only if} $X$ is finite-dimensional.
\end{corollary}
\begin{proof}
If $T$ is compact, then so is $TT^{-1} = \id_X$, meaning that $X$ is finite-dimensional by \ref{identityCompactFiniteDimensional}. Conversely, all operators on a finite-dimensional Banach space are compact.
\end{proof}

\begin{definition}
Let $X$ be a Banach space. The \udef{Calkin algebra} is the quotient $\Bounded(X)/\Compact(X)$.
\end{definition}
TODO: quotient algebra ($[A][B] = [AB]$)

\begin{proposition}
Let $[T]\in\Bounded(X)/\Compact(X)$. Then the following are equivalent:
\begin{enumerate}
\item $[T]$ is invertible in the Calkin algebra;
\item $\exists S\in\Bounded(X):$ both $\vec{1}-TS$ and $\vec{1}-ST$ are compact;
\item $T$ has closed range and finite-dimensional kernel and cokernel. 
\end{enumerate}
\end{proposition}
\begin{proof}
Point 1. and 2. are easily equivalent: $[S]$ is an inverse of $[T]$ if and only if $[\vec{1}] = [S][T] = [ST]$ and $[\vec{1}] = [T][S] = [TS]$. Then
\[ [\vec{1}] = [ST] \iff [ST - \vec{1}] = [0] \qquad [\vec{1}] = [TS] \iff [TS - \vec{1}] = [0] \]
and $[F]=[0]$ if and only if $F$ is compact.

TODO
\end{proof}

\subsubsection{Fredholm operators}
TODO: Moore-Penrose pseudoinverse??

\begin{definition}
An operator $T\in\Bounded(X,Y)$ between Banach spaces is called a \udef{Fredholm operator} if $T$ has a finite-dimensional kernel and cokernel.

The \udef{Fredholm index} of $T$ is defined as
\[ \Index T \defeq \dim\ker T - \dim\coker T.  \]

We denote the space of Fredholm operators from $X$ to $Y$ as $\Fred(X,Y)$. If $X=Y$, we write $\Fred(X)$.
\end{definition}

\begin{example}
\begin{enumerate}
\item If $X=Y$ is finite-dimensional, then all operators are Fredholm with index $0$.
\item A self-adjoint operator on a Hilbert space with finite kernel is Fredholm with Fredholm index $0$.
\item The left shift $S_l:\ell^2(\N)\to\ell^2(\N): (x_n)_n\mapsto (x_{n+1})_n$ has index $1$.
\item The right shift $S_r = S_l^*$ has index $-1$.
\end{enumerate}
\end{example}

\begin{lemma} \label{FredholmOperatorClosedRange}
A Fredholm operator has closed range.
\end{lemma}

\begin{lemma}
Let $T\in\Bounded(H)$ be a bounded operator on a Hilbert space. Then $\dim\coker T = \dim\ker T^*$.
\end{lemma}
\begin{proof}
TODO (is it correct?) $\ker(T^*) = \im(T)^\perp$.
\end{proof}


\begin{proposition}
Let $S,T\in\Fred(X)$, $\lambda\in\F$ and $K\in\Compact(X)$. Then
\begin{enumerate}
\item $\Index(ST) = \Index(S)+\Index(T)$;
\item $\Index(T+K) = \Index(T)$;
\item $\Index(\lambda T) = \Index(T)$, if $\lambda \neq 0$;
\item $\Index(T) = 0$ \textup{if and only if} $T=K'+L$ for some compact $K'$ and invertible $L$.
\end{enumerate}
Let $T\in\Fred(H)$ for some Hilbert space $H$. Then
\begin{enumerate} \setcounter{enumi}{4}
\item $\Index(T^*) = -\Index(T)$.
\end{enumerate}
\end{proposition}
TODO: prove using next lemma:
\begin{lemma}
Let the commutative diagram
\[ \begin{tikzcd}
0 \rar & X \dar{T} \rar & Y \dar{S} \rar & Z \dar{R} \rar & 0 \\
0 \rar & X \rar & Y \rar & Z \rar & 0
\end{tikzcd} \]
have short exact rows. If any two of $T,S,R$ are Fredholm, then so is the third and
\[ \Index S = \Index T + \Index R. \]
\end{lemma}
\begin{proof}
TODO snake lemma to obtain long exact
\[ 0\to \ker T \to \ker S\to \ker R \to \coker T \to \coker S \to \coker R \to 0. \]
\end{proof}
\begin{corollary} \mbox{}
\begin{enumerate} 
\item Let $T\in\Fred(X)$ and $S\in\Fred(Y)$ be Fredholm, then so is $T\oplus S$ with
\[ \Index(T\oplus S) = \Index(T)+\Index(S). \]
\item Let $T\in\Fred(X,Y)$ and $S\in\Fred(Y,Z)$ be Fredholm, then so is $ST$ with
\[ \Index(ST) = \Index(T)+\Index(S). \]
\item Let $K\in\Compact(X)$ be compact, then $\id_X+K$ is Fredholm with
\[ \Index(\id_X+K) = 0. \]
\end{enumerate}
\end{corollary}


\begin{lemma}[Fredholm alternative] \label{FredholmAlternative}
Let $T$ be a Fredholm operator of index zero. Then either $T$ is bijective, or it is neither injective nor surjective.
\end{lemma}
\begin{proof}
The operator $T$ is injective iff $\dim\ker(T) = 0$ and surjective iff $\dim\coker(T) = 0$.
\end{proof}



\section{Isometries}
In this section we do not assume isometries to be linear.

\subsection{Isometries on $\R$}
\begin{lemma} \label{sumSquaredIntervals}
Let $x,y,z\in\R$. Then
\[ (x-y)^2 + (y-z)^2 - (x-z)^2 = 2(x-y)(z-y). \]
\end{lemma}
Thus also $(y-x)^2 + (z-y)^2 \leq (z-x)^2$ if $x\leq y\leq z$.
\begin{proof}
We calculate
\begin{align*}
(x-y)^2 + (y-z)^2 - (x-z)^2 &= \cancel{x^2} + \cancel{z^2} + 2y^2 + -2xy-2yz -\cancel{x^2} -\cancel{z^2} + 2xz \\
&= 2\big(y^2 + -xy-yz + xz\big) \\
&= 2(x-y)(z-y).
\end{align*}
\end{proof}

\begin{lemma}
All isometries of $\R$ are of the form $f: \R\to\R: x\mapsto ax+b$ with $a = \pm 1$.
\end{lemma}
\begin{proof}
Let $f$ be an isometry. Define, for all $x\neq y\in \R$,
\[ \lambda_{x,y} \defeq \frac{f(x)-f(y)}{x-y}. \]
Then
\[ \lambda_{x,y}^2 = \frac{\big(f(x)-f(y)\big)^2}{(x-y)^2} = \frac{\big|f(x)-f(y)\big|^2}{|x-y|^2} = 1, \]
so $\lambda_{x,y} = \pm 1$ and, in particular, $\lambda_{x,y} = \lambda_{x,y}^{-1}$


Fix $x\in \R$. Since $f(y) = -\lambda_{x,y}y +\lambda_{x,y}x + f(x)$, it is enough to show that $\lambda_{x,z} = \lambda_{x,y}$ for all $y,z\in \R$. In this case $a = -\lambda_{x,y}$ and $b = \lambda_{x,y}x + f(x)$.

First note
\[ \lambda_{x,y} = \frac{f(x)-f(y)}{x-y} = \frac{f(y)-f(x)}{y-x} = \lambda_{y,x}, \]
and calculate
\begin{align*}
f(x) &= \lambda_{x,y}(x-y) + f(y) \\
&= \lambda_{x,y}(x-y) + \lambda_{y,z}(y-z) + f(z) \\
&= \lambda_{x,y}(x-y) + \lambda_{y,z}(y-z) + \lambda_{z,x}(z-x) + f(x),
\end{align*}
so $-\lambda_{y,z}(y-z) = \lambda_{x,y}(x-y) + \lambda_{x,z}(z-x)$. Squaring gives
\[ (y-z)^2 = (x-y)^2 + (z-x)^2 + 2\lambda_{x,y}\lambda_{x,z}(z-x)(x-y) \]
and thus
\[ \lambda_{x,y}\lambda_{x,z} = - \frac{(x-y)^2 + (z-x)^2 - (y-z)^2}{2(z-x)(x-y)} = \frac{(x-y)^2 + (z-x)^2 - (y-z)^2}{2(z-x)(y-x)} = 1, \]
by \ref{sumSquaredIntervals}. So $\lambda_{x,z} = \lambda_{x,y}$.
\end{proof}

\subsection{Affine isometries}

\begin{proposition}
Let $V,W$ be real normed spaces and $f: V\to W$ an isometry. Then $f$ is affine \textup{if and only if} $f\big(\frac{1}{2}x + \frac{1}{2}y\big) = \frac{1}{2}\big(f(x) + f(y)\big)$ for all $x,y\in V$.
\end{proposition}
\begin{proof}
The direction $\Rightarrow$ is immediate from the definition.

For the other direction, observe that \ref{midpointPreservingMapLemma} implies that $f\big(\lambda x + (1-\lambda)y\big) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda$ of the form $\frac{k}{2^m}$. The set of all such $\lambda$ is dense in $\R$ and $f$ is continuous by \ref{isometriesUniformlyContinuous}. By \ref{denseSetDeterminesContinuousFunction}, this implies that$f\big(\lambda x + (1-\lambda)y\big) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda\in\R$, i.e.\ $f$ is affine.
\end{proof}
\begin{corollary}
Let $V,W$ be real normed spaces and $f: V\to W$ an isometry. If $\cball_W(0,1)$ is strictly convex, then $f$ is affine.
\end{corollary}
\begin{proof}
Since $f$ is isometric, we have
\begin{align*}
\norm{f\Big(\frac{x+y}{2}\Big) - f(x)} &= \norm{\frac{x+y}{2} - x} = \frac{\norm{x-y}}{2} \\
\norm{f\Big(\frac{x+y}{2}\Big) - f(y)} &= \norm{\frac{x+y}{2} - y} = \frac{\norm{x-y}}{2}, 
\end{align*}
so $\frac{2}{\norm{x-y}}\Big(f(x) - f\big(\frac{x+y}{2}\big)\Big)$ and $\frac{2}{\norm{x-y}}\Big(f\big(\frac{x+y}{2}\big) - f(y)\Big)$ are unit vectors and 
\begin{align*}
\norm{\frac{2}{\norm{x-y}}\bigg(f(x) - f\Big(\frac{x+y}{2}\Big)\bigg) + \frac{2}{\norm{x-y}}\bigg(f\Big(\frac{x+y}{2}\Big) - f(y)\bigg)} &= \frac{2}{\norm{x-y}}\norm{f(x) - f(y)} \\
&= 2.
\end{align*}
By \ref{strictConvexityUnitBall}, this implies that $f(x) - f\big(\frac{x+y}{2}\big) = f\big(\frac{x+y}{2}\big) - f(y)$, or $f\big(\frac{x+y}{2}\big) = \frac{f(x) + f(y)}{2}$.
\end{proof}

\begin{theorem}[Mazur-Ulam theorem]
Let $V,W$ be real normed spaces. Then every surjective isometry $f: V\to W$ is affine.
\end{theorem}
\begin{proof}

\end{proof}

\begin{example}
Bijective isometry between complex spaces that is real-affine, but not complex-affine.
\end{example}

\begin{theorem}[Beckman-Quarles]

\end{theorem}

\begin{example}
Counterexamples for $\R$ and $L^2$.
\end{example}

\section{Completions and constructions}
\subsection{The lattice of sub-Banach spaces}
\begin{definition}
Let $X$ be a Banach space and $Y\subseteq X$. We call $Y$ a \udef{sub-Banach space} if $Y$ is a subalgebra of $X$ as a vector space and the norm on $X$, restricted to $Y$, makes $Y$ a complete uniform space.
\end{definition}

\begin{proposition} \label{subBanachSpaceLattice}
Let $X$ be a Banach space. The set of sub-Banach spaces of $X$ forms a complete $\wedge$-subsemilattice of $\powerset(X)$. The Moore closure is given by $\closure\circ \Span$.
\end{proposition}
\begin{proof}
By \ref{topologicalMetricSubuniformity} a subset $Y$ of $X$ is complete as a metric subspace iff it is complete as a uniform subspace. Since $X$ is Hausdorff and complete, this is equivalent to $Y$ being closed, by \ref{closedComplete}.

Thus $Y$ is a sub-Banach space iff it is a vector subspace and closed and the sub-Banach spaces form a complete $\wedge$-subsemilattice by \ref{intersectionCompleteSemilatticesCompleteSemilattice}, \ref{subalgebrasCompleteWedgeSubsemilattice} and \ref{propertiesTopology}.

The form of the Moore closure is given by \ref{MooreClosureIntersectionCompleteSublattices}, if we can show that $\closure\big(\Span(Y)\big)$ is a vector space for all $Y\subseteq X$. Indeed, let $F,G$ be filters in $\Span(Y)$ that converge to some $x,y\in X$. Then, for all $\lambda\in \F$, we have $F+\lambda G \to x+\lambda y$. Since $F+\lambda G \in \Span(Y)$, we have $x+\lambda y \in \closure\big(\Span(Y)\big)$.
\end{proof}

\subsection{Completions}
\begin{lemma} \label{embeddingInCompletionLinear}
Let $X$ be a normed vector space with completion $\hat{X}$. Then
\begin{enumerate}
\item there exist unique operations of addition and scalar multiplication on $\hat{X}$ that make $\hat{X}$ a Banach space;
\item the embedding $\hat{}: X \hookrightarrow \hat{X}$ is linear.
\end{enumerate}
\end{lemma}
\begin{proof}

\end{proof}


\begin{proposition}
Let $X$ be a normed vector space with completion $\hat{X}$ and $\sSet{Y, \xi}$ a complete vector space. Let $T: X\to Y$ be a bounded linear operator. Then $T$ has a unique continuous extension $\hat{T}: \hat{X} \to Y$ and
\begin{enumerate}
\item $\hat{T}$ is linear;
\item $\norm{\hat{T}} = \norm{T}$.
\end{enumerate}
\end{proposition}
\begin{proof}
We have that $T$ is uniformly continuous by \ref{uniformContinuityGroupHomomorphism}. Thus the continuous extension $\hat{T}$ exists and is unique by \ref{uniformlyContinuousExtensionToCompletion}.

(1) Take $a,b\in \hat{X}$ and $\lambda \in \F$. Then, by \ref{sequentialLemma} and \ref{sequentialInherenceAdherence}, we can find sequences $\seq{a_n}, \seq{b_n}$ such that $\seq{\hat{a}_n}, \seq{\hat{b}_n}$ converge to $a$ and $b$ in the completion.

For all $x,y\in X$, we have $\widehat{x+\lambda y} = \hat{x} + \lambda \hat{y}$ by \ref{embeddingInCompletionLinear}, so
\[ \hat{T}\big(\hat{x} + \lambda \hat{y}\big) = \hat{T}\big(\widehat{x+\lambda y}\big) = T(x+\lambda y) = T(x) + \lambda T(y) = \hat{T}(\hat{x}) + \lambda \hat{T}(\hat{y}). \]

Now we can compute
\begin{align*}
\hat{T}(a+ \lambda b) &= \lim_{n\to \infty}\hat{T}\big(\hat{a_n} + \lambda \hat{b_n}\big) \\
&= \lim_{n\to \infty}\hat{T}(\hat{a_n}) + \lambda \hat{T}(\hat{b_n}) \\
&= \lim_{n\to \infty}\hat{T}(\hat{a_n}) + \lambda \lim_{n\to \infty}\hat{T}(\hat{b_n}) \\
&= \hat{T}(a) + \lambda \hat{T}(\hat{b}).
\end{align*}
This shows linearity.
\end{proof}


\begin{proposition}
The completions of a space with respect to two different norms are isomorphic \textup{if and only if} the norms are equivalent.
\end{proposition}

TODO move down
\subsection{Tensor products}
TODO Ryan
\url{https://math.stackexchange.com/questions/2712906/does-mathcalb-mathcalh-mathcalh-otimes-mathcalh-in-infinite-dime}
\url{https://math.stackexchange.com/questions/35191/operator-norm-and-tensor-norms?noredirect=1&lq=1}

\subsection{Direct sums}

For arbitrary direct sums we can generalise: now that we have a concept of limits, we can relax the requirement that all but finitely many terms be zero. Instead we require that the sequence of norms is bounded in some way. This gives a whole family of related concepts of direct sum, named for which sequence space the sequence of norms belongs to.
\begin{definition}
Let $\{V_i\}_{i\in I}$ be an arbitrary family of normed spaces over a field $\F$ and let $\ell(I,\R)$ be a normed space of sequences in $\R$ indexed by $I$. Then the \udef{$\ell$-direct sum} is the vector space with as field
\[ \bigoplus_{i\in I}^\ell V_i = \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad (\norm{v_i}_{V_i})_{i\in I}\in \ell(I,\F) }. \]
The vector operations are defined pointwise and the norm is given by the $\ell$-norm $\norm{\big. \seq{\norm{v_i}_{V_i}}_{i\in I}}_\ell$.

In particular we have, for all $1\leq p<\infty$, the \udef{$\ell^p$-direct sum}
\[ \bigoplus_{i\in I}^p V_i \defeq \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad \sqrt[p]{\sum_{i\in I}\norm{v_i}_{V_i}^p}<\infty} \]
and the \udef{$\ell^\infty$-direct sum}
\[ \bigoplus_{i\in I}^\infty V_i \defeq \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad \sup_{i\in I}\norm{v_i}_{V_i}<\infty}. \]
\end{definition}

TODO: Relation to convergence vector space direct sum.

TODO: in general these direct sums are not complete (not Banach spaces), even if each $V_i$ is a Banach space.

\begin{proposition}
For any sequence space that is a Banach space the direct sum is a Banach space. TODO: in particular algebraic direct sum as $c_{00}$? (one possible norm)? and finite direct sums?
\end{proposition}

\subsubsection{Finite direct sums}

\begin{proposition}
Let $V,W$ be normed spaces. Let $\norm{\cdot}_1, \norm{\cdot}_2$ be norms on $\R^2$. Then the norms
\end{proposition}



\subsubsection{Direct sum of identical spaces}
\begin{proposition}
Let $V$ be a Banach space over $\F$, $I$ an arbitrary index set and $\ell(I,\F)$ a banach sequence space.
\[ \bigoplus_{i\in I}^\ell V \cong \ell\otimes V \]
\end{proposition}

\subsubsection{Internal direct sums}
\begin{proposition} \label{directSumClosedSubspacesBanachSpace}
Let $X$ be a Banach space and $A,B\subseteq X$ subspaces such that $A\cap B = \{0\}$ and $A\oplus^i B$ is closed in $X$. Then $A\oplus^i B$ is a convergence direct sum \textup{if and only if} $A$ and $B$ are closed subspaces of $X$.
\end{proposition}
TODO criteria for $A\oplus^i B$ being closed in $X$: \url{https://arxiv.org/abs/1509.06445}.
\begin{proof}
The direction $\Rightarrow$ follows from \ref{convergenceDirectSumsClosedSubspaces} and the fact that $\closure_X(A) = \closure_{A\oplus^i B}(A)$ by \ref{subspaceAdherence}.

Now assume $A,B$ closed. By \ref{internalConvergenceDirectSumEquivalents} it is enough to show that $P_A: A\oplus^i B \to A: x+y \mapsto x$ is continuous. We first show that $P_A$ is a closed operator, using \ref{closedGraphEquivalence}.

Suppose $\seq{a_n+b_n} \in (A\oplus^i B)^\N$ is a convergent sequence with limit $c\in X$ and $P_A(a_n+b_n) = a_n \to a$. Since $A$ is closed, we have $a\in A$. Then $b_n = (a_n+b_n)- a_n \to c-a$ and $c-a\in B$ because $B$ is closed. Now we have, $P_A(c) = P_A(a+c-a) = a$, so $P_A$ is closed.

Since $A\oplus^i B$ is closed, it is a Banach space and we can apply the closed graph theorem \ref{BanachClosedGraphTheorem}.
\end{proof}



\subsection{Quotients of normed spaces}
\begin{proposition} \label{quotientNormProperties}
Let $\sSet{V, \norm{\cdot}}$ be a seminormed space and $U\subseteq V$ a subspace. Then
\begin{align*}
\norm{[x]_U}_{V/U} \defeq& \inf\setbuilder{\norm{x+u}}{u\in U} \\
=& \inf\setbuilder{\norm{y}}{y\in [x]_U}
\end{align*}
is
\begin{enumerate}
\item a well-defined seminorm on $V/U$;
\item continuous;
\item $\ker\big(\norm{[\cdot]_U}_{V/U}\big) = \overline{U}$;
\item $\ker\big(\norm{\cdot}_{V/U}\big) = \setbuilder{[v]_U}{v \in \overline{U}}$;
\item a norm \textup{if and only if} $U$ is closed in $V$.
\end{enumerate}
\end{proposition}
Note that we are allowing $\norm{\cdot}$ to be a seminorm on $V$, rather than a norm.
\begin{proof}
The equality in the definition is immediate, since $y \in [x]_U$ is equivalent to $y-x \in U$ and thus equivalent to the existence of $u\in U$ such that $y = x+u$. 

(1) First we check that $\norm{\cdot}_{V/U}$ is well-defined. Take $x,y\in V$ such that $[x]_U = [y]_U$. Then $x-y \in U$. For all $u\in U$ we have
\begin{align*}
\norm{x+u} &= \norm{y + (x-y)+u} \in \setbuilder{\norm{y+u}}{u\in U} \\
\norm{y+u} &= \norm{x + (y-x)+u} \in \setbuilder{\norm{x+u}}{u\in U}, 
\end{align*}
which implies that $\norm{[x]_U}_{V/U} = \norm{[y]_U}_{V/U}$.

For absolute homogeneity, take $\lambda\in\F\setminus\{0\}$. Then $\lambda x+U = \lambda(x+U)$, so
\[ \norm{\lambda x}_{V/U} = \inf\setbuilder{\norm{\lambda x+u}}{u\in U} = \inf\setbuilder{|\lambda|\norm{(x+u)}}{u\in U} = |\lambda| \norm{x}_{V/U}. \]
For subadditivity, we have
\begin{align*}
\norm{x+y}_{V/U} &= \inf\setbuilder{\norm{x+y+u}}{u\in U} \\
&= \inf\setbuilder{\norm{x+y+u+v}}{u\in U, v\in U} \\
&\leq \inf\setbuilder{\norm{x+u}+\norm{y+v}}{u,v\in U} \\
&= \norm{x}_{V/U} + \norm{y}_{V/U}.
\end{align*}

(2) The function $N: V\to \R^+: x\mapsto \norm{[x]_U}_{V/U}$ is a seminorm. It is bounded by $1$ on $\cball_V(0,1)$, since
\[ \norm{[x]_U}_{V/U} = \inf\setbuilder{\norm{x+u}}{u\in U} \leq \norm{x+0} = \norm{x}. \]
It is continuous by \ref{sublinearContinuity}. And thus $\norm{\cdot}_{V/U}$ is continuous by \ref{characteristicPropertyInitialFinalConvergence}.

(3) For all $v\in U$, we have
\[ 0 \leq \norm{[v]_U}_{V/U} \defeq \inf\setbuilder{\norm{v+u}}{u\in U} \leq \norm{v + (-v)} = 0, \]
so $v\in \ker\big(\norm{[\cdot]_U}_{V/U}\big)$. This implies $U \subseteq \ker\big(\norm{[\cdot]_U}_{V/U}\big)$. Since the kernel is closed, by (2), we have $\overline{U} \subseteq \ker\big(\norm{\cdot}_{V/U}\big)$.

Conversely, suppose $v\in \ker\big(\norm{[\cdot]_U}_{V/U}\big)$. Then, by \ref{sequencesToExtrema}, there exists a sequence $\seq{u_n}$ in $U$ such that $\norm{v+ u_n} \to 0$. Thus $\seq{-u_n}$ converges to $v$ in $V$ and $v\in \overline{U}$.

(4) We have $[v]_U \in \ker\big(\norm{\cdot}_{V/U}\big) \iff \norm{[v]_U}_{V/U} = 0 \iff v\in \ker\big(\norm{[\cdot]_U}_{V/U}\big)$.

(5) We have that
\begin{align*}
\text{$\norm{\cdot}_{V/U}$ is a norm} &\iff \ker\big(\norm{\cdot}_{V/U}\big) = \{[0]_U\} \\
&\iff \forall v\in \overline{U}: \; [v]_U = [0]_U \\
&\iff \forall v\in \overline{U}: \; v\in U \\
&\iff U = \overline{U}.
\end{align*}
\end{proof}

\begin{proposition} \label{quotientNormFromKernelSeminorm}
Let $\sSet{V,\norm{\cdot}}$ be a seminormed space and $N \defeq \ker\big(\norm{\cdot}\big)$. Then $\norm{[v]_N}_{V/N} = \norm{v}$ for all $v\in V$.
\end{proposition}
\begin{proof}
For all $u\in N$ we have
\[ \norm{v} = \big|\norm{v} - \norm{-u}\big| \leq \norm{v + u} \leq \norm{v}+\norm{u} = \norm{v}, \]
by the reverse triangle inequality \ref{reverseTriangleInequality}, so $\norm{v} = \norm{v+u}$. Thus
\[ \norm{[v]_N}_{V/N} = \inf \setbuilder{\norm{v+u}}{u\in N} = \inf\{\norm{v}\} = \norm{v}. \]
\end{proof}

\begin{lemma} \label{approximationToQuotientNormLemma}
Let $\sSet{V,\norm{\cdot}}$ be a seminormed space, $U\subseteq V$ a subspace, $x\in V$ and $\epsilon > 0$. Then there exists $y\in [x]_U$ such that $\norm{y} \leq \norm{[x]_U}_{V/U} + \epsilon$.
\end{lemma}
\begin{proof}
We have $\norm{[x]_U}_{V/U} = \inf\setbuilder{\norm{y}}{y\in [x]_U}$. If there was no such $y$, then $\norm{[x]_U}_{V/U} + \epsilon$ would have been a lower bound, which is impossible, since $\norm{[x]_U}_{V/U}$ is not greater than $\norm{[x]_U}_{V/U} + \epsilon$.
\end{proof}

\begin{proposition} \label{quotientSeminormConvergenceIsQuotientConvergence}
Let $\sSet{V, \norm{\cdot}}$ be a seminormed space and $U\subseteq V$ a subspace. Then the pseudometric convergence on $V/U$ w.r.t.\ the seminorm $\norm{\cdot}_{V/U}$ is equal to the quotient convergence.
\end{proposition}
\begin{proof}
Since the quotient convergence makes $\norm{\cdot}_{V/U}$ continuous, by \ref{quotientNormProperties}, and the quotient convergence is a vector space convergence, by \ref{quotientConvergenceVectorSpaceConvergence}, the quotient convergence is stronger than the initial vector space convergence w.r.t.\ the seminorm $\norm{\cdot}_{V/U}$. By \ref{seminormConvergenceInitialVectorSpaceConvergence} this is equal to the seminorm convergence.

For the opposite inclusion, it is enough to compare the convergences at $0$. Suppose $F \in \powerfilters(V/U)$ such that $F$ converges to $[0]_U\in V/U$ in the seminorm convergence. We need to prove $F\to [0]_U$ in the quotient convergence.

For all $A\in F$ and $\epsilon > 0$, define
\[ A_\epsilon \defeq \bigcup_{[x]_U\in A}\setbuilder{y\in [x]_U}{\norm{y} \leq \norm{x}_{U/V} + \epsilon}. \]
Then consider $G \defeq \upset\setbuilder{A_\epsilon}{A\in F, \epsilon > 0}$. It is a filter since $A_{\epsilon_1}\cap B_{\epsilon_2} \supseteq (A\cap B)_{\min\{\epsilon_1, \epsilon_2\}}$. We claim $G\to 0$ in $V$ and $[\cdot]_{U}^{\imf\imf}(G) \subseteq F$, which is enough to prove that $F\to [0]_U$ in $V/U$.

For the first claim, take $\delta >0$. Then $A \defeq \ball_{V/U}([0]_U, \delta / 2) \in F$ and, for all $y\in A_{\delta / 2}$, there exists $[x]_U\in A$ such that $y\in [x]_U$ and
\[ \norm{y} \leq \norm{[x]_U}_{V/U} + \delta / 2 < \delta / 2 + \delta / 2 = \delta, \]
so $A_{\delta / 2} \subseteq \ball_V(0, \delta)$. Since this holds for all $\delta > 0$, we have that $G \to 0$ in $V$.

For the second claim, take arbitrary $A_\epsilon \in G$. It is enough to prove that $A\subseteq [\cdot]_{U}^{\imf}(A_\epsilon)$. Indeed, take arbitrary $[x]_U \in A$. By \ref{approximationToQuotientNormLemma}, there exists $y\in [x]_U$ such that $\norm{y} \leq \norm{[x]_U}_{V/U} + \epsilon$. Since $y\in A_\epsilon$ and $[y]_U = [x]_U$, we have $[x]_U \in [\cdot]_{U}^{\imf}(A_\epsilon)$.
\end{proof}

\begin{proposition} \label{quotientBanachSpace}
Let $X$ be a Banach space and $U\subseteq X$ a closed subspace. Then $X/U$ is a Banach space.
\end{proposition}
TODO: does this follow from a more general principle???
\begin{proof}
The space $X/U$ is normed by \ref{quotientNormProperties} and \ref{quotientSeminormConvergenceIsQuotientConvergence}. We just need to show that it is complete.

Let $\seq{[x_n]_U}$ be a Cauchy sequence in $X/U$. There exists a subsequence $\seq{x_{n_k}}_k$ such that $\norm{[x_{n_k}]_U - [x_{n_{k-1}}]_U}_{X/U} \leq 2^{-k}$ for all $k\in \N\setminus\{0\}$.

For all $k\in \N\setminus\{0\}$, we can find $y_{k} \in [x_{n_k} - x_{n_{k-1}}]_U$ such that
\[ \norm{y_k} \leq \norm{[x_{n_k}]_U - [x_{n_{k-1}}]_U}_{X/U} + 2^{-k} \leq 2^{-k}+2^{-k} = 2^{-k+1}. \]
Define the sequence $\seq{z_k}$ recursively by $z_0 = x_{n_0}$ and $z_{k+1} = z_{k}+ y_{k+1}$. We have that $z_k \in [x_{n_k}]_U$ for all $k\in \N$. For all $l\leq m \in \N$, we have
\begin{align*}
\norm{z_m - z_l} &= \norm{\sum_{k=l}^{m-1} z_{k+1} - z_{k}} \\
&= \norm{\sum_{k=l}^{m-1} y_{k+1}} \\
&\leq \sum_{k=l}^{m-1}\norm{y_{k+1}} \\
&\leq \sum_{k=l}^{m-1}2^{-k} \leq \sum_{k=l}^{\infty}2^{-k} = 2^{-l}\Big(\sum_{k=0}^{\infty}2^{-k}\Big) = 2^{1-l}.
\end{align*}
This implies that $\seq{z_k}$ is a Cauchy sequence in $X$. Since $X$ is a Banach space, it has a limit $z$. By continuity of $[\cdot]_U$, we have that $\seq{[x_{n_k}]_U} = \seq{[z_k]_U} \to [z]_U$.
\end{proof}

\section{Unbounded operators}
Should be: not-necessarily-bounded operators.
\subsection{Operators bounded below}
TODO: also unbounded operators!
\begin{definition}
Let $T$ be a linear operator. We say $T$ is \udef{bounded below} if
\[ \exists b>0:\forall v\in \dom(T): \quad \norm{Tv}\geq b\norm{v} \]
\end{definition}

\begin{proposition} \label{boundedBelow}
Let $T\in \Lin(V, W)$ be an operator. Then $T$ has a bounded inverse $T^{-1}: \im(T)\to V$ \textup{if and only if} $T$ is bounded below by some constant $b$.

In this case
\[ \displaystyle\norm{T^{-1}} = \left(\inf_{x\neq 0}\frac{\norm{Tx}}{\norm{x}}\right)^{-1} \leq \frac{1}{b}. \]
\end{proposition}
\begin{proof}
First assume $T$ bounded below.
To show $T$ is injective, take $x_1,x_2\in \dom T$ such that $Tx_1 = Tx_2$. Then
\[ 0 = \norm{Tx_1 - Tx_2} = \norm{T(x_1 - x_2)} \geq b\norm{x_1 - x_2} \geq 0. \]
So $\norm{x_1 - x_2} = 0$ and thus $x_1=x_2$.
The existence of $T^{-1}$ is then clear. For boundedness notice that $T^{-1}y \in \dom(T)$, so because $T$ is bounded below,
\[ b\norm{T^{-1}y} \leq \norm{TT^{-1}y} = \norm{y} \quad\implies\quad \norm{T^{-1}y} \leq \frac{1}{b}\norm{y}. \]

This also shows that $\norm{T^{-1}} \leq 1/b$ for all lower bounds $b$. In other words $1/\norm{T^{-1}} \geq \inf_{x\neq 0}\norm{Tx}/\norm{x}$.

Now assume $T^{-1}$ bounded. Then for all $x\in\dom(T)$: $\norm{x} = \norm{T^{-1}Tx} \leq \norm{T^{-1}}\norm{Tx}$, so $T$ is bounded below by $1/\norm{T^{-1}}$.

This also shows that $1/\norm{T^{-1}}$ is a lower bound, so $1/\norm{T^{-1}} \leq \inf_{x\neq 0}\norm{Tx}/\norm{x}$.
\end{proof}



\subsection{Closed operators and graph norm}
\begin{definition}
Let $T:\dom(T)\subseteq X\to Y$ be an operator. Then $T$ is a \udef{closed operator} if $\graph(T)$ is closed in $X\oplus Y$.
\end{definition}
This is not the same as a closed map in the topological sense!

\subsubsection{The graph norm}
Let $L:V\to W$ be a linear map between normed spaces. The graph of $L$
\[ \setbuilder{(v,w)\in V\oplus W}{w = Lv} \]
has a natural norm inherited from the direct sum:
\[ \norm{(v,Lv)} = \norm{v}_V + \norm{Lv}_W. \]
This norm can also be seen as a norm on $V$: the \udef{graph norm} induced by $L$ is defined as
\[ \norm{v}_L := \norm{v}_V + \norm{Lv}_W. \]

\begin{proposition}
Let $L: \sSet{V, \norm{\cdot}_V}\to \sSet{W, \norm{\cdot}_W}$ be a linear map between normed spaces. Then $L: \sSet{V, \norm{\cdot}_L}\to \sSet{W, \norm{\cdot}_W}$ is bounded with norm $K$.
Also
\begin{enumerate}
\item $K \leq 1$;
\item $K < 1$ \textup{if and only if} $L: \sSet{V, \norm{\cdot}_V}\to \sSet{W, \norm{\cdot}_W}$ is bounded.
\end{enumerate}
\end{proposition}
\begin{proof}
Take any $v\in V$. Then
\[ \norm{L(v)}_W \leq \norm{L(v)}_W + \norm{v}_V = \norm{v}_L. \]
This shows the $L$ is bounded and the norm is less than or equal to $1$.

Now we can write
\[ K = \sup_{v\in V\setminus\{0\}}\frac{\norm{L(v)}_W}{\norm{v}_L} = \sup_{v\in V\setminus\{0\}}\frac{\norm{L(v)}_W}{\norm{v}_V + \norm{L(v)}_W} \defeq \sup_{v\in V\setminus\{0\}}K_v, \]
where we have set $K_v \defeq \frac{\norm{L(v)}_W}{\norm{v}_V + \norm{L(v)}_W}$.

First assume $L: \sSet{V, \norm{\cdot}_V}\to \sSet{W, \norm{\cdot}_W}$ is bounded. Then, from $\norm{L_v}_W = K_v \big(\norm{L(v)}_W + \norm{v}_V\big)$, we calculate
\[ K_v\norm{v}_V = \norm{L(v)}_W(1-K_v) \leq \norm{L}\;\norm{v}_V(1-K_v), \]
which implies $K_v \leq \norm{L}(1-K_v)$. This can be written as $K_v \leq \frac{\norm{L}}{1+ \norm{L}}$.
Thus $K = \sup_v K_v \leq \frac{\norm{L}}{1+ \norm{L}} < 1$.

Now assume $L: \sSet{V, \norm{\cdot}_V}\to \sSet{W, \norm{\cdot}_W}$ is unbounded. We write
\[ K_v = \frac{\norm{L(v)}_W}{\norm{v}_V + \norm{L(v)}_W} = \frac{\norm{L(v)}_W + \norm{v}_V - \norm{v}_V}{\norm{v}_V + \norm{L(v)}_W} = 1 - \frac{\norm{v}_V}{\norm{v}_V + \norm{L(v)}_W}. \]
We can find a sequence $\seq{v_n}$ of unit vectors such that $\norm{L(v_n)}_W \to \infty$. Then $K_{v_n} = 1 - \frac{1}{1+\norm{L(v_n)}_W} \to 1$. Thus
\[ K = \sup_{v\in V}K_v \geq \sup_{v\in \seq{v_n}}K_v \geq \limsup_{n}K_{v_n} \geq \lim_{n}K_{v_n} = 1, \]
so $K = 1$.
\end{proof}


\begin{lemma} \label{graphNormConvergenceLemma}
Let $T: X\not\to Y$ be an operator between normed spaces and $\seq{x_n}$ a sequence in $\dom(T)$. Then the following are equivalent:
\begin{enumerate}
\item $x_n \overset{\norm{\cdot}_T}{\longrightarrow} x$;
\item $(x_n, Tx_n) \overset{\norm{\cdot}_{X\oplus Y}}{\longrightarrow} (x, Tx)$;
\item $x_n\overset{\norm{\cdot}_X}{\longrightarrow} x$ and $Tx_n\overset{\norm{\cdot}_Y}{\longrightarrow} Tx$.
\end{enumerate}
\end{lemma}
\begin{proof}
We have the equivalences
\begin{align*}
x_n \overset{\norm{\cdot}_T}{\longrightarrow} x &\iff \norm{x_n - x}_T \longrightarrow 0 \\
&\iff \norm{x_n - x}_X + \norm{Tx_n - Tx}_Y \longrightarrow 0 \\
&\iff \norm{(x_n - x, Tx_n - Tx)}_{X\oplus Y} \longrightarrow 0 \\
&\iff \norm{(x_n, Tx_n) - (x, Tx)}_{X\oplus Y} \longrightarrow 0 \\
&\iff (x_n, Tx_n) \overset{\norm{\cdot}_{X\oplus Y}}{\longrightarrow} (x, Tx).
\end{align*}
Now if $x_n\overset{\norm{\cdot}_X}{\longrightarrow} x$ and $Tx_n\overset{\norm{\cdot}_Y}{\longrightarrow} Tx$, then $\norm{x_n - x}_X + \norm{Tx_n - Tx}_Y \longrightarrow 0$. 

Conversely, from
\[ 0 \leq \norm{x_n - x}_X\leq \norm{x_n - x}_X + \norm{Tx_n - Tx}_Y, \]
we get $x_n\overset{\norm{\cdot}_X}{\longrightarrow} x$ by the squeeze theorem (TODO ref). We similarly get $Tx_n\overset{\norm{\cdot}_Y}{\longrightarrow} Tx$.
\end{proof}
\begin{corollary}
The graph norm is strong than then original norm. Both norms are equivalent on $\dom(T)$ \textup{if and only if} $T$ is bounded.
\end{corollary}
\begin{corollary}
Let $T: X\not\to Y$ be an operator between normed spaces. Then the topology induced by the graph norm is equal to the initial topology w.r.t. $\{\id_X: X\to \sSet{X,\norm{\cdot}_X}, T\}$.
\end{corollary}

\subsubsection{Closed operators}

\begin{proposition} \label{boundedBelowClosedRange}
Let $T\in \Lin(X, Y)$ be a closed operator between Banach spaces that is bounded below. Then $\im(T)$ is closed.
\end{proposition}
\begin{proof}
Let $T$ be bounded below by $b$ and let $\seq{Tx_n}$ be a Cauchy sequence in $\im(T)$. Then $\norm{x_m - x_n} \leq \frac{1}{b}\norm{T(x_m - x_n)}$, so $\seq{x_n}$ is also Cauchy by \ref{CauchyCriterion}.

So we can find $x\in X,y\in Y$ such that $x_n\to x$ and $Tx_n\to y$. By closedness of $T$, we have $Tx = y$ and thus $y\in\im(T)$.
\end{proof}

\begin{proposition}
Let $X,Y$ be Banach spaces and $S,T\in \Lin(X,Y)$ with $\dom(S) = \dom(T)$. If $S$ is a closed operator and there exist $\alpha,\beta,\gamma\in \R^+$ such that $0 < \gamma \leq 1$ and $\beta < 1/\gamma$ and
\[ \norm{(S-T)u} \leq \alpha \norm{u} + \beta\norm{Su}^\gamma{u}^{1-\gamma} \qquad\text{for all $u\in \dom(S) = \dom(T)$,} \]
then $T$ is also closed.
\end{proposition}
\begin{proof}
TODO Jeribi.
\end{proof}

\subsubsection{Closable operators}
\begin{definition}
A linear operator is called \udef{closable} if it has closed extension.
\end{definition}

\begin{proposition} \label{closableCriterion}
A linear operator $T$ is closable \textup{if and only if} for all sequences $\seq{x_n}\subset\dom(T)$
\[ \left(x_n\to 0 \land T(x_n)\to v\right) \quad\implies\quad v = 0. \]
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{lemma}
A closable operator $T$ has a minimal closed extension $\overline{T}$, which is given by the closure of the graph of $T$.
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{lemma} \label{domImClosureOperator}
Let $T$ be a closable operator. Then
\begin{enumerate}
\item $\dom(T)$ is dense in $\dom(\overline{T})$;
\item $\im(T)$ is dense in $\im(\overline{T})$.
\end{enumerate}
\end{lemma}
\begin{proof}
We have $(x,\overline{T}x)\in\graph(\overline{T})$ iff there exists a sequence $\seq{x_n}$ in $\dom(T)$ such that $\seq{x_n,Tx_n}\overset{\norm{\cdot}_{X\oplus Y}}{\longrightarrow} (x,\overline{T}x)$. So we can conclude using \ref{graphNormConvergenceLemma}.
\end{proof}

\begin{lemma}
Let $X,Y$ be normed spaces and $T$ a closable operator such that $\overline{T}$ is injective. Then $\overline{T}^{\,-1} = \overline{T^{-1}}$.
\end{lemma}
\begin{proof}
TODO
\end{proof}

\subsubsection{Domain and core}
\begin{definition}
Let $T: X\not\to Y$ be a closed operator between normed spaces and $D\subseteq \dom(T)$ a subspace. We call $D$ a \udef{core} or \udef{essential domain} for $T$ if $\setbuilder{(x,Tx)}{x\in D}$ is dense in $\graph(T)\subseteq X\oplus Y$.
\end{definition}

\begin{proposition} \label{operatorCoreCriterion}
Let $T: X\not\to Y$ be a closed operator between normed spaces and $D\subseteq \dom(T)$ a subspace. Then $D$ is a core of $T$ \textup{if and only if} $D$ is dense in $\dom(T)$ w.r.t. the graph norm $\norm{\cdot}_T$ of $T$.
\end{proposition}
Note that the norm is bounded by the graph norm, so the graph norm topology is finer than the norm topology by \ref{normComparison}. Thus $\closure_{\norm{\cdot}_T}(D) \subseteq \closure_{\norm{\cdot}}(D)$ and it is not enough for $D$ to be norm dense in $\dom(T)$.
\begin{proof}
Immediate by \ref{graphNormConvergenceLemma}.
\end{proof}






\chapter{Differentiation}
\url{file:///C:/Users/user/Downloads/978-1-4614-3894-6.pdf}

\url{file:///C:/Users/user/Downloads/2011_Bookmatter_TheRicciFlowInRiemannianGeomet.pdf}

TODO: move to section on convergence vector spaces.

TODO: move real analysis above here.


\section{Directional derivatives}
One possible perspective: All derivatives are based on directional derivatives.

TODO: Hadamard derivative.

\begin{definition}
Let $V,W$ be convergence vector spaces and $f:U\subseteq V\to W$ a function. For $a\in U$ and $u\in V$, we call
\[ \partial_u f|_a \defeq \lim_{t\to 0} \frac{f(a+tu) - f(a)}{t} \]
the \udef{directional derivative} of $f$ at $a$ in the direction $u$, if it exists.
\end{definition}

For a given function $f:V\to W$, the directional derivative is a partial function of both a direction and a point:
\[ (V\times V) \not\to W:\quad (u,a) \mapsto \partial_u f(a)  \]

Partial application in the first argument gives a function
\[ \partial_u f:\; V\not\to W:\; a\mapsto \partial_u f(a) \defeq \partial_u f|_a \]
that is also referred to as the \udef{directional derivative} of $f$ in the direction $u$.



\subsection{Partial derivatives}
\begin{definition}
Let $V,W$ be convergence vector spaces, $f:U\subseteq V\to W$ a function defined on a subset $U$ and $u\in V$.
We call the function
\[ \partial_u f: U\to W: a\mapsto \partial_u f|_a \]
the \udef{partial derivative} of $f$ in the direction $u$. We also write $\pd{f}{u}$ for $\partial_u f$.
\begin{itemize}
\item If $V = \R^n$ with the standard basis $\mathcal{E} = \seq{\vec{e}_i}_{i=1}^n$, then we will often write $\partial_{\vec{e}_i}f$ as $\pd{f}{x^i}$.
\item If $V = \R^2$, then we often write the standard basis as $\seq{\vhat{x}, \vhat{y}}$ and the directional derivatives $\partial_{\vhat{x}}f$ and $\partial_{\vhat{y}}f$ are written as $\partial_{x}f$ and $\partial_{y}f$.
\item If $V = \R^3$, then we often write the standard basis as $\seq{\vhat{x}, \vhat{y}, \vhat{z}}$ and the additional directional derivative $\partial_{\vhat{z}}f$ is written as $\partial_{z}f$.
\end{itemize}
\end{definition}
TODO notation $D^\alpha$ for multiindex $\alpha$. Also $|\alpha| = \sum_i \alpha_i$.


\subsection{Gateaux derivative}
\begin{definition}
Let $f:V\to W$ be a function between convergence vector spaces. If $\partial_u f|_a$ exists for all $u\in V$, then we call the partial application of the directional derivative in the second argument
\[ \diff{_af}: V\to W: u\mapsto \diff{_af}(u) \defeq \partial_u f|_a = \lim_{t\to 0} \frac{f(a+tu) - f(a)}{t} \]
the \udef{Gateaux differential} of $f$ at the point $a$.

If $\diff{_af}: V\to W$ is a continuous linear map, we will refer to it as the \udef{Gateaux derivative}.
\end{definition}
The Gateaux differential is homogeneous even if it is not linear:
\begin{lemma} \label{GateauxDifferentialHomogeneous}
Let $f:V\to W$ be a function between normed spaces and $a,u\in V$. If $\partial_u f$ is defined at $a$, then
\[ \diff{_a f}(\lambda u) = \partial_{\lambda u}f(a) = \lambda\partial_u f(a) = \lambda \diff{_a f}(u) \qquad \forall \lambda\in\F. \]
\end{lemma}
\begin{proof}
$\partial_{\lambda u}f(a) = \lim_{t\to 0} \frac{f(a+t\lambda u) - f(a)}{t} = \lim_{t\lambda\to 0} \frac{f(a+t\lambda u) - f(a)}{t \lambda / \lambda} = \lambda\partial_u f(a)$.
\end{proof}


\begin{example}
The function
\[ f: \R^2\to \R: (x,y) \mapsto \begin{cases}
\frac{x^2y}{x^2+y^2} & (x,y)\neq (0,0) \\0 & (x,y) = (0,0)
\end{cases} \]
has a Gateaux differential $\diff{_{\vec{0}}f}$ that is defined everywhere, but is non-linear.
Indeed, take $(x_0, y_0)\in R^2$. Then
\begin{align*}
\diff{_{\vec{0}}f}(x_0, y_0) &= \lim_{t\to 0}\frac{f(tx_0, ty_0) - f(0,0)}{t} \\
&= \lim_{t\to 0}\frac{1}{t}\frac{(tx_0)^2(ty_0)}{(tx_0)^2+(ty_0)^2} \\
&= \lim_{t\to 0}\frac{x_0y_0}{x_0^2+y_0^2} = \frac{x_0y_0}{x_0^2+y_0^2}.
\end{align*}
This is clearly non-linear in $(x_0, y_0)$.
\end{example}


\subsection{Frchet derivative}
\url{https://link.springer.com/content/pdf/bbm%3A978-3-642-16286-2%2F1.pdf}
\url{http://www.m-hikari.com/ams/ams-password-2008/ams-password17-20-2008/behmardiAMS17-20-2008.pdf}

\begin{definition}
Let $V,W$ be normed vector spaces and $f: V\to W$ a function.
If $f$ has a (bounded linear) Gateaux derivative at $a$ and convergence in the definition of the derivative
\[ \diff{_af}: V\to W: u\mapsto \diff{_af}(u) \defeq \partial_u f|_a = \lim_{t\to 0} \frac{f(a+tu) - f(a)}{t} \]
is uniform in all $u$ on the $\sphere(\vec{0},1)$, then we say the function is \udef{(Frchet) differentiable} at $a$ and has \udef{Frchet derivative} $\diff{_af}$.

We may also write $\diff{f}$, leaving the $a$ implicit.
\end{definition}
TODO: for CVS: uniform on some vicinity??

\begin{proposition} \label{FrechetDerivativeAsymptotics}
Let $V,W$ be normed vector spaces, $f:V\to W$ a function and $a\in V$.

Then $f$ is Frchet differentiable at $a$ \textup{if and only if} there exists a bounded linear map $A: V\to W$ such that $f(a+x)$ can be written as
\[ f(a+x) = f(a) + A(x) + o\big(\norm{x}\big) \qquad \text{as} \qquad x\to 0. \]
In this case $A = \diff{_af}$.
\end{proposition}
\begin{proof}
First assume $f$ is Frchet differentiable at $a$. Then
\begin{multline*}
\forall \varepsilon>0:\exists \delta>0: \; \forall u\in S(\vec{0},1): \forall t\in\R: \; t< \delta \implies \varepsilon > \\ \norm{\frac{f(a+tu) - f(a)}{t} - \diff{_af}(u)} = \frac{\norm{f(a+tu) - f(a)- \diff{_af}(tu)}}{|t|} = \frac{\norm{f(a+tu) - f(a)- \diff{_af}(tu)}}{\norm{tu}}.
\end{multline*}

Now each vector $x$ in $V$ can be written as $tu$ for some $t\in\R$ and $u\in S(\vec{0},1)$, so this can be written as
\[ \forall \varepsilon>0:\exists \delta>0: \; \forall x\in V: \; \norm{x}< \delta \implies  \varepsilon > \frac{\norm{f(a+x) - f(a)- \diff{_af}(x)}}{\norm{x}} \]
which is exactly the statement $f(a+x) = f(a) + \diff{_af}(x) + o(x)$ as $x\to 0$.

The logic can be reversed to obtain the equivalence.
\end{proof}

\begin{proposition} \label{FrechetDifferentiableImpliesContinuous}
Let $V,W$ be normed vector spaces, $f:V\to W$ a function and $a\in V$. If $f$ is Frchet differentiable at $a$, then $f$ is continuous at $a$.
\end{proposition}
\begin{proof}
Assume $f$ is has Frchet derivative $A$. Then
\[ 0 = \lim_{x\to a} \norm{f(x) - f(a) - \diff{_af}(x-a)} = \norm{\lim_{x\to a}f(x) - f(a) - \diff{_af}(\lim_{x\to a} x-a)} = \norm{\lim_{x\to a}f(x) - f(a)}. \]
\end{proof}

\begin{example}
The result \ref{FrechetDifferentiableImpliesContinuous} does not hold for Gateaux derivatives. Every linear functional is Gateaux differentiable, but there exist discontinuous linear functionals.
\end{example}

\begin{proposition}
If there exists a basis $\beta$ of $V$ such that the partial derivatives of $f:U\subseteq V\to W$ w.r.t. $\beta$ exist and are continuous in $a\in V$, then $f$ is Frchet differentiable in $a$.
\end{proposition}
\begin{proof}

\end{proof}
TODO for finite dimensions! Expand to criterion for Gateaux to Frchet.
\begin{example}

\end{example}

\subsection{Functions with real domain}
If $V = \R$, then there is, up to scalar multiplication, only one direction in $V$, namely $1$. We denote the directional derivative $f'(a) \defeq \partial_1 f|_a$.


\begin{definition}
Let $\sSet{V,\xi}$ be a convergence vector space, $f: \R \to V$ a function and $a\in \R$. We call $f$ \udef{differentiable} at $a$ if the directional derivative $\partial_1f|_a$ exists.

The partial function $f': \R\not\to V: x\mapsto f'(x) \defeq \partial_1f|_x$ is called the \udef{derivative} of $f$. It is also denoted $\od{f(x)}{x}$, in which case $f'(a)$ is denoted $\left.\od{f(x)}{x}\right|_{x=a}$.
\end{definition}

\begin{lemma} \label{elementaryRealDomainDerivative}
Let $\sSet{V,\xi}$ be a convergence vector space, $f: \R \to V$ a function and $a\in \R$. Then
\[ f'(a) = \lim_{t\to 0} \frac{f(a+t) - f(a)}{t}. \]
\end{lemma}
We mean, in particular, that $f$ is differentiable at $a$ if and only if the limit exists.
\begin{proof}
Restatement of the definition.
\end{proof}

\begin{proposition} \label{realDomainDerivativeIsFrechet}
Let $\sSet{V,\xi}$ be a convergence vector space, $f: \R \to V$ a function and $a\in \R$. Then the following are equivalent:
\begin{enumerate}
\item the derivative $f'(a)$ exists;
\item $\diff{_af}: \R \to \R$ exists as a Gateaux differential;
\item $\diff{_af}: \R \to \R$ exists as a Gateaux derivative;
\item $\diff{_af}: \R \to \R$ exists as a Frchet derivative.
\end{enumerate}
\end{proposition}
We have $f'(a) = \diff{_af}(1)$.
\begin{proof}
$(1) \Rightarrow (2)$ For all $\lambda \in \R$, we have, by \ref{GateauxDifferentialHomogeneous}, that
\[ \diff{_af}(\lambda) = \lambda \diff{_af}(1) = \lambda \partial_1f|_a = f'(a) \]
exists.

$(2) \Rightarrow (3)$ Every Gateaux differential is homogeneous, \ref{GateauxDifferentialHomogeneous}. Since the domain of $\diff{_af}$ is $\R$, this also implies that $\diff_{af}$ is linear.

$(3) \Rightarrow (4)$ For the non-trivial direction, we need to check uniformity of the convergence on $\sphere(\vec{0}, 1) = \{-1,1\}$. Since this is a finite set, this follows easily.
\end{proof}



\begin{lemma} \label{directionalDerivativeAsOD}
Let $V,W$ be convergence vector spaces, $f: V\to W$ a function, $a,u\in V$ and $t'\in \R$. Then
\[ \partial_u f|_{a+t'u} = \left.\dod{f(a+tu)}{t}\right|_{t=t'}. \]
\end{lemma}
In particular $\partial_u f|_a = \left.\dod{f(a+tu)}{t}\right|_{t=0}$.
\begin{proof}
We calculate, using \ref{elementaryRealDomainDerivative},
\begin{align*}
\left.\dod{f(a+tu)}{t}\right|_{t=t'} &= \lim_{t\to 0}\frac{f\big(a+(t+t')u\big) - f(a+t'u)}{t} \\
&= \lim_{t\to 0}\frac{f\big(a+t'u + tu \big) - f(a+t'u)}{t} = \partial_uf|_{a+t'u}.
\end{align*}
\end{proof}
\begin{corollary} \label{partialDerivativeImpliesContinuityAlongLine}
Let $V,W$ be convergence vector spaces, $f: V\to W$ a function, $a,u\in V$. Suppose $\partial_uf|_a$ exists. Then the function $t\mapsto f(a+tu)$ is continuous at $t=0$.
\end{corollary}
\begin{proof}
By the lemma we have that $t\mapsto f(a+tu)$ is differentiable at $t=0$. This implies continuity by \ref{realDomainDerivativeIsFrechet} and \ref{FrechetDifferentiableImpliesContinuous}.
\end{proof}






\section{Rules of calculus}
\begin{proposition}[Chain rule for Gateaux and Frchet derivative] \label{FrechetChainRule}
Let $U$ be a convergence vector space, $V,W$ be normed spaces and $f: U\to V$, $g: V\to W$ be functions. Suppose $f$ has a Gateaux differential at $a\in U$ and $g$ is Frchet differentiable at $f(a)$, then $g\circ f$ has a Gateaux differential at $a$ defined by
\[ \diff{_a}\big(g\circ f\big) = \big(\diff{_{f(a)}}g\big) \circ \big(\diff{_a}f\big). \]
In addition, if $\diff{_a}f$ is linear / bounded / a Frchet derivative, then $\diff{_a}\big(g\circ f\big) $ is too.
\end{proposition}
\begin{proof}
Take arbitrary $u\in V$.

We calculate, using the fact that $\diff{_{f(a)}g}$ is a bounded linear function and \ref{FrechetDerivativeAsymptotics},
\begin{align*}
\diff{_{f(a)}g}\big(\diff{_af}(u)\big) &= \diff{_{f(a)}g}\Big(\lim_{t\to 0}\frac{f(a+tu) - f(a)}{t}\Big) \\
&= \lim_{t\to 0}\frac{1}{t}\diff{_{f(a)}g}\big(f(a+tu) - f(a)\big) \\
&= \lim_{t\to 0}\frac{1}{t}\Big(g\big(f(a) + f(a+tu) - f(a)\big) - g\big(f(a)\big) + h\big(f(a+tu) - f(a)\big)\Big) \\
&= \lim_{t\to 0}\frac{(g\circ f)(a+tu) - (g\circ f)(a)}{t} + \lim_{t\to 0}\frac{h\big(f(a+tu) - f(a)\big)}{t} \\
&= \diff{_a}\big(g\circ f\big)(u) + \lim_{t\to 0}\frac{h\big(f(a+tu) - f(a)\big)}{t},
\end{align*}
where $h(x) \in o\big(\norm{x}\big)$ as $x\to 0$. We just need to prove $\lim_{t\to 0}\frac{h\big(f(a+tu) - f(a)\big)}{t} = 0$. Indeed, we have
\begin{align*}
\lim_{t\to 0}\frac{\norm{h\big(f(a+tu) - f(a)\big)}}{t} &= \lim_{t\to 0}\frac{\norm{h\big(f(a+tu) - f(a)\big)}}{\norm{f(a+tu) - f(a)}}\frac{\norm{f(a+tu) - f(a)}}{t} \\
&= \lim_{t\to 0}\frac{\norm{h\big(f(a+tu) - f(a)\big)}}{\norm{f(a+tu) - f(a)}}\cdot \lim_{t\to 0}\norm{\frac{f(a+tu) - f(a)}{t}} \\
&= 0\cdot \diff{_af}(u).
\end{align*}
Since composiing linear / bounded functions results in a linear / bounded function, these properties are transferred from $\diff{_a}f$ to $\diff{_a}(g\circ f)$.

Finally, suppose $\diff{_a}f$ is a Frchet derivative. We verify that $\diff{_a}(g\circ f)$ is a Frchet derivative using \ref{FrechetDerivativeAsymptotics}. Take $u\in U$. Then
\begin{align*}
(g\circ f)(a+x) &= g\big(f(a) + \diff{_af}(x) + h(x)\big) \\
&= (g\circ f)(a) + \diff{_{f(a)}g}\big(\diff{_af}(x) + h(x)\big) + h'\big(\diff{_af}(x) + h(x)\big) \\
&= (g\circ f)(a) + (\diff{_{f(a)}g}\circ \diff{_af})(x) + \diff{_{f(a)}g}\big(h(x)\big) + h'\big(\diff{_af}(x) + h(x)\big),
\end{align*}
where $h(x), h'(x) \in o\big(\norm{x}\big)$. We need to show that $\diff{_{f(a)}g}\big(h(x)\big) + h'\big(\diff{_af}(x) + h(x)\big) \in o\big(\norm{x}\big)$. Indeed,
\begin{align*}
\frac{\norm{\diff{_{f(a)}g}\big(h(x)\big) + h'\big(\diff{_af}(x) + h(x)\big)}}{\norm{x}} &\leq \norm{\diff{_{f(a)}g}}\frac{\norm{h(x)}}{\norm{x}} + \frac{\norm{h'\big(\diff{_af}(x) + h(x)\big)}}{\norm{\diff{_af}(x) + h(x)}}\frac{\norm{\diff{_af}(x) + h(x)}}{\norm{x}} \\
&\leq \norm{\diff{_{f(a)}g}}\frac{\norm{h(x)}}{\norm{x}} + \frac{\norm{h'\big(\diff{_af}(x) + h(x)\big)}}{\norm{\diff{_af}(x) + h(x)}}\Big(\norm{\diff{_af}} + \frac{\norm{h(x)}}{\norm{x}}\Big) \\
&\to \norm{\diff{_{f(a)}g}}\cdot 0 + 0\cdot \Big(\norm{\diff{_af}} + 0\Big) = 0 \qquad \text{as $x\to 0$}.
\end{align*}
\end{proof}

\begin{proposition} \label{directionalDerivativeConstructions}
Let $U,V,W$ be convergence vector spaces, $f,g: U\to V$, $h: U\to \F$, $a\in U$, $u\in U$ and $\lambda\in\F$. Let $T\in \contLin(V,W)$ be a continuous linear function. Suppose $\partial_uf|_a$ and $\partial_u g|_a$ exist. Then
\begin{enumerate}
\item $\partial_u(f+g)|_a$ exists and $\partial_u(f+g)|_a = \partial_uf|_a + \partial_u g|_a$;
\item $\partial_u(hf)|_a$ exists and $\partial_u(hf)|_a = (\partial_uh|_a)\cdot f(a) + h(a)\cdot(\partial_u f|_a)$;
\item $\partial_u(T\circ f)|_a = T\big(\partial_uf|_a\big)$;
\item $\partial_u(\lambda f)_a$ exists and $\partial_u(\lambda f)_a = \lambda \partial_uf|_a$;
\item define $h: x\mapsto f(a+\lambda x)$. Then $\partial_u h|_0$ exists and $\partial_uh|_0 = \lambda \partial_u f|_a$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We calculate
\begin{align*}
\partial_u(f+g)|_a &= \lim_{t\to 0} \frac{(f+g)(a+tu) - (f+g)(a)}{t} \\
&= \lim_{t\to 0} \Big(\frac{f(a+tu) - f(a)}{t} + \frac{g(a+tu) - g(a)}{t} \Big) \\
&= \lim_{t\to 0}\frac{f(a+tu) - f(a)}{t} + \lim_{t\to 0} \frac{g(a+tu) - g(a)}{t} \\
&= \partial_uf|_a + \partial_u g|_a.
\end{align*}

(2) We have that the function $t\mapsto f(a+tu)$ is continuous at $0$ by \ref{partialDerivativeImpliesContinuityAlongLine}. Then we calculate
\begin{align*}
\partial_u(hf)|_a &= \lim_{t\to 0} \frac{(hf)(a+tu) - (hf)(a)}{t} \\
&= \lim_{t\to 0} \Big(\frac{h(a+tu)f(a+tu) - h(a)f(a+tu)}{t} + \frac{h(a)f(a+tu) - h(a)f(a)}{t}\Big) \\
&= \lim_{t\to 0} \Big(\frac{h(a+tu) - h(a)}{t}f(a+tu)\Big) + \lim_{t\to 0}h(a)\frac{f(a+tu) - f(a)}{t} \\
&= \Big(\lim_{t\to 0} \frac{h(a+tu) - h(a)}{t}\Big)f(a) + h(a)\lim_{t\to 0}\frac{f(a+tu) - f(a)}{t} \\
&= (\partial_uh|_a)\cdot f(a) + h(a)\cdot(\partial_u f|_a).
\end{align*}

(3) \begin{align*}
\partial_u(T\circ f)|_a &= \lim_{t\to 0} \frac{(T\circ f)(a+tu) - (T\circ f)(a)}{t} \\
&= \lim_{t\to 0} T\Big(\frac{f(a+tu)-f(a)}{t}\Big) \\
&= T\Big(\lim_{t\to 0} \frac{f(a+tu)-f(a)}{t}\Big) \\
&= T\big(\partial_uf|_a\big).
\end{align*}

(4) This follows from (2) setting $h = \constant{\lambda}$. Alternatively, this follows from (3), since multiplication by $\lambda$ is a continuous linear operator.

(5) We calculate
\begin{align*}
\partial_uh|_0 &= \lim_{t\to 0} \frac{h(tu) - h(0)}{t} \\
&= \lim_{t\to 0} \frac{f(a+ \lambda tu) - f(a)}{t} \\
&= \lim_{t\to 0} \lambda\frac{f(a+ \lambda tu) - f(a)}{\lambda t} \\
&= \lambda\lim_{t\to 0} \frac{f(a+ tu) - f(a)}{t} \\
&= \lambda\partial_uf|_a.
\end{align*}
\end{proof}
\begin{corollary} \label{derivativeOfConstructions}
Let $U,V,W$ be convergence vector spaces, $f,g: U\to V$ functions and $a\in U$. Let $T\in \contLin(V,W)$ be a continuous linear function and $\lambda \in \F$. Suppose $f,g$ are Gateaux differentiable at $a$. Then
\begin{enumerate}
\item $\diff{_a(f+g)}$ exists and $\diff{_a(f+g)} = \diff{_af} + \diff{_ag}$;
\item $\diff{_a(T\circ f)}$ exists and $T\circ \diff{_af}$;
\item $\diff{_a(f+\lambda g)}$ exists and $\diff{_a(f+\lambda g)} = \diff{_af} + \lambda \diff{_ag}$.
\end{enumerate}
\end{corollary}
If $V,W$ are normed spaces, then point (2) follows from the chain rule \ref{FrechetChainRule}.

\begin{lemma}
Let $V$ be a vector space, $W$ a convergence vector space and $T\in\Lin(V,W)$ a linear function. For all $a\in V$, we have $\diff{_aT} = T$.
\end{lemma}
\begin{proof}
Take arbitrary $u\in V$. Then
\[ \diff_{aT}(u) = \lim_{t\to 0} \frac{T(a+tu) - T(a)}{t} = \lim_{t\to 0} \frac{tT(u)}{t} = T(u). \]
\end{proof}

\begin{proposition} \label{derivativeBilinearFunction}
Let $B: V_1 \oplus V_2 \to W$ be a bilinear function. Then, for $(x,y),(a,b)\in V_1\oplus V_2$
\[ \partial_{(x,y)}B|_{(a,b)} = B(x,b) + B(a,y). \]
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
\partial_{(x,y)}B|_{(a,b)} &= \lim_{t\to 0} \frac{B(a+tx, b+ty) - B(a,b)}{t} \\
&= \lim_{t\to 0} \frac{1}{t} (B(a,b) + tB(a,y) + tB(x,b) + t^2B(x,y) - B(a,b)) \\
&=B(x,b) + B(a,y) + \lim_{t\to 0} tB(x,y) \\
&= B(x,b) + B(a,y).
\end{align*}
\end{proof}

\begin{proposition}[Leibniz rule] \label{LeibnizDerivativeRule}
Let $U,V_1,V_2$ be convergence vector spaces, $f: U\to V_1$, $g: U\to V_2$ functions and $u,a\in U$. Let $B: V_1\oplus V_2 \to W$ is a continuous bilinear function. Suppose $\partial_uf|_a$ and $\partial_ug|_a$ exist. Then $\partial_u B\circ (f,g)|_a$ exists and is given by
\[ \partial_u B\circ (f,g)|_a = B\big(\partial_uf|_a, g(a)\big) + B\big(f(a), \partial_ug|_a\big). \]
\end{proposition}
Note $(f,g)$ is a function $U\to V_1\oplus V_2$. The proof makes essential use of the bilinearity, rather than linearity, of $B$.
\begin{proof}
We calculate
\begin{align*}
\partial_u B\circ (f,g)|_a &= \lim_{t\to 0} \frac{B\big(f(a+tu), g(a+tu)\big) - B\big(f(a), g(a)\big)}{t} \\
&= \lim_{t\to 0} \frac{B\big(f(a+tu), g(a+tu)\big) - B\big(f(a), g(a)\big) - B\big(f(a+tu), g(a)\big) + B\big(f(a+tu), g(a)\big)}{t} \\
&= \lim_{t\to 0} B\Big(f(a+tu), \frac{g(a+tu)-g(a)}{t}\Big) + B\Big(\frac{f(a+tu) - f(a)}{t}, g(a)\Big) \\
&= B\Big(\lim_{t\to 0}f(a+tu), \lim_{t\to 0}\frac{g(a+tu)-g(a)}{t}\Big) + B\Big(\lim_{t\to 0}\frac{f(a+tu) - f(a)}{t}, g(a)\Big) \\
&= B\Big(f(a), \partial_ug|_a\Big) + B\Big(\partial_uf|_a, g(a)\Big),
\end{align*}
where we have used that the convergence on $V_1\oplus V_2$ is the product convergence, see \ref{finiteDirectSumIsProduct}. We have also used the fact that $t\mapsto f(a+tu)$ is continuous at $0$, which is given by \ref{partialDerivativeImpliesContinuityAlongLine}.
\end{proof}


\subsection{Mean value theorem}
\subsubsection{Scalar-valued functions}
I.e.\ functions with scalar codomain.


TODO: requires Rolle's theorem, so move that up.

\begin{theorem}[Mean value theorem] \label{meanValueTheorem}
Let $a<b\in \R$ and $f: \interval{a,b}\to \R$ be a continuous real function that is differentiable on $\interval[o]{a,b}$. Then there exists $c\in \interval[o]{a,b}$ such that
\[f'(c) = \frac{f(b)-f(a)}{b-a}. \]
\end{theorem}
\begin{proof}
Define $g(x) = f(x) - x\frac{f(b)-f(a)}{b-a}$. Then
\begin{multline*}
g(a) = f(a) - a\frac{f(b)-f(a)}{b-a} = \frac{bf(a) - af(a)}{b-a} - a\frac{f(b)-f(a)}{b-a} = \\ \frac{bf(a)-af(b)}{b-a} = \frac{bf(a)-bf(b) + bf(b)-af(b)}{b-a} = \\ \frac{bf(b) - af(b)}{b-a} - \frac{bf(b) - bf(a)}{b-a} = f(b) - b \frac{f(b)-f(a)}{b-a} = g(b),
\end{multline*}
so we can apply Rolle's theorem \ref{RollesTheorem} to $g$. Thus there exists $c\in \interval[o]{a,b}$ such that $0 = g'(c) = f'(c) - \frac{f(b)-f(a)}{b-a}$. Thus $f'(c) = \frac{f(b)-f(a)}{b-a}$.
\end{proof}
\begin{corollary}
Let $a,b, M\in \R$ and $f:\interval{a,b}\to \R$ a function differentiable on $\interval[o]{a,b}$. If $|f'(x)| \leq M$ for all $x\in \interval[o]{a,b}$, then $f$ is Lipschitz continuous with Lipschitz constant $M$.
\end{corollary}
\begin{corollary} \label{partialDerivativeMeanValueTheorem}
Let $V$ be a vector space, $a,u\in V$, $t\in\R^+$ and $f: V\to \R$ a function whose partial derivative $\partial_u f$ exists on $a+\interval{0,t}\cdot u$. Then there exists $h\in \interval[o]{0,t}$ such that
\[ \frac{f(a+tu) - f(a)}{t} = \partial_u f(a+hu). \]
\end{corollary}
\begin{proof}
Consider the function $g: \interval{0,t} \to \R: t'\mapsto f(a+t'u)$. It is continuous and differentiable by \ref{partialDerivativeImpliesContinuityAlongLine} and \ref{directionalDerivativeAsOD}.

Thus we can apply the mean value theorem \ref{meanValueTheorem} to $g$: there exists $h\in \interval[o]{0,t}$ such that
\[ \frac{f(a+tu) - f(a)}{t} = \left.\dod{f(a+tu)}{t}\right|_{t=h} = \partial_u f(a+hu). \]
The last equation is again an application of \ref{directionalDerivativeAsOD}.
\end{proof}

TODO: \url{https://ejde.math.txstate.edu/Volumes/2012/34/cakmak.pdf}

\begin{lemma} \label{boundDerivativeAndFixedPointGivesRange}
Let $a,b\in \R$ and $f:\interval{a,b}\to \R$ be a continuous function that is differentiable on $\interval[o]{a,b}$. If $f$ has a fixed point and $0\leq f' \leq 1$ everywhere, then $\im(f)\subseteq \interval{a,b}$. 
\end{lemma}
\begin{proof}
Let $x^*\in \interval{a,b}$ be a fixed point, then $f(x^*) = x^* \in \interval{a,b}$. Take arbitrary $x\in \interval{a,b}\setminus\{x^*\}$. Then
\[ \frac{f(x) - x^*}{x - x^*} = \frac{f(x) - f(x^*)}{x - x^*} = f'(c) \]
for some $c\in \interval[o]{a,b}$ by the mean value theorem \ref{meanValueTheorem}. Thus $0\leq \frac{f(x) - x^*}{x - x^*} \leq 1$.

First assume $x^* \leq x$, so $x - x^* \geq 0$. Then $0 \leq f(x) - x^* \leq x - x^*$, so $x^* \leq f(x) \leq x$. Since $x^*, x\in \interval{a,b}$, we have $f(x)\in \interval{a,b}$.

Now assume $x\leq x^*$, so $x - x^* \leq 0$. Then $x - x^* \leq f(x) - x^* \leq 0$, so $x \leq f(x) \leq x^*$. Since $x^*, x\in \interval{a,b}$, we have $f(x)\in \interval{a,b}$.
\end{proof}

\subsubsection{Vector-valued functions}

\begin{proposition}[Mean value theorem for vector-valued functions] \label{meanValueTheoremVectorFunctions}
Let $X$ be a normed space. Let $a<b\in \R$ and $f: \interval{a,b}\to X$ be a continuous real function that is differentiable on $\interval[o]{a,b}$. Then there exists $c\in \interval[o]{a,b}$ such that
\[ \frac{\norm{f(b)-f(a)}}{b-a} \leq \norm{f'(c)}. \]
\end{proposition}
TODO: straightforward generalisation to seminorms of the form $|h|$, where $h$ is a linear functions. Is this worthwhile?
\begin{proof}
The claim is trivially true if $f(b) = f(a)$. We now assume $f(b) \neq f(a)$.

Let $\omega: X\to \F$ be the functional $\omega_{f(b)-f(a)}$ defined in \ref{existenceBoundedFunctionalOfSameNorm} which sends $f(b)-f(a)$ to $\norm{f(b)-f(a)}$ and has norm $1$. 
Set $g \defeq \omega \circ f: \interval{a,b}\to \F$.
Now we calculate, applying the scalar mean value theorem \ref{meanValueTheorem} to $g$ (from which we obtain $c$) and using \ref{derivativeOfConstructions},
\begin{align*}
\frac{\norm{f(b)-f(a)}}{b-a} &= \Big|\frac{\norm{f(b)-f(a)}}{b-a}\Big| \\
&= \Big|\frac{\omega\big(f(b)-f(a)\big)}{b-a}\Big| \\
&= \Big|\frac{(\omega\circ f)(b)-(\omega \circ f)(a)}{b-a}\Big| \\
&= \Big|\frac{g(b)- g(a)}{b-a}\Big| \\
&\leq \big|g'(c)\big| \\
&= \big|(\omega \circ f)'(c)\big| \\
&= \big|\omega\big(f'(c)\big)\big| \\
&\leq \norm{\omega}\,\norm{f'(c)} \\
&= \norm{f'(c)}.
\end{align*}
\end{proof}
TODO: we have used the mean value theorem for scalared-valued functions $g: \interval{a,b}\to \F$, not the real mean valued theorem. We need to prove this version first, mayby like in the following proof, although it's very redundant. Or some generalised version?

TODO: in the special case of Hilbert spaces we have the following:
\begin{proof}
The claim is trivially true if $f(b) = f(a)$. We now assume $f(b) \neq f(a)$.

Consider the function $g(t) \defeq \inner{f(b)-f(a), f(t)}$. Since $g$ is real-valued, there exists $c\in \interval[o]{a,b}$ such that $g'(c) = \frac{g(b)-g(a)}{b-a}$. Then $g(b)-g(a) = \norm{f(b)-f(a)}^2$ and $g'(c) = \inner{f(b)-f(a), f'(t)}$ (TODO ref). By the Cauchy-Schwarz inequality \ref{CauchySchwarz}, we have
\[ \frac{\norm{f(b)-f(a)}^2}{b-a} = \left|\frac{\norm{f(b)-f(a)}^2}{b-a}\right| = \left|\frac{g(b)-g(a)}{b-a}\right| = |g'(c)| = |\inner{f(b)-f(a), f'(t)}| \leq \norm{f(b)-f(a)} \norm{f'(t)}. \]
Dividing by $\norm{f(b)-f(a)}$ yields the result.
\end{proof}
\begin{corollary} \label{meanValueTheoremGateauxDerivative}
Let $V$ be a vector space and $W$ a normed vector space, $a,u\in V$, $t\in\R^+$ and $f: U\subseteq V\to W$ a function that is continuous on $a+\interval{0,1}\cdot u$ and whose partial derivative $\partial_u f$ exists on $a+\interval[o]{0,t}\cdot u$. Then there exists $h\in \interval[o]{0,t}$ such that
\[ \norm{f(a+tu) - f(a)} = t\norm{\partial_u f(a+hu)}. \]
\end{corollary}
\begin{proof}
Consider the function $g: \interval{0,t} \to W: t'\mapsto f(a+t'u)$. It is continuous and differentiable by \ref{partialDerivativeImpliesContinuityAlongLine} and \ref{directionalDerivativeAsOD}.

Thus we can apply the mean value theorem \ref{meanValueTheorem} to $g$: there exists $h\in \interval[o]{0,t}$ such that
\[ \frac{\norm{f(a+tu) - f(a)}}{t} \leq \norm{\left.\dod{f(a+tu)}{t}\right|_{t=h}} = \norm{\partial_u f(a+hu)}. \]
The last equation is again an application of \ref{directionalDerivativeAsOD}.
\end{proof}


\subsection{The Jacobian}
\begin{definition}
Let $f:U\subseteq\R^m\to\R^n$ be a function. Then $A_{\diff{f}}$ is a matrix with
\[ [A_{\diff{f}}]_{ij} = [\diff{f}\vec{e}_j]_i = \left[\pd{f}{x^j}\right]_i. \]
This matrix is called the \udef{Jacobian} $J_f$.
\end{definition}

\subsection{Interchange of limits}

\begin{proposition} \label{derivativeOfUniformLimit}
Let $X$ be a Banach space, $a,b\in \R$ and $\seq{f_n: \interval{a,b} \to X}_{n\in\N}$ a sequence of differentiable functions. Suppose
\begin{itemize}
\item $\seq{f_n'}$ is a uniform Cauchy sequence;
\item there exists $c\in \interval{a,b}$ such that $\seq{f_n(c)}$ is a Cauchy sequence;
\end{itemize}
then $\seq{f_n}$ converges uniformly to a differentiable function $f: \interval{a,b}\to X$ and $f' = g$.
\end{proposition}
\begin{proof}
Define the functions
\[ g_{x,n}: \R\to X: h\mapsto \frac{f_n(x+h) - f_n(x)}{h}. \]
We first show that $\seq{g_{x,n}}_{n\in \N}$ is a uniform Cauchy sequence. Take arbitrary $\epsilon > 0$. Since $\seq{f_n'}$ is uniform Cauchy, there exists $N\in \N$ such that for all $m,n\geq N$ and all $t\in \interval{a,b}$, we have $\norm{f_n'(t) - f_m'(t)} \leq \epsilon$, by \ref{metricCauchySequence}. By \ref{derivativeOfConstructions}, the function $f_n - f_m$ is differentiable with $(f_n - f_m)' = f_n' - f_m'$, so we can use the mean value theorem \ref{meanValueTheoremVectorFunctions} to calculate, for arbitrary $t\in \interval{a,b}$,
\begin{align*}
\norm{g_{x,n}(h) - g_{x,m}(h)} &= \frac{\norm{f_n(x+h) - f_n(x) - \big(f_m(x+h) - f_m(x)\big)}}{|h|} \\
&= \frac{\norm{\big(f_n - f_m\big)(x) - \big(f_n - f_m\big)(x+h)}}{|h|} \\
&\leq \norm{(f_n - f_m)'(t)} \\
&= \norm{f_n'(t) - f_m'(t)} \leq \epsilon.
\end{align*}
This implies that $\seq{g_{x,n}}_{n\in \N}$ is a uniform Cauchy sequence by \ref{metricCauchySequence}.

Next we show that $\seq{f_n}$ is a uniform Cauchy sequence. Take arbitrary $\epsilon > 0$. Now we can find $N_1\in \N$ such that $\norm{g_{x,n}(t) - g_{x,m}(t)} \leq \frac{\epsilon}{2(b-a)}$ for all $m,n\geq N_1$ and $t\in \interval{a,b}$, by \ref{metricCauchySequence}. We can also find $N_2 \in \N$ such that $\norm{f_n(c) - f_m(c)} \leq \frac{\epsilon}{2}$, also by \ref{metricCauchySequence}. Set $N\defeq \max\{N_1, N_2\}$.
Take arbitrary $m,n\geq N$ and $x\in \interval{a,b}$. We calculate
\begin{align*}
\norm{f_n(x) - f_m(x)} &\leq \norm{f_n(x) - f_n(c) - f_m(x) + f_m(c)} + \norm{f_n(c) - f_m(c)} \\
&\leq \norm{f_n(x) - f_n\big(x + (c-x)\big) - f_m(x) + f_m\big(x + (c-x)\big)} + \frac{\epsilon}{2} \\
&\leq \norm{(c-x)g_{x,n}(c-x) - (c-x)g_{x,m}(c-x)} + \frac{\epsilon}{2} \\
&= |c-x|\norm{g_{x,n}(c-x) - g_{x,m}(c-x)} + \frac{\epsilon}{2} \\
&\leq |c-x|\frac{\epsilon}{2(b-a)} + \frac{\epsilon}{2} \leq \frac{\epsilon}{2}  + \frac{\epsilon}{2}  = \epsilon.
\end{align*}
We conclude that $\seq{f_n}$ is a uniform Cauchy sequence by \ref{metricCauchySequence}.

By \ref{supmetricComplete} and the fact that $X$ is complete, there exists a function $f: \interval{a,b}\to X$ such that $f_n \to f$.

By continuity of addition and scalar multiplication (and the fact that $f_n\to f$), we have $g_{x,n}(h) \to \frac{f(x+h) - f(x)}{h}$ as $n\to \infty$. Set $g_{x}: \R\to X: h\mapsto \frac{f(x+h) - f(x)}{h}$, so $g_{x,n} \to g_x$ pointwise. By \ref{uniformCauchyPointwiseConvergentUniformConvergent}, $g_{x,n} \to g_x$ uniformly.

By definition of the derivative, we have $g_{x,n}(h) \to f_n'(x)$ as $h\to 0$. If $f$ is differentiable, then its derivative at $x$ is given by $\lim_{h\to 0}g_x(h)$.

Thus, using \ref{swapLimitsUniformConvergenceMetricSpaces}, we calculate,
\begin{align*}
\lim_{n\to \infty} f_n'(x) &= \lim_{n\to \infty} \lim_{h\to 0}g_{x,n}(h) \\
&= \lim_{h\to 0} \lim_{n\to \infty} g_{x,n}(h) \\
&= \lim_{h\to 0} g_{x}(h) \\
&= f'(x).
\end{align*}
\end{proof}
TODO: if we add the assumption that the $f'_n$ are continuous, then we have the following proof (TODO: extend to vector case)
\begin{proof}
By \ref{weakSecondTheoremCalculus}, we have
\[ f_n(x) = f_n(c) + \int_c^x f'_n(t)\diff{t}. \]
Now set
\begin{align*}
f(x) \defeq& \lim_{n\to \infty} \Big(f_n(c) + \int_c^x f'_n(t)\diff{t}\Big) \\
=& f(c) + \lim_{n\to \infty}\int_c^x f'_n(t)\diff{t} \\
=& f(c) + \int_c^x g(t)\diff{t},
\end{align*}
where the existence of the limit and its value follow from \ref{interchangeLimitIntegralUniformLimit}.

Since $g$ is continuous (TODO ref), we have $f' = g$ by \ref{firstFundamentalTheoremCalculus}.

Finally, we just need to show that the convergence of $f_n\to f$ is uniform. We use \ref{metricUniformConvergence}. Take arbitrary $\epsilon >0$. Let $N\in \N$ be such that $|f(c) - f_n(c)| \leq \epsilon / 2$ and $|g(x) - f'_n(x)| \leq \frac{\epsilon}{2(b-a)}$ for all $x\in \interval{a,b}$ and $n\geq N$. Then take arbitrary $x\in \interval{a,b}$ and calculate
\begin{align*}
|f(x) - f_n(x)| &= \bigg|\Big(f(c)+\int_c^x g(t) \diff{t}\Big) - \Big(f_n(c)+\int_c^x f_n'(t) \diff{t}\Big)\bigg| \\
&\leq \big|f(c) - f_n(c)\big| + \Big|\int_c^x \big(g(t)- f_n'(t)\big)\diff{t}\Big| \\
&\leq \frac{\epsilon}{2} + \frac{\epsilon}{2(b-a)}\int_a^b\diff{t} \\
&= \frac{\epsilon}{2} + \frac{\epsilon}{2(b-a)}(b-a) = \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.
\end{align*}
By \ref{metricUniformConvergence} this concludes the proof. 
\end{proof}

\begin{example}
Let $f_n: \interval{0,2\pi} \to \R: x\mapsto \frac{\sin(nx)}{n}$. Then $\seq{f_n}$ converges uniformly to $0$, but $f_n'(x) = \cos(nx)$, which does not converge, even pointwise.
\end{example}


\begin{theorem}[Schwarz's theorem] \label{SchwarzTheorem}
Let $V$ be a real convergence vector space, $f:U\subseteq V\to \R$ a real-valued function defined on an open convex subset $U$ and $u,v\in V$. Suppose $\partial_u\partial_v f$ and $\partial_v\partial_u f$ exist with domain $U$ and one of them is continuous, then $\partial_u\partial_v f = \partial_v\partial_u f$.
\end{theorem}
We also write $\md{f}{2}{u}{}{v}{} = \md{f}{2}{v}{}{u}{}$. In particular, if one is continuous, then the other is also continuous.

This theorem is also known as Clairaut's theorem.
\begin{proof}
WLOG we may assume $\partial_v\partial_u f$ continuous. Take arbitrary $a\in U$. We now calculate
\begin{align*}
\partial_u\partial_v f(a) &= \lim_{t\to 0}\frac{1}{t}\big(\partial_v f(a+tu) - \partial_v f(a)\big) \\
&= \lim_{t\to 0}\frac{1}{t}\Big(\lim_{t'\to 0}\frac{1}{t'}\big(f(a+tu+t'v) - f(a+tu)\big) - \lim_{t'\to 0}\frac{1}{t'}\big(f(a+t'v) - f(a)\big)\Big) \\
&= \lim_{t\to 0}\lim_{t'\to 0}\frac{1}{tt'}\Big(\big(f(a+tu+t'v) - f(a+tu)\big) - \big(f(a+t'v) - f(a)\big)\Big).
\end{align*}
Fix $\epsilon > 0$ as in \ref{rectangleSubsetLemma} and take arbitrary $t,t'\in \interval{0, \epsilon}$. By \ref{directionalDerivativeConstructions}, $f(a+t'v) - f(a)$ is differentiable in the direction $u$. Then there exists, by \ref{partialDerivativeMeanValueTheorem}, $h(t) \in \interval{0,t}$ and $h'(t')\in \interval{0,t'}$ such that
\begin{align*}
\frac{1}{tt'}\Big(\big(f(a+tu+t'v) - f(a+tu)\big) - &\big(f(a+t'v) - f(a)\big)\Big) \\
&= \frac{1}{t'}\partial_u\Big(f\big(a+h(t)u+t'v\big) - f\big(a+h(t)u\big)\Big) \\
&= \frac{1}{t'}\Big(\partial_uf\big(a+h(t)u+t'v\big) - \partial_uf\big(a+h(t)u\big)\Big) \\
&= \partial_v\partial_uf\big(a+h(t)u+h'(t)v\big).
\end{align*}
As $t\to 0$ and $t'\to 0$, $h(t)$ and $h'(t')$ tend to zero by the squeeze theorem \ref{squeezeTheoremNets}. By continuity of $\partial_v\partial_uf$, we have
\begin{align*}
\partial_u\partial_v f(a) &= \lim_{t\to 0}\lim_{t'\to 0}\frac{1}{tt'}\Big(\big(f(a+tu+t'v) - f(a+tu)\big) - \big(f(a+t'v) - f(a)\big)\Big) \\
&= \lim_{t\to 0}\lim_{t'\to 0}\partial_v\partial_uf\big(a+h(t)u+h'(t)v\big) \\
&= \partial_v\partial_u f(a).
\end{align*}
\end{proof}
\begin{corollary}
Let $V$ be a real locally convex TVS, $f:U\subseteq V\to \R$ a real-valued function defined on an open subset $U$ and $u,v\in V$. Suppose $\partial_u\partial_v f$ and $\partial_v\partial_u f$ exist with domain $U$ and one of them is continuous, then $\partial_u\partial_v f = \partial_v\partial_u f$.
\end{corollary}
\begin{proof}
Take $a\in U$. Since $U$ is open, $a$ has a vicinity $U_a$ that is a subset of $U$. By local convexity and \ref{locallyConvexTVSLocallyConvexOpen}, we can find an open convex neighbourhood $U'_a$ of $a$ that is a subset of $U_a$. By restricting $f$ to $U'_a$, we can apply the theorem.
\end{proof}


\begin{proposition}
Let $V,W$ be normed spaces and $f: V\to W$ be a function. Suppose
\begin{itemize}
\item there exists an open set $U\subseteq V$ such that $\diff{_af}$ exists as a Gateaux derivative (i.e.\ is a bounded linear function) for all $a\in U$;
\item the function $U \to \Bounded(V,W): a\mapsto \diff{_af}$ is continuous;
\end{itemize}
Then $\diff{_af}$ is a Frchet derivative for all $a\in U$.
\end{proposition}
In particular, $f$ is continuous on $U$.

This proof uses the mean value theorem. For a proof using the fundamental theorem of calculus, see 
\url{https://math.stackexchange.com/questions/3745037/a-frech%C3%A9t-differentiable-function-to-has-a-continuous-derivative-in}
\begin{proof}
Since $U$ is open, we can find $\delta >0$ such that $\cball(a, \delta) \subseteq U$. We define the function
\[ g: \cball(0, \delta) \to W: x\mapsto f(a+x) - \diff{_af}(x), \]
which has Gateaux derivative $\diff{_bg} = \diff{_{a+b}f} - \diff{_af}$, by \ref{derivativeOfConstructions}. Now we calculate, for all $x\in \cball(a,\delta)$,
\begin{align*}
\frac{\norm{f(a+x) - f(a) - \diff{_af}(x)}}{\norm{x}} &= \frac{\norm{g(x) - g(0)}}{\norm{x}} \\
&\leq \frac{\norm{\diff{_{hx}g}(x)}}{\norm{x}} \\
&= \frac{\norm{\diff{_{a+hx}f}(x) - \diff{_af}(x)}}{\norm{x}} \\
&\leq \norm{\diff{_{a+hx}f} - \diff{_af}},
\end{align*}
where the $h \in \interval[o]{0,1}$ depends on $x$ and is given by \ref{meanValueTheoremGateauxDerivative}. The last inequality is by definition of the operator norm.

Since $h(x)$ is bounded, we have that $hx \to 0$ as $x\to 0$. Thus $\norm{\diff{_{a+hx}f} - \diff{_af}}$ also converges to $0$ and $\frac{\norm{f(a+x) - f(a) - \diff{_af}(x)}}{\norm{x}}$ converges to $0$. We conclude that the derivative is a Frchet derivative by \ref{FrechetDerivativeAsymptotics}.
\end{proof}

\subsection{Derivative of determinant}
\begin{proposition} \label{DerivativeDeterminantAtIdentity}
Let $\F$ be a field and $A\in \F^{n\times n}$. Then $\diff{_{\mathbb{1}_n}\det} = \Tr$, where the derivative is a Frchet derivative.
\end{proposition}
In other words, the Frchet derivative of the determinant function $\det: \F^{n\times n}\to \F$ at $\mathbb{1}_{n}$ is the trace function $\Tr:\F^{n\times n}\to \F$.
\begin{proof}
We use \ref{FrechetDerivativeAsymptotics} to show that it is the Frchet derivative. To that end, we expand using \ref{expansionDeterminantAroundIdentity}:
\begin{align*}
\det(\mathbb{1}_n + X) &= \det(\mathbb{1}_n + \frac{\norm{X}}{\norm{X}}X) \\
&= 1 + \norm{X}\Tr\Big(\frac{X}{\norm{X}}\Big) + {\norm{X}}^2 P({\norm{X}}) \\
&= \det(\mathbb{1}_n) + \Tr(X) + {\norm{X}}^2 P({\norm{X}}),
\end{align*}
so we just need to show that ${\norm{X}}^2 P({\norm{X}})$ is $o(X)$ as $X\to 0$. For $X$ such that $\norm{X} \leq 1$, we use the bound in \ref{expansionDeterminantAroundIdentity} (and the fact that $\norm{X/\norm{X}} = 1$) to calculate
\begin{align*}
0\leq \frac{{\norm{X}}^2 P({\norm{X}})}{\norm{X}} &= {\norm{X}} P({\norm{X}}) \\
&\leq {\norm{X}} P(1) \\
&\leq \norm{X}\big(1 + n + n!2^n\big).
\end{align*}
The last term converges to $0$ as $X\to 0$, so ${\norm{X}}^2 P({\norm{X}})$ is $o(X)$ as $X\to 0$ by the squeeze theorem \ref{squeezeTheoremNets}.
\end{proof}

\begin{proposition}[Jacobi's formula] \label{JacobisFormula}
Let $U\subseteq \R$ be an open set and $A: U\to \F^{n\times n}$ a differentiable function. Then
\[ \od{}{t}\Big|_{t=t_0}\det\big(A(t)\big) = \Tr\Big(\adj\big(A(t_0)\big)A'(t_0)\Big). \]
If $A(t_0)$ is invertible, then
\[ \od{}{t}\Big|_{t=t_0}\det\big(A(t)\big) = \det\big(A(t_0)\big)\Tr\Big(A^{-1}(t_0)A'(t_0)\Big). \]
\end{proposition}
\begin{proof}
We first prove the second assertion, assuming $A(t_0)$ is invertible. Using \ref{DerivativeDeterminantAtIdentity} and the chain rule \ref{FrechetChainRule}, we calculate
\begin{align*}
\dod{}{t}\Big|_{t=t_0}\det\big(A(t)\big) &= \dod{}{t}\Big|_{t=t_0}\det\big(A(t_0)A(t_0)^{-1}A(t)\big) \\
&= \det\big(A(t_0)\big)\dod{}{t}\Big|_{t=t_0}\det\big(A(t_0)^{-1}A(t)\big) \\
&= \det\big(A(t_0)\big)\big(\diff{_{\mathbb{1}_n}\det}\big)\Big(A(t_0)^{-1}\dod{}{t}\Big|_{t=t_0}A'(t)\Big) \\
&= \det\big(A(t_0)\big)\Tr\Big(A(t_0)^{-1}A'(t_0)\Big).
\end{align*}
For the first assertion, first assume $A(t_0)$ is invertible. Then, by \ref{inverseAdjunctDeterminant},
\begin{align*}
\od{}{t}\Big|_{t=t_0}\det\big(A(t)\big) &= \det\big(A(t_0)\big)\Tr\big(A^{-1}(t_0)A'(t_0)\big) \\
&= \Tr\Big(\det\big(A(t_0)\big)A^{-1}(t_0)A'(t_0)\Big) \\
&= \Tr\Big(\adj\big(A(t_0)\big)A'(t_0)\Big).
\end{align*}
Now suppose $A(t_0)$ is not invertible. Since the invertible elements are dense in the space of all matrices, by \ref{finiteDimensionsInvertiblesDense}, we can find a sequence $\seq{B_n}$ of invertible matrices that converges to $A(t_0)$ by \ref{existenceConvergentSequenceDenseSubset}. TODO: finish argument!
\end{proof}
\begin{corollary}
Let $B\in \F^{n\times n}$. Then $\det(e^{B}) = e^{\Tr(B)}$.
\end{corollary}
\begin{proof}
Setting $A(t) = e^{t B}$ in the Jacobi formula gives $\od{}{t}\det(e^{tB}) = \det(e^{tB})\Tr(e^{-tB}Be^{tB}) = \det(e^{tB})\Tr(B)$. Solving this differential equation yields $\det(e^{tB}) = e^{t\Tr(B)}$.
\end{proof}

\section{Differentiation in a convergence algebra}

\begin{proposition}[Leibniz rule] \label{LeibnizRuleDerivativeAlgebra}
Let $A$ be a normed algebra and $a,b\in (\R \to A)$ elements that have derivatives. Then
\[ (ab)' = a'b + ab'. \]
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
0 &= 0\cdot a'(t)b'(t) = \lim_{\epsilon \to 0} \epsilon a'(t)b'(t) \\
&= \lim_{\epsilon \to 0} \epsilon \frac{a(t+\epsilon) - a(t)}{\epsilon}\frac{b(t+\epsilon) - b(t)}{\epsilon} \\
&= \lim_{\epsilon \to 0}\frac{a(t+\epsilon)b(t+\epsilon) - a(t+\epsilon)b(t) - a(t)b(t+\epsilon) + a(t)b(t)}{\epsilon} + \frac{a(t)b(t)}{\epsilon} - \frac{a(t)b(t)}{\epsilon} \\
&= \lim_{\epsilon \to 0} \frac{a(t+\epsilon)b(t+\epsilon) - a(t)b(t)}{\epsilon} - \frac{a(t+\epsilon) - a(t)}{\epsilon}b(t) - a(t)\frac{b(t+\epsilon) - b(t)}{\epsilon} \\
&= (ab)' - a'b - ab'.
\end{align*}
\end{proof}

\begin{proposition} \label{derivativeIdempotent}
Let $A$ be an algebra and $p\in A$ such that $p^2 = p$ and $p'$ exists. Then
\begin{enumerate}
\item $p' = pp'+ p'p$;
\item $pp'p = 0$;
\item $(p')^2 = p'pp' + p(p')^2p$;
\item $p^{\prime\prime} = 2(p')^2 + pp^{\prime\prime} + p^{\prime\prime}p$;
\item $pp^{\prime\prime}p = -2p(p')^2p$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We calculate $p' = (p^2)' = pp'+ p'p$.

(2) Multiply (1) by $p$ on the left and right.

(3) We calculate $(p')^2 = (pp'+ p'p)(pp'+ p'p) = pp'pp' + pp'p'p + p'ppp' + p'pp'p = 0p + pp'p'p + p'pp' + p'0$.

(4) Take derivative of (1).

(5) Multiply (4) by $p$ on the left and right.
\end{proof}
\begin{corollary}
Let $\Tr$ be a trace functional on $A$ and $p\in A$ as before. Then $\Tr(p') = 0$.
\end{corollary}
\begin{proof}
$\Tr(p') = \Tr(pp'+ p'p) = \Tr(p^2p')+ \Tr(p'p^2) = \Tr(pp'p) + \Tr(pp'p) = 2\Tr(0) = 0$.
\end{proof}
\begin{proposition}
Let $p_0,p_1$ be differentiable idempotents such that $p_0p_1 = 0 = p_1p_0$. Then $p_0'p_1 = -p_0p_1'$.
\end{proposition}
\begin{proof}
We have $0 = p_0p_1$, so $0 = 0' = p_0'p_1 + p_0p_1'$.
\end{proof}
\begin{corollary}
Let $p_0,p_1$ be differentiable idempotents such that $p_0p_1 = 0 = p_1p_0$. Then
\begin{enumerate}
\item $p_1p_0'p_0 = -p_1p_1'p_0$;
\item $p_1p_0'p_1 = 0$;
\item $p_1(p_0')^2p_1 = p_1p_1'p_0p_1'p_1$;
\item $p_0(p_0')^2p_1 = 0$;
\item $p_1p^{\prime\prime}_0p_1 = 2p_1(p_0')^2p_1$.
\end{enumerate}
If in addition $p_2$ is a differentiable idempotent such that $p_0p_2 = 0 p_2p_0$ and $p_1p_2 = 0 p_2p_1$, then
\begin{enumerate} \setcounter{enumi}{5}
\item $p_1p_0'p_2 = 0$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) We have
\[ p_1p_0'p_0 = p_1(p_1p_0')p_0 = -p_1(p_1'p_0)p_0 = -p_1p_1'p_0. \]

(2) We have $p_1p_0'p_1 = -(p_1'p_0)p_1 = -p_1'(p_0p_1) = 0$.

(3) We have $p_1(p_0')^2p_1 = (p_1p_1p_0')(p_0'p_1p_1) = (p_1p_1'p_0)(p_0p_1'p_1)$.

(4) We have $p_0(p_0')^2p_1 = (p_0p_0')(p_0'p_1) = -(p_0p_0')(p_0p_1') = -(p_0p_0'p_0)p_1' = 0$.

(5) We have, using \ref{derivativeIdempotent}, $p_1p^{\prime\prime}_0p_1 = p_1(2(p'_0)^2 + p_0p_0^{\prime\prime} + p_0^{\prime\prime}p_0)p_1 = 2p_1(p_0')^2p_1$.

(6) We have $(p_1p_0')p_2 = - (p_1'p_0)p_2 = - p_1'(p_0p_2) = 0$.
\end{proof}
\begin{corollary} \label{derivativeIdempotentOffDiagonal}
Let $p_0,p_1$ be differentiable idempotents such that $p_0p_1 = 0 = p_1p_0$ and $p_0 + p_1 = 1$. Then
\begin{enumerate}
\item $p_0' = p_0p_0'p_1 + p_1p_0'p_0$;
\item $p_0p_0' = p_0'p_1$ and $p_0'p_0 = p_1p_0'$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) We have
\[ p_0' = (p_0 + p_1)p_0'(p_0 + p_1) = \cancel{p_0p_0'p_0} + p_0p_0'p_1 + p_1p_0'p_0 + \cancel{p_1p_0'p_1}. \]

(2) We have, using point (1),
\[ p_0p_0' = p_0(p_0p_0'p_1) + p_0(p_1p_0'p_0) = p_0p_0'p_1 = (p_0p_0'p_1)p_1 + (p_1p_0'p_0)p_1 = p_0'p_1. \]
The other equation is similar.
\end{proof}


\begin{proposition}
Let $A$ be a convergence algebra and $a: U\subseteq\C \to A$.

If $a$ is differentiable and $a(t)^{-1}$ exists for all $t\in \R$, then
\[ \od{a^{-1}}{t} = -a^{-1}a'a^{-1}. \]
\end{proposition}
\begin{proof}
We have
\begin{align*}
\od{}{t}a^{-1} &= \lim_{h\to 0}\frac{a(t+h)^{-1} - a(t)^{-1}}{h} \\
&= \lim_{h\to 0} -\frac{a(t)^{-1}a(t+h)a(t+h)^{-1} - a(t)^{-1}a(t)a(t+h)^{-1}}{h} \\
&= \lim_{h\to 0} -\frac{a(t)^{-1}\Big(a(t+h) - a(t)\Big)a(t+h)^{-1}}{h} \\
&= -a(t)^{-1}\left(\lim_{h\to 0}\frac{\big(a(t+h) - a(t)\big)a(t+h)^{-1}}{h}\right)\Big(\lim_{h\to 0}a(t+h)^{-1}\Big) \\
&= -a(t)^{-1}a'a^{-1}.
\end{align*}
\end{proof}


\section{Analytic functions}
TODO: multiindex notation
\begin{definition}
Let $M$ be a convergence module over a ring $R$ and $n\in\N$. A function $f: R^n\to M$ is called \udef{analytic} if
\[ f(x) = \sum_{I\in\N^n}c_I(x-x_0)^I \]
where $c_I\in M^n$ and $x_0\in R^n$.
\end{definition}

\subsection{Taylor expansion}
Radius of convergence

\subsection{Properties of analytic functions}
\begin{proposition}
Let $f$ be an analytic function. Then
\begin{enumerate}
\item $f$ is continuous;
\item $f$ is differentiable.
\end{enumerate}
\end{proposition}

\section{Classification of spaces}
\begin{definition}
Let $X,Y$ be subsets of normed vector spaces and $X$ be open. We call a function $f: X\to Y$
\begin{itemize}
\item \udef{smooth} at $x_0\in V$ if all derivatives of $f$ at $x_0$ exist;
\item \udef{analytic} at $x_0\in V$ if the Taylor series of $f$ at $x_0$ exists and has non-zero radius of convergence.
\end{itemize}
\end{definition}
\begin{lemma}
Let $f: X\to Y$ be a smooth function. Then all derivatives are continuous.
\end{lemma}

\begin{definition}
Let $X,Y$ be subsets of normed vector spaces and $X$ be open.
\begin{itemize}
\item $\cont^r(X,Y)$ is the space of functions in $(X \to Y)$ whose first $r$ derivatives exist and are continuous;
\item $\cont^\infty(X,Y)$ is the space of functions in $(X \to Y)$ that are smooth at all points in $X$;
\item $\cont^\omega(X,Y)$ is the space of functions in $(X \to Y)$ that are analytic at all points in $X$.
\end{itemize}
If $Y = \C$, we write $\cont^r(X), \cont^\infty(X)$ and $\cont^\omega(X)$. We can also use subscripts $_0$ and $_c$ to denote the extra conditions of vanishing at infinity and having compact support.
\end{definition}







\chapter{Banach algebras}
In this part we set $\F \in \{\R, \C\}$. Usually operator algebras are assumed to be complex. We will attempt to give results for real algebras where possible.
\begin{definition}
A \udef{normed algebra} is an associative algebra $A$ over $\F$ with norm $\norm{\cdot}$ such that $(\F, A,+, \norm{\cdot})$ is a normed space and we have \udef{submultiplicativity}, i.e.
\[ \forall x,y\in A: \quad \norm{xy}\leq\norm{x}\norm{y}. \]
We say $A$ is \udef{unital} if there exists a unit element $\vec{1}\in A$ such that
\[ \forall x\in A: \vec{1}\cdot x = x = x\cdot \vec{1} \qquad \text{and} \qquad \norm{\vec{1}} = 1. \]
\end{definition}
TODO: which results also hold for normed algebras?
\begin{definition}
A \udef{Banach algebra} is a normed algebra that is also a Banach space.
\end{definition}

\begin{proposition}
Let $X,Y$ be normed spaces. Then $\Bounded(X,Y)$ is a unital normed algebra.

The algebra $\Bounded(X,Y)$ is a Banach algebra \textup{if and only if} $Y$ is a Banach space.
\end{proposition}
\begin{proof}
We have $\Bounded(X,Y)\subseteq \Lin(X,Y)$. which is an algebra by \ref{linearMapsAlgebra}. Closure under multiplication follows from \ref{operatorNormIsNorm} and \ref{existenceOperatorNorm}. Submultiplicativity is also given by \ref{operatorNormIsNorm}.

The algebra is unital because the identity operator is bounded.

The condition for $\Bounded(X,Y)$ to be a Banach algebra is given by \ref{boundedOperatorsFormBanachSpace}.
\end{proof}

\begin{lemma} \label{multiplicationContinuous}
Let $A$ be a Banach algebra. The multiplication map $\cdot: A\times A \to A: (x,y)\mapsto xy$ is continuous.
\end{lemma}
\begin{proof}
Because $A\times A$ is a metric space, we can combine \ref{sequentialContinuity} and \ref{convergenceFiniteProductTopology} to conclude that the multiplication map is continuous iff $x_ny_n \to xy$ whenever $x_n \to x$ and $y_n \to y$.

Assume $x_n \to x$ and $y_n \to y$. Then
\begin{align*}
\norm{x_ny_n - xy} &= \norm{x_ny_n - xy_n + xy_n - xy} \leq \norm{(x_n-x)y_n}+ \norm{x(y_n-y)}\\ 
&\leq \norm{x_n-x}\cdot\norm{y_n}+ \norm{x}\cdot\norm{y_n-y} = \norm{x_n-x}\cdot\norm{y_n-y+y}+ \norm{x}\cdot\norm{y_n-y}\\
&\leq \norm{x_n-x}\cdot(\norm{y_n-y} + \norm{y})+ \norm{x}\cdot\norm{y_n-y} \to 0
\end{align*}
\end{proof}
As a consequence multiplication by a fixed factor, $x\mapsto cx$ or $x\mapsto xc$ for some $c$, is also continuous, by \ref{productInclusionsContinuous}. This is also immediate from the boundedness of multiplication $\norm{xy}\leq\norm{x}\norm{y}$ and \ref{boundedLinearMaps}.

\begin{lemma}
Let $A$ be a Banach algebra and $D\subset A$ a subset. Suppose $a\in A$ commutes with all elements of $D$, then $a$ commutes with the closure $\overline{D}$.
\end{lemma}
\begin{proof}
Take an arbitrary element $d\in \overline{D}$. Take an arbitrary $\epsilon >0$. Then we can find an $x\in D$ such that $\norm{x-d}\leq \epsilon$. Then, using that $a$ and $x$ commute,
\begin{align*}
\norm{ad - da} &= \norm{a(d+x-x) - (d+x-x)} \\
&= \norm{a(d-x) - (d-x)a} \leq 2\epsilon \norm{a}.
\end{align*}
Because we can choose $\epsilon$ arbitrarily small, $\norm{ad - da}$ must be zero.
\end{proof}

\begin{proposition} \label{smallestBanachAlgebra}
Let $A$ be a Banach algebra and $S\subset A$ a subset. Then
\[ \mathcal{B}(S) \defeq \overline{\Span}\setbuilder{s_1\cdot s_2 \cdot \ldots \cdot s_k}{k\geq 1, s_1,\ldots, s_k \in S} \]
is the smallest Banach subalgebra in $A$ that contains $S$.
\end{proposition}

\section{Unitisation}
\begin{definition}
Let $A$ be a Banach algebra. Then the \udef{unitisation} of $A$ is the algebra $A^\dagger = A\oplus \F$ with multiplication
\[ (x,\lambda)\cdot (y,\mu) = (xy+\lambda y + \mu x, \lambda\mu) \]
and a norm that extends the norm $\norm{\cdot}$ on $A$ to a norm on $A^\dagger$. In other words, there is an isometric embedding
\[ A \hookrightarrow A^\dagger: x\mapsto (x,0). \]
\end{definition}
TODO: is $A^\dagger$ necessarily complete?
\begin{lemma}
For any Banach algebra $A$, $A^\dagger$ is a unital Banach algebra with unit $\vec{1} = (0,1)$.
\end{lemma}
\begin{proof}
TODO: is $A^\dagger$ necessarily complete?
\end{proof}
It is possible to use multiple norms for the unitisation.
\begin{proposition} \label{normsOfUnitisation}
Let $A$ be a Banach algebra. Of the possible norms for $A^\dagger$, the $1$-norm
\[ \norm{(x,\lambda)}_1 = \norm{x}+|\lambda| \]
is minimal and the operator norm
\[ \norm{(x,\lambda)}_{op} = \sup\setbuilder{\norm{xa + \lambda a}}{a\in A \land \norm{a}\leq 1} \]
is maximal. All possible norms are equivalent.
\end{proposition}
\begin{proof}
TODO: prove the operator norm is actually a norm and isometric.
\end{proof}

\begin{definition}
We define
\[ \tilde{A} \defeq \begin{cases}
A & \text{if $A$ unital} \\
A^\dagger & \text{if $A$ non-unital.}
\end{cases} \]
If a Banach algebra $A$ is unital, we can identify $\F$ with $\F\cdot \vec{1} \subseteq A$.
\end{definition}

Alternatively we could define $\tilde{A}$ as the smallest unital Banach algebra containing $A$.

\begin{lemma} \label{algebraIdealInUnitisation}
Let $A$ be a Banach algebra. Then $A$ is a maximal ideal of $A^\dagger$.
\end{lemma}

\begin{lemma}
Let $A$ be a Banach algebra. We have the split exact sequence
\[ \begin{tikzcd}
0 \rar & A \rar[hook, "\iota"] & A^\dagger \rar[shift left, "\pi_2"] & \lar[hook, shift left, "\lambda"] \F \rar & 0.
\end{tikzcd} \]
\end{lemma}

\begin{lemma} \label{unitalProjectionsAlgebraHomomorphisms}
Let $A$ be a Banach algebra. Then
\begin{enumerate}
\item $\proj_2: A^\dagger \to \F$ is a unital algebra homomorphism;
\item $\proj_1: \setbuilder{(a,0)}{a\in A}\subseteq A^\dagger \to A$ is an isometric algebra homomorphism.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Linearity and unitality are immediate. For multiplicativity, take $(a,\lambda), (b, \mu)\in A^\dagger$. Then
\[ \proj_2\big((a,\lambda)(b, \mu)\big) = \proj_2\big(ab+\lambda b + \mu a, \lambda\mu\big) = \lambda\mu = \proj_2(a,\lambda)\proj_2(b,\mu). \]

(2) Linearity and unitality are immediate. For multiplicativity, take $(a,0), (b, 0)\in A^\dagger$. Then
\[ \proj_1\big((a,0)(b, 0)\big) = \proj_1\big(ab+0 b + 0 a, 0\big) = ab = \proj_1(a,0)\proj_2(b,0). \]
Finaly, $\proj_1$ is isometric since the norm on $A^\dagger$ extends the norm on $A$ by definition.
\end{proof}

\begin{lemma} \label{unitalExtensionLinearFunction}
Let $A,B$ be Banach algebras. Every linear function $\Psi:A\to B$ extends uniquely to a linear function $\Psi^\dagger: A^\dagger \to B^\dagger$ that maps $\vec{1}_{A^\dagger}$ to $\vec{1}_{B^\dagger}$. This function is given by:
\[ \Psi^\dagger: A^\dagger \to B^\dagger: (a,\lambda) \mapsto (\Psi(a),\lambda). \]
In particular, every algebra homomorphism extends uniquely to a unital algebra homomorphism.
\end{lemma}
By ``$\Psi^\dagger$ extend $\Psi$'', we mean that $\Psi^\dagger\big(\iota(x)\big) = \iota\big(\Psi(x)\big)$ for all $x\in A$.
\begin{proof}
We want $\Psi^\dagger((a,0)) = (\Psi(a),0)$ for all $a\in A$. Because $\Psi$ is unital, we have $\Psi^\dagger((\vec{0},1)) = (\vec{0},1)$. So
\[ \Psi^\dagger((a,\lambda)) = \Psi^\dagger((a,0))+\lambda \Psi^\dagger((\vec{0},1)) = (\Psi(a),0) + \lambda(\vec{0},1) = (\Psi(a),\lambda). \]
\end{proof}
\begin{corollary} \label{projectionOnACommutes}
Let $\pi_1: A^\dagger \to A$ be the projection on the first component: $\pi_1(a,\alpha) = a$.

The unital extension $\Psi^\dagger$ commutes with $\pi_2$:
\[ \pi_2\circ\Psi^\dagger = \Psi^\dagger \circ \pi_2 = \Psi\circ \pi_2. \]
Restricted to $A$, this is equal to $\Psi$.
\end{corollary}

\begin{definition}
As before we set, for $\Psi: A \to B$ an algebra homomorphism
\[ \tilde{\Psi} = \begin{cases}
\Psi & \text{if $A$ unital} \\
\Psi^\dagger & \text{if $A$ non-unital.}
\end{cases} \]
Thus $\tilde{\Psi}$ is a function on $\tilde{A}$.
\end{definition}

\begin{lemma} \label{DaggerMorphismProperties}
Let $A,B$ be Banach algebras and $\Psi:A\to B$ and algebra homomorphism. Then
\begin{enumerate}
\item $\im(\Psi^\dagger) = (\im\Psi)^\dagger$;
\item $\ker(\Psi^\dagger) = \ker(\Psi)\oplus\{0\}$;
\item $\Psi^\dagger$ is injective \textup{if and only if} $\Psi$ is injective;
\item $\Psi^\dagger$ is surjective \textup{if and only if} $\Psi$ is surjective;
\item $\norm{\Psi^\dagger} = \max\{\norm{\Psi},1\}$;
\item $\Psi^\dagger$ is isometric \textup{if and only if} $\Psi$ is isometric.
\end{enumerate}
\end{lemma}
\begin{proof}
The third point follows from the second and \ref{injectivityKernelTriviality}.
\end{proof}

\begin{definition}
Let $A$ be a Banach algebra. We define the \udef{scalar mapping} to be
\[ s = \lambda\circ \pi: A^\dagger \to A^\dagger: (a,\lambda) \mapsto (0,\lambda). \]
\end{definition}
Notice that $\pi\circ s = \pi$.

\subsection{Approximate units}
\begin{definition}
Let $A$ be a Banach algebra. A net $(e_\lambda)_{\lambda\in\Lambda}$ is an \udef{approximate unit} if
\begin{enumerate}
\item $\norm{e_\lambda}\leq 1$ for all $\lambda$;
\item $a = \lim_{\lambda\to \infty} e_\lambda \cdot a = \lim_{\lambda\to \infty} a \cdot e_\lambda$.
\end{enumerate}
We call $(e)_\lambda$ is an \udef{increasing approximate unit} if $\lambda_0 \leq \lambda_1$ implies $0\leq e_{\lambda_0} \leq e_{\lambda_1}$.
\end{definition}
\begin{lemma}
If $A$ is unital, any approximate unit in $A$ converges to $\vec{1}$.
\end{lemma}
\begin{proof}
We have $\vec{1} = \lim_{\lambda\to\infty}e_\lambda\cdot \vec{1} = \lim_{\lambda\to\infty}e_\lambda$.
\end{proof}

\section{Neumann series}
\begin{proposition}[Neumann series] \label{NeumannSeries}
Let $A$ be a unital Banach algebra and $x\in A$. 
If $\norm{x}<1$, then $\vec{1}-x$ is invertible with inverse
\[ (\vec{1}-x)^{-1} = \sum_{n=0}^\infty x^n \qquad\text{and}\qquad \norm{(\vec{1} - x)^{-1}} \leq \frac{1}{1-\norm{x}}. \]
Equivalently, if $\norm{\vec{1}-x}< 1$, then $x$ is invertible with inverse
\[ x^{-1} = \sum_{n=0}^\infty(\vec{1}-x)^n. \]
\end{proposition}
\begin{proof}
Since $\norm{x^n}\leq \norm{x}^n$ for all $n\geq 1$ and $\sum \norm{x}^n$ is a convergent geometric series, the series $\sum x^n$ is convergent by \ref{absoluteConvergenceImpliesConvergence}.

Also
\[ \norm{(\vec{1} - x)^{-1}} = \norm{\sum_{i=0}^\infty x^i} \leq \sum_{i=0}^\infty \norm{x}^i = \frac{1}{1-\norm{x}} \]
by the geometric series.
\end{proof}
We can in fact weaken the requirement of $\norm{x}<1$ to $\exists k\in\N: \norm{x^k}<1$:
\begin{corollary} \label{NeumannSeriesEventuallyContractive}
Let $A$ be a unital Banach algebra and $x\in A$ such that $\norm{x^k}<1$ for some $k>0$. Then $\vec{1} - x$ is invertible and $(\vec{1} - x)^{-1} = \sum_{i=0}^\infty x^i$.
\end{corollary}
\begin{proof}
We know that the Neumann series $\sum_{i=0}^\infty(x^k)^i$ converges. So
\[ \sum_{i=0}^\infty x^i = (\vec{1} + x + x^2 +\ldots + x^{k-1})\sum_{i=0}^\infty(x^k)^i \]
converges.

To show this convergent sequence acturally gives the correct inverse, we calculate
\begin{align*}
(\vec{1}-x)(\vec{1} + x + x^2 +\ldots + x^{k-1})(\vec{1} - x^k)^{-1} &= \Big(\vec{1} + x + x^2 +\ldots + x^{k-1} - x - x^2 -\ldots - x^{k}\Big)(\vec{1} - x^k)^{-1} \\
&= (\vec{1} - x^k)(\vec{1} - x^k)^{-1} = \vec{1}.
\end{align*}
This shows that $\sum_{i=0}^\infty x^i$ is the correct right inverse. To show it is also a left inverse, we expand $(\vec{1} - x^k)^{-1}(\vec{1} + x + x^2 +\ldots + x^{k-1})(\vec{1}-x)$.
\end{proof}


\begin{lemma}
Let $A$ be a unital Banach algebra and $a,b\in A$ with $\norm{a} < 1$. Then $\sum_{n=0}^\infty a^nb$ is the unique fixed point of $x\mapsto ax+b$.
\end{lemma}
\begin{proof}
The function $x\mapsto ax+b$ is a contraction if and only if $\norm{x}<1$. So it has a unique fixed point (TODO). Starting the fixed point iteration at $b$ yields the series:
\begin{align*}
b &\mapsto  ab+b \\
ab+b &\mapsto a^2b + ab + b \\
&\hdots.
\end{align*}
\end{proof}
This gives an alternate proof of the convergence of the Neumann series.

\subsection{The set of invertible elements}
\begin{proposition} \label{openSetInvertibles}
Let $x\in\GL(A)$ and $y\in A$ such that $\norm{y} < \norm{x^{-1}}^{-1}$, then
\begin{enumerate}
\item $(x-y)^{-1} = x^{-1}\sum_{i=0}^\infty(x^{-1}y)^i$ for all $y\in A$ such that $\norm{y}\leq \norm{x^{-1}}^{-1}$;
\item $\ball(x,\norm{x^{-1}}^{-1})\subset \GL(A)$;
\item the invertible elements $\GL(A)$ form an open subset of $A$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) From $\norm{x^{-1}y} \leq \norm{x^{-1}}\,\norm{y} < \norm{x^{-1}}\,\norm{x^{-1}}^{-1} = 1$, we have that $(\vec{1} - x^{-1}y)$ is invertible with a Neumann series expansion. We then have
\[ x^{1}\sum_{i=0}^\infty (x^{-1}y)^i = x^{-1}(\vec{1} - x^{-1}y)^{-1} = (x-y)^{-1}. \]

(2) Any element in  $\ball(x,\norm{x^{-1}}^{-1})$ is of the form $x-z$, where $\norm{z} < \norm{x^{-1}}^{-1}$.

(3) This follows from (2) by \ref{interior}.
\end{proof}
TODO: this also works if $x$ is closed, bijective linear operator on a Banach space (i.e.\ not necessarily bounded).

\begin{proposition} \label{inverseMapContinuous}
The map $^{-1}: \GL(A)\to\GL(A): x\mapsto x^{-1}$ is continuous.
\end{proposition}
\begin{proof}
Take a convergent sequence $(x_n)\subset\GL(A)$ with limit $x$. We wish to prove $(x_n^{-1})$ converges to $x^{-1}$, because then the map is continuous by \ref{sequentialContinuity}. We can choose an $n_0$ such that $\forall n\geq n_0: x_n \in B(x,\norm{x^{-1}}^{-1})$. From now on we consider only the tails $(x_n)_{n=n_0}^\infty$ and $(x_n^{-1})_{n=n_0}^\infty$, which have the same limits. Then
\[ \norm{x^{-1}}\cdot\norm{x-x_n} < \norm{x^{-1}}\cdot\norm{x^{-1}}^{-1} = 1. \]
Also
\[ \norm{\vec{1} - x^{-1}x_n} = \norm{x^{-1}(x-x_n)} \leq \norm{x^{-1}}\cdot\norm{x-x_n} < 1. \]
We calculate, using the inequalities to apply the Neumann series formula and geometric series formula:
\begin{align*}
\norm{x_n^{-1} - x^{-1}} &= \norm{(x_n^{-1}x - \vec{1})x^{-1}} = \norm{((x^{-1}x_n)^{-1} - \vec{1})x^{-1}} \\
&= \norm{\left(\sum_{k=0}^\infty[\vec{1} - x^{-1}x_n]^k - \vec{1}\right)x^{-1}} = \norm{\left(\sum_{k=1}^\infty[\vec{1} - x^{-1}x_n]^k\right)x^{-1}} \\
&\leq \sum_{k=1}^\infty\norm{\vec{1} - x^{-1}x_n}^k\cdot\norm{x^{-1}} = \sum_{k=1}^\infty\norm{x^{-1}(x - x_n)}^k\cdot\norm{x^{-1}} \\
&\leq \norm{x^{-1}}\sum_{k=1}^\infty\norm{x - x_n}^k\cdot\norm{x^{-1}}^{k} = \norm{x^{-1}}\sum_{k=0}^\infty\norm{x - x_n}^k\cdot\norm{x^{-1}}^{k} - \norm{x^{-1}} \\
&= \frac{\norm{x^{-1}}}{1-\norm{x - x_n}\cdot\norm{x^{-1}}}-\norm{x^{-1}} = \frac{\norm{x - x_n}\cdot\norm{x^{-1}}^2}{1-\norm{x - x_n}\cdot\norm{x^{-1}}}.
\end{align*}
As the right-hand side converges to $0$, so must the left-hand side. Thus $(x_n^{-1})$ converges to $x^{-1}$.
\end{proof}

\begin{lemma} \label{finiteDimensionsInvertiblesDense}
Let $A$ be a finite-dimensional unital Banach algebra. Then $\GL(A)$ is dense in $A$.
\end{lemma}
\begin{proof}
TODO: any set of eigenvectors with different eigenvalues is linearly independent. Since there are only finitely many linearly independent vectors, there are only finitely many eigenvalues. Let $\epsilon$ be the lowest absolute value of an eigenvalue of $a$, excluding $0$. Then $\mu \vec{1} - a$ is invertible for all $\mu\in ball_\F(0,\epsilon)$.
\end{proof}

\section{Quotient algebras}
\begin{proposition}
Let $A$ be a Banach algebra and $J\subset A$ a closed (two-sided) ideal. Then $A/J$ is a Banach algebra.
\end{proposition}
\begin{proof}
We know that $A/J$ is a Banach space by \ref{quotientBanachSpace} and an algebra by (TODO ref). We just need to check that the quotient norm is submultiplicative.

Take $a,b\in A$ and $\epsilon>0$. Then there exist $x,y\in J$ such that $\norm{a+J} + \epsilon > \norm{a+x}$ and $\norm{b+J} + \epsilon > \norm{b+y}$. Then
\begin{align*}
\big(\norm{a+J} + \epsilon\big)\big(\norm{b+J} + \epsilon\big) &> \norm{a+x}\;\norm{b+y} \\
&\geq \norm{(a+x)(b+y)} = \norm{ab + (ay+xb+xy)} \geq \norm{ab + J}.
\end{align*}
Taking $\epsilon\to 0$, gives $\norm{a+J}\;\norm{b+J} \geq \norm{ab + J}$.
\end{proof}

\begin{proposition}
Let $A$ be a unital Banach algebra and $J\subseteq A$ a proper ideal. Then $\norm{\vec{1}}_{A/J} = 1$.
\end{proposition}
\begin{proof}
Since $0\in J$, we have $\norm{\vec{1}}_{A/J} \leq 1$ and it is enough to show that $\norm{\vec{1}+z} \geq 1$ for all $z\in J$. Suppose, towards a contradiction, that $\norm{\vec{1}+z} < 1$, so $z\in \ball(\vec{1},1)$. This means that $z$ is invertible by \ref{openSetInvertibles} and thus that $J$ is not proper by \ref{properIdealNoUnit}.
\end{proof}
\begin{corollary}
Let $A$ be a unital Banach algebra and $J$ a proper ideal. Then
\begin{enumerate}
\item $\vec{1} \notin \overline{J}$;
\item if $J$ is maximal, then it is closed.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) We have $\vec{1}\notin J$ by \ref{properIdealNoUnit}. There does not exist a sequence in $J$ that converges to $\vec{1}$ because $1 = \norm{\vec{1}}_{A/J} = \inf_{z\in J}\norm{\vec{1}-z}$.

(2) Let $J\subseteq A$ be a maximal ideal in $A$. It is enough to show that the closure $\overline{J}$ is also an ideal. Since $\vec{1}\notin \overline{J}$ this ideal is proper.

The fact that $\overline{J}$ is an ideal follows straight from the continuity of addition and multiplication.
\end{proof}

\begin{proposition} \label{commutativeBanachAlgebraIdeals}
Let $A$ be a commutative unital Banach algebra and $\mathcal{J}$ is a maximal ideal. Then 
\begin{enumerate}
\item if $A$ is complex, then $A/\mathcal{J} \cong \C$;
\item if $A$ is real, then $A/\mathcal{J} \cong \R$ or $A/\mathcal{J} \cong \C$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) If $A$ is commutative, $A/\mathcal{J}$ is a field by TODO ref. It is also a unital Banach algebra (TODO), so $A/\mathcal{J} \cong \C$ by the Gelfand-Mazur theorem, \ref{GelfandMazur}.

(2) TODO
\end{proof}

\section{Finite elements}
elements of the socle. \url{https://link.springer.com/content/pdf/10.1023/A:1009717500980.pdf}

have finite spectrum 

\url{http://matwbn.icm.edu.pl/ksiazki/sm/sm104/sm10431.pdf}

\section{Real and complex Banach algebras}
TODO: define $A_\R$ and $A_\C$.

\begin{proposition} \label{preservationAlgebraicPropertiesComplexificationRealification}
$\GL(A) = \GL(A_\C)\cap A$ etc.
\end{proposition}

\section{The spectrum}
TODO: remove unital requirement.
\begin{definition}
Let $A$ be a complex Banach algebra. The \udef{spectrum} of an element $x\in A$ is defined as
\[ \spec(x) = \spec_A(x) \defeq \setbuilder{\lambda\in\C}{\lambda\cdot \vec{1} - x \in \tilde{A} \;\text{is not invertible}}. \]
If $A$ is a real Banach algebra, then the spectrum of $x\in A$ is defined as
\[ \spec_A(x) \defeq \spec_{A_\C}(x) = \setbuilder{\lambda\in\C}{\lambda\cdot \vec{1} - x \in \widetilde{A_\C} \;\text{is not invertible}}.  \]
The \udef{resolvent set} of an element $x\in A$ is
\[ \res(x) = \C\setminus\spec(x) \]
and its \udef{resolvent map} is
\[ R_x: \res(x) \to A : \lambda\mapsto (\lambda\cdot\vec{1}  - x)^{-1}. \]
The \udef{spectral radius} of $x\in A$ is
\[ \spr(x) = \sup\setbuilder{|\lambda|}{\lambda\in\spec(x)}. \]
\end{definition}
As we will later show that the spectrum is compact (\ref{spectrumCompact}), we may equivalently write
\[ \spr(x) = \max\setbuilder{|\lambda|}{\lambda\in\spec(x)}. \]

\begin{lemma} \label{zeroSpectrumNonunitalAlgebra}
Let $A$ be a non-unital Banach algebra. Then $0\in\spec_A(a)$ for all $a\in A$.
\end{lemma}
\begin{proof}
Because $A\subset A^\dagger$ as an ideal, we have $A\cdot A^\dagger \subseteq A$ and thus $\vec{1} \notin A\cdot A^\dagger$. This means that no $a\in A$ has an inverse in $A^\dagger$.
\end{proof}

\begin{lemma}
Let $A$ be a real Banach algebra. Then for all $a\in A$ and $\mu_1,\mu_2\in \R$:
\begin{enumerate}
\item $\spec(a) = \overline{\spec(a)}$;
\item $\mu_1 + \mu_2 i \in \spec(a)$ \textup{if and only if} $(a-\mu_1)^2+\mu_2^2$ is not invertible in $\tilde{A}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Assume $\lambda \notin \spec(a)$, so $(\lambda-a)^{-1}$ exists. Then
\[ \vec{1} = \overline{\vec{1}} = \overline{(\lambda-a)}\overline{(\lambda-a)^{-1}} = (\overline{\lambda}-a)\overline{(\lambda-a)^{-1}}, \]
so $\overline{\lambda}-a$ is invertible and $\overline{\lambda}\notin \spec(a)$. The converse is identical, using $\overline{\lambda}$.

(2) By (1), $(\mu_1 + \mu_2 i)\vec{1}- a$ is invertible if and only if $(\mu_1 - \mu_2 i)\vec{1}-a$ is invertible. Because $(\mu_1 + \mu_2 i)\vec{1}-a$ and $(\mu_1 - \mu_2 i)\vec{1}-a$ commute, this is equivalent to saying
\[ \big((\mu_1 + \mu_2 i)-a\big)\big((\mu_1 - \mu_2 i)-a\big) = (\mu_1-a)^2 + \mu_2^2 \]
is invertible in $\widetilde{A_\C}$, by \ref{productInvertibility} and thus also in $A$ by \ref{preservationAlgebraicPropertiesComplexificationRealification}.
\end{proof}

\begin{proposition}
Let $B$ be a complex Banach algebra and $A = B_\R$, then for all $a\in A$
\[ \spec_A(a) = \spec_B(a) \cup \overline{\spec_B(a)}. \]
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition} \label{spectrumCompact}
For any $x\in A$, the spectrum $\spec(x)$ is a compact subset of $\setbuilder{\lambda\in\C}{|\lambda|\leq \norm{x}}$.

In particular, $\spr(x) \leq \norm{x}$.
\end{proposition}
\begin{proof}
Let $\lambda\in \C$ be such that $|\lambda|>\norm{x}$, then
\[ 1 > \frac{\norm{x}}{|\lambda|} = \frac{\lambda - (\lambda - \norm{x})}{|\lambda|} = \norm{\vec{1} - \left(\vec{1} - \frac{x}{\lambda}\right)}. \]
By \ref{NeumannSeries}, $\vec{1} - x/\lambda$ is invertible and thus so is $\lambda-x$.

It is then enough to show that $\spec(x)$ is closed. By \ref{openSetInvertibles}, $\GL(A)$ is open and the set of non-invertibles $A\setminus \GL(A)$ is closed. Consider $f: \C \to A: \lambda \mapsto \lambda - x$. Then $\spec(x) = f^{-1}[A\setminus \GL(A)]$ is the preimage of a closed set under a continuous map, and hence is closed.
\end{proof}

\begin{lemma}[Polynomial spectral mapping] \label{polynomialSpectralMapping}
Let $A$ be a Banach algebra and $p$ a complex polynomial. Then
\[ p^{\imf}\big(\spec(x)\big) \subseteq \sigma\big(p(x)\big). \]
\end{lemma}
\begin{proof}
Take $\lambda\in\spec(x)$ so $p(\lambda)\in p^{\imf}\big(\spec(x)\big)$. Then $y\mapsto p(\lambda)-p(y)$ is a polynomial with zero at $\lambda$, so we can factorise it as $(\lambda - y)q(y)$ for some other polynomial $q$ by the fundamental theorem of algebra (TODO ref).

Now suppose, towards a contradiction, that $p(\lambda)\notin \sigma\big(p(x)\big)$. Then $p(\lambda) - p(x) = (\lambda - x)q(x) = q(x)(\lambda - x)$ has an inverse, so $(\lambda - x)$ is invertible by \ref{productInvertibility}. This contradicts $\lambda\in\spec(x)$.
\end{proof}

\begin{proposition}
Let $A$ be a unital Banach algebra and $x,y\in A$. Then $\vec{1} - xy$ is invertible \textup{if and only if} $\vec{1} - yx$ is invertible.
\end{proposition}
\begin{proof}
Assume $\vec{1} - xy$ invertible. Then the inverse of $\vec{1} - yx$ is
\[ y(\vec{1} - xy)^{-1}x + \vec{1}. \]
\end{proof}
\begin{corollary} \label{spectrumSwappedElements}
Let $A$ be a unital Banach algebra and $x,y\in A$. Then
\[ \spec(xy)\cup\{0\} = \spec(yx)\cup\{0\}. \]
\end{corollary}
TODO: is there a link to Jacobson's lemma??
\begin{proof}
Assuming $\lambda \neq 0$, we have $\lambda\in\spec(xy) \iff \frac{xy}{\lambda} - \vec{1}$ is invertible.
\end{proof}
It is important to include $0$: there are cases when $0\in \spec(xy)$, but $0\notin \spec(yx)$.

\begin{lemma} \label{spectrumOfImage}
Let $A,B$ be unital Banach algebras and $\Psi: A\to B$ a unital algebra homomorphism. Then for all $x\in A$: $\spec(\Psi(x)) \subseteq \spec(x)$ and hence $\spr(\Psi(x)) \leq \spr(x)$.
\end{lemma}
\begin{proof}
By contraposition: Assume $\lambda\notin\spec(x)$, then $x-\lambda$ has an inverse, call it $a$. Then $(\Psi(x) - \lambda)$ has an inverse by
\[ (\Psi(x) - \lambda)\Psi(a) = \Psi(x-\lambda)\Psi(a) = \Psi((x-\lambda)a) = \Psi(\vec{1}) = \vec{1},\]
meaning $\lambda \notin \spec(\Psi(x))$.
\end{proof}

In general if $B$ is a subalgebra of a Banach algebra $A$, then for any $x\in B$, $\spec_B(x) \supseteq \spec_A(x)$.


\begin{lemma} \label{spectrumIdempotent}
Let $A$ be a unital Banach algebra and $p\in A$ an idempotent element. Then $\spec(p) \subseteq \{0,1\}$.
\end{lemma}
\begin{proof}
Take $\lambda \in \C\setminus \{0,1\}$. Then $x-\lambda\vec{1}$ has an inverse given by $-\lambda^{-1} + (1 - \lambda)^{-1}\lambda^{-1}x$, indeed
\[ (x-\lambda)\left( - \frac{1}{\lambda} + \frac{x}{(1-\lambda)\lambda} \right) = \vec{1} - \frac{x}{\lambda} + \frac{x-x\lambda}{(1-\lambda)\lambda} =  \vec{1} - \frac{x}{\lambda} + \frac{x}{\lambda} = \vec{1}. \]
The inverse is two-sided since it commutes.
\end{proof}



\subsection{Resolvents and pseudoresolvents}

\begin{proposition} \label{secondNeumannSeries}
Let $A$ be a Banach algebra, $x\in A$ and $|\lambda| > \inf_{n\to \infty}\norm{x^n}^{1/n}$, then
\begin{enumerate}
\item $\lambda\in\res(x)$;
\item $R_x(\lambda) = \sum_{n=0}^\infty\frac{x^n}{\lambda^{n+1}}$;
\item $\norm{R_x(\lambda)} \leq \frac{1}{|\lambda|-\norm{x}}$.
\end{enumerate}
\end{proposition}
\begin{proof}
Take arbitrary $\epsilon > 0$. We can find an $n\in\N$ such that $|\lambda| > \norm{x^n}^{1/n}$, so $1 > \norm{\left(\frac{x}{\lambda}\right)^n}$. By \ref{NeumannSeriesEventuallyContractive}, we have that $(\vec{1} - \frac{x}{\lambda})$ is invertible and
\[ R_x(\lambda) = \lambda^{-1} \left(\vec{1} - \frac{x}{\lambda}\right)^{-1} = \lambda \sum_{n=0}^\infty\left(\frac{x}{\lambda}\right)^n = \sum_{n=0}^\infty\frac{x^n}{\lambda^{n+1}}. \]
Finally $\norm{R_x(\lambda)} \leq \sum_{n=0}^\infty\frac{\norm{x}^n}{|\lambda|^{n+1}} = \frac{1}{|\lambda|(1-\frac{\norm{x}}{\lambda})} = \frac{1}{|\lambda|-\norm{x}}$.
\end{proof}

\begin{lemma} \label{BanachAlgebraResolventMultiplication}
Let $A$ be a unital Banach algebra, $a\in A$ and $\lambda\in\res(a)$. Then
\[ aR_a(\lambda) = \lambda R_a(\lambda) - \vec{1}. \]
\end{lemma}
\begin{proof}
We have $\id_X = (\lambda \id_X - T)R_T(\lambda) = \lambda R_T(\lambda) - TR_T(\lambda)$.
\end{proof}

\subsubsection{Pseudoresolvents and first resolvent identity}
\begin{definition}
Let $A$ be a Banach algebra. A function $\mathcal{R}:\Lambda \subseteq \C \to A$ is called a \udef{pseudoresolvent} if, for all $\lambda,\mu\in\Lambda$
\[ \mathcal{R}(\lambda) - \mathcal{R}(\mu) = (\mu-\lambda)\mathcal{R}(\lambda)\mathcal{R}(\mu). \]
This equation is known as the (first) \udef{resolvent identity}.
\end{definition}

Note that if a pseudoresolvent $\mathcal{R}$ is zero anywhere, it is identically zero.

\begin{lemma}
Let $\mathcal{R}:\Lambda \subseteq \C \to A$ be a pseudoresolvent on a Banach algebra $a$ and $\lambda,\mu\in\Lambda$. Then $\mathcal{R}(\lambda)\mathcal{R}(\mu) = \mathcal{R}(\mu)\mathcal{R}(\lambda)$.
\end{lemma}
\begin{proof}
If $\lambda = \mu$, then the result is immediate. If $\lambda \neq \mu$, then
\[ \mathcal{R}(\lambda)\mathcal{R}(\mu) = (\mu-\lambda)^{-1}\big(\mathcal{R}(\lambda) - \mathcal{R}(\mu)\big) = (\lambda - \mu)^{-1}\big(\mathcal{R}(\mu) - \mathcal{R}(\lambda)\big) = \mathcal{R}(\mu)\mathcal{R}(\lambda). \]
\end{proof}

\begin{proposition} \label{firstNeumannSeries}
Let $\mathcal{R}:\Lambda \subseteq \C \to A$ be a pseudoresolvent and $\lambda_0,\lambda\in\Lambda$ such that $|\lambda-\lambda_0|\,\norm{\mathcal{R}(\lambda_0)} < 1$. Then
\[ \mathcal{R}(\lambda) = \sum_{n=0}^\infty(\lambda_0 - \lambda)^n \mathcal{R}(\lambda_0)^{n+1} \qquad\text{and}\qquad \norm{\mathcal{R}(\lambda)} \leq \frac{1}{\norm{\mathcal{R}(\lambda_0)}^{-1} - |\lambda_0-\lambda|}. \]
In particular $\mathcal{R}$ is analytic and can be analytically continued to $\Lambda \cup \ball(\lambda_0, \norm{\mathcal{R}(\lambda_0)}^{-1})$. The analytic continuation still satisfies the resolvent identity.
\end{proposition}
\begin{proof}
By assumption we have $(\lambda - \lambda_0)\mathcal{R}(\lambda_0)$ is a contraction, so $(\id_X - (\lambda - \lambda_0)\mathcal{R}(\lambda_0))^{-1}$ exists and has a Neumann series expansion by \ref{NeumannSeries}. From the resolvent identity, we get
\[ \mathcal{R}(\lambda)\big(\id_X - (\lambda_0 - \lambda)\mathcal{R}(\lambda_0)\big) = \mathcal{R}(\lambda_0), \]
so, using the Neumann series expansion,
\begin{align*}
\mathcal{R}(\lambda) &= \big(\id_X - (\lambda_0 - \lambda)\mathcal{R}(\lambda_0)\big)^{-1}\mathcal{R}(\lambda_0) \\
&= \sum_{n=0}^\infty(\lambda_0 - \lambda)^n \mathcal{R}(\lambda_0)^{n+1}.
\end{align*}
This series converges in norm for all $\lambda\in \C$ such that $|\lambda-\lambda_0| < \norm{\mathcal{R}(\lambda_0)}^{-1}$. Running the equalities in reverse gives the resolvent identity.

The norm estimate is also given by \ref{NeumannSeries}:
\begin{align*}
\norm{\mathcal{R}(\lambda)} &\leq \norm{\big(\id_X - (\lambda_0 - \lambda)\mathcal{R}(\lambda_0)\big)^{-1}}\norm{\mathcal{R}(\lambda_0)} \\
&\leq \frac{1}{1 - |\lambda_0-\lambda|\,\norm{\mathcal{R}(\lambda_0)}}\norm{\mathcal{R}(\lambda_0)} \\
&= \frac{1}{\norm{\mathcal{R}(\lambda_0)}^{-1} - |\lambda_0-\lambda|}.
\end{align*}
\end{proof}
\begin{corollary} \label{derivativePseudoresolvent}
Let $\mathcal{R}:\Lambda \subseteq \C \to A$ be a pseudoresolvent. Then
\begin{enumerate}
\item $\mathcal{R}'(\lambda) = -\mathcal{R}(\lambda)^2$ for all $\lambda\in\Lambda$;
\item $\mathcal{R}^{(n)}(\lambda) = n!(-1)^n \mathcal{R}(\lambda)^{n+1}$ for all $n\in \N$.
\end{enumerate}
In particular the map $\mathcal{R}$ is holomorphic on its domain of definition.
\end{corollary}
\begin{proof}
We calculate
\begin{align*}
\mathcal{R}'(\lambda) &= \lim_{\mu\to\lambda} \frac{\mathcal{R}(\mu) - \mathcal{R}(\lambda)}{\mu-\lambda} \\
&= \lim_{\mu\to\lambda} \frac{\mathcal{R}(\mu) - \mathcal{R}(\lambda)}{\mu-\lambda} \\
&= \lim_{\mu\to\lambda} -\mathcal{R}(\lambda)\mathcal{R}(\mu) = -\mathcal{R}(\lambda)\lim_{\mu\to\lambda} \mathcal{R}(\mu) = -\mathcal{R}(\lambda)^2.
\end{align*}
For the last equality we have used the fact that $\mathcal{R}$ is continuous, which follows from its analyticity.
\end{proof}

\begin{proposition} \label{firstResolventIdentity}
Let $A$ be a Banach algebra and $x\in A$. Then the resolvent map
\[ R_x: \res(x)\to A: \lambda \mapsto (\lambda\cdot\vec{1} - x)^{-1} \]
is a pseudoresolvent.
\end{proposition}
\begin{proof}
We first note that $R_x(\lambda), R_x(\mu)$ commute for any $\lambda,\mu\in\spec(x)$, by \ref{commutationInverse}.

We then calculate
\begin{align*}
R_x(\lambda) - R_x(\mu) &= R_x(\lambda)(\mu - x)R_x(\mu) - R_x(\lambda)(\lambda - x)R_x(\mu) \\
&= \mu R_x(\lambda)R_x(\mu) - R_x(\lambda)xR_x(\mu) - \lambda R_x(\lambda)R_x(\mu) + R_x(\lambda)xR_x(\mu) \\
&= \mu R_x(\lambda)R_x(\mu) - \cancel{R_x(\lambda)xR_x(\mu)} - \lambda R_x(\lambda)R_x(\mu) + \cancel{R_x(\lambda)xR_x(\mu)} \\
&= (\mu - \lambda)R_x(\lambda)R_x(\mu).
\end{align*}
\end{proof}

\subsubsection{Second resolvent identity}
\begin{proposition}[Second resolvent identity] \label{secondResolventIdentity}
Let $A$ be a Banach algebra and $x,y\in A$. Then for all $\lambda \in \res(x)\cap \res(y)$ we have
\[ R_x(\lambda) - R_y(\lambda) = R_x(\lambda)\big(x-y\big)R_y(\lambda). \]
\end{proposition}
\begin{proof}
We have
\begin{align*}
R_x(\lambda)\big(x-y\big)R_y(\lambda) &= R_x(\lambda)\big(\lambda\vec{1}-y - (\lambda\vec{1} - x)\big)R_y(\lambda) \\
&= R_x(\lambda)\cancel{(\lambda\vec{1}-y)R_y(\lambda)} - \cancel{R_x(\lambda)(\lambda\vec{1} - x)}R_y(\lambda) \\
&= R_x(\lambda) - R_y(\lambda).
\end{align*}
\end{proof}
We can obtain the first resolvent identity from the second by setting $y = (\lambda-\mu)\vec{1} + x$. Then $R_{(\lambda-\mu)\vec{1} + x}(\lambda) = R_x(\mu)$, so
\begin{align*}
R_x(\lambda) - R_x(\mu) &= R_x(\lambda) - R_{(\lambda-\mu)\vec{1} + x}(\lambda) \\
&= R_x(\lambda)(x - (\lambda-\mu)\vec{1} + x)R_{(\lambda-\mu)\vec{1} + x}(\lambda) \\
&= (\lambda - \mu)R_x(\lambda)R_{(\lambda-\mu)\vec{1} + x}(\lambda) \\
&= (\lambda - \mu)R_x(\lambda)R_x(\mu).
\end{align*}

\begin{corollary}
Let $x,y\in A$ and $\lambda\in \res(x)\cap \res(y)$ be such that $\norm{R_x(\lambda)(y-x)}<1$. Then 
\[ R_y(\lambda) = \sum_{n=0}^\infty \big(R_x(\lambda)(y-x)\big)^nR_x(\lambda) \quad\text{and}\quad \norm{R_y(\lambda)} \leq \frac{1}{\norm{R_x(\lambda)}^{-1} - \norm{y-x}}. \]
\end{corollary}
\begin{proof}
We have
\[ R_x(\lambda) = R_y(\lambda) + R_x(\lambda)(x-y)R_y(\lambda) = \big(\vec{1} + R_x(\lambda)(x-y)\big)R_y(\lambda) = \big(\vec{1} - R_x(\lambda)(y-x)\big)R_y(\lambda). \]
Because $R_x(\lambda)(y-x)$ was assumed a contraction, $\vec{1} + R_x(\lambda)(x-y)$ has an inverse given by its Neumann series, \ref{NeumannSeries}, so
\begin{align*}
R_y(\lambda) &= \big(\vec{1} - R_x(\lambda)(y-x)\big)^{-1} R_x(\lambda) \\
&= \sum_{n=0}^\infty \big(R_x(\lambda)(y-x)\big)^n R_x(\lambda).
\end{align*}
The norm estimate is also given by \ref{NeumannSeries}:
\begin{align*}
\norm{R_y(\lambda)} &\leq \norm{\big(\vec{1} - R_x(\lambda)(y-x)\big)^{-1}}\norm{R_x(\lambda)} \\
&\leq \frac{1}{1 - \norm{R_x(\lambda)(y-x)}}\norm{R_x(\lambda)} \\
&\leq \frac{1}{1 - \norm{R_x(\lambda)}\,\norm{(y-x)}}\norm{R_x(\lambda)} \\
&= \frac{1}{\norm{R_x(\lambda)}^{-1} - \norm{y-x}}.
\end{align*}
\end{proof}

\subsubsection{Properties of the spectrum}

\begin{proposition} \label{spectrumNonEmpty}
Let $A$ be a Banach algebra and $x\in A$. Then $\spec(x)\neq \emptyset$.
\end{proposition}
\begin{proof}
Assume, towards a contradiction, that $\res(x) = \C$. Then the resolvent norm $\lambda\mapsto \norm{R_x(\lambda)}$ is an entire function by \ref{derivativePseudoresolvent}. By \ref{secondNeumannSeries} we have $\lim_{|\lambda|\to\infty}\norm{R_x(\lambda)} = 0$. By Liouville's theorem \ref{liouvilleTheoremAnalysis}, we have that $\norm{R_x(\lambda)}$ is identically zero.

Now we have
\[ 1 = \norm{\vec{1}} = \norm{R_x(\lambda)(\lambda-x)} \leq \norm{R_x(\lambda)}\,\norm{\lambda - x} = 0. \]
This is a contradiction.
\end{proof}
\begin{corollary}[Gelfand-Mazur] \label{GelfandMazur}
Let $A$ be a unital complex Banach algebra. If every non-zero element is invertible, then $A=\C\cdot \vec{1}$.
\end{corollary}
\begin{proof}
Suppose $x\in A\setminus (\C\cdot\vec{1})$. Then $\spec(x) = \emptyset$, contradicting the theorem.
\end{proof}
In other words, $\C$ is the only normed complex division algebra.


\begin{proposition}[Spectral radius formula] \label{spectralRadiusFormula}
Let $A$ be a Banach algebra and $x\in A$. Then
\[ \spr(x) = \lim_{n\to\infty}\norm{x^n}^{1/n} = \inf_{n\in\N}\norm{x^n}^{1/n}. \]
\end{proposition}
TODO: sometimes Fekete's lemma \ref{FeketesLemma} is used in the proof. Does this help us?

TODO: fill out references!!
\begin{proof}
The contraposition of (1) in \ref{secondNeumannSeries} gives that $\lambda\in\spec(x)$ implies $|\lambda| \leq \inf_{n\to \infty}\norm{x^n}^{1/n}$. Thus
\[ \spr(x) \leq \inf_{n\in\N}\norm{x^n}^{1/n} \leq \liminf_{n\in\N}\norm{x^n}^{1/n} \leq \limsup_{n\in\N}\norm{x^n}^{1/n}. \]
The result follows if we can prove $\limsup_{n\to\infty}\norm{x^n}^{1/n}\leq \spr(x)$.

Consider the function $zR_x\big(z^{-1}\big)$, which is holomorphic on $\ball(0,\spr(x)^{-1})\setminus\{0\}$ by \ref{derivativePseudoresolvent}. Take an arbitrary continuous linear functional $f\in \dual{A}$. Then $f\big(zR_x(z^{-1})\big)$ is holomorphic (TODO ref).

For all $z\in \ball(0,\norm{x}^{-1})\setminus\{0\}\subseteq \ball(0,\spr(x)^{-1})\setminus\{0\}$, we have the series expansion
\[ f\big(zR_x(z^{-1})\big) = f\Big(z\sum_{n=0}^\infty \frac{x^n}{z^{-n-1}}\Big) = f\Big(\sum_{n = 0}^\infty (zx)^n\Big) = \sum_{n = 0}^\infty f\big((zx)^n\big), \]
by \ref{secondNeumannSeries}. By TODO ref, this series expansion must also hold on the larger disk $\ball(0,\spr(x)^{-1})$, which means that $f\big((zx)^n\big) \to 0$ by TODO ref.

Since $f\in A^*$ was arbitrary, the sequence $\seq{(zx)^n}$ is weakly convergent and so norm-bounded (with, say, bound $C$) by \ref{weaklyConvergentSequenceNormBounded} for all $z\in \ball(0,\spr(x)^{-1})$. Thus
\[ |z|\limsup_{n\to\infty}\norm{x^n}^{1/n} = \limsup_{n\to\infty}\norm{(zx)^n}^{1/n} \leq \limsup_{n\to\infty}C^{1/n} = \lim_{n\to\infty} C^{1/n} = 1, \]
and so $\limsup_{n\to\infty}\norm{x^n}^{1/n} \leq |z|^{-1}$. Since $z\in \ball(0,\spr(x)^{-1})$ was picked arbitrarily, we have $\limsup_{n\to\infty}\norm{x^n}^{1/n} \spr(x)$.
\end{proof}



\subsection{Quasinilpotent operators}
\begin{definition}
Let $A$ be a Banach algebra and $x\in A$. If $\spec(x) = \{0\}$, then $x$ is called \udef{quasinilpotent}
\end{definition}
\url{https://www.jstor.org/stable/2042882?seq=1}
\url{https://www.researchgate.net/profile/Zbigniew-Slodkowski/publication/265547661_A_note_on_quasinilpotent_elements_of_a_Banach_algebra/links/5e7e8f94458515efa0b0fe83/A-note-on-quasinilpotent-elements-of-a-Banach-algebra.pdf?origin=publication_detail}

\url{https://www.cambridge.org/core/services/aop-cambridge-core/content/view/AC3CBD3000D16515D0BD83C07B703186/S0013091500015352a.pdf/finite_dimensionality_nilpotents_and_quasinilpotents_in_banach_algebras.pdf}

\url{https://www.cambridge.org/core/services/aop-cambridge-core/content/view/C8F26DDF45A29D689C726A29D8F0BC2A/S0017089500008429a.pdf/algebraic-ideals-of-semiprime-banach-algebras.pdf}

\begin{proposition} \label{nilpotentQuasinilpotent}
Every nilpotent element is quasinilpotent. The converse holds for finite elements in semisimple Banach algebras (TODO correct version of finite).
\end{proposition}
\begin{proof}
Let $x$ be a nilpotent element in a (unital) Banach algebra $A$.
By the spectral radius formula \ref{spectralRadiusFormula}, we have
\[ \spr(x) = \lim_{n\to\infty}\norm{x^n}^{1/n} = 0. \]
This implies $\spec(x) = \{0\}$.

Now assume $x$ a finite quasimilpotent element. Then $\dim(xAx) = n$ and so $x^2, \ldots x^{n+2}$ a linearly dependent and thus there exists a polynomial $p$ of degree at most $n+2$ such that $p(x) = 0$. We can factorise $p(x) = x^kq(x)$ where $q$ is some polynomial such that $q(0) \neq 0$ and $k\in \N$. Then by spectral mapping, we have that $\spec(q(x)) = q(\spec(x)) = q(\{0\}) \neq \{0\}$. Thus $q(x)$ is invertible and we have
\[ x^k = x^kq(x)q(x)^{-1} = 0\cdot q(x)^{-1} = 0. \]
This means $x$ is nilpotent.
\end{proof}

\subsection{Characters}
\begin{definition}
Let $A$ be a Banach algebra. A \udef{character} on $A$ is a non-zero algebra homomorphism $A\to\C$.

The \udef{character space} (or \udef{(Gelfand) spectrum}) $\hat{A}$ is the set of characters on $A$, equipped with the pointwise convergence.
\end{definition}
In other words, a character on $A$ is a non-zero multiplicative linear functional $A\to \C$.
In particular, if $A$ is a real algebra, a character is still complex valued, but now  an $\R$-linear functional on $A$. 

\begin{lemma} \label{unitalCharacterExtension}
Let $A$ be a Banach algebra and $\varphi\in \hat{A}$. Set $\tilde{\varphi}: A^\dagger \to \C: (a,\lambda) \mapsto \varphi(a)+\lambda$. Then $\tilde{\varphi} \in \widehat{A^\dagger}$.
\end{lemma}

If $A$ is a real algebra, then we set
\[ \hat{A} = \setbuilder{\varphi|_A}{\varphi\in \widehat{A_\C}}. \]

\begin{proposition} \label{charactersUnital}
Let $A$ be a Banach algebra and $\varphi$ a character on $A$, then $\varphi$ is continuous and
\begin{enumerate}
\item $\varphi(\vec{1}) = 1$ if $A$ is unital;
\item $\norm{\varphi}\leq 1$;
\item $\norm{\varphi} = 1$ if $A$ contains an approximate unit.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We prove that $\varphi$ is unital if $A$ is unital: As $\varphi \neq 0$, we have $\varphi(x)\neq 0$ for some $x\in A$. Then $\varphi(x) = \varphi(x\cdot\vec{1}) = \varphi(x)\varphi(\vec{1})$, so $\varphi(\vec{1}) = 1$.

(2) If we can show $\norm{\tilde{\varphi}} \leq 1$, then $\norm{\varphi}\leq 1$ follows by \ref{DaggerMorphismProperties}. To this end suppose that for some $x\in \tilde{A}$, $|\tilde{\varphi}(x)|>\norm{x}$. Then $x-\tilde{\varphi}(x)$ is invertible by \ref{spectrumCompact}. Thus
\begin{align*}
1 = \tilde{\varphi}(\vec{1}) &= \tilde{\varphi}\big((x-\tilde{\varphi}(x))^{-1}(x-\tilde{\varphi}(x))\big) \\
&= \tilde{\varphi}(x-\tilde{\varphi}(x))^{-1}\tilde{\varphi}(x-\tilde{\varphi}(x)) \\
&= \tilde{\varphi}(x-\tilde{\varphi}(x))^{-1}\big(\tilde{\varphi}(x)-\tilde{\varphi}(x)\big) \\
&= \tilde{\varphi}(x-\tilde{\varphi}(x))^{-1}\cdot 0 = 0,
\end{align*}
which is a contradiction. Then $\norm{\tilde{\varphi}} \leq 1$ and thus $\norm{\varphi} \leq 1$.

(3) By \ref{boundedLinearMaps}, $\varphi$ is continuous. Let $\seq{e_\lambda}_{\lambda\in\Lambda}$ be an approximate unit.
Then, for all $x\in A$, we have
\[ \varphi(x) = \varphi(\lim_\lambda x\cdot e_\lambda) = \lim_\lambda\varphi(x\cdot e_\lambda) = \lim_\lambda \varphi(x)\cdot \varphi(e_\lambda). \]
Since $\varphi\neq 0$, we can take $x$ such that $\varphi(x)\neq 0$. Then
\[ 1 = \varphi(x)^{-1}\varphi(x) = \varphi(x)^{-1}\cdot\big(\lim_\lambda \varphi(x)\cdot \varphi(e_\lambda)\big) = \lim_\lambda \varphi(x)^{-1}\varphi(x)\cdot \varphi(e_\lambda) = \lim_\lambda \varphi(e_\lambda). \]
This implies $\sup_{\lambda}|\varphi(e_\lambda)| \geq 1$. Because $\norm{e_\lambda}\leq 1$ for all $\lambda\in\Lambda$, we have
\[ 1 \leq \sup_{\lambda}|\varphi(e_\lambda)| \leq \sup_{\lambda}\frac{|\varphi(e_\lambda)|}{\norm{e_\lambda}} \leq \norm{\varphi}. \]
The other inequality is given by (2).
\end{proof}

\begin{lemma}
Let $A$ be a real Banach algebra. Let $\varphi$ be a character on $A$. Then $\varphi$ is continuous \textup{if and only if} $\im(\varphi)\subseteq \R$.
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{proposition} \label{characterSpaceLocallyCompact}
Let $A$ be a commutative Banach algebra. Then the character space $\hat{A}$
\begin{enumerate}
\item is locally compact and Hausdorff;
\item is compact if and only if $A$ unital.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) The character space is Hausdorff by \ref{weakTopologyLCTVS}. The unit ball in the dual $\dual{A}$ is weak-$*$-compact by the Banach-Alaoglu theorem \ref{alaogluTheorem}. The set of all algebra homomorphisms $\hat{A}\cup \{\underline{0}\}$ is a closed subset of $(A\to \C)$ (TODO ref), so it is weak-$*$-compact by \ref{compactClosedSets}.

Now $\hat{A} = \big(\hat{A}\cup \{\underline{0}\}\big)\setminus \{\underline{0}\}$ is locally closed by \ref{locallyClosedEquivalents} (using the fact that $\{\underline{0}\}$ is closed by \ref{FrechetCharacterisation}). Thus it is locally compact by \ref{locallyCompactSubspace}.

(2) TODO
\end{proof}

\begin{proposition} \label{commutativeSameSpectrum}
Let $A$ be  a  unital  Banach  algebra,  and  suppose  that $S\subseteq A$ is a subset  of  pairwise commuting elements.  Then there exists a unital commutative Banach subalgebra $C\subseteq A$ with $S\subseteq C$ such that
\[ \forall s\in S: \quad \spec_A(s) = \spec_C(s). \]
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition} \label{characterMaximalIdealsComplex}
Let $A$ be a complex unital commutative Banach algebra. Then we have a bijection
\[ \ker: \hat{A} \twoheadrightarrowtail \{\text{maximal ideals in $A$}\}: \varphi \mapsto \ker(\varphi).  \]
\end{proposition}
\begin{proof}
First we verify that for each character $\varphi$ the kernel is a maximal ideal. Indeed applying \ref{splittingMap} to $\varphi$ we get an isomorphism $A/\ker\varphi \cong \im\varphi = \C$, meaning $\ker(\varphi)$ has codimension 1 and thus is a maximal proper subspace. By \ref{kernelIsIdeal}, $\ker(\varphi)$ is an ideal.

To prove $\ker$ is injective: let $\ker(\varphi) = \ker(\psi)$. Take some $a\in A$, which we can uniquely write as $\lambda+x$, with $x\in\ker(\varphi) = \ker(\psi)$, as $A = \Span(\vec{1})\oplus \ker(\varphi)$. Then
\[ \varphi(a) = \varphi(\lambda\cdot\vec{1} + x) = \lambda\varphi(\vec{1}) + 0 = \lambda = \lambda\psi(\vec{1}) + 0 = \psi(\lambda\cdot\vec{1} + x) = \psi(a), \]
so $\varphi = \psi$.

For surjectivity, take a maximal ideal $\mathcal{J}$.
By \ref{commutativeBanachAlgebraIdeals}, $A/\mathcal{J}\cong \C$, so the quotient map $A\to A/\mathcal{J}\cong \C$ can be seen as a character with kernel $\mathcal{J}$.
\end{proof}

\begin{example}
We know that $A$ is a maximal ideal in $A^\dagger$ by \ref{algebraIdealInUnitisation}. It is the kernel of the character $\proj_2: A^\dagger \to \C: (x,\lambda) \mapsto \lambda$. We take this character as the basepoint of $\widehat{A^\dagger}$.
\end{example}

\begin{lemma} \label{compactificationOfCharacterSpaceIsCharacterSpaceOfUnitisation}
Let $A$ be a complex commutative Banach algebra. Then
\[ \left(\begin{smallmatrix}
\widetilde{(-)} \\ \proj_2
\end{smallmatrix}\right): \hat{A}^\dagger \to \widehat{A^\dagger}: \varphi \mapsto \begin{cases}
\tilde{\varphi} & (\varphi \in \hat{A}) \\
\proj_2 & (\varphi = \infty)
\end{cases} \]
is a homeomorphism.
\end{lemma}
TODO: clean up proof
\begin{proof}
The function $\widetilde{(-)}: \hat{A} \to \widehat{A^\dagger}: \varphi \to \varphi^\dagger$ is well-defined by \ref{unitalCharacterExtension}.

We first show that $\left(\begin{smallmatrix}
\widetilde{(-)} \\ \proj_2
\end{smallmatrix}\right)$ is a bijection. For injectivity, take $\varphi_1 \neq \varphi_2\in \hat{A}^\dagger$. Suppose $\varphi_1, \varphi_2\in \hat{A}$, then clearly $\widetilde{\varphi_1} \neq \widetilde{\varphi_2}$. Now suppose $\varphi_2 = \infty$, then $\left(\begin{smallmatrix}
\widetilde{(-)} \\ \proj_2
\end{smallmatrix}\right)(\varphi_2) = \widetilde{\constant{0}} \neq \widetilde{\varphi_1}$, since $\constant{0}\notin \hat{A}$.

For surjectivity, take $\varphi\in \widehat{A^\dagger}$. Then $\ker(\varphi)$ is a maximal ideal in $A^\dagger$ by \ref{characterMaximalIdealsComplex}. Suppose $\varphi \neq \proj_2$, then $\ker(\varphi) \neq A$, so $\varphi|_A$ is a non-zero algebra homomorphism and thus $\varphi|_A\in \hat{A}$.

Finally it is enough to show continuity of $\left(\begin{smallmatrix}
\widetilde{(-)} \\ \proj_2
\end{smallmatrix}\right)$ by \ref{compactToHausdorffHomeomorphism}. We start by showing the continuity of $\widetilde{(-)}: \hat{A} \to \widehat{A^\dagger}: \varphi \to \varphi^\dagger$. Take arbitrary $(a,\lambda)\in A^\dagger$. Then \[ \evalMap_{(a,\lambda)}|_{\widehat{A^\dagger}}\circ \widetilde{(-)} = \evalMap_a|_{\hat{A}} + \lambda, \]
which is clearly continuous, so $\widetilde{(-)}: \hat{A} \to \widehat{A^\dagger}$ is continuous by \ref{characteristicPropertyInitialFinalConvergence}.

We use \ref{universalPropertyAlexandroffCompactification} to show the continuity of $\left(\begin{smallmatrix}
\widetilde{(-)} \\ \proj_2
\end{smallmatrix}\right)$. Take a closed subset $C\subseteq \widehat{A^\dagger}\setminus \{\proj_2\}$, then $\widetilde{(-)}^{\preimf}(C)$ is a closed subset of $\hat{A}$. It is also a closed subset of $\hat{A}\cup\{\constant{0}\}$. Indeed suppose, towards a contradiction, that $F\in \powerfilters(\hat{A}\cup\{\constant{0}\})$ converges pointwise to $\varphi \notin \widetilde{(-)}^{\preimf}(C)$. Then $\varphi = \constant{0}$, since $\widetilde{(-)}^{\preimf}(C)$ is a closed subset of $\hat{A}$, and for all $a\in A$ we have $\upset\evalMap_a^{\imf\imf}(F) \to 0$. Then, for all $\lambda \in \C$, we have $\evalMap_{(a,\lambda)}\Big(\widetilde{(-)}^{\imf\imf}(F)\Big) \to \lambda$ and thus $\widetilde{(-)}^{\imf\imf}(F) \to \proj_2$. Since $\widetilde{(-)}^\imf\Big(\widetilde{(-)}^\preimf(C)\Big)\subseteq C$, this means $\proj_2\in C$, which is a contradiction.

Thus $\widetilde{(-)}^{\preimf}(C)$ is compact in $\hat{A}\cup \{\constant{0}\}$ by \ref{compactClosedSets} and so compact in $\hat{A}$ by \ref{compactSetCompactSubspace}.
\end{proof}

TODO characters in real Banach algebra.


\subsubsection{The Gelfand transform}
\begin{definition}
Let $A$ be a unital commutative Banach algebra. The \udef{Gelfand transform} of $A$ is the map
\[ \evalMap_{-}|_{\hat{A}} : A\to \cont(\hat{A}): x\mapsto \evalMap_x \]
where $\evalMap_x(\varphi) = \varphi(x)$ for all $x\in A, \varphi\in\hat{A}$.
\end{definition}
Note that $\evalMap_{-}|_{\hat{A}}$ does not mean $\big(\evalMap_{-}\big)|_{\hat{A}}$.


Note that $\evalMap_{-}|_{\hat{A}}$ is well-defined, in the sense that $\evalMap_{x} \in \cont(\hat{A})$, because we have equipped $\hat{A}$ with the weak-$*$ topology, which makes each $\evalMap_{x}$ continuous.

\begin{lemma} \label{GelfandTransformHomomorphism}
Let $A$ be a commutative Banach algebra. Then the Gelfand transform $\evalMap_{-} : A\to \cont(\hat{A})$ is an algebra homomorphism.
\end{lemma}
\begin{proof}
We need to show that $\evalMap_{-}$ is both linear and multiplicative. First note $\evalMap_{x}(\lambda \varphi + \psi) = \lambda \varphi(x) + \psi(x) = \lambda\evalMap_{x}(\varphi) + \evalMap_{x}(\psi)$ and then $\evalMap_{x}(\varphi\psi) = \varphi(x)\psi(x) = \evalMap_{x}(\varphi)\evalMap_{x}(\psi)$.
\end{proof}

\begin{proposition} \label{spectrumFromSpectrum}
Let $A$ be a unital complex commutative Banach algebra and $x\in A$. Then
\[ \spec(x) = \setbuilder{\varphi(x)}{\varphi\in\hat{A}} = \evalMap_x^{\imf}(\hat{A}). \]
\end{proposition}
\begin{proof}
Suppose $\lambda = \varphi(x)$ for some $\varphi\in\hat{A}$. Then $\varphi(x) = \lambda \varphi(\vec{1}) = \varphi(\lambda\cdot \vec{1})$, since $\varphi$ is unital, by \ref{charactersUnital}.

Thus $x-\lambda\cdot \vec{1}\in\ker(\varphi)$, which is a proper ideal (since $\varphi \neq \constant{0}$). So $x-\lambda\cdot \vec{1}\notin\GL(A)$ by \ref{nonInvertibleGeneratedIdeals} and $\lambda \in\spec(x)$.

Suppose $\lambda \in \spec(x)$. Because $x-\lambda\cdot \vec{1}$ is non-invertible, the ideal generated by it is proper, by \ref{nonInvertibleGeneratedIdeals}, and $x-\lambda\cdot \vec{1}$ lies in a maximal ideal, by \ref{idealLatticeCoatomic}. By \ref{characterMaximalIdealsComplex}, this means $x-\lambda\cdot \vec{1} \in \ker(\varphi)$ for some $\varphi\in\hat{A}$ and then $\lambda = \varphi(x)$.
\end{proof}
\begin{corollary}
Let $A$ be a unital complex commutative Banach algebra and $x\in A$. Then
\[ \norm{\evalMap_{x}} = \spr(x) \leq \norm{x} \]
\end{corollary}
\begin{proof}
For all $x\in A$:
\[ \norm{\evalMap_x} = \sup_{\varphi\in\hat{A}}\left( \frac{|\varphi(x)|}{\norm{\varphi}} \right) = \sup_{\varphi\in\hat{A}}(|\varphi(x)|) = \spr(x) \leq \norm{x} \]
where we have used that $\norm{\varphi} = 1$ by \ref{charactersUnital}, $\spec(x) = \setbuilder{|\varphi(x)|}{\varphi\in\hat{A}}$ by \ref{spectrumFromSpectrum} and the inequality is from \ref{spectrumCompact}.
\end{proof}
\begin{corollary}
Let $A$ be a unital complex commutative Banach algebra. The Gelfand transform is a unital, norm-contractive Banach algebra homomorphism.
\end{corollary}
\begin{proof}
Algebra homomorphism is given by \ref{GelfandTransformHomomorphism}. Norm-contractiveness (and thus continuity) follows from the previous corollary.

To see that $\evalMap_{-}$ is unital, note that $\evalMap_{\vec{1}}(\varphi) = 1$ by for all $x\in A$ and $\varphi\in \hat{A}$ by \ref{charactersUnital}. Thus $\evalMap_{\vec{1}} = \constant{1}$, which is the unit of $\cont(\hat{A})$.
\end{proof}

\subsection{The commutant and bicommutant}
\begin{lemma} \label{commutantBanachSubalgebra}
Let $A$ be a Banach algebra and $B\subseteq A$ a subset. Then
\begin{enumerate}
\item $\comm{B}$ is a Banach subalgebra;
\item if $A$ is unital, then $\vec{1}\in \comm{B}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) TODO We need to show: closure under addition, multiplication and scalar multiplication. Also completeness of $\comm{B}$, which follows since $\comm{B}$ is closed, by continuity of $[x,-]$, for all $x\in B$.

Most of this can presumably be offloaded to general theory, but a bit scattered at the moment.

(2) Immediate, since $\vec{1}$ commutes with all elements of $A$ and thus, in particular, with all elements of $B$.
\end{proof}

\begin{lemma}
Let $A$ be a Banach algebra and $B\subseteq A$ a subset. If the elements of $B$ commute pairwise, then $\bicomm{B}$ is commutative.
\end{lemma}
\begin{proof}
The commutativity of $B$ is expressed as $B\subseteq \comm{B}$. Applying the polar map twice gives $\bicomm{B}\subseteq \comm{\big(\bicomm{B}\big)}$. Thus $\bicomm{B}$ is commutative.
\end{proof}

\begin{proposition}
Let $A$ be a unital Banach algebra, $B\subseteq A$ a subset and $x\in \comm{B}$. Then $\spec_A(x) = \spec_{\comm{B}}(x)$.
\end{proposition}
\begin{proof}
It is enough to prove that $u\in \comm{B}$ has an inverse in $\comm{B}$ iff $u$ has an inverse in $A$. The direction $\Leftarrow$ is immediate.

Assume $u\in \comm{B}$ has an inverse $u^{-1}\in A$. For all $b\in B$, we have $ub = bu$. Multiplying by $u^{-1}$ on both the left and the right, we have $bu^{-1} = u^{-1}b$. This implies $u^{-1}\in \comm{B}$.
\end{proof}
\begin{corollary} \label{spectrumInCommutativeAlgebra}
Let $A$ be a unital Banach algebra and suppose that $B\subset A$ is a set of pairwise commuting elements. Then there exists a unital commutative Banach subalgebra $C$ such that $B\subset C\subset A$ and
\[ \spec_A(x) = \spec_C(x) \qquad \text{for all $x\in B$.} \]
\end{corollary}

\begin{proposition} \label{spectrumAlgebraicOperationsOnCommutingElements}
Let $A$ be a unital Banach algebra and $x,y\in A$ be commuting elements. Then
\begin{enumerate}
\item $\spec(x+y) \subseteq \spec(x) + \spec(y)$;
\item $\spec(xy) \subseteq \spec(x) \cdot \spec(y)$.
\end{enumerate}
\end{proposition}
\begin{proof}
By \ref{spectrumInCommutativeAlgebra}, we can compute all the spectra in a commutative Banach algebra $C$. We then calculate, using \ref{spectrumFromSpectrum},
\begin{align*}
\spec(x+y) &= \setbuilder{\varphi(x+y)}{\varphi\in \hat{C}} \\
&= \setbuilder{\varphi(x)+\varphi(y)}{\varphi\in \hat{C}} \\
&\subseteq \setbuilder{\varphi(x)}{\varphi\in \hat{C}} + \setbuilder{\varphi(y)}{\varphi\in \hat{C}} \\
&= \spec(x) + \spec(y),
\end{align*}
and
\begin{align*}
\spec(xy) &= \setbuilder{\varphi(xy)}{\varphi\in \hat{C}} \\
&= \setbuilder{\varphi(x)\varphi(y)}{\varphi\in \hat{C}} \\
&\subseteq \setbuilder{\varphi(x)}{\varphi\in \hat{C}} \cdot \setbuilder{\varphi(y)}{\varphi\in \hat{C}} \\
&= \spec(x) \cdot \spec(y).
\end{align*}
\end{proof}

\subsection{Holomorphic functional calculus}
ISem.

\url{https://noncommutativeanalysis.wordpress.com/2014/01/02/advanced-analysis-notes-18-the-holomorphic-functional-calculus-i/}.

\url{https://noncommutativeanalysis.wordpress.com/2014/01/05/advanced-analysis-notes-19-the-holomorphic-functional-calculus-ii-definition-and-basic-properties/#more-2773}.

Works for all elements, not just normal ones.

\begin{example}
Power series are not satisfactory, since a holomorphic function may have different power series expansions at different points. When defining a functional calculus, we need to take the wole of the spectrum of each element into account. We cannot join together different power series at different points in the spectrum.

For example, take $a = \begin{pmatrix}
2 & 0 \\ 0 & 0
\end{pmatrix}$ and $f(z) = (1-z)^{-1}$. Then $f(z)$ is holomorphic and
\begin{itemize}
\item $f(z) = \sum_{n\in\N}z^n$ around $0$;
\item $f(z) = \sum_{n\in\N}-(2 - z)^n$ around $2$.
\end{itemize}
Since $\spec(a) = \{0,2\}$, we somehow need to combine these two series. Plugging $a$ into either yields a non-convergent series.
\end{example}

\subsubsection{Analyticity}
\begin{definition}
Let $U\subseteq \C$ be an open set, $X$ a Banach space and $f: U\to X$ a function. Then
\begin{itemize}
\item $f$ is called \udef{weakly analytic} if $\rho\circ f: U\to \C$ is analytic for all $\rho\in \dual{X}$;
\item $f$ is called \udef{strongly analytic} if, for all $z_0 \in U$, the limit
\[ \od{f}{z}\Big|_{z_0} = \lim_{z\to z_0} \frac{f(z) - f(z_0)}{z-z_0} \]
exists in the norm convergence.
\end{itemize}
\end{definition}

\begin{lemma}
Let $U\subseteq \C$ be an open set, $X$ a Banach space and $f: U\to X$ a function. If $f$ is strongly analytic, then $f$ is continuous and weakly analytic.
\end{lemma}
\begin{proof}
Suppose $f$ is strongly analytic. Then
\[ \lim_{z\to z_0} f(z) - f(z_0) =  \lim_{z\to z_0}\frac{f(z) - f(z_0)}{z-z_0}\cdot (z-z_0) = \od{f}{z}\Big|_{z_0} \big(\lim_{z\to z_0}z-z_0\big) = \od{f}{z}\Big|_{z_0}\cdot 0 = 0, \]
so $\lim_{z\to z_0} f(z) = f(z_0)$, which means that $f$ is continuous.

Take $\rho\in \dual{X}$. Then
\begin{align*}
\dod{\rho\circ f}{z}\Big|_{z_0} &= \lim_{z\to z_0} \frac{(\rho \circ f)(z) - (\rho \circ f)(z_0)}{z-z_0} \\
&= \lim_{z\to z_0} \rho\Big(\frac{f(z) - f(z_0)}{z-z_0}\Big) \\
&= \rho\Big(\lim_{z\to z_0}\frac{f(z) - f(z_0)}{z-z_0}\Big) \\
&= \rho\Big(\dod{f}{z}\Big|_{z_0}\Big).
\end{align*}.
\end{proof}

\begin{proposition} \label{weakAnalyticityConsequences}
Let $U\subseteq \C$ be an open set, $X$ a Banach space, $f: U\to X$ a weakly analytic function and $\Gamma$ a Jordan curve in $U$ whose interior is also in $U$. Then
\begin{enumerate}
\item $f$ is continuous;
\item $\oint_{\Gamma}f(z)\diff{z} = 0$ and $f(w) = \frac{1}{2\pi i}\oint_{\Gamma}\frac{f(z)}{z-w}\diff{z}$ for all $w$ in the interior of $\Gamma$;
\item $f$ is strongly analytic.
\end{enumerate}
\end{proposition}
TODO integrals well-defined because $f$ is uniformly continuous on $\Gamma$.
\begin{proof}
(1) Pick $z_0\in U$ and $r>0$ small enough such that $\ball(z_0, r)\subseteq U$. Take arbitrary $\rho\in\dual{X}$. Since $\rho\circ f$ is analytic, the function
\[ z\mapsto \frac{(\rho\circ f)(z) - (\rho\circ f)(z_0)}{z-z_0} = \rho\Big(\frac{f(z) - f(z_0)}{z-z_0}\Big) \]
is continuous on $\ball{z_0, r}$ and thus bounded on $\ball{z_0, r}$ by the extreme value theorem \ref{extremeValueTheorem}. 

This means that the set
\[ \setbuilder[\Big]{\frac{f(z) - f(z_0)}{z-z_0}}{z\in\ball(z_0, r)} \]
is weakly bounded, and thus bounded by \ref{weaklyBoundedIffBounded}. Thus there exists $M \geq 0$ such that
\[ \norm{f(z) - f(z_0)} \leq M(z-z_0) \]
for all $z\in\ball(z_0,r)$. This implies that $f$ is continuous at $z_0$.

(2) For all $\rho\in\dual{X}$, we have
\[ \rho\Big(\oint_{\Gamma}f\diff{z}\Big) = \oint_\Gamma \rho\circ f \diff{z} = 0, \]
by \ref{boundedOperatorUnderIntegral} and \ref{CauchyTheorem}. By the Hahn-Banach theorem, \ref{locallyConvexDualPair}, we have $\oint_{\Gamma}f\diff{z} = 0$.

Again, take arbitrary $\rho\in\dual{X}$. Then
\begin{align*}
\rho\Big(f(w) - \frac{1}{2\pi i}\oint_{\Gamma}\frac{f(z)}{z-w}\diff{z}\Big) &= (\rho\circ f)(w) - \frac{1}{2\pi i}\oint_{\Gamma}\frac{(\rho \circ f)(z)}{z-w}\diff{z} \\
&= \frac{1}{2\pi i}\oint_{\Gamma}\frac{(\rho \circ f)(z)}{z-w}\diff{z} - \frac{1}{2\pi i}\oint_{\Gamma}\frac{(\rho \circ f)(z)}{z-w}\diff{z} = 0,
\end{align*}
by \ref{boundedOperatorUnderIntegral} and \ref{CauchyIntergralFormula}. By the Hahn-Banach theorem, \ref{locallyConvexDualPair}, we have $f(w) = \frac{1}{2\pi i}\oint_{\Gamma}\frac{f(z)}{z-w}\diff{z}$.

(3) Take $z_0\in U$ and $r>0$ such that $\cball(z_0, r)\subseteq U$. By \ref{continuityExpandedDomain}, we can restict our attention to $\ball(z_0, r)$. Take $z\in \ball(z_0, r)$. Then, using point (2) and the resolvent identity \ref{firstResolventIdentity}, we have
\begin{align*}
\frac{f(z)- f(z_0)}{z-z_0} &= \frac{1}{2\pi i (z-z_0)}\Big(\oint_{|z'- z_0| = r}\frac{f(z')}{z'-z}\diff{z'} - \oint_{|z'- z_0| = r}\frac{f(z')}{z'-z_0}\diff{z'}\Big) \\
&= \frac{1}{2\pi i (z-z_0)}\oint_{|z'- z_0| = r}f(z')\Big(\frac{(z'-z)^{-1} - (z'-z_0)^{-1}}{z-z_0}\Big)\diff{z'} \\
&= \frac{1}{2\pi i (z-z_0)}\oint_{|z'- z_0| = r}f(z')(z'-z)^{-1}(z'-z_0)^{-1}\diff{z'} \\
&= \frac{1}{2\pi i}\oint_{|z'- z_0| = r}f(z')\Big(\frac{1}{(z'-z_0)^2} - \frac{1}{(z'-z_0)^2} + \frac{1}{(z'-z)(z'-z_0)}\Big)\diff{z'} \\
&= \frac{1}{2\pi i}\oint_{|z'- z_0| = r}f(z')\Big(\frac{1}{(z'-z_0)^2} + \frac{(z'-z_0)-(z'-z)}{(z'-z_0)^2(z'-z)}\Big)\diff{z'} \\
&= \frac{1}{2\pi i}\oint_{|z'- z_0| = r}f(z')\Big(\frac{1}{(z'-z_0)^2} + \frac{z-z_0}{(z'-z_0)^2(z'-z)}\Big)\diff{z'} \\
&= \frac{1}{2\pi i}\oint_{|z'- z_0| = r}\frac{f(z')}{(z'-z_0)^2}\diff{z'} + \frac{z-z_0}{2\pi i}\oint_{|z'- z_0| = r}\frac{f(z')}{(z'-z_0)^2(z'-z)}\diff{z'}.
\end{align*}
The first part is independent of $z$. For the second part, the integral is bounded as $z\to z_0$ (since $z'$ stays $r$ away from $z_0$ and $z$ goes to $z_0$). Since the prefactor goes to $0$, this means that the second term goes to zero. Thus the limit $\lim_{z\to z_0}\frac{f(z)-f(z_0)}{z-z_0}$ exists and is equal to $\frac{1}{2\pi i}\oint_{|z'- z_0| = r}\frac{f(z')}{(z'-z_0)^2}\diff{z'}$.
\end{proof}

\subsubsection{Holomorphic functional calculus}

\begin{lemma}
Let $K\subseteq \C$ be a compact set and $U$ an open set containing $K$. Then $K$ lies in the union of interiors of a finite set of simple Jordan curves.
\end{lemma}
\begin{proof}
Essentially by compactness. TODO smooth interpolation of curves.
\end{proof}

\begin{theorem}[Holomorphic functional calculus] 
\label{holomorphicFunctionalCalculus} \label{holomorphicSpectralMapping}
Let $A$ be a unital Banach algebra and $x\in A$. Consider the function
\[ \Phi_x: \holomorphic(\spec(x),\C) \to A: f\mapsto f(x)\defeq \frac{1}{2\pi i}\oint_\Gamma f(z)R_x(z)\diff{z}. \]
Here $\Gamma$ is any finite union of simple Jordan curves that contains $\spec(x)$ such that $f$ is holomorphic in a region that contains $\Gamma$ and its interior. Then
\begin{enumerate}
\item $\Phi_x$ is well-defined: it does not depend on the particular curve $\Gamma$;
\item $\Phi_x$ is an algebra homomorphism;
\item $\Phi_x$ is continuous if $\holomorphic(\spec(x),\C)$ is equipped with continuous convergence;
\item $\Phi_x(\id_\C) = x$ and $\Phi_x(\underline{1}) = \vec{1}$;
\item if $A$ is unital, then $\spec(\Phi_x(f)) = f^\imf\big(\spec(x)\big)$.
\end{enumerate}
\end{theorem}
From points (2) and (3), it follows that $\Phi_x(p) = p(x)$ for any polynomial $p\in \C[X]$.

Note that equipping $\holomorphic(\spec(x),\C)$ with continuous convergence is not the same as uniform convergence, because functions in $\holomorphic(\spec(x),\C)$ may have unbounded domain, even though $\spec(x)$ is compact.
\begin{proof}
(1) TODO: from \ref{weakAnalyticityConsequences}.

(2) Take $f,g \in \holomorphic(\spec(x),\C)$ and $\lambda\in \C$. Then $\spec(x) \subseteq \dom(f)\cap \dom(g)$ and we can find a curve $\Gamma$ in $\dom(f)\cap \dom(g)$. By linearity of the integral, we have
\begin{align*}
\Phi_x\big(f+\lambda g\big) &= \frac{1}{2\pi i}\oint_\Gamma \big(f(z)+\lambda g(z)\big)R_x(z)\diff{z} \\
&= \frac{1}{2\pi i}\oint_\Gamma f(z)R_x(z)\diff{z} + \frac{\lambda}{2\pi i} \oint_\Gamma g(z)R_x(z)\diff{z} \\
&= \Phi_x(f) + \lambda \Phi_x(g).
\end{align*}
To show multiplicativity, take two curves $\Gamma_1, \Gamma_2$ in $\dom(f)\cap \dom(g)$ such that $\Gamma_2$ is in the interior of $\Gamma_1$ and $\spec(x)$ is in the interior of $\Gamma_2$. Then, using the first resolvent identity \ref{firstResolventIdentity}, we have
\begin{align*}
\Phi_x(f)\Phi_x(g) &= \Big(\frac{1}{2\pi i}\Big)^2\oint_{\Gamma_1}f(z)R_x(z)\diff{z}\oint_{\Gamma_2}g(z')R_x(z')\diff{z'} \\
&= \Big(\frac{1}{2\pi i}\Big)^2\oint_{\Gamma_1}\oint_{\Gamma_2}f(z)g(z')R_x(z)R_x(z')\diff{z'}\diff{z} \\
&= \Big(\frac{1}{2\pi i}\Big)^2\oint_{\Gamma_1}\oint_{\Gamma_2}f(z)g(z')\Big(\frac{R_x(z) - R_x(z')}{z'- z}\Big)\diff{z'}\diff{z} \\
&= \Big(\frac{1}{2\pi i}\Big)^2\oint_{\Gamma_1}\oint_{\Gamma_2}f(z)g(z')\frac{R_x(z)}{z'- z}\diff{z'}\diff{z} + \oint_{\Gamma_1}\oint_{\Gamma_2}f(z)g(z')\frac{R_x(z')}{z- z'}\diff{z'}\diff{z} \\
&= \frac{1}{2\pi i}\oint_{\Gamma_2}\Big(\frac{1}{2\pi i}\oint_{\Gamma_1}\frac{f(z)}{z- z'}\diff{z}\Big)g(z')R_x(z')\diff{z'} + \frac{1}{2\pi i}\oint_{\Gamma_1}f(z)R_x(z)\Big(\frac{1}{2\pi i}\oint_{\Gamma_2}\frac{g(z')}{z'- z}\diff{z'}\Big)\diff{z},
\end{align*}
where the order of integration has been swapped in the second integral using Fubini's theorem (TODO ref for vector-valued functions).

We have $\frac{1}{2\pi i}\oint_{\Gamma_1}\frac{f(z)}{z- z'}\diff{z} = f(z')$ for all $z'\in \Gamma_2$ by Cauchy's integral formula \ref{CauchyIntergralFormula}, since all these $z'$ lie inside $\Gamma_1$. We have $\oint_{\Gamma_2}\frac{g(z')}{z'- z}\diff{z'} = 0$ for all $z\in \Gamma_1$ by Cauchy's theorem \ref{CauchyTheorem}, since all these $z$ lie outside $\Gamma_2$.
Thus
\[ \Phi_x(f)\Phi_x(g) = \frac{1}{2\pi i}\oint_{\Gamma_2}f(z')g(z')R_x(z')\diff{z'} = \frac{1}{2\pi i}\oint_{\Gamma_2}(f\cdot g)(z')R_x(z')\diff{z'} = \Phi_x(f\cdot g). \]

(3) By \ref{continuousConvergenceCompactOpen}, \ref{uniformConvergenceOnCompactsIsCompactOpenConvergence}, \ref{spectrumCompact} and \ref{metricUniformConvergence}, the continuous convergence is metric and thus sequential, by \ref{pseudometricSpaceFrechetUrysohn}, so it is enough to check sequential convergence.

TODO: Using uniform metric we can see that the sequence is uniformly bounded and so we can use the bounded convergence theorem (TODO ref).

(4) Take $x\in A$ and let $\Gamma$ be a circle centred at $0$ with radius larger than $\norm{x}$. Then, using \ref{secondNeumannSeries} and (3), we have
\begin{align*}
\Phi_x(\id_{\spec(x)}) &= \frac{1}{2\pi i}\oint_\Gamma zR_x(z)\diff{z} \\
&= \frac{1}{2\pi i}\oint_\Gamma \sum_{n=0}^\infty z\frac{x^n}{z^{n+1}}\diff{z} \\
&= \sum_{n=0}^\infty x^n \frac{1}{2\pi i}\oint_\Gamma  z^{-n}\diff{z} = a,
\end{align*}
where we have used that $\oint_\Gamma  z^{-n}\diff{z} = 2\pi i$ if $n = 1$ and is zero otherwise, by the residue formula \ref{residueFormula}.

Similarly, we calculate
\begin{align*}
\Phi_x(\constant{1}) = \frac{1}{2\pi i}\oint_\Gamma R_x(z)\diff{z} = \sum_{n=0}^\infty x^n \frac{1}{2\pi i}\oint_\Gamma  z^{-n-1}\diff{z} = \vec{1}.
\end{align*}

(5) First take $\lambda\in\spec(x)$. We need to show that $f(\lambda)\in \spec(\Phi_x(f))$, so $f(\lambda)\vec{1} - \Phi_x(f)$ is not invertible. Now $z\mapsto f(\lambda)- f(z)$ is holomorphic and equals $0$ at $z=\lambda$. Then $f(z) - f(\lambda) = (\lambda - z)g(z)$ for some holomorphic $g$ by \ref{holomorphicZeroFactorisationLemma}. Now, by (2) and (4), we have
\[ f(\lambda)\vec{1} - \Phi_x(f) = \Phi_x\big(f(\lambda)\constant{1} - f\big) = \Phi_x\big((\lambda\constant{1} - \id)\cdot g(z)\big) = (\lambda\vec{1} - x)\Phi_x(g). \]
If $f(\lambda)\vec{1} - \Phi_x(f)$ were invertible, then $\lambda\vec{1} - x$ would be invertible with inverse $\big(f(\lambda)\vec{1} - \Phi_x(f)\big)^{-1}\Phi_x(g)$. Since this is not true (by assumption $\lambda\in\spec(x)$), we have that $f(\lambda)\vec{1} - \Phi_x(f)$ is not invertible.

Now suppose $\mu\notin f^\imf\big(\spec(x)\big)$. Then $\lambda - f(z)$ is non-zero on $\spec(x)$, so $\spec(x) \subseteq f^\preimf(\C\setminus\{0\})$. Also $f^\preimf(\C\setminus\{0\})$ is open by \ref{preimageOpenClosed}. The function $(\constant{\lambda} - f)^{-1}$ is holomorphic on $f^\preimf(\C\setminus\{0\})$ and thus an element of $\holomorphic(\spec(x),\C)$. Then $\Phi_x\big((\constant{\lambda} - f)^{-1}\big)$ is the inverse of $\lambda \vec{1} - \Phi_x(f)$, so $\lambda \notin \spec\big(\Phi_x(f)\big)$.
\end{proof}

Alternative proof for (5) in \url{https://noncommutativeanalysis.wordpress.com/2014/01/05/advanced-analysis-notes-19-the-holomorphic-functional-calculus-ii-definition-and-basic-properties/}

\subsubsection{Riesz eigenprojections}
Holomorphic functional calculus applied to
\[ \chi_{S,\delta}: A\to \{0,1\}: x\mapsto \begin{cases}
1 & d(x,S) \leq \delta \\
0 & \text{otherwise}.
\end{cases} \]

TODO: spectral measure with only disconnected parts in $\sigma$-algebra??

TODO: $P_\Delta$ and $E_\Delta \defeq \im P_\Delta$.

\begin{lemma}
$\spec(T|_{E_\Delta}) = \spec(T)\cap\Delta$.
\end{lemma}

\begin{definition}
We call $\dim E_\lambda$ the \udef{algebraic multiplicity} of $\lambda$.
\end{definition}

\subsubsection{Frobenius covariants}
TODO $P_\lambda$ is a Frobenius covariant. \url{https://en.wikipedia.org/wiki/Frobenius_covariant}

TODO cfr. Lagrange polynomial??

\subsubsection{The exponential}
\begin{definition}
Let $A$ be a Banach algebra and $a\in A$. We define the \udef{exponential} of $a$ by holomorphic functional calculus:
\[ e^a \defeq \Phi_a(\exp) = \frac{1}{2\pi i}\oint_\Gamma e^zR_a(z)\diff{z}. \]
We also denote the exponential by $\exp(a)$.

We call the function $\exp: A\to A: a\mapsto e^a$ the \udef{exponential mapping}.
\end{definition}

\begin{proposition}
Let $A$ be a Banach algebra and $a\in A$. Then
\[ e^a = \sum_{i=0}^\infty \frac{a^i}{i!}. \]
\end{proposition}
\begin{proof}
TODO
\end{proof}
Note that $\exp(0) = \vec{1}$.

\begin{lemma} \label{continuityExp}
The exponential mapping is continuous.
\end{lemma}
\begin{proof}
TODO - see Coleman
\end{proof}

\begin{proposition} \label{factorisationCommutingExponentials}
Let $A$ be a unital Banach algebra and $a,b\in A$. If $a$ and $b$ commute, then
\[ \exp(a+b) = \exp(a)\exp(b). \]
\end{proposition}
\begin{proof}
TODO - Coleman
\end{proof}
\begin{corollary}
Let $a\in A$. Then $\exp(a)\in \GL(A)$ and $\exp(a)^{-1} = \exp(-a)$.
\end{corollary}
\begin{proof}
As $a$ and $-a$ commute, we have $\exp(-a)\exp(a) = \exp(0) = \vec{1}$.
\end{proof}

\begin{lemma}
Let $A$ be a Banach algebra and $a\in A$. Then
\[ \exp(a) = \lim_{n\to\infty} \left(\vec{1} + \frac{a}{n}\right)^n. \]
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{lemma}
Let $A$ be a Banach algebra and $p\in A$ an idempotent. Then
\[ e^p = \vec{1} + (e-1)p. \]
\end{lemma}
\begin{proof}
We calculate
\[ e^p = \vec{1} + \sum_{k=1}^\infty \frac{p^k}{k!} = \vec{1} + \sum_{k=1}^\infty \frac{p}{k!} = \vec{1} + p\sum_{k=1}^\infty \frac{1}{k!} = \vec{1} + (e-1)p. \]
\end{proof}

\begin{lemma}
Let $A$ be a Banach algebra, $a\in A$ and $b\in \GL(A)$. Then $\exp(bab^{-1}) = b\exp(a)b^{-1}$.
\end{lemma}
\begin{proof}
TODO
\end{proof}

TODO: $\exp(a^*) = \exp(a)^*$ is Banach-$*$-algebra.

TODO: correct setting for this:
\begin{proposition}
$\det(e^a) = e^{\Tr(a)}$.
\end{proposition}


\subsection{Jordan decomposition}
\subsubsection{Eigennilpotent}
\begin{definition}
Let $a$ be a finite element in a semisimple Banach algebra and $\lambda\in \spec(a)$. The \udef{eigennilpotent operator} of $a$ at $\lambda$ is defined as
\[ D_{\lambda} \defeq (a-\lambda)P_{\lambda}. \]
\end{definition}
This definition works because we can find a $\delta < d(\lambda, \spec(a)\setminus\{\lambda\})$.

\begin{lemma}
Let $a$ be a finite element in a semisimple Banach algebra and $\lambda\in \spec(a)$. The eigennilpotent operator $D_\lambda$ is nilpotent.
\end{lemma}
\begin{proof}
By spectral mapping \ref{holomorphicSpectralMapping}, $D_\lambda$ is quasinilpotent. Because $a$ is finite, it is nilpotent by \ref{nilpotentQuasinilpotent}.
\end{proof}



\subsubsection{Jordan vectors}
\begin{definition}
Let $V$ be a finite dimensional vector space and $T$ an operator on $V$. A \udef{Jordan vector} of $T$ belonging to the eigenvalue $\lambda$ is a vector $x\in V$ such that
\[ (\lambda\id_V - T)^kx = 0 \]
for some $k\in \N$. The least such $k$ is called the \udef{degree} of $x$ and is denoted $\deg_J(x)$.
\end{definition}
Eigenvectors are Jordan vectors of degree $1$.

\begin{proposition}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$ $\lambda\in\spec(T)$ and $x\in V$. Then $x$ is a Jordan vector of $T$ belonging to the eigenvalue $\lambda$ \textup{if and only if} $x\in E_\lambda$.
\end{proposition}
\begin{proof}
Let $x\in E_\lambda$. Then $x = P_\lambda x$ and thus
\[ (\lambda\id_V - T)^kx = (\lambda\id_V - T)^kP_\lambda x = \big((\lambda\id_V - T)P_\lambda\big)^k x = D_\lambda^k x, \]
which is zero for some $k$ because $D_\lambda$ is nilpotent.

Conversely, assume $x$ is a Jordan vector of $T$ belonging to the eigenvalue $\lambda$. We can write $x = x_1+x_2 \in E_{\lambda}\oplus E_{\C\setminus\{\lambda\}}$.
Then (because $E_\lambda$ is reducing for $T-\lambda\id_V$)
\[ 0 = (\lambda\id_V - T)^kx = (\lambda\id_V - T)^kx_1 + (\lambda\id_V - T)^kx_2 \in E_{\lambda}\oplus E_{\C\setminus\{\lambda\}} \]
Thus we have $(\lambda\id_V - T)^kx_1 = 0$ and $(\lambda\id_V - T)^kx_2 = 0$ separately.
Now $T-\lambda\id_V$ is invertible on $E_{\C\setminus\{\lambda\}}$, so $x_2 = 0$ (TODO ref). This means that $x = x_1 \in E_\lambda$.
\end{proof}

\begin{definition}
Let $m = \deg_N(D_\lambda)$. Then we have
\[ \{0\} \subsetneq \ker(\lambda\id_V - T) \subsetneq \ker(\lambda\id_V - T)^2 \subsetneq \ldots \subsetneq \ker(\lambda\id_V - T)^{m-1} \subsetneq \ker(\lambda\id_V - T)^m = V. \]
We define $E^k_\lambda \defeq \ker(\lambda\id_V - T)^k$. In particular
\begin{itemize}
\item $E^1_\lambda$ is the \udef{geometric eigenspace};
\item $E^{m-1}_\lambda$ is the \udef{algebraic eigenspace}.
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$, $\lambda\in\spec(T)$ and $x\in E_\lambda$. Then
\begin{enumerate}
\item $1 \leq \dim\ker(\lambda\id_V - T) \leq \dim E_\lambda$;
\item $1 \leq \deg_J(x) \leq \dim_E\lambda$.
\end{enumerate}
\end{lemma}
The lemma says the geometric multiplicity is smaller than the algebraic multiplicity.
\begin{proof}
Every eigenvector is a Jordan vector, so $\ker(\lambda\id_V - T) \subseteq E_\lambda$.

For all $k\in\N$ smaller then the degree of $x$, $(\lambda\id_V - T)^kx$ is a Jordan vector and thus in $E_\lambda$. TODO all $(\lambda\id_V - T)^kx$ are linearly independent (like in \ref{nilpotentQuasinilpotent})
\end{proof}

\begin{definition}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$ and $\lambda\in\spec(T)$. The eigenvalue $\lambda$ is called
\begin{itemize}
    \item \udef{simple} if the algebraic multiplicity is $1$;
    \item \udef{semisimple} if every Jordan vector in $E_\lambda$ has degree $1$;
    \item \udef{prime} if the geometric multiplicity is $1$.
\end{itemize}
If all eigenvalues of $T$ are semisimple, then $T$ is called a \udef{diagonal operator}.
\end{definition}

\begin{lemma}
An operator $T$ is diagonal iff $T$ is of the form $\sum_j a_jP_j$, where $a_j\in \F$ and $P_j$ are projectors that commute pairwise.
\end{lemma}

\subsubsection{Characteristic polynomial and equation}
\begin{definition}
Let $V$ be a finite dimensional vector space and $T$ an operator on $V$. The \udef{characteristic polynomial} $p_T(x)$ of $T$ is the polynomial
\[ p_T(x) \defeq \det(x\id_V - T). \]
\end{definition}

\begin{proposition}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$ and $\spec(T) = \{\lambda_j\}_{j=1}^r$. Then
\[ p_T(x) = \prod_{j=1}^r(x - \lambda_j)^{\dim E_{\lambda_j}}. \]
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
A number $\lambda\in \C$ is an eigenvalue of $T$ \textup{if and only if} it is a root of $p_T(x)$.
\end{corollary}

\begin{definition}
The equation $p_T(x) = 0$ is the \udef{characteristic equation} of $T$.
\end{definition}

\subsubsection{Spectral representation}
\begin{proposition}
Let $V$ be a finite dimensional complex vector space and $T$ an operator on $V$. There exists a unique decomposition $T = S + D$ such that
\begin{itemize}
\item $S$ is diagonal;
\item $D$ is nilpotent;
\item $SD = DS$.
\end{itemize}
If $\spec(T) = \{\lambda_j\}_{j=1}^r$, this decomposition is given by
\[ T = \sum_{j=1}^r \lambda_r P_{\lambda_r} + \sum_{j=1}^r D_{\lambda_r}. \]
\end{proposition}

\subsubsection{Partial fraction decomposition of the resolvent}
For any operator $T$ on a vector space $V$ with eigenvalue $\lambda_0$, the resolvent $R_T(\lambda)$ has a pole at $\lambda_0$.

\begin{proposition}
Let $T$ be an operator on a finite dimensional vector space $V$ and $\lambda_0\in\spec(T)$. Then the Laurent expansion of $R_T(\lambda)$ around $\lambda_0$ is of the form
\[ R_T(\lambda) = \frac{P_0}{\lambda-\lambda_0} + \sum_{n=1}^{\deg_N(D_0)-1}\frac{D_0^{n}}{(\lambda - \lambda_0)^{n+1}} + \sum_{n=0}^\infty(-1)^n S_0^{n+1}(\lambda - \lambda_0)^n, \]
where $P_0\defeq P_{\lambda_0}, D_0\defeq D_{\lambda_0}$ and $S_0$ is some fixed operator.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{definition}
The holomorphic part of the Laurent expansion of $R_T(\lambda)$ at $\lambda_0$ is called the \udef{reduced resolvent} of $T$ w.r.t. $\lambda_0$:
\[ S_{T,\lambda_0}(\lambda) \defeq \sum_{n=0}^\infty(-1)^n S_0^{n+1}(\lambda - \lambda_0)^n = R_T(\lambda) - \left(\frac{P_0}{\lambda-\lambda_0} + \sum_{n=1}^{\deg_N(D_0)-1}\frac{D_0^{n}}{(\lambda - \lambda_0)^{n+1}}\right). \]
\end{definition}

\begin{proposition}
Let $T$ be an operator on a finite dimensional vector space $V$ and $\lambda_0\in\spec(T)$. Then
\[ R_{T|_{(\id_V-P_0)}}(\lambda) = S_{T,\lambda_0}|_{\id_V-P_0}(\lambda). \]
\end{proposition}

\begin{proposition}
Let $T$ be an operator on a finite dimensional vector space $V$ with $\spec(T) = \{\lambda_j\}_{j=1}^r$. The partial fraction decomposition of $R_T(\lambda)$ is given by
\[ R_T(\lambda) = \sum_{j=1}^r\left(\frac{P_{\lambda_j}}{\lambda - \lambda_j} +\sum_{n=1}^{\deg_N(D_{\lambda_j})-1}\frac{D_{\lambda_j}^n}{(\lambda - \lambda_j)^{n+1}}\right). \]
The partial fraction decomposition of $S_{T,\lambda_k}(\lambda)$ is given by
\[ S_{T,\lambda_k}(\lambda) = \sum_{\substack{j=1 \\ j\neq k}}^r\left(\frac{P_{\lambda_j}}{\lambda - \lambda_j} +\sum_{n=1}^{\deg_N(D_{\lambda_j})-1}\frac{D_{\lambda_j}^n}{(\lambda - \lambda_j)^{n+1}}\right). \]
\end{proposition}
\begin{proof}
The poles of $R_T(\lambda)$ are exactly the eigenvalues of $T$. There are finitely many of them, so we can use partial fraction decomposition, \ref{partialFractionDecomposition}. We just need to show that the holomorphic part is zero. For that we note that $\lim_{\lambda \to \infty} R_T(\lambda) = 0$ and all principal parts tend to $0$ at infinity as well. Thus the holomorphic part also tends to $0$, making it bounded. By Liouville's theorem, \ref{liouvilleTheoremAnalysis}, we get that it is identically zero.
\end{proof}
\begin{corollary}[Sylvester-Lagrange formula]
Let $f$ be a holomorphic function on an open set that contains $\spec(T)$. Then
\[ f(T) = \sum_{j=1}^r\left(f(\lambda_j)P_{\lambda_j} +\sum_{n=1}^{\deg_N(D_{\lambda_j})-1}\frac{f^{(n)}(\lambda_j)D_{\lambda_j}^n}{n!}\right). \] 
\end{corollary}
\begin{proof}
We have
\[ f(T) = \oint_\Gamma f(\lambda)R_T(\lambda)\diff{\lambda} = 2\pi i\sum_{j=1}^r \Res_{\lambda_j}f(\lambda)R_T(\lambda) \]
by the residue theorem (TODO ref for operators).
\end{proof}
\begin{corollary}[Cayley-Hamilton]
Let $p_T(x)$ be the characteristic polynomial of $T$. Then $p_T(T) = 0$.
\end{corollary}
\begin{proof}
Since $p_T(x) = \prod_{j=1}^r(x - \lambda_j)^{\dim E_{\lambda_j}}$ and $\dim E_{\lambda_j} \geq \deg_N(D_{\lambda_j})$, we see that $p_T(\lambda)R_T(\lambda)$ has no poles and is holomorphic, meaning that $oint_\Gamma f(\lambda)R_T(\lambda)\diff{\lambda} = 0$ by Cauchy's theorem (TODO ref for operators).
\end{proof}

\subsubsection{Normal operators}


\begin{proposition}
If $T$ is a normal operator, then $P_\lambda = P^*_\lambda$ and $D_\lambda = D^*_\lambda = 0$.
\end{proposition}
This means normal operators are diagonalisable.
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Let $V$ be a finite dimensional complex vector space and $T$ a normal operator
on $V$ with $\spec(T) = \{\lambda_j\}_{j=1}^r$.
\begin{enumerate}
\item We have the spectral decompositions
\[ T = \sum_{j=1}^r \lambda_r P_{\lambda_r} \qquad\text{and}\qquad T^* = \sum_{j=1}^r \overline{\lambda_r} P_{\lambda_r}. \]
\item We have
\[ R_T(\lambda) = \sum_{j = 1}^r \frac{P_{\lambda_j}}{\lambda - \lambda_j} \qquad \text{and} \qquad S_{T,\lambda_k}(\lambda) = \sum_{\substack{j = 1 \\ j\neq k}}^r \frac{P_{\lambda_j}}{\lambda - \lambda_j} \]
\end{enumerate}
\end{corollary}

\subsubsection{Jordan decomposition}
TODO matrix representation + matrix representation of Lagrange-Sylvester. See Baumgrtel



\section{Tensor products}
\subsection{Algebraic tensor product}
\begin{proposition}
Let $A$ be a Banach algebra, $a\in A$ and $p\in A$ an idempotent. Then
\begin{enumerate}
\item $e^{p\otimes a} = (\vec{1}-p)\otimes \vec{1} + p\otimes e^a$;
\item $e^{a\otimes p} = \vec{1}\otimes (\vec{1}-p) + e^a\otimes p$.
\end{enumerate}
In particular, $e^{\vec{1}\otimes a} = \vec{1}\otimes e^a$ and $e^{a\otimes \vec{1}} = e^a\otimes \vec{1}$.
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
e^{p\otimes a} &= \sum_{k=0}^\infty \frac{(p\otimes a)^k}{!k} \\
&= \sum_{k=0}^\infty \frac{p^k\otimes a^k}{!k} \\
&= \vec{1}\otimes \vec{1} + \sum_{k=1}^\infty \frac{p\otimes a^k}{!k} \\
&= \vec{1}\otimes \vec{1} + p\otimes\left(\sum_{k=1}^\infty \frac{a^k}{!k}\right) \\
&= \vec{1}\otimes \vec{1} + p\otimes\left(\sum_{k=0}^\infty \frac{a^k}{!k}\right) - p\otimes \vec{1} \\
&= (\vec{1} - p)\otimes \vec{1} + p\otimes e^a,
\end{align*}
where we have used the continuity of $p\otimes -$ (which is bounded by $\norm{p}$).
\end{proof}
\begin{corollary}
Let $A$ be a Banach algebra and $a,b\in A$. Then $e^a\otimes e^b = e^{a\otimes \vec{1} + \vec{1}\otimes b}$.
\end{corollary}
\begin{proof}
We calculate
\[ e^a\otimes e^b = (e^a\otimes \vec{1})(\vec{1}\otimes e^b) = e^{a\otimes \vec{1}}e^{\vec{1}\otimes b} = e^{a\otimes \vec{1} + \vec{1}\otimes b}, \]
using \ref{factorisationCommutingExponentials} with the fact that $a\otimes \vec{1}$ and $\vec{1}\otimes b$ commute.
\end{proof}

