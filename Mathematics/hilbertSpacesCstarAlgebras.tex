\chapter{Inner product and Hilbert spaces}

\chapter{Hilbert spaces}
\begin{definition}
An \udef{inner product} on a vector space $V$ is a function
\[ \inner{\cdot,\cdot}: V\times V \to \mathbb{F}  \]
that has the following properties:
\begin{itemize}[leftmargin=4.5cm]
\item[\textbf{Linearity}] in the \emph{second}\footnote{Some authors take linearity in the first component.} component
\[\inner{v,\lambda_1 w_1 + \lambda_2 w_2} = \lambda_1\inner{v,w_1} + \lambda_2\inner{v,w_2},\]
where $\lambda_1,\lambda_2 \in \mathbb{F}$ and $v,w_1,w_2\in V$.
\item[\textbf{Conjugate symmetry}\footnote{This is for $\mathbb{F} = \C$. For $\mathbb{F} = \R$ this reduces to normal symmetry $\inner{v,w} = \inner{w,v}$.}] $\inner{v,w} = \overline{\inner{w,v}}$ for all $v,w\in V$.
\item[\textbf{Positivity}\footnote{By conjugate symmetry we know that $\inner{v,v}$ is a real number, so this condition makes sense.}] $\inner{v,v} \geq 0$ for all $v\in V$.
\item[\textbf{Definiteness}]$\inner{v,v} = 0$ if and only if $v= 0$.
\end{itemize}
An \udef{inner product space} or \udef{pre-Hilbert space} $(\mathbb{F}, V,+,\inner{\cdot,\cdot})$ is a vector space $(\mathbb{F}, V,+)$ together with an inner product $\inner{\cdot,\cdot}$ on $V$.

\begin{itemize}
\item A real finite-dimensional inner product space is called a \udef{Euclidean space}.
\item A \udef{Hilbert space} is an inner product space that is complete as a metric space.
\end{itemize}
\end{definition}

A finite-dimensional inner product space is automatically a Hilbert space by proposition \ref{finiteDimComplete}.


\section{Inner product spaces}
\begin{lemma}
An inner product over a complex vector space $V$ is anti-linear in the first component.
\end{lemma}

\begin{lemma} \label{nonDegeneracyInnerProduct}
Definiteness implies the inner product on $V$ is non-degenerate:
\[ [\forall u\in V:\inner{u,v} = 0] \implies v = 0. \]
\end{lemma}
The converse is not true.

There are some generalised notions of inner product:
\begin{definition}
Let $V$ be a complex vector space.
\begin{enumerate}
\item A \udef{sesquilinear form} is a function $V\times V\to \C$ that is linear in the second component and anti-linear in the first.
\item A \udef{Hermitian form} is a conjugate symmetric sesquilinear form.
\item A \udef{pre-inner product} is a positive Hermitian form, i.e.\ an inner product without the requirement of definiteness.
\end{enumerate}
\end{definition}

\begin{example}
\begin{enumerate}
\item The \udef{standard inner product} on $\R^n$ is given by
\[ \inner{a,b} = \inner{\begin{bmatrix}
a_1 \\ \vdots \\ a_n
\end{bmatrix},\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix}} = \begin{bmatrix}
a_1 & \hdots & a_n
\end{bmatrix}\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix} = a^\transp b \]
This is also known as the \udef{dot product} $a\cdot b$.
\item The \udef{standard inner product} on $\C^n$ is given by
\[ \inner{a,b} = \inner{\begin{bmatrix}
a_1 \\ \vdots \\ a_n
\end{bmatrix},\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix}} = \begin{bmatrix}
\bar{a}_1 & \hdots & \bar{a}_n
\end{bmatrix}\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix} = \bar{a}^\transp b \]
\item The \udef{Frobenius inner product} on $\C^{m\times n}$ is given by
\[ \inner{A,B}_F =  \Tr(\overline{A}^\transp B) = \overline{\vectorisation_C(A)}^\transp \vectorisation_C(B)\]
\item On the vector space $\mathcal{C}[a,b]$ of continuous real functions on $[a,b]$, we can take the inner product
\[ \inner{f,g} = \int_a^b f(x)\cdot g(x) \diff{x}. \]
\end{enumerate}
\end{example}

\begin{definition}
Two vectors $u,v \in V$ are \udef{orthogonal} if $\inner{u,v} =0$. This is denoted $u\perp v$.
\end{definition}
\begin{lemma} \label{elementaryOrthogonality}
Let $V$ be an inner product space.
\begin{enumerate}
\item $0$ is the only vector orthogonal to itself.
\item $0$ is orthogonal to all $v\in V$;
\item Let $x,y\in V$. If, for all $v\in V$, $\inner{v,x} = \inner{v,y}$, then $x=y$.
\end{enumerate}\end{lemma}
\begin{proof}
The first is a consequence of definiteness, the second a consequence of linearity: $\inner{v,0} = \inner{v,0\cdot0} = 0\inner{v,0} = 0$.

The third is also a consequence of linearity: assume $\forall v\in V: \inner{v,x} = \inner{v,y}$, then $\inner{v,x-y}=0$ and $x-y$ is orthogonal to all $v\in V$ and in particular to $0$. Thus $x-y$ must be zero.
\end{proof}

\begin{proposition}
Every inner product gives rise to a norm, defined by
\[ \norm{\cdot} = \sqrt{\inner{\cdot,\cdot}}. \]
\end{proposition}
\begin{proof}
The only non-trivial part is the triangle inequality. This will be proved later using the Cauchy-Schwarz inequality.
\end{proof}


\begin{lemma}
Let $V$ be an inner product space. Then
\[ \norm{v+w}^2 = \norm{v}^2+\norm{w}^2+2\Re\inner{v,w} \]
\end{lemma}
\begin{lemma} \label{orthogonalDecomposition}
Let $v,w\in V$, with $w\neq 0$. We can decompose $v$ as a multiple of $w$ and a vector $u$ orthogonal to $w$:
\[ v = cw+u = \left(\frac{\inner{v,w}}{\norm{w}^2}\right)w + \left( v- \frac{\inner{v,w}w}{\norm{w}^2} \right). \]
\end{lemma}
\begin{proof}
The only thing to check is $\inner{w, v- \frac{\inner{v,w}w}{\norm{w}^2}} = 0$, which is a simple calculation.
\end{proof}

\subsection{Pythagoras and Cauchy-Schwarz}
\begin{theorem}[Pythagorean theorem] \label{Pythagoras}
Suppose $u\perp v$. Then $\norm{u+v}^2 = \norm{u}^2 + \norm{v}^2$.
\end{theorem}
\begin{proof}
\[ \norm{u+v}^2 = \inner{u+v,u+v} = \inner{u,u}+ \inner{u,v} + \inner{v,u} + \inner{v,v} = \norm{u}^2 + \norm{v}^2. \]
\end{proof}

\begin{theorem}[Cauchy-Schwarz-Bunyakovsky inequality.] \label{CauchySchwarz}
Let $V$ be a vector space with a pre-inner product $\inner{\cdot,\cdot}$. Let $v,w\in V$. Then
\[ |\inner{v,w}|^2\leq \inner{v,v}\cdot\inner{w,w}. \]
Suppose $\inner{\cdot,\cdot}$ is definite (i.e.\ an inner product), then
this is an equality \textup{if and only if} $v$ and $w$ are scalar multiples.
\end{theorem}
This result is also known as the Cauchy-Schwarz inequality, or the CSB inequality.
\begin{proof}
Consider 
\[ \inner{v-\lambda w, v-\lambda w} = \inner{v,v}-\lambda\inner{v,w}-\overline{\lambda}\inner{w,v} + |\lambda|^2 \inner{w,w} \geq 0. \]
Suppose $\inner{v,w}=re^{i\theta}$ (if $\mathbb{F} = \R$, then $\theta=0$ or $\theta = \pi$). The inequality must still hold for all $\lambda$ of the form $te^{-i\theta}$ for some $t\in \R$. The inequality thus becomes
\[ 0\leq \inner{v,v}-te^{-i\theta}re^{i\theta}-te^{i\theta}re^{-i\theta} + t^2 \inner{w,w} = \inner{v,v}-2rt + t^2 \inner{w,w}. \]
On the right we have a quadratic formula in $t$. This may never be negative and the discriminant may therefore not be positive. Calculating the discriminant gives $(2r)^2 - 4\inner{v,v}\inner{w,w}$. Thus
\[ 0\geq r^2 - \inner{v,v}\inner{w,w} = |\inner{v,w}|^2 - \inner{v,v}\inner{w,w}. \]
\end{proof}
In the case of an inner product, there is a simpler proof:
\begin{proof}
Take the decomposition from lemma \ref{orthogonalDecomposition} and apply the Pythagorean theorem to obtain
\[ \norm{v}^2 = \frac{|\inner{v,w}|^2}{\norm{w}^2} + \norm{u}^2 \geq \frac{|\inner{v,w}|^2}{\norm{w}^2}. \]
This also shows the claim about scalar multiples.
\end{proof}
\begin{corollary} \label{innerBoundedFunctionals}
Let $V$ be an inner product space. The functions
\[\inner{v,\cdot}: V\to \mathbb{F}: x\mapsto \inner{v,x} \]
are bounded linear functionals for all $v\in V$.
\end{corollary}
\begin{corollary} \label{preInnerProductCSBZero}
Let $V$ be a vector space with a pre-inner product $\inner{\cdot,\cdot}$. Then
\[ \inner{x,x}=0\lor\inner{y,y}=0 \quad\implies\quad \inner{x,y} = 0. \]
\end{corollary}
\begin{definition}
The Cauchy-Schwarz inequality allows us to define the \udef{angle} $\theta$ between two vectors $v,w$ by
\[ \cos\theta = \frac{\inner{v,w}}{\norm{v}\norm{w}}.\]
\end{definition}
\begin{lemma}
If $v\perp w$, then the angle between them is $\pi/2 + k\pi$.
\end{lemma}

TODO CS special case of HÃ¶lder inequality.

\begin{theorem}[Triangle inequality]
Let $v,w\in V$. Then
\[ \norm{v+w} \leq \norm{v}+\norm{w} \]
This inequality is an equality if and only if one of $u,v$ is a nonnegative multiple of the other. Also
\begin{enumerate}
\item $\big|\norm{v}-\norm{w}\big|\leq \norm{v-w}$;
\item $\big|\norm{v}-\norm{w}\big| \leq \norm{v+w} \leq \norm{v}+\norm{w}$.
\end{enumerate}
\end{theorem}
\begin{proof}
We calculate
\begin{align*}
\norm{v+w}^2 &= \norm{v}^2+\norm{w}^2+2\Re\inner{v,w} \\
&\leq \norm{v}^2+\norm{w}^2+2|\inner{v,w}| \\
&\leq \norm{v}^2+\norm{w}^2+2\norm{v}\norm{w} \\
&= (\norm{v}+\norm{w})^2.
\end{align*}
The other inequalities are the reverse triangle inequalities \ref{reverseTriangleInequality}.
\end{proof}

\subsection{Parallelogram law and polarisation}
\begin{theorem}[Parallelogram law] \label{parallelogramLaw}
Let $V$ be an inner product space and $v,w\in V$. Then
\[ \norm{v+w}^2 + \norm{v-w}^2 = 2 (\norm{v}^2+\norm{w}^2). \]
\end{theorem}
\begin{proof}
We calculate
\begin{align*}
\norm{v+w}^2 + \norm{v-w}^2 = \inner{v+w, v+w}+\inner{v-w,v-w} = 2(\norm{v}^2 + \norm{w}^2).
\end{align*}
\end{proof}
\begin{corollary}[Appolonius' identity] \label{AppoloniusIdentity}
Let $V$ be an inner product space and $x,y,z\in V$. Then
\[ \norm{z-x}^2 + \norm{z-y}^2 = \frac{1}{2}\norm{x-y}^2 + 2\norm*{z-\frac{1}{2}(x+y)}^2. \]
\end{corollary}
\begin{proof}
Apply the parallelogram law to $u = \frac{1}{2}(z-x)$ and $v = \frac{1}{2}(z-y)$.
\end{proof}

\begin{proposition}[Ptolemy's inequality] \label{PtolemyInequality}
Let $V$ be an inner product space. Then the norm satisfies $\forall u,v,w\in V$
\[ \norm{u-v}\;\norm{w} + \norm{v-w}\;\norm{u} \geq \norm{u-w}\;\norm{v}. \]
\end{proposition}

Polarisation identities allow us to recover the inner product from the norm.
\begin{theorem}[Polarisation identities] \label{polarisationIdentities}
\mbox{}
\begin{enumerate}
\item For real inner product spaces, $\mathbb{F} = \R$:
\begin{align*}
\inner{v,w} &= \frac{1}{2}(\norm{v+w}^2 - \norm{v}^2-\norm{w}^2) \\
&= \frac{1}{2}(\norm{v}^2 + \norm{w}^2-\norm{v-w}^2) \\
&= \frac{1}{4}(\norm{v+w}^2 - \norm{v-w}^2) = \frac{1}{4}\sum_{k=0}^1 (-1)^k\norm{v+(-1)^k w}^2.
\end{align*}
\item For complex inner product spaces, $\mathbb{F} = \C$:
\[ \inner{x,y} = \frac{1}{4}\sum_{k=0}^3 i^k\norm{i^k x+y}^2. \]
\item For general sesquilinear forms:
\[ S(x,y) = \frac{1}{4}\sum_{k=0}^3 i^k S(i^k x+y, i^k x+y). \]
\end{enumerate}
\end{theorem}
\begin{corollary} \label{HermitianRealQuadratic}
A sesquilinear form $S$ is Hermitian \textup{if and only if} $S(v,v)$ is real for all $v\in V$.
\end{corollary}
\begin{proof}
The direction $\Rightarrow$ is obvious (conjugate symmetry gives $S(v,v) = \overline{S(v,v)}$). For the other direction, consider
\[ \begin{cases}
S(u+iv, u+iv) = S(u,u) + S(v,v) + i\Big(S(u,v) - S(v,u)\Big) \\
S(u+v, u+v) = S(u,u) + S(v,v) + \Big(S(u,v) + S(v,u)\Big).
\end{cases} \]
Taking the imaginary part gives
\[ \begin{cases}
\Im S(u+iv, u+iv) = 0 = \Re\Big(S(u,v) - S(v,u)\Big) \\
\Im S(u+v, u+v) = 0 = \Im\Big(S(u,v) + S(v,u)\Big).
\end{cases} \]
\end{proof}
\begin{proof}[Alternative proof]
We can also prove the direction $\Leftarrow$ by direct calculation:
\begin{align*}
\overline{S(u,v)} &= \frac{1}{4}\sum^3_{k=0}(-i)^kS(u+i^kv,u+i^kv) & &\text{Using the fact that $S(u+i^kv,u+i^kv)$ is real} \\
&= \frac{1}{4}\sum^3_{k=0}(-i)^kS\Big((i^k)(v+(-i)^ku),(i^k)(v+(-i)^ku)\Big) & &\text{Using $i^k(-i)^k=1$}\\
&= \frac{1}{4}\sum^3_{k=0}(-i)^k\cancel{(i^k)}\cancel{(-i^k)}S(v+(-i)^ku,v+(-i)^ku) & &\text{Using (conjugate) linearity}\\
&= \frac{1}{4}\sum^3_{k=0}i^kS(v+i^ku,v+i^ku) & &\text{Substituting $k\to k+2$}\\
&= S(v,u).
\end{align*}
\end{proof}
Not all norms on vector spaces can be obtained from an inner product. If a norm can be obtained from an inner product, we can use polarisation to recover the inner product. If a norm cannot be obtained from an inner product, the putative inner product suggested by polarisation will turn out not to be an inner product.
\begin{theorem}[Jordan-von Neumann]
Let $\sSet{V,\norm{\cdot}}$ be a real or complex normed space. The following are equivalent:
\begin{enumerate}
\item the polarisation yields an inner product;
\item the parallelogram law holds;
\item Appolonius' identity holds;
\item Ptolemy's inequality holds.
\end{enumerate}
\end{theorem}
TODO! (For other fields??)
\begin{proof}
If polarisation yields an inner product, then we have an inner product space and thus the parallelogram law and Ptolemy's inequality hold by \ref{parallelogramLaw} and \ref{PtolemyInequality}.

The polarisation identities immediately imply
\begin{itemize}
\item Conjugate symmetry:
\[ \inner{x,y} = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ y}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{x+ i^{-k}y}^2 = \frac{1}{4}\sum_{k'=0}^3\overline{i^{k'}}\norm{i^{k'}y+ x}^2 = \overline{\inner{y,x}}. \]
\item Positivity and definiteness:
\[ \inner{x,x} = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ x}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{(i^k+1)x}^2 = \frac{\norm{x}^2}{4}\sum_{k=0}^3 i^k\cdot |1+i^k|^2 = \norm{x}^2 \]
\end{itemize}
Now assume Appolonius' identity, \ref{AppoloniusIdentity}, holds. We need to show linearity in second component.
We can calculate
\begin{align*}
\inner{x,y_1} + \inner{x,y_2} &= \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ y_1}^2 + \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ y_2}^2 = \frac{1}{4}\sum_{k=0}^3i^k\Big(\norm{i^k x+ y_1}^2\frac{1}{4} + \norm{i^k x+ y_2}^2\Big) \\
&= \frac{1}{4}\sum_{k=0}^3i^k\left(\frac{1}{2}\norm{y_1-y_2}^2 + 2\norm{i^k+\frac{y_1+y_2}{2}}\right) \\
&= 2\frac{1}{4}\sum_{k=0}^3i^k\left(\norm{i^k+\frac{y_1+y_2}{2}}\right) \\
&= 2\inner{x,\frac{y_1+ y_2}{2}}.
\end{align*}
Setting $y_2 = 0$ and $y_1 = 2y$, this yields $\inner{x,2y} = 2\inner{x,y}$, which also means that
\[ \inner{x,y_1+y_2} = 2\inner{x,\frac{y_1+ y_2}{2}} = \inner{x,y_1} + \inner{x,y_2}. \]
By induction, we can prove that this putative inner product is linear for all positive rational scalars. By continuity this result extends to all positive scalars.

Finally we check
\begin{align*}
\inner{x,-y} &= \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x - y}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^{k-2} x + y}^2 = -\frac{1}{4}\sum_{k'=0}^3i^{k'}\norm{i^{k'} x + y}^2 = -\inner{x,y} \\
\inner{x,iy} &= \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x + iy}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^{k-1} x + y}^2 = i\frac{1}{4}\sum_{k'=0}^3i^{k'}\norm{i^{k'} x + y}^2 = i\inner{x,y}.
\end{align*}
TODO Ptolemy inequality.
\end{proof}
\begin{corollary}
The space $l^p$ is an inner product space \textup{if and only if} $p=2$.
\end{corollary}
\begin{proof}
The inner product on $l^2$ is defined by $\inner{x_n, y_n} = \sum_{n=1}^\infty \overline{x_n}y_n$.

If $p\neq 2$ we can find a counterexample to the parallelogram law: let $x=(1,1,0,0,\ldots)\in l^p$ and $y = (1,-1,0,0,\ldots)\in l^p$. Then
\[ \norm{x}_p = \norm{y}_p = 2^{1/p} \qquad \text{and} \qquad \norm{x+y} = \norm{x-y} = 2 \]
and the parallelogram law is then not valid if $p\neq 2$.
\end{proof}

\section{Orthogonal and orthonormal sets of vectors}
\subsection{Orthogonal complements}
\begin{definition}
Let $A$ be a subset of an inner product space $V$. The \udef{orthogonal complement} $A^\perp$ of $A$ is the set of vectors in $V$ that are orthogonal to every vector in $A$:
\[ A^\perp = \{ v\in V\;|\; \inner{v,a}=0\; \forall a\in A \}. \]
\end{definition}

\begin{proposition} \label{OrthogonalComplementProperties}
Let $A,B$ be \emph{subsets} of an inner product space $V$.
\begin{enumerate}
\item $A^\perp$ is a subspace of $V$;
\item $A^\perp = \Span(A)^\perp$;
\item $\{0\}^\perp = V$;
\item $V^\perp = \{0\}$;
\item $A\cap A^\perp \subset \{0\}$;
\item If $A\subset B$, then $B^\perp \subset A^\perp$.
\end{enumerate}
\end{proposition}

We can also consider the orthogonal complement of a subspace with respect to another subspace, not the full space.
\begin{definition}
Let $A\subseteq B$ be subsets of an inner product space $V$. The \udef{orthogonal complement} of $A$ with respect to $B$ is the set of vectors in $B$ that are orthogonal to every vector in $A$:
\[ B\ominus A = \{ b\in B\;|\; \inner{b,a}=0\; \forall a\in A \}. \]
\end{definition}

\begin{lemma} \label{ominusSubspace}
Let $V$ be an inner product space and $A\subseteq B \subseteq V$ subsets. Then $B\ominus A = B\cap A^\perp$.
\end{lemma}
\begin{proof}
Take $v\in B\ominus A$. This is equivalent to $v\in B \land \forall u\in A: \inner{v,u} =0$ and thus equivalent to $v\in B \land v\in A^\perp$.
\end{proof}

\begin{proposition} \label{perpUnderIsometry}
Let $V$ be an inner product space, let $A\subseteq B\subseteq V$ be subsets and $T:V\to V$ an isometry. Then
\begin{enumerate}
\item if $A\perp B$, then $T[A]\perp T[B]$;
\item $T[B\ominus A] = T[B]\ominus T[A]$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) If $\inner{a,b}=0$ for all $a\in A, b\in B$, then $\inner{T(a), T(b)} =0$, meaning $T[A]\perp T[B]$.

(2) Take $v\in T[B\ominus A]$. This is equivalent to the existence of $x\in B$ such that $T(x) = v$ and $\inner{x,y}=0$ for all $y\in A$. By isometry $\inner{x,y}=0 \iff \inner{T(x), T(y)}=0$ for all $y\in A$. So, equivalently, $v\in T[B]\ominus T[A]$. 
\end{proof}

\begin{proposition} \label{orthogonalComplementClosed}
Let $A$ be a \emph{subset} of an inner product space $V$. Then $A^\perp$ is closed and $\overline{A}^\perp = A^\perp$. This can be rephrased as
\[ \overline{A}^\perp = \overline{A^\perp} = A^\perp. \]
Also
\[\overline{A} \subset (A^\perp)^\perp. \]
\end{proposition}
\begin{proof}
Let $x\in \overline{A^\perp}$. Then there exists a sequence $(x_i)$ in $A^\perp$ that converges to $x$. For all $a\in A$, the functional $\inner{a,\cdot}:y\mapsto \inner{a,y}$ is bounded (by Cauchy-Schwarz). Thus all these functionals are continuous. Applying any one to the sequence $x_i$ gives a sequence of zeros. Thus $\inner{a,x} = 0$ for all $a\in A$. Thus $x\in A^\perp$ and hence $A^\perp \supset \overline{A^\perp}$ meaning $A^\perp$ is closed.

Now $\overline{A}\supset A$, so $\overline{A}^\perp \subset A^\perp$. For the other inclusion, take an $x\in A^\perp$. Take an arbitrary $y\in \overline{A}$. Then there exists a sequence $(y_i)$ in $A$ that converges to $y$. Apply the bounded functional $\inner{x,\cdot}$ to the sequence $(y_i)$, yielding a sequence of zeros. Thus $\inner{x,y}=0$. Thus $x\in \overline{U}^\perp$.

Finally let $a\in \overline{A}$. Take a sequence $a_i\to a$. Take an arbitrary element $x\in A^\perp$. As before $\inner{x,a} = \lim_i\inner{x,a_i} = 0$. So $a\in (A^\perp)^\perp$.
\end{proof}
\begin{corollary} \label{orthogonalComplementDenseSpace}
Let $A$ be a subset of $V$. If $\Span(A)$ is dense in $V$, then $A^\perp = \{0\}$. 
\end{corollary}
\begin{proof}
\[ A^\perp = \Span(A)^\perp = \overline{\Span(A)}^\perp = V^\perp = \{0\}. \]
\end{proof}
\begin{corollary} \label{perpToDenseSet}
Let $x\in V$. If there exists a dense set $S$ such that $x\perp y$ for all $y\in S$, then $x=0$.
\end{corollary}
\begin{proof}
If such an $S$ exists, then $x\in S^\perp = \overline{S}^\perp = V^\perp = \{0\}$.
\end{proof}
\begin{proposition}
Let $U$ be a finite-dimensional subspace of an inner product space $V$.
\begin{enumerate}
\item $V=U\oplus U^\perp$;
\item $U = (U^\perp)^\perp$.
\end{enumerate}
\end{proposition}
Notice that $V$ may be infinite dimensional!
\begin{proof}
We start with the first point. The sum $U + U^\perp$ is definitely direct, $U\oplus U^\perp$, by proposition \ref{OrthogonalComplementProperties} and the criterion for a direct sum, proposition \ref{directSumCriterion}. Clearly $U\oplus U^\perp\subseteq V$, so we just need to show that $V \subseteq U\oplus U^\perp$.

To that end, take a vector $v\in V$. Let $\{e_i\}_{i=1}^n$ be an orthonormal basis of $U$. We can write
\[ v = \left(v - \sum_{i=1}^n\inner{v,e_i}e_i\right) + \left(\sum_{i=1}^n\inner{v,e_i}e_i\right). \]
The first part is an element of $U^\perp$, the second of $U$, so $v\in U\oplus U^\perp$.

For the second point: any finite-dimensional subspace $U$ is automatically closed, so $U = \overline{U} \subset (U^\perp)^\perp$, by proposition \ref{orthogonalComplementClosed}. For the other inclusion, take $v\in (U^\perp)^\perp$. By the first point, we can write $v = v_1 + v_2$ where $v_1\in U$ and $v_2\in U^\perp$. Because $v\in (U^\perp)^\perp$ and $v_2\in U^\perp$, we must have
\[ 0 = \inner{v_2, v} = \inner{v_2, v_1+v_2} = \inner{v_2, v_1} + \inner{v_2,v_2} = \norm{v_2}. \]
So $v=v_1\in U$.
\end{proof}

TODO all projection results for projection onto finite dim? See proposition before Bessel inequality. In fact better: projection onto summand of direct sum! Put under decompositions.

\begin{proposition} \label{linearDeMorgan}
Let $W_1,W_2$ be subspaces of an inner product space $V$. Then
\[ (W_1+W_2)^\perp = W_1^\perp \cap W_2^\perp. \]
\end{proposition}
\begin{proof}
For a vector $v\in V$,
\[  v\in (W_1+W_2)^\perp \implies \forall x\in W_1\cup W_2: \inner{v,x} = 0 \implies v\in W_1^\perp \cap W_2^\perp \]
and
\begin{align*}
v\in W_1^\perp \cap W_2^\perp &\implies \forall x\in W_1, y\in W_2: \inner{v,x} = 0 = \inner{v,y} \\
&\implies \forall x\in W_1, y\in W_2:\inner{v, x+y} = 0 \implies v\in (W_1+W_2)^\perp.
\end{align*}
\end{proof}

A result dual to proposition \ref{linearDeMorgan} holds for finite-dimensional spaces:
\begin{proposition}
Let $W_1,W_2$ be subspaces a finite-dimensional space $V$. Then
\[ (W_1\cap W_2)^\perp = W_1^\perp + W_2^\perp. \]
\end{proposition}
\begin{proof}
We start by applying proposition \ref{linearDeMorgan} to $W_1^\perp$ and $W_2^\perp$:
\[ (W_1^\perp+W_2^\perp)^\perp = (W_1^\perp)^\perp \cap (W_2^\perp)^\perp = W_1 \cap W_2. \]
Taking the orthogonal complement of both sides gives the result. In infinite dimensions $(W_1^\perp+W_2^\perp)$ is not necessarily closed. 
\end{proof}

\subsection{Orthogonal sets and sequences}
\begin{definition}
\begin{itemize}
\item A set of vectors $D$ is called \udef{orthogonal} if for any two vectors $v,w\in D$, $v\perp w$ \textup{if and only if} $v\neq w$.
\item A set of vectors $D$ is called \udef{orthonormal} if for any two vectors $v,w\in D$,
\[ \inner{v,w} = \begin{cases}
0 & (v\neq w) \\ 1 & (v=w)
\end{cases}. \]
\end{itemize}
In particular an orthonormal set is an orthogonal set of unit vectors.
\end{definition}

\begin{lemma} \label{orthogonalLinearlyIndependent}
Every orthogonal set of vectors is linearly independent.
\end{lemma}
\begin{lemma}
Every subset of an orthogonal (resp. orthonormal) set is orthogonal (resp. orthonormal).
\end{lemma}

\begin{theorem}[Gram-Schmidt procedure]
Every finite set of linearly independent vectors $D = \{v_1,\ldots, v_n\}$ can be transformed into an orthonormal set $D' = \{e_1,\ldots,e_n\}$ with the same number of vectors such that the spans are the same: $\Span(D') = \Span(D)$.
\end{theorem}
\begin{proof}
The procedure goes as follows:
\begin{align*}
e_1 &= \frac{v_1}{\norm{v_1}} \\
e_2 &= \frac{v_2 - \inner{e_1,v_2}e_1}{\norm{v_2 - \inner{e_1,v_2}e_1}} \\
&\hdots \\
e_j &= \frac{v_j - \inner{e_1,v_j}e_1- \ldots - \inner{e_{j-1},v_j}e_{j-1}}{\norm{v_2 - \inner{e_1,v_2}e_1- \ldots - \inner{e_{j-1},v_j}e_{j-1}}} \\
&\hdots
\end{align*}
\end{proof}

If we only need an orthogonal set $\{y_1,\ldots,y_n\}$, not an orthonormal one, we can use the procedure
\[ y_{k+1} = v_{k+1} - \sum_{i=1}^k \frac{\inner{v_{k+1}, y_i}}{\inner{y_i,y_i}}y_i. \]

\begin{lemma} \label{orthogonality}
Let $(\mathbb{F}, V,+,\inner{\cdot,\cdot})$ be an inner product space. Then
\[ \inner{v,w}=0 \qquad \iff \qquad \forall a\in\mathbb{F}:\;\norm{v}\leq\norm{v+aw}.  \]
\end{lemma}
\begin{proof}
The implication $\Rightarrow$ is a consequence of the Pythagorean theorem. For the other implication, assume $\forall a\in\mathbb{F}:\;\norm{v}\leq\norm{v+aw}$. Then
\[ \norm{v}^2 \leq \norm{v-aw}^2 = \norm{v}^2 - 2\Re\inner{v,aw} + \norm{aw}^2 \]
which implies $2\Re\inner{v,aw} \leq a^2\norm{w}^2$. Let $\inner{v,w} = re^{i\theta}$. (If $\mathbb{F} = \R$, then $\theta=0$.) Then in particular the inequality holds for all $a=te^{i\theta}$ with $t\in\R$. This yields
\[ 2\Re(te^{-i\theta}re^{i\theta}) \leq t^2\norm{w}^2 \qquad \text{or}\qquad 2rt\leq t^2\norm{w}^2. \]
Letting $t\geq 0$, we can divide out a $t$: $2r\leq t\norm{w}^2$. Then letting $t\to 0$ gives $r=0$ and thus $\inner{v,w}=0$.
\end{proof}

\begin{proposition}
Let $V$ be an inner product space and $D = \{e_1,\ldots, e_n\}$ a finite orthonormal set of vectors. Then $\forall v\in V$
\[ \inf_{c_i\in\mathbb{F}}\norm{v-\sum_{i=1}^nc_ie_i} = \norm{v-\sum_{i=1}^n\inner{e_i,v}e_i} \]
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
\norm{v-\sum_{i=1}^nc_ie_i}^2 &= \inner{v-\sum_{i=1}^nc_ie_i,v-\sum_{j=1}^nc_je_j} \\
&= \norm{v} - \sum_{j=1}^n c_j\inner{v,e_j} - \sum_{i=1}^n\bar{c}_i\inner{e_i,v} + \sum_{i,j=1}^n\bar{c}_ic_j\inner{e_i,e_j} \\
&= \norm{v} - 2\Re\left(\sum_{i=1}^nc_i\overline{\inner{e_i,v}}\right) + \sum_{i=1}^n|c_i|^2 \\
&= \sum_{i=1}^n\left(|c_i|^2 - 2\Re\left(\sum_{i=1}^nc_i\overline{\inner{e_i,v}}\right) + |\inner{e_i,v}|^2\right) +\norm{v} - \sum_{i=1}^n|\inner{e_i;v}|^2 \\
&= \sum_{i=1}^n|c_i - \inner{e_i,v}|^2 +\norm{v} - \sum_{i=1}^n|\inner{e_i,v}|^2.
\end{align*}
This is clearly minimised when $c_i = \inner{e_i,v}$.
\end{proof}
\begin{corollary}
Let $v\in\Span(D)$, then $v = \sum_{i=1}^n \inner{e_i,v}e_i$.
\end{corollary}
We call the numbers $\inner{e_i,v}$ the \udef{Fourier coefficients} of $v$ w.r.t. $D$.
\begin{proof}
In this case $\inf_{c_i\in\mathbb{F}}\norm{v-\sum_{i=1}^nc_ie_i} = 0$.
\end{proof}
\begin{corollary}[Bessel inequality] \label{BesselInequality}
Let $\{e_i\}_{i\in I}$ be an orthonormal family and $v\in V$, then
\[ \sum_{i\in I}|\inner{e_i,v}|^2 = \sup \left\{\sum_{\substack{i\in I' \subset I\\ I' \;\text{finite}}} |\inner{e_i,v}|^2 \right\} \leq \norm{v}^2. \]
\end{corollary}
\begin{proof}
In the previous proof,
\[ 0 \leq \norm{v-\sum_{i=1}^nc_ie_i}^2 = \sum_{i=1}^n|c_i - \inner{e_i,v}|^2 +\norm{v} - \sum_{i=1}^n|\inner{e_i,v}|^2 = \norm{v} - \sum_{i=1}^n|\inner{e_i,v}|^2. \]
Where we have set $c_i = \inner{e_i,v}$. Thus the supremum must also be $\leq \norm{v}$.
\end{proof}
\begin{corollary}
For any $v\in V$, $\inner{e_i,v} = 0$ except for countably many $i\in I$. \label{countableComponents}
\end{corollary}
\begin{proof}
Ref TODO. \url{https://proofwiki.org/wiki/Uncountable_Sum_as_Series}.
\end{proof}
TODO: link with metric topology being sequential?

\begin{corollary}[Riemann-Lebesgue lemma] \label{RiemannLebesgueLemma}
For any sequence $\seq{e_i}_{i\in J \subset I}$, we have
\[ \lim_{i\in J} \inner{e_i,v} = 0. \]
\end{corollary}

\begin{corollary}
We can also obtain the Cauchy-Schwarz inequality from the Bessel inequality.
\end{corollary}
\begin{proof}
Let $x,y\in V$. Then $\{x/\norm{x}\}$ is an orthonormal set. Applying the Bessel inequality for $y$ gives $\norm{y}^2 \geq |\inner{x/\norm{x}, y}|^2 \implies |\inner{x,y}|^2\leq \norm{x}^2\norm{y}^2 \implies |\inner{x,y}| \leq \norm{x}\;\norm{y}$.
\end{proof}

\subsection{Orthonormal bases}
\begin{definition}
Let $D$ be an orthonormal set of vectors in an inner product space $V$, then $D$ is said to be
\begin{enumerate}
\item \udef{maximal}, if it is a maximal element in the set of orthonormal sets ordered by inclusion;
\item \udef{total}, if the smallest closed subspace that includes $D$ is $V$ (i.e.\ $\Span(D)$ is dense in $V$);
\item an \udef{orthonormal basis} (o.n. basis) or a \udef{Hilbert basis} if any vector in $V$ can be written as a (possibly infinite) linear combination of elements of $D$.
\end{enumerate}
\end{definition}
\begin{note}
Hilbert bases are in general not Hamel bases.  e.g\, take $\R^\mathbb{N}$. Then 
\begin{align*}
(1,0,0,&\ldots), \\
(0,1,0,&\ldots), \\
(0,0,1,&\ldots), \\
&\ldots
\end{align*}
is an orthonormal basis, but not a Hamel basis (consider $(1,1,1,\ldots)$).
\end{note}

\begin{proposition} \label{exitenceMaximalOrthonormalSet}
\begin{itemize}
\item Every vector space has a maximal orthonormal set.
\item Every orthonormal set can be extended to a maximal orthonormal set.
\end{itemize}
\end{proposition}
\begin{proof}
The first statement follows easily from the second. The second statement is proved using Zorn's lemma. Let $S$ be an orthonormal set. Define
\[ \mathcal{A} = \{ D\subset V \;|\; S\subset D \; \text{and $D$ is orthonormal} \} \]
ordered by inclusion. It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union which is still orthonormal. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. This is by definition an orthonormal basis.

In the finite-dimensional case this can also be proved using the Gram-Schmidt procedure.
\end{proof}

\begin{proposition}
If $V$ is finite-dimensional, then the notions of maximal orthonormal set, total orthonormal set and orthonormal set coincide. Such an orthonormal set is also a (Hamel) basis of $V$.
\end{proposition}
\begin{proof}
Corollaries of Gram-Schmidt.
\end{proof}

\begin{lemma} \label{characterisationMaximalOrthonormalSet}
Let $V$ be an inner product space and $D$ an orthonormal set. Then
\begin{enumerate}
\item $D$ is maximal \textup{if and only if} $D^\perp = \{0\}$;
\item if $D$ is an orthonormal basis, then $D$ is maximal.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) All possible vectors with which to extend $D$ are elements of $D^\perp$. 

(2) Assume $D$ an o.n. basis. Then $D^\perp = (\Span(D))^\perp = V^\perp = \{0\}$, using \ref{OrthogonalComplementProperties} and \ref{orthogonalComplementDenseSpace}.
\end{proof}
There are maximal orthonormal families that are not bases.
\begin{example}
Consider the space $l^2(\N)$ and take the subspace $X$ generated by the family of elements
\[ \left( \sum_{n=1}^\infty n^{-1}e_n, e_2,e_3,e_4,\ldots \right) \]
with the inner product induced by the inner product of $l^2$. In this space $F=\{e_2,e_3,\ldots\}$ is orthonormal and maximal, but not an orthonormal basis.
\end{example}

Maximal orthonormal families are easy to construct, but do not have the nice properties of orthonormal bases (see below). We would really like the concepts of orthonormal basis and maximal orthonormal family to coincide. We will see they coincide exactly in Hilbert spaces (see \ref{criterionHilbertSpace}).

\begin{proposition} \label{totalONBParsevalEquivalence} \label{plancherel}
Let $V$ be an inner product space and $D = \{e_i\}_{i\in I}$ an orthonormal set. The following are equivalent:
\begin{enumerate}
\item $D$ is an orthonormal basis of $V$;
\item $D$ is total in $V$;
\item for all $v,w\in V$,
\[ \inner{v,w} = \sum_{i\in I}\inner{v,e_i}\inner{e_i,w}; \]
\item \textup{(Parseval's identity)} for all $v\in V$,
\[ \norm{v}^2 = \sum_{i\in I}|\inner{e_i,v}|^2; \]
\item for all $v\in V$: if $v\perp D$, then $v=0$;
\item \textup{(Plancherel formula)} for all $v\in V$,
\[ v = \sum_{i\in I}\inner{e_{i},v}e_{i}. \]
\end{enumerate}
\end{proposition}
\begin{proof}
We proceed round-robin-style.
\begin{itemize}[leftmargin=2cm]
\item[$\boxed{(1) \Rightarrow (2)}$] Assume $D$ an o.n. basis. Then there exists a net of partial sums converging to any element $v\in V$. Each of these partial sums is a finite linear combination of elements in $D$ and thus this net is a net in $\Span(D)$. This means $v\in\overline{\Span(D)}$.
\item[$\boxed{(2) \Rightarrow (3)}$] Fix $v,w\in V$. Because $V$ is a metric spaces and thus sequential, we can find sequences $(v_j)_{j\in J}$ and $(w_k)_{k\in K}$ in $\Span(D)$ converging to $v$ and $w$. Now the linear maps $u\mapsto \overline{\inner{u, e_i}}$ and $u\mapsto \inner{e_i, u}$ are bounded by Cauchy-Schwarz and thus continuous by theorem \ref{boundedLinearMaps} (TODO corollary CSB). Then we can calculate, using the fact that each $v_j$ and $w_k$ is a finite linear combination of $e_i$,
\begin{align*}
\inner{v,w} &= \inner{\lim_{j}v_j, \lim_k w_k} = \lim_{j}\lim_{k}\inner{v_j,w_k} \\
&= \lim_{j}\lim_{k}\inner{\sum_{i=1}^{N_{j}}\inner{e_i,v_j}e_i,\sum_{i'=1}^{N_k}\inner{e_{i'},w_k}e_{i'}} \\
&= \lim_{j}\lim_{k}\sum_{i=1}^{N_{j}}\sum_{i'=1}^{N_k}\inner{v_j,e_i}\inner{e_{i'},w_k}\inner{e_i,e_{i'}} = \lim_{j}\lim_{k}\sum_{i=1}^{N_{j}}\sum_{i'=1}^{N_k}\inner{v_j,e_i}\inner{e_{i'},w_k}\delta_{i,i'} \\
&= \lim_{j}\lim_{k}\sum_{i=1}^{\min\{N_{j},N_{k}\}}\inner{v_j,e_i}\inner{e_i,w_k} \\
&= \lim_{j}\lim_{k}\sum_{i\in I}\inner{v_j,e_i}\inner{e_i,w_k} \\
&= \sum_{i\in I}\lim_{j}\lim_{k}\inner{v_j,e_i}\inner{e_i,w_k} \\
&= \sum_{i\in I}\inner{v,e_i}\inner{e_i,w}.
\end{align*}
For the interchange of the limits and the summation in the penultimate equality we can use Tannery's theorem, \ref{tannery}. Indeed $|\inner{e_i,w_k}|$ is bounded by $\norm{w_k}$ by the Bessel inequality. By the continuity of the norm we have $\lim_k \norm{w_k} = \norm{w}$, so the sequence $\norm{w_k}$ is bounded.
\item[$\boxed{(3) \Rightarrow (4)}$] Set $v=w$.
\item[$\boxed{(4) \Rightarrow (5)}$] If $v\perp D$, then
\[ \norm{v}^2 = \sum_{i\in I}|\inner{e_i,v}|^2 = 0 \qquad\text{which implies $v=0$.} \]
\item[$\boxed{(5) \Rightarrow (6)}$] The vector $v-\sum_{i\in I}\inner{e_i,v}e_i$ is perpendicular to $D$:
\[ \forall e_j\in D: \quad \inner{e_j, v-\sum_{i\in I}\inner{e_i,v}e_i} = \inner{e_j, v}-\sum_{i\in I}\inner{e_i,v}\inner{e_j,e_i} = \inner{e_j, v} - \inner{e_j, v} = 0. \]
So $v-\sum_{i\in I}\inner{e_i,v}e_i = 0$ and the Plancherel formula holds.
\item[$\boxed{(6) \Rightarrow (1)}$] By definition of o.n. basis.
\end{itemize}
\end{proof}

\begin{lemma}
Let $V$ be an inner product space. If $D$ is an orthonormal basis of $V$, then it is also an orthonormal basis of $\overline{V}$, the completion of $V$.
\end{lemma}
\begin{proof}
Let $D$ be an o.n. basis. By \ref{totalONBParsevalEquivalence} $\Span(D)$ is dense in $V$, meaning it is also dense in $\overline{V}$, by \ref{denseSubsetOfDenseSubspaceIsDense}. Thus $D$ is total in $\overline{V}$ and an o.n. basis by \ref{totalONBParsevalEquivalence}.
\end{proof}

\subsubsection{Cardinality and separable inner product spaces}
\url{https://arxiv.org/pdf/1606.03869.pdf}
\begin{definition}
An inner product space is \udef{separable} if it is separable as a metric space, i.e.\ it admits a countable dense subset.
\end{definition}

\begin{proposition}
Given a vector space $V$, any two maximal orthonormal sets have the same cardinality.
\end{proposition}
\begin{proof}
Take $D = \{e_i\}_{i\in I}$ and $D' = \{f_j\}_{j\in J}$ maximal orthonormal sets.
\end{proof}

\begin{proposition}
An inner product space is separable \textup{if and only if} it admits an orthonormal basis with at most countably many vectors.
\end{proposition}
\begin{proof}
TODO infinite-dimensional analog of the Gram-Schmidt process
\end{proof}
\begin{corollary}
Any separable inner product space has an orthonormal basis.
\end{corollary}

\begin{proposition}
Not every inner product space has an orthonormal basis.
\end{proposition}
\begin{proof}
\url{https://en.wikipedia.org/wiki/Inner_product_space#Orthonormal_sequences}
\url{https://groups.google.com/g/sci.math.research/c/1SA_3h1whQo?pli=1}
\url{https://www.angelfire.com/journal/mathematics/innerproduct.pdf}
\url{https://arxiv.org/pdf/1009.1441.pdf}
\end{proof}


\section{Maps on inner product spaces}

\begin{lemma}[Continuity of inner product]
Let $V$ be an inner product space. Then the inner product is a continuous function $V\times V \to \mathbb{F}$.
\end{lemma}
\begin{proof}
We show that if $x_n \to x$ and $y_n \to y$, then $\inner{x_n,y_n}\to \inner{x,y}$. By the triangle and Cauchy-Schwarz inequalities
\begin{align*}
|\inner{x_n,y_n}-\inner{x,y}| &= |\inner{x_n,y_n}-\inner{x_n,y}+\inner{x_n,y} - \inner{x,y}| \\
&\leq |\inner{x_n, y_n-y}| + |\inner{x_n-x, y}| \\
&\leq \norm{x_n}\norm{y_n-y} + \norm{x_n-x}\norm{y}.
\end{align*}
Because the right-hand side converges to $0$, the left-hand side must too.
\end{proof}

\begin{lemma} \label{equalityOfMapsInnerProductSpaces}
Let $V$ be an inner product space and $S,T\in\Hom(V)$. Then $S=T$ \textup{if and only if}
\[ \forall v,w\in V: \inner{Tv,w} = \inner{Sv,w}. \]
\end{lemma}
\begin{proof}
The direction $\boxed{\Rightarrow}$ is obvious. For the other direction, use
\[ 0 = \inner{Tv,w} - \inner{Sv,w} = \inner{(T-S)v,w} \]
for all $v,w$. In particular set $w$ equal to $(T-S)v$. Then $\norm{(T-S)v} = 0$ for all $v\in V$. By the definiteness of the norm we have $(T-S)v = 0$, meaning $Tv = Sv$.
\end{proof}

\subsection{Bounded operators}
\begin{lemma} \label{operatorNormInnerProduct}
Let $T\in\Bounded(V,W)$, then
\begin{align*}
\norm{T} &= \sup_{w\in \im(T),v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \\
&= \sup\setbuilder{|\inner{w,Tv}|}{w\in \im(T)\;\land\; v\in\dom{T}\;\land\; \norm{w} = 1 = \norm{v}} \\
&= \sup_{w\in W,v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \\
&= \sup\setbuilder{|\inner{w,Tv}|}{w\in W\;\land\; v\in\dom{T}\;\land\; \norm{w} = 1 = \norm{v}}.
\end{align*}
\end{lemma}
\begin{proof}
We prove
\[ \norm{T} \leq \sup_{w\in \im(T),v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \leq \sup_{w\in W,v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \leq \norm{T}. \]
The first two inequalities follow from the characterisation \ref{operatorNorm}
\[ \norm{T} = \sup_{v \in \dom(T)} \frac{\norm{Tv}}{\norm{v}} = \sup_{v \in \dom(T)} \frac{\inner{Tv,Tv}}{\norm{Tv}\,\norm{v}} \]
and the inclusions
\begin{align*}
\setbuilder{\frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}}}{v\in\dom(T), w = Tv} &\subseteq \setbuilder{\frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}}}{v\in\dom(T), w\in\im(T)}\\
&\qquad\quad\subseteq \setbuilder{\frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}}}{v\in\dom(T), w\in V}.
\end{align*}
The last equality follows from the Cauchy-Schwarz inequality \ref{CauchySchwarz}:
\[ \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \leq \frac{\norm{w}\,\norm{Tv}}{\norm{w}\,\norm{v}} = \frac{\norm{Tv}}{\norm{v}} \leq \frac{\norm{T}\,\norm{v}}{\norm{v}} = \norm{T} \]
for all $v\in\dom(T), w\in V$. 
\end{proof}

\subsection{Isometries}
\begin{lemma}
Let $V,W$ be inner product spaces. Let $f:V\to W$ be a function. Then $f$ preserves the metric (i.e.\ is an isometry) \textup{if and only if} $f$ also preserves the inner product:
\[ \forall x,y \in V: \quad \inner{f(x),f(y)}_W = \inner{x,y}_V. \]
\end{lemma}
The proof is a simple application of the polarisation identities.

\begin{definition}
Let $V,W$ be an inner product spaces. A linear map $U\in\Hom(V,W)$ is called \udef{unitary} if it is an isometry and invertible.

Unitary operators on real vector spaces are also called \udef{orthogonal operators}.
\end{definition}
Because every isometry is injective (see lemma \ref{isometryInjective}), it is enough for a linear map to be isometric and surjective to be unitary.

\begin{lemma}
Every unitary map is bounded and has norm $1$.
\end{lemma}
\begin{proof}
Let $U: V\to W$ be a unitary map between inner product spaces. Then $\forall v\in V: \norm{U(v)} = \norm{v}$.
\end{proof}

Unitary operators transform orthonormal bases to orthonormal bases:
\begin{proposition}
Let $T\in \Hom(V,W)$ with $V,W$ inner product spaces and let $V$ have an orthonormal basis $\{e_i\}_{i\in I}$. Then $T$ is unitary \textup{if and only if} $\{Te_i\}_{i\in I}$ is an orthonormal basis of $W$.
\end{proposition}
\begin{proof}
Assume $T$ unitary. The family $\{Te_i\}_{i\in I}$ is certainly orthonormal, by preservation of the inner product. Now let $w\in W$ and so $T^{-1}w\in V$. By the Plancherel formula, proposition \ref{plancherel}, we can write
\[ T^{-1}w = \sum_{n=1}^\infty \inner{e_{i_n},T^{-1}w}e_{i_n} = \lim_{N\to\infty}\sum_{n=1}^N \inner{e_{i_n},T^{-1}w}e_{i_n} \]
and so
\[ w = TT^{-1}w = T\lim_{N\to\infty}\sum_{n=1}^N \inner{e_{i_n},T^{-1}w}e_{i_n} = \lim_{N\to\infty}\sum_{n=1}^N \inner{e_{i_n}T^{-1}w}Te_{i_n} \]
because $T$ is bounded and thus continuous, by theorem \ref{boundedLinearMaps}.
Thus $\{Te_i\}_{i\in I}$ is an orthonormal basis of $W$.

Conversely, assume $\{Te_i\}_{i\in I}$ is an orthonormal basis of $W$. We first prove $T$ is bounded, which is a simple application of Parseval's identity, proposition \ref{totalONBParsevalEquivalence}:
\[ \norm{Tv}^2 = \sum_{i\in I}|\inner{Te_i,Tv}|^2 = \sum_{i\in I}|\inner{e_i,v}|^2 = \norm{v}^2. \]
The rest of the proof is again an application of the Plancherel formula.
\end{proof}

\begin{lemma}
Let $U$ be a unitary map. If $\lambda$ is an eigenvalue of $U$, then $|\lambda| = 1$.
\end{lemma}
\begin{proof}
Let $v$ be an eigenvector associated to the eigenvalue $\lambda$. Then
\[ \inner{v,v} = \inner{L(v),L(v)} = \inner{\lambda v, \lambda v} = \lambda^2\inner{v,v},  \]
so $\lambda^2 = 1$.
\end{proof}

\subsection{Symmetric operators}
\begin{definition}
Let $(\mathbb{F},V,+,\inner{\cdot,\cdot})$ be an inner product space. A linear operator $L$ is called \udef{symmetric} if, $\forall v,w\in \dom(L)$
\[ \inner{L(v),w} = \inner{v,L(w)}. \]
\end{definition}

\begin{proposition}
Let $V$ be an inner product space and $L$ a symmetric operator on $V$. Then eigenvectors of $L$ associated to different eigenvalues are orthogonal.
\end{proposition}
\begin{proof}
Let $v,w$ be eigenvectors of $L$ with eigenvalues $\lambda, \mu$ such that $\lambda \neq \mu$. Then
\[ \lambda\inner{v,w} = \inner{\lambda v,w}=\inner{L(v),w} = \inner{v,L(w)} = \inner{v,\mu w} = \mu \inner{v,w} \]
and consequently $\inner{v,w} =0$.
\end{proof}

\subsection{Impact on subspaces}
\subsubsection{Invariant and reducing subspaces}
\begin{definition}
Let $V$ be an inner product space and $T$ a linear operator on $V$.
\begin{itemize}
\item A subspace $U\subseteq V$ is said to be \udef{invariant} under $T$ if $T[U] \subset U$.
\item A subspace $U\subseteq V$ is said to be \udef{reducing} for $T$ if both $U$ and $U^\perp$ are invariant under $T$.
\end{itemize}
\end{definition}

\section{Energy forms}
\begin{definition}
Let $T$ be an operator on an inner product space $V$. The \udef{energy form} of $T$ is the map
\[ \inner{\cdot, \cdot}_T: \dom(T)\times \dom(T) \to \F: (x,y) \mapsto \inner{x,Ty}. \]
We also define the associated quadratic form
\[ Q_T: \dom(T)\to \F: x\mapsto \inner{x,x}_T = \inner{x,Tx}. \]
\end{definition}
Energy forms are clearly sesquilinear.

\begin{lemma} \label{quadraticFormInverseOperator}
Let $T$ be an invertible operator. Then
\[ Q_{T^{-1}}(x) = \overline{Q_T(T^{-1}(x))}. \]
\end{lemma}
\begin{proof}
For all $x\in V$
\[ Q_{T^{-1}}(x) = \inner{x,T^{-1}x} = \inner{TT^{-1}x, T^{-1}x} = \overline{\inner{T^{-1}x, T(T^{-1}x)}} = \overline{Q_T(T^{-1}(x))}. \]
\end{proof}

\begin{lemma} \label{sameEnergyFormSameOperator}
Two operators $T_1,T_2\in \Lin(V)$ have the same energy form \textup{if and only if} $T_1 = T_2$.
\end{lemma}
\begin{proof}
This is a consequence of \ref{elementaryOrthogonality}.
\end{proof}

\begin{lemma} \label{energyFormHermitianSymmetric}
The energy form of an operator $T$ is Hermitian \textup{if and only if} $T$ is symmetric.
\end{lemma}
\begin{proof}
For all $x,y$: $\inner{x,Ty} = \overline{\inner{y, Tx}}$ iff $\inner{x,Ty} = \inner{Tx, y}$.
\end{proof}
\begin{corollary} \label{symmetricRealQuadraticForm}
If $T$ is symmetric, then $Q_T$ is real-valued.
\end{corollary}
\begin{proof}
Assume $T$ symmetric, then for all $u\in\dom(T)$
\[ Q_T(u) = \inner{u,Tu} = \inner{Tu,u} = \overline{\inner{u,Tu}} = \overline{Q(u)}. \]
\end{proof}

So the energy form associated to a symmetric operator is Hermitian. We typically would like our energy forms to be pre-inner products. This is exactly the case for positive operators.

\subsection{Positive operators}
\begin{definition}
Let $T$ be an operator on an inner product space $V$. Then $T$ is called \udef{positive} if the associated energy form is positive: for all $x\in V$
\[ Q_T(x) = \inner{x,x}_T = \inner{x,Tx} \geq 0. \]
We write $A \geq 0$. We also say
\begin{itemize}
\item $A$ is \udef{strictly positive}, denoted $A > 0$, if $Q_T(u) > 0$;
\item $A$ is \udef{negative} if $-A$ is positive;
\item $A$ is \udef{positive definite}, \udef{strongly positive} or \udef{coercive} if there exists a constant $k>0$ such that
\[ Q_T(x) \geq k\norm{x}^2 > 0. \]
\end{itemize}
\end{definition}

\begin{lemma} \label{positiveOperatorSymmetric}
If $T$ is a positive operator on a complex inner product space, then $T$ is symmetric.
\end{lemma}
\begin{proof}
If $T$ is a positive operator on a complex inner product space $V$, then $Q_T(x)$ is in particular real for all $x\in V$. This is equivalent to $\inner{\cdot,\cdot}_T$ being Hermitian by \ref{HermitianRealQuadratic}, which is in turn equivalent to the symmetry of $T$ by \ref{energyFormHermitianSymmetric}.
\end{proof}

On a real inner product space there may exist positive operators that are not symmetric.
\begin{example}
Let $V= \R^2$ and $T: \R^2 \to\R^2: (x,y)\mapsto (y,-x)$. Then
\[ \forall (x,y)\in V: \quad Q_T\big((x,y)\big) = \inner{(x,y), (y,-x)} = xy -xy = 0 \geq 0, \]
so $T$ is positive. But $T$ is not symmetric. Indeed $\inner{(0,y), T(x,0)} = -xy$ and $\inner{T(0,y), (x, 0)} = xy$.
\end{example}

\begin{lemma}
Let $A$ be an bounded operator on a Hilbert space $H$. Then $A^*A$ and $AA^*$ are positive. Also $A^*A$ is strictly positive \textup{if and only if} $A$ is injective.
\end{lemma}
\begin{proof}
For all $x\in H$:
\[ \inner{A^*Ax,x} = \inner{Ax,Ax} = \norm{Ax}^2 \geq 0 \qquad \inner{AA^*x,x} = \inner{A^*x,A^*x} = \norm{A^*x}^2 \geq 0. \]
If $A$ is injective, then its kernel is $\{0\}$ and thus $\norm{Ax}^2 > 0$ for all $x\in H\setminus\{0\}$.
\end{proof}

\begin{lemma}
Let $T$ be an invertible operator on an inner product space $V$. Then $Q_T[V] = Q_{T^{-1}}[V]$.
\end{lemma}
\begin{proof}
Immediate from \ref{quadraticFormInverseOperator}.
\end{proof}
\begin{corollary}
Let $T$ be an invertible operator. Then $T$ is positive (definite) \textup{if and only if} $T^{-1}$ is positive (definite).
\end{corollary}

\begin{lemma} \label{positiveOperatorPositiveEnergyForm}
Let $T$ be an operator. The energy form $\inner{\cdot,\cdot}_T$ is positive (and thus a pre-inner product) \textup{if and only if} $T$ is a positive operator.
\end{lemma}

\subsubsection{Energy norm}
\begin{definition}
Let $T$ be a positive operator. Then
\[ \norm{\cdot}_T: \dom(T) \to [0,+\infty[: x\mapsto \norm{x}_T = \sqrt{Q_T(x)} \]
is the \udef{energy norm} associated to $T$.
\end{definition}

\begin{lemma}
The energy norm of a positive operator determines a pseudometric topology.
\end{lemma}

\begin{definition}
The topology generated by the energy norm is called the \udef{energy topology} and convergence in the energy topology is called \udef{convergence in energy}.
\end{definition}

\begin{proposition}
Let $T$ be a positive operator. Then
\begin{enumerate}
\item the energy topology is coarser than the norm topology;
\item the topologies are the same on $\dom(T)$ if $T$ is positive definite.
\end{enumerate}
\end{proposition}

\subsubsection{The partial order on operators}
\begin{definition}
We define an \udef{operator partial order} by
\[ A\leq B \qquad\iff\qquad B-A \geq 0. \]
\end{definition}
TODO: restrict to bounded operators??

\begin{lemma}
The operator partial order is a partial order on the set of operators on an inner product space.
\end{lemma}

\subsubsection{Induced topology}
We consider the topology induced by an energy norm $\norm{\cdot}_T$.

\begin{proposition} \label{energyNormTopology}
Let $V$ be an inner product space and $T$ a positive operator on $V$. Then
\begin{enumerate}
\item if $T$ is bounded, then $\norm{\cdot}_T$ is bounded by $\norm{\cdot}$;
\item $\norm{\cdot}$ is bounded by $\norm{\cdot}_{T+\id}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) If $T$ is bounded, then $\forall v\in V$
\[ \norm{v}_T = \sqrt{\inner{v,Tv}} = \sqrt{|\inner{v,Tv}|} \leq \sqrt{\norm{v}^2\norm{T}} = \sqrt{\norm{T}}\norm{v}, \]
where we have used the Cauchy-Schwarz inequality \ref{CauchySchwarz}.

(2) For all $v\in V$ we have
\[ \norm{v} \leq \norm{v} + \norm{v}_T = \inner{v,v} + \inner{v,Tv} = \inner{v,(T+\id)v} = \norm{v}_{T+\id}. \]
\end{proof}
\begin{corollary}
Let $V$ be an inner product space and $T$ a positive operator on $V$. Then
\begin{enumerate}
\item if $T$ is bounded, then the topology induced by $\norm{\cdot}$ is finer than the topology induced by $\norm{\cdot}_T$;
\item the topology induced by $\norm{\cdot}_{T+\id}$ is finer than the topology induced by $\norm{\cdot}$.
\end{enumerate}
\end{corollary}
\begin{proof}
This follows straight from \ref{normComparison}.
\end{proof}

\subsection{Dissipative operators}
\begin{definition}
Let $T$ be an operator on $H$. Then $T$ is \udef{dissipative} if, for all $x\in\dom(T)$
\[ \Im \inner{x,Tx} \geq 0. \]
\end{definition}

\subsection{Rayleigh quotient}
\begin{definition}
Let $T$ be a linear operator on an inner product space $V$. The \udef{Rayleigh quotient} for $T$ is 
\[ J_T: \dom(T)\setminus\{0\}\to \F: u\mapsto \frac{Q(u)}{\norm{u}^2} = \frac{\inner{u,Tu}}{\norm{u}^2}. \]
We may also write just $J$ if the intended operator $T$ is clear.
\end{definition}

\begin{lemma}
Let $T\in\Lin(V)$ be a linear operator and $J_T$ the associated Rayleigh quotient. Then for all $u\in V$:
\[ J_T(u) = J_T\left(\frac{u}{\norm{u}}\right). \]
\end{lemma}

\subsubsection{Numerical range}
\url{https://users.math.msu.edu/users/shapiro/pubvit/downloads/numrangenotes/numrange_notes.pdf}

\url{https://pskoufra.info.yorku.ca/files/2016/07/Numerical-Range.pdf}

\url{http://www.math.wm.edu/~ckli/nrnote}

\url{https://link-springer-com.ezproxy.ulb.ac.be/content/pdf/10.1007%2F978-3-319-01448-7.pdf}

\url{https://projecteuclid.org/journalArticle/Download?urlId=10.1307%2Fmmj%2F1028997958}

\begin{definition}
Let $T$ be a linear operator on an inner product space $V$ and $J_T$ the Rayleigh quotient of $T$. The range $\NumRange(T) \defeq \im(J_T)$ is known as the \udef{numerical range}.
\end{definition}

The numerical range of $T$ can equivalently be defined as the image of the unit sphere under the quadratic form associated to $T$.

\begin{lemma}
Let $T$ be a linear operator on an inner product space $V$ and $J_T$ the Rayleigh quotient of $T$. Then
\begin{align*}
\NumRange(T) &= J_T[\setbuilder{u\in V}{\norm{u} = 1} \cap \dom(T)] \\
&= Q_T[\setbuilder{u\in V}{\norm{u} = 1}\cap \dom(T)].
\end{align*}
\end{lemma}

\begin{lemma}
Let $V$ be an inner product space over a field $\F$, $\lambda,\mu\in \F$ and $T$ an operator on $V$. Then
\[ W(\lambda T + \mu) = \lambda W(T) + \mu. \]
\end{lemma}

\begin{theorem}[Toeplitz-Hausdorff theorem]
Let $V$ be an inner product space and $T$ an operator on $V$. Then $W(T)$ is convex.
\end{theorem}
\begin{proof}
TODO \url{https://www.ams.org/journals/proc/1970-025-01/S0002-9939-1970-0262849-9/S0002-9939-1970-0262849-9.pdf}

\url{https://www.cambridge.org/core/services/aop-cambridge-core/content/view/BA251EBB1E1DE08DBD3D84964F65938B/S0008439500058197a.pdf/the-toeplitz-hausdorff-theorem-explained.pdf}
\end{proof}

\begin{proposition}
Let $V$ be an inner product space and $T$ an operator on $V$. If $V$ is finite dimensional, then $W(T)$ is compact.
\end{proposition}
\begin{proof}
Heine-Borel. TODO.
\end{proof}

\begin{lemma}
Let $V$ be an inner product space and $T$ a bounded symmetric operator on $V$. Then
\begin{enumerate}
\item the directional derivative $\partial_v(J_T(u))$ exists if $u\neq 0$ and is equal to (TODO remove and place in proof?)
\[ \partial_v(J_T)|_u = \frac{\inner{u,u}\Big( \inner{v,Tu} + \inner{u,Tv} \Big) - \inner{u,Tu}\Big(\inner{u,v}+\inner{v,u}\Big)}{\inner{u,u}^2}; \]
\item $u\in V\setminus \{ 0 \}$ is a critical point of $J_T$ \textup{if and only if} $u$ is an eigenvector of $T$ with corresponding eigenvalue $\lambda = J_T(u)$.
\end{enumerate}
\end{lemma}
\begin{proof}
TODO: critical point in $\C$ v $\R$?? (For symmetric operators $J$ is real valued)
\ref{derivativeBilinearForm}
\end{proof}

\subsubsection{Numerical radius}
\begin{definition}
Let $T$ be a linear operator on an inner product space $V$. Then
\[ \nr(T) \defeq \sup_{u\in \dom(T)\setminus\{0\}} |J_T(u)| \]
is the \udef{numerical radius}.
\end{definition}
If $Q_T$ is the quadratic form associated to an operator $T$, we have
\[ |Q_T(u)| \leq \norm{u}^2\nr(T). \]

\begin{lemma}
Let $T$ be a linear operator on an inner product space $V$ and $J_T$ the Rayleigh quotient of $T$. Then
\begin{align*}
\nr(T) &= \sup_{\substack{u\in \dom(T) \setminus\{0\}\\ \norm{u} = 1}} |J_T(u)| \\
&= \sup_{\substack{u\in \dom(T) \setminus\{0\}\\ \norm{u} = 1}} |Q_T(u)|.
\end{align*}
\end{lemma}

\begin{proposition} \label{normNumRadius}
Let $T$ be an operator on an inner product space $V$.
\begin{enumerate}
\item If $T$ is bounded, then $\forall u\in \dom(T)\setminus\{0\}$
\[ |J_T(u)| \leq \nr(T) \leq \norm{T}. \]
\item If $T$ is symmetric, then $T$ is bounded with $\norm{T} = \nr(T)$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) The first claim follows simply from the Cauchy-Schwarz inequality \ref{CauchySchwarz}
\[ |J(u)| \leq \frac{\norm{u}\,\norm{Tu}}{\norm{u}^2} = \frac{\norm{Tu}}{\norm{u}} \leq \frac{\norm{T}\norm{u}}{\norm{u}} = \norm{T}. \]

(2) For the second claim we need to also show the inverse inequality. By \ref{operatorNormInnerProduct} it is enough to show that $|\inner{w,Tv}| \leq \nr(T)$ for all $v\in \dom(T)$ and $w\in\im(T)$ with $\norm{v} = 1 = \norm{w}$.

Take arbitrary unit vectors $v,w\in V$ and let $\theta$ be such that $|\inner{w,Tv}| = e^{i\theta}\inner{w,Tv}$. Then $\inner{e^{-i\theta}w,Tv}$ is real, so, viewing it as a sesquilinear form, the imaginary parts of the polarisation identity \ref{polarisationIdentities} cancel:
\begin{align*}
\inner{e^{-i\theta}w,Tv} &= \frac{1}{4}\sum_{k=0}^3i^k \inner{(i^ke^{-i\theta}w + v), T((i^ke^{-i\theta}w + Tv))} \\
&= \frac{1}{4}\Big( \inner{v+e^{-i\theta}w, T(v+e^{-i\theta}w)} - \inner{v-e^{-i\theta}w, T(v-e^{-i\theta}w)} \Big),
\end{align*}
where we have used that the quadratic form is real by \ref{symmetricRealQuadraticForm}.

Thus
\begin{align*}
|\inner{w,Tv}| &= |\inner{e^{-i\theta}w,Tv}| \\
&= \frac{1}{4}\Big(\inner{v+e^{-i\theta}w, T(v+e^{-i\theta}w)} - \inner{v-e^{-i\theta}w, T(v-e^{-i\theta}w)} \Big) \\
&\leq \frac{1}{4}\Big( |\inner{v+e^{-i\theta}w, T(v+e^{-i\theta}w)}| + |\inner{v-e^{-i\theta}w, T(v-e^{-i\theta}w)}| \Big) \\
&\leq \frac{1}{4}\nr(T)\Big( \norm{v+e^{-i\theta}w}^2 + \norm{v-e^{-i\theta}w}^2 \Big) \\
&= \frac{1}{4}\nr(T)\Big( 2\norm{v}^2 + 2\norm{w}^2 \Big) = \nr(T),
\end{align*}
where we have used the fact that $v,w$ are unit vectors and the parallelogram law \ref{parallelogramLaw}.
\end{proof}
\begin{corollary}
If $T$ is a symmetric operator; it is bounded iff $J_T$ is bounded above and below:
\[ \forall u\in\dom(T): \; k \leq J_T(u) \leq K \]
for some $k,K\in \R$.
\end{corollary}
\begin{corollary}
If $T$ is symmetric and bounded, then
\[ \norm{T} = \sup_{\norm{u}\leq 1} |\inner{u,Tu}|. \]
\end{corollary}








\section{Examples of Hilbert spaces}
\subsection{The $\ell^2$ spaces}
Sequence spaces $\ell^p$ Hilbert iff $p=2$. (TODO: other sequence spaces?)

\subsection{Direct sum}
Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. By considering them as Banach spaces we can take the $\ell^2$-direct sum. (TODO: other sequence spaces?)
\begin{proposition}
Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. The $\ell^2$-direct sum is a Hilbert space.
\end{proposition}
This gives the conventional interpretation of the \udef{Hilbert space direct sum}: it is the $\ell^2$-direct sum of the summands as Banach spaces.

\section{Strong and weak convergence}
\subsection{Weak convergence of vectors}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. The \udef{weak convergence} on $\mathcal{H}$ is the initial convergence w.r.t.
\[ \setbuilder{\inner{x,-}: \mathcal{H}\to \F}{x\in\mathcal{H}}. \]
Thus $F \overset{w}{\longrightarrow} v$ \textup{if and only if} $\inner{x, F} \longrightarrow \inner{x,v}$ for all $x\in\mathcal{H}$.

We denote the weak convergence $\overset{w}{\longrightarrow}$ or $\lim^w$ and write $\mathcal{H}^w$ to denote the convergence space $\sSet{\mathcal{H},\lim^w}$.
\end{definition}
By \ref{initialVectorSpaceConvergence} we have that the weak convergence is a vector space convergence.

\begin{lemma} \label{normConvergenceFinerThenWeakConvergence}
Norm convergence is finer than weak convergence.
\end{lemma}
\begin{proof}
The functions in $\setbuilder{\inner{x,-}: \mathcal{H}\to \F}{x\in\mathcal{H}}$ are also continuous in the norm convergence.
\end{proof}

\begin{example}
Let $\seq{e_n}$ be an orthonormal basis. Then the Riemann-Lebesgue lemma, \ref{RiemannLebesgueLemma}, gives $e_n \overset{w}{\longrightarrow} 0$, even though clearly $e_n \not\to 0$.
\end{example}

So norm convergence is strictly finer.

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space, $x\in \mathcal{H}$ and $\seq{x_n}\subseteq \mathcal{H}$. Then
\begin{enumerate}
\item $x_n \overset{w}{\longrightarrow} x$ implies $\norm{x} \leq \liminf \norm{x_n}$;
\item $x_n \longrightarrow x$ \textup{if and only if} $x_n \overset{w}{\longrightarrow} x$ and $\norm{x_n}\to \norm{x}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have
\[ \norm{x}^2 = \inner{x,x} = \lim \inner{x,x_n} = \liminf\inner{x,x_n} \leq \norm{x}\liminf\norm{x_n}, \]
where the limit is in $\R$. We use that for convergent sequences, $\lim = \liminf$.

(2) The direction $\Rightarrow$ is clear form \ref{normConvergenceFinerThenWeakConvergence} and the continuity of the norm.

For the converse, we have
\[ \norm{x-x_n}^2 = \norm{x}-2\Re(\inner{x,x_n}) + \norm{x_n} \to 0, \]
because $\norm{x_n} \to \norm{x}$ and $\inner{x,x_n} \to \inner{x,x} = \norm{x}^2$.
\end{proof}

\url{https://math.stackexchange.com/questions/1461363/closed-unit-ball-of-hilbert-space-sequentially-compact-in-weak-topology}

\subsubsection{Weak Cauchy filters}
\begin{definition}
Cauchy filters in $\mathcal{H}^w$ are called \udef{weak Cauchy filters}.
\end{definition}


\begin{proposition}
Let $\mathcal{H}$ be a real or complex Hilbert space.
A filter $F$ in $\powerfilters(\mathcal{H})$ is a weak Cauchy filter \textup{if and only if} $\inner{x,F}$ converges in $\F$ for all $x$.
\end{proposition}
\begin{proof}
The following steps are equivalent:
\begin{itemize}
\item $F$ is weak Cauchy;
\item $F-F \overset{w}{\longrightarrow} 0$;
\item $\inner{x, F} - \inner{x,F} \to 0$ for all $x\in \mathcal{H}$;
\item $\inner{x, F}$ is Cauchy in the scalar field for all $x\in\mathcal{H}$;
\item $\inner{x, F}$ converges for all $x\in\mathcal{H}$.
\end{itemize}
The last equivalence follows because $\R$ and $\C$ are complete.
\end{proof}
\begin{corollary}
Every weak Cauchy filter is bounded in norm.
\end{corollary}
\begin{proof}
Consider the family of bounded linear operator $\inner{F,-}$. Then $\inner{F,-}$ converges pointwise, by the proposition. By Banach-Steinhaus, \ref{BanachSteinhaus}, we have $\sup_{y\in F}\norm{\inner{y,-}} = \sup_{y\in F}\norm{y} <\infty$.
\end{proof}

Note that the previous proposition does not mean a weak Cauchy filter is necessarily weakly convergent, as there may not exist a vector $v$ such that $\inner{x,F}$ converges to $\inner{x,v}$ for all $x\in\mathcal{H}$. However, we have the following proposition:

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space. Then $\mathcal{H}^w$ is Cauchy complete.
\end{proposition}
\begin{proof}
Let $F$ be a weak Cauchy filter. Let $\seq{e_i}_{i\in I}$ be an orthonormal basis of $\mathcal{H}$. Let $c_i = \lim \inner{e_i, F}$. Then $F \overset{w}{\longrightarrow} \sum_{i\in I}c_i e_i$.

First we check this sum is well-defined. TODO!!
\end{proof}

TODO: $\inner{F,F}$ weak convergent?????


\subsection{Strong and weak convergence of operators}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. Then the
\begin{itemize}
\item \udef{strong operator topology} is the topology of pointwise convergence on $\Lin(\mathcal{H})$;
\item \udef{weak operator topology} is the topology of pointwise convergence on $\Lin(\mathcal{H}^w)$.
\end{itemize}
We write $F \overset{SOT}{\longrightarrow}A$ and $F \overset{WOT}{\longrightarrow}A$ for the convergence of $F$ to $A$ in the strong and weak operator topologies.
\end{definition}


TODO introduce shifts earlier.
\begin{example}
Consider the left and right shifts $S_l$ and $S_r$ on $\ell^2(\N)$. Then
\begin{itemize}
\item $S_l \overset{SOT}{\longrightarrow} 0$, but $S_l \overset{norm}{\not\longrightarrow} 0$;
\item $S_r \overset{WOT}{\longrightarrow} 0$, but $S_r \overset{SOT}{\not\longrightarrow} 0$.
\end{itemize}
In general taking adjoints is not continuous w.r.t. strong operator convergence, because $S_l \overset{SOT}{\longrightarrow} 0$, but $S_r = S_l^* \overset{SOT}{\not\longrightarrow} 0$.
\end{example}

\url{https://math.stackexchange.com/questions/1054288/the-set-of-all-normal-operators-on-a-hilbert-space-is-not-strongly-closed}


Note normal operators not $SOT$-closed!
\begin{proposition}
If $\seq{A_n}$ is a sequence of normal operators that converges to a normal operator $A$ in the strong operator topology, then $A_n^* \overset{SOT}{\longrightarrow} A^*$.
\end{proposition}

\begin{proposition}
Let $\seq{A_n}$ be a sequence of bounded operators on a Hilbert space and $A\in\Lin(\mathcal{H})$. Then
\begin{enumerate}
\item if $A_n \overset{SOT}{\longrightarrow} A$, then $\norm{A}\leq \liminf\norm{A_n}$;
\item If $A_nx \longrightarrow Ax$ for all $x$ in a dense subset of $\mathcal{A}$ and $\seq{A_n}$ is a bounded sequence, then $A_n \overset{SOT}{\longrightarrow} A$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) For all unit vectors $x$ we have
\[ \norm{Ax} = \norm{\lim_{SOT}A_nx} = \lim_{n\to\infty}\norm{A_nx} \leq  \]
\end{proof}
TODO: same for WOT.

TODO: Cauchy sequences are bounded. Does this follow form general Banach theory?


\section{Tools to study operators}
\subsection{Spectrum}
\subsection{Numerical range}
\begin{lemma}
Let $T$ be an operator on a Hilbert space. If $\lambda\in\cspec(T)$, then there exists a Weyl sequence for $\lambda$. 
\end{lemma}
\begin{proof}
If $\lambda\in\cspec(T)$, then $R_T(\lambda)$ is densely defined.
\end{proof}

\begin{proposition}[Spectral inclusion property of numerical range] \label{spectralInclusionNumericalRange}
Let $T$ be an operator on a Hilbert space. Then
\begin{enumerate}
\item $\pspec(T)\subseteq \NumRange(T)$;
\item $\rspec(T) \subseteq \NumRange(T)$;
\item $\apspec(T) \subseteq \overline{\NumRange(T)}$.
\end{enumerate}
In particular $\cspec(T) \subseteq \overline{\NumRange(T)}$.
\end{proposition}
\begin{proof}
(1) Let $\lambda \in \pspec(T)$. Then $Tx = \lambda x$ for some unit vector $x$ and so
\[ \inner{x,Tx} = \inner{x,\lambda x} = \lambda \inner{x,x} = \lambda\norm{x}^2 = \lambda, \]
which means that $\lambda \in \NumRange(A)$.

(2) Let $\lambda \in \rspec(T)$. Then there exists a unit vector $x \in \im(\lambda\id - T)^\perp$, so
\[ 0 = \inner{x,(\lambda\id-T)x} = \lambda\inner{x}^2 - \inner{x,Tx} = \lambda - \inner{x,Tx}, \]
which means that $\lambda \in \NumRange(A)$.

(3) Let $\lambda \in \apspec(T)$. Then there exists a Weyl sequence $\seq{e_n}$ for $\lambda$ by \ref{WeylSequence}. Then
\[ \norm{(\lambda\id - T)e_n} = \norm{e_n}\;\norm{(\lambda\id - T)e_n} \geq |\inner{e_n, (\lambda\id - T)e_n}| = |\lambda - \inner{e_n,Te_n}| \to 0. \]
Thus $\lambda\in\overline{\NumRange(T)}$.

Finally we have $\cspec(T)\subseteq\apspec(T)$ by \ref{approximateSpectrum} and so $\cspec(T)\subseteq\overline{\NumRange(T)}$.
\end{proof}
\begin{corollary}
If $T$ is a closed operator on a Hilbert space, then $\spec(T)\subseteq \overline{\NumRange(T)}$.
\end{corollary}

\section{Projectors and minimisation problems}
Every subspace is a convex, non-empty subset.
\begin{theorem}[Hilbert projection theorem]
Let $\mathcal{H}$ be a Hilbert space, $K$ a closed, convex, non-empty subset of $\mathcal{H}$.
\begin{enumerate}
\item There exists a unique element of $K$ of least norm. i.e.\ there exists a unique $k_0\in K$ such that
\[ \norm{k_0} = \inf\setbuilder{\norm{k}}{k\in K}. \]
i.e.\ $\min\setbuilder{\norm{k}}{k\in K}$ exists.
\item For any $h\in\mathcal{H}$ there exists a unique point $k_0$ in $K$ such that
\[ \norm{h-k_0} = \inf\{\norm{h-k}\;|\; k\in K\}. \]
We use this to define the distance $d(h,K) \defeq \norm{h-k_0}$.
\item If $K$ is a (closed) subspace, then $k_0$ is also the unique point in $K$ such that $(h-k_0)\perp K$.
\end{enumerate}
\end{theorem}
The idea for the first part of the proof is to take a sequence $\seq{\norm{k_i}}\to \inf\setbuilder{\norm{k}}{k\in K}$. By the parallelogram law $\seq{k_i}$ is Cauchy and by completeness it has a limit $k_0$.
\begin{proof}
(1) We can find a sequence $\seq{k_i}$ in $K$ such that $\norm{k_i}$ converges to $d = \inf\setbuilder{\norm{k}}{k\in K}$ by \ref{sequenceToSupInf}. By the parallelogram law
\begin{align*}
\norm{k_i-k_j}^2 &= 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4\norm{\frac{1}{2}(k_i+k_j)}^2 \\
&\leq 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4d^2
\end{align*}
the sequence $\seq{k_i}$ is Cauchy. So it converges to some $k_0$ in $K$ because $K$ is a closed subset of a complete space.

To prove uniqueness, take another $k_0'\in K$ such that $\norm{k_0'}=d$. By convexity $\tfrac{1}{2}(k_0 +k_0')\in K$, hence
\[ d\leq \norm{\tfrac{1}{2}(k_0+k_0')}\leq \tfrac{1}{2}(\norm{k_0}+\norm{k_0'}) = d. \]
So $\norm{\tfrac{1}{2}(k_0+k_0')} = d$. The parallelogram law gives
\[ d^2 = \norm{\frac{k_0+k_0'}{2}}^2 = d^2- \norm{\frac{k_0-k_0'}{2}}^2; \]
hence $\norm{k_0 - k_0'}^2 = 0$ and thus $h_0=k_0$.

(2) The element $k_0$ considered in point 1. is the point closest to a particular choice for $h$, namely $h=0$. For other $h$ consider the set $K-h$, which is again closed and convex.

(3) For all $k\in K$ and $a\in \mathbb{F}$, we have
\[ \norm{h-k_0}\leq \norm{h-k_0+ak} \]
and thus, by lemma \ref{orthogonality}, $(h-k_0)\perp k$, meaning $(h-k_0)\perp K$.

For the converse (i.e.\ uniqueness), suppose $f_0\in K$ such that $(h-f_0)\perp K$. Then for all $f\in K$ we have $(h-f_0)\perp (f_0 -f)$ so that
\begin{align*}
\norm{h-f}^2 &= \norm{(h-f_0) + (f_0-f)}^2 \\
&= \norm{h-f_0}^2 + \norm{f_0 - f}^2 \geq \norm{h-f_0}^2.
\end{align*}
So $\norm{h-f_0}=\inf\{\norm{h-k}\;|\; k\in K\} = d(h,K)$ and thus $f_0=k_0$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed vector subspace. Then $\mathcal{H} = K^\perp \oplus K$.
\end{corollary}
\begin{proof}
We need to prove every vector $x\in \mathcal{H}$ has a unique decomposition of the form
\[ x = y+z \qquad y\in K,\; z\in K^\perp. \]

Such a decomposition exists: we can take $y=k_0$ and $z = x-k_0$. We have already proved uniqueness. We can also give another argument for uniqueness: assume another such decomposition $x=y'+z'$. Then $y-y'= z-z'$ where the left side is in $K$ and the right in $K^\perp$. The only element in $K\cap K^\perp$ is $0$, so $y=y'$ and $z=z'$.
\end{proof}
The ability to make such decompositions in general is unique to Hilbert spaces, see theorem \ref{criterionHilbertSpace}.

\subsection{Orthogonal projection and decomposition}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. Given a subspace $K$ and an element $x \in \mathcal{H}$, we call the unique element $y\in K$ of the decomposition $K\oplus K^\perp$ the \udef{orthogonal projection} of $x$ on $K$. It is denoted $P_K(x)$. This defines a function $P_K:\mathcal{H}\to K$ called the \udef{orthogonal projection} on $K$.
\end{definition}

\begin{proposition}
Let $P$ be the orthogonal projection on a closed subspace $K$. Then
\begin{enumerate}
\item $P$ is a linear operator on $\mathcal{H}$;
\item $\norm{Px}\leq \norm{x}$ for all $x\in\mathcal{H}$;
\item $P^2 = P$;
\item $\ker P = K^\perp$ and $\im P = K$;
\item $\id_\mathcal{H} - P$ is the orthogonal projection of $\mathcal{H}$ onto $K^\perp$.
\end{enumerate}
\end{proposition}
\begin{proof}
These are mostly direct results of the decomposition. In particular 5. follows if we know $K^\perp$ is closed, which it is by proposition \ref{orthogonalComplementClosed}.
\end{proof}
\begin{corollary} \label{HilbertClosedSpaceOrthogonalDecomposition}
Let $\mathcal{H}$ ba a Hilbert space and $K$ a closed subspace, then $\mathcal{H} = K\oplus K^\perp$.
\end{corollary}
\begin{proof}
Let $P$ be the orthogonal projection on $K$. Then by \ref{directSumKernelImageIdempotent}
\[ \mathcal{H} = \im P \oplus \ker P = K\oplus K^\perp. \]
\end{proof}
\begin{corollary} \label{doubleComplementClosure}
Let $\mathcal{H}$ be a Hilbert space.
\begin{enumerate}
\item If $K$ is a subspace, then $(K^\perp)^\perp = \overline{K}$ is the closure of $K$.
\item If $A$ is a subset, then $(A^\perp)^\perp$ is the closed linear span of $A$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Assume $K$ is closed. Then using $0=(I-P_K)x\;\; \Leftrightarrow \;\; x=P_Kx$, we see
\[ (K^\perp)^\perp = \ker(I-P_K) = \im P_K = K. \]
Then, if $K$ is not closed, $(K^\perp)^\perp = (\overline{K}^\perp)^\perp = \overline{K}$, by proposition \ref{orthogonalComplementClosed}.

(2) Using \ref{OrthogonalComplementProperties} we calculate $(A^\perp)^\perp = (\Span(A)^\perp)^\perp = \overline{\Span(A)}$.
\end{proof}
\begin{corollary} \label{denseZeroComplement}
Let $A$ be a subset of a Hilbert space $\mathcal{H}$. Then $\Span(A)$ is dense in $\mathcal{H}$ \textup{if and only if} $A^\perp = \{0\}$.
\end{corollary}
\begin{proof}
The subspace $\Span(A)$ is dense in $\mathcal{H}$ iff $\overline{\Span(A)} = \mathcal{H}$ iff $(\Span(A)^\perp)^\perp = (A^\perp)^\perp = \mathcal{H}$ iff $A^\perp = \{0\}$.

In the last step we have used that $A^\perp$ is closed so that $((A^\perp)^\perp)^\perp = \overline{A^\perp} = A^\perp$, see \ref{orthogonalComplementClosed}.
\end{proof}

\subsubsection{Existence of orthonormal bases}
\begin{corollary}
Let $D$ be an orthonormal subset of a Hilbert space $\mathcal{H}$, then $D$ is an ortonormal basis \textup{if and only if} it is maximal.
\end{corollary}
\begin{proof}
This is a restatement of the previous corollary in the language of \ref{characterisationMaximalOrthonormalSet}.
\end{proof}
\begin{corollary}
Every Hilbert space has an orthonormal basis.
\end{corollary}
\begin{proof}
Every inner product space has a maximal orthonormal set by \ref{exitenceMaximalOrthonormalSet}. This maximal orthonormal set is an orthonormal set by the proposition.
\end{proof}
\begin{corollary} \label{HilbertOnBasisMaximal}
An orthonormal subset of a Hilbert space is an orthonormal basis \textup{if and only if} it is maximal.
\end{corollary}

\begin{lemma}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed subspace. Let $\{e_i\}_{i\in I}$ be an orthonormal basis of $K$. Then
\[ P_K(x) = \sum_{i\in I} \inner{e_i,x}e_i. \]
\end{lemma}
\begin{proof}
We can extend $\{e_i\}_{i\in I}$ to an orthonormal basis $\{e_i\}_{i\in J}$ of $\mathcal{H}$. Then
\[ x = \sum_{i\in J}\inner{e_i,x}e_i = \sum_{i\in I}\inner{e_i,x}e_i + \sum_{i\notin I}\inner{e_i,x}e_i, \]
which is clearly a decomposition in $K\oplus K^\perp$. This is unique, so we have found $P_K(x)$.
\end{proof}

\subsubsection{When are inner product spaces complete?}
Notice that some of the results obtained for Hilbert spaces have one direction that is generally true for inner product spaces: in any inner product space we have
\begin{itemize}
\item $\overline{K}\subset (K^\perp)^\perp$;
\item $\Span(A)$ dense in $\mathcal{H}$ implies $A^\perp = \{0\}$;
\item if $D$ is an orthonormal basis, then it is maximal.
\end{itemize}
See \ref{orthogonalComplementClosed}, \ref{orthogonalComplementDenseSpace} and \ref{characterisationMaximalOrthonormalSet}.

The converses are only true for Hilbert spaces.
\begin{theorem} \label{criterionHilbertSpace}
Let $H$ be an inner product space. If any of the following hold, $H$ is a Hilbert space:
\begin{enumerate}
\item For any orthonormal set $D$,
\[ \text{$D$ is maximal} \quad\implies\quad \text{$D$ is an orthonormal basis.} \]
\item For any subset $A$, $A^\perp = \{0\}$ implies $\Span(A)$ is dense in $H$.
\item For any subspace $K$, we have $(K^\perp)^\perp = \overline{K}$.
\item For all closed subspaces $K$ we can decompose $H = K\oplus K^\perp$.
\end{enumerate}
\end{theorem}
\begin{proof}
We prove the first statement implies $H$ is a Hilbert space. The other three imply the first and thus that $H$ is a Hilbert space.
\begin{enumerate}
\item We prove the contrapositive: assume $H$ is not complete, we wish to show that 1. does not hold, i.e.\ there exists a maximal orthonormal subset of $H$ that is not an orthonormal basis.

Let $\mathcal{H}$ be the completion of $H$ and take a unit vector $v\in \mathcal{H}\setminus H$. Now working in the completion, we have the decomposition $\Span\{v\}\oplus \Span\{v\}^\perp$. Consider the subspace $\Span\{v\} + H = \Span\{v\}\oplus(H\cap \Span\{v\}^\perp)$. We can extend $\{v\}$ to a maximal orthonormal set $\{v\}\cup D$ by \ref{exitenceMaximalOrthonormalSet}.

We claim $D$ is the orthonormal set we want:

Firstly it is maximal.
Assume, towards a contradiction, that $D$ is not maximal in $H$, so there exists an orthonormal set $D'\supsetneq D$. Take $w\in D'\setminus D$ and let $w'$ be the normalisation of $w - \inner{v,w}v$. Then $w' \perp v$ and $w' \perp D$, so $\{v\}\cup D\cup\{w'\}$ is an orthonormal set in $\Span\{v\} + H$, which contradicts the maximality of $\{v\}\cup D$.

Secondly it cannot be total. Indeed if $\Closure_H(\Span(D)) = H\cap\overline{\Span(D)}$ were equal to $H$, then $H \subseteq \overline{\Span(D)}$ and thus $\mathcal{H} = \overline{H} \subseteq \overline{\Span(D)} \subseteq \mathcal{H}$, meaning $\overline{\Span(D)} = \mathcal{H}$. But $v\notin \overline{\Span(D)}$, so $\overline{\Span(D)} \neq \mathcal{H}$.

\item 2. clearly implies 1. We can also adapt the proof above to show 2. implies $H$ is a Hilbert space:
Assume $H$ is not complete and let $\mathcal{H}$ be the completion of $H$. There exists a $v\in \mathcal{H}\setminus H$. All orthogonal complements are taken in the completion.
The set
\[ U \defeq H\cap\{v\}^\perp \]
is not dense in $\mathcal{H}$ for the same reason $D$ was not total above. We claim that the orthogonal complement of $U$ in $H$ is $\{0\}$:
\[ U^\perp\cap H = \{0\}. \]
First we claim $U$ is dense in $\{v\}^\perp$: take a $w\in \{v\}^\perp$ and let $(x_n)_{n\in\N}\subseteq H$ converge to $w$ (this is possible because $w\in\mathcal{H}$ and $H$ is dense in $\mathcal{H}$). Fix some $x\in H$ such that $\inner{x,v}\neq 0$, then we have the following sequence in $U$ that converges to $w$:
\[ n\mapsto x_n - \inner{x_n,v}\frac{x}{\inner{x,v}}. \]
Then because $U$ is dense in $\{v\}^\perp$,
\[ U^\perp\cap H = \overline{U}^\perp\cap H = (\{v\}^\perp)^\perp \cap H = \Span\{v\}\cap H = \{0\}. \]
\item Assume 3. Let $D$ be a maximal orthonormal set. Then
\[ \overline{\Span(D)} = (\Span(D)^\perp)^\perp = (D^\perp)^\perp = \{0\}^\perp = H, \]
so $D$ is an orthonormal basis.
\item Assume 4. Let $D$ be a maximal orthonormal set. Then $D^\perp$ is a closed subspace, so
\[ H  = D^\perp \oplus (D^\perp)^\perp = \{0\} \oplus (D^\perp)^\perp = (\Span(D)^\perp)^\perp = \overline{\Span(D)}. \]
\end{enumerate}
\end{proof}

\subsubsection{Orthogonal decomposition}
\begin{theorem}
 A Banach space such all of its closed subspaces are complemented is isomorphic to a Hilbert space.
\end{theorem}
\begin{proof}
TODO Lindestrauss and Tzafriri in 1971. Only real??
\end{proof}

\begin{proposition} \label{directSumOrthogonalClosed}
Let $\mathcal{H}$ be a Hilbert space and let $\{V_i\}_{i\in I}$ be a family of closed, (pairwise) orthogonal subspaces. Then
\[ \bigoplus_{i\in I}V_i \qquad \text{is a closed subspace of $\mathcal{H}$.} \]
\end{proposition}
\begin{proof}
Let $(v_n)$ be a Cauchy sequence in $\bigoplus_{i\in I}V_i$ which converges to $w$. Let $v_{i,n}$ be the component of $v_n$ in $V_i$. By orthogonality we have
\[ \norm{v_n-v_m}^2 = \sum_{i\in I}\norm{v_{i,n}-v_{i,m}}^2. \]
Then
\[ \norm{v_{i,n}-v_{i,m}} \leq \norm{v_n-v_m} \]
which implies $(v_{i,n})_n$ is a Cauchy sequence in the closed space $V_i$ which therefore converges to $w_i\in V_i$. Now there are only a finite number of $i$ for which there exist non-zero $v_{i,n}$ (TODO proof!!!!). So then
\[ \lim_n v_n = \lim_n \sum_{i\in I}v_{i,n} = \sum_{i\in I}w_i \in \bigoplus_{i\in I}V_i \]
where the interchange of limits and last equality follow because the sums are finite.
\end{proof}

\begin{lemma} \label{cancellationOminus}
Let $\mathcal{H}$ be a Hilbert space and $A\supseteq B \supseteq C$ subspaces with $B$ closed. Then
\[ (A\ominus B)\oplus (B\ominus C) = A\ominus C.\]
\end{lemma}
\begin{proof}
Take $v\in(A\ominus B)\oplus (B\ominus C)$. Then either $\{v\}\perp C$ or $\{v\}\perp B$, but this implies $\{v\}\perp C$, so $v\in A\ominus C$.

Take $v\in A\ominus C$. We can uniquely write $v = v_1 + v_2 \in (A\ominus B)\oplus B = A$. We just need to show that $v_2\in B\ominus C$. Indeed assume $\inner{c,v_2}\neq 0$ for some $c\in C$. Then
\[ \inner{c, v} = \inner{c, v_1+v_2} = \inner{c,v_1}+\inner{c,v_2} = \inner{c,v_2} \neq 0, \]
so $v\notin A\ominus C$, a contradiction.
\end{proof}

\subsection{Projection and minimisation in finite-dimensional spaces}

\begin{lemma}
Let $K$ be a subspace of $\F^n$ spanned by the orthonormal basis $\{\vec{u}_i\}_{i=1}^k$. Then
\[ P_K = QQ^* \qquad\text{where}\qquad Q = \begin{bmatrix}
\vec{u}_1 & \vec{u}_2 & \hdots & \vec{u}_k
\end{bmatrix}. \]
\end{lemma}
\begin{proof}
$P_K(\vec{x}) = \sum_{i=1}^k\vec{u}_i\inner{\vec{u}_i,\vec{x}} = \sum_{i=1}^k\vec{u}_i \vec{u}_i^*\vec{x} = \left(\sum_{i=1}^k\vec{u}_i \vec{u}_i^*\right)\vec{x} = QQ^*\vec{x}$.
\end{proof}
\begin{corollary}
For any matrix $A$ with QR factorisation $A=QR$, we have
\[ P_{\Col(A)} = QQ^*. \]
\end{corollary}
In general $P_{\Col(A)} = A(A^*A)^{-1}A^*$.

\begin{proposition}[Normal equations]
Let $\{\vec{v}_i\}_{i=1}^k$ be linearly independent set of vectors in $\F^n$. Set $K = \Span\{\vec{v}_i\}_{i=1}^k$. Then for all $\vec{x}\in \F^n$
\[ P_K(\vec{x}) = \sum_{i=1}^k c_i \vec{v}_i, \]
where $\begin{bmatrix}
c_1 & c_2 & \hdots & c_k
\end{bmatrix}^\transp$ is the solution of
\[ \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix}\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{x}} \\ \inner{\vec{v}_2,\vec{x}} \\ \vdots \\ \inner{\vec{v}_k,\vec{x}}
\end{bmatrix}. \]
This system of linear equations is consistent, yielding a unique solution.
\end{proposition}
The equations in this proposition are known as \udef{normal equations} and the matrix
\[ G(\vec{v}_1, \ldots, \vec{v}_k) \defeq \begin{bmatrix}
\vec{v}_1^* \\ \vec{v}_2^* \\ \vdots \\ \vec{v}_k^*
\end{bmatrix}\begin{bmatrix}
\vec{v}_1 & \vec{v}_2 & \hdots & \vec{v}_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix} \]
is known as the \udef{Gram matrix} or \udef{Grammian}.
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A\in\F^{m\times n}$, $\vec{b}\in\F^m$ and $\vec{x}_0\in\F^n$. Then
\[ \min_{\vec{x}\in\F^n}\norm{\vec{b}-A \vec{x}} = \norm{\vec{b} - A \vec{x}_0} \]
if and only if
\[ A^*A \vec{x}_0 = A^* \vec{b}. \]
\end{proposition}
We regard $\vec{x}_0$ as the ``best approximate solution'' to the (not necessarily consistent) system $A \vec{x} = \vec{b}$.
\begin{proof}
TODO
\end{proof}

\subsection{Riesz representation}
\begin{theorem}[Riesz-FrÃ©chet representation theorem] \label{rieszRepresentation}
Let $\mathcal{H}$ be a Hilbert space. For every continuous linear functional $\omega\in \mathcal{H}'$, there exists a unique $v_\omega\in\mathcal{H}$ such that
\[ \omega(x) = \inner{v_\omega, x} \qquad \forall x\in\mathcal{H}. \]
Moreover, $\norm{v_\omega}_\mathcal{H} = \norm{\omega}_{\mathcal{H}'}$.
\end{theorem}  

The idea of the proof is as follows: consider $\mathcal{H} \cong \ker\omega \oplus \im\omega$. So we can find a subspace $U\subseteq \mathcal{H}$ such that $\mathcal{H} = \ker\omega\oplus U$. Clearly $\dim U = \dim\im\omega = \dim\F = 1$. Between $1$-dimensional spaces there can only be one linear map, up to rescaling. This map is given by $x\mapsto \inner{v,x}$ for some $v\in U$, where the scaling determines the $v$. So we choose $v$ such that $\omega|_U = x\mapsto \inner{v,x}$.

Now we want extend this form of $\omega|_U$ to the whole of $\mathcal{H}$. This works exactly if $v\in(\ker\omega)^\perp$. So we need $U=(\ker\omega)^\perp$ which is true if and only if $\mathcal{H} = \ker\omega\oplus U = \ker\omega\oplus (\ker\omega)^\perp$, which only works in general if $\ker\omega$ is closed and $\mathcal{H}$ is a Hilbert space. Now $\ker\omega$ is closed if and only if it is continuous, by \ref{continuousMapCriterion}.

With this idea we give a full proof:
\begin{proof}
If $\ker\omega = \mathcal{H}$, we can take $v_\omega = 0$.

Assume $\ker\omega\neq \mathcal{H}$, then $(\ker\omega)^\perp\neq \{0\}$ by \ref{denseZeroComplement}, because $\ker\omega$ is closed (\ref{continuousMapCriterion}). So we can take a non-zero $u\in (\ker\omega)^\perp$. We can choose it such that $\omega(u) = 1$, by rescaling. Now let $h\in\mathcal{H}$. We can write $h = (h - \omega(h)u)+\omega(h)u\in\ker\omega\oplus (\ker\omega)^\perp$, because $\omega(h - \omega(h)u) = 0$. So
\[ 0 = \inner{u,h - \omega(h)u} = \inner{u,h} - \omega(u)\norm{u}^2. \]
If $v_\omega = \norm{u}^{-2}u$, then $\omega(h) = \inner{v_\omega, h}$ for all $h\in\mathcal{H}$.

For uniqueness: assume we can find two vectors $v_\omega,v_\omega'$ such that for all $h\in\mathcal{H}$ we have $\omega(h) = \inner{v_\omega, h} = \inner{v_\omega', h}$. Then $v_\omega - v_\omega'\perp \mathcal{H}$, so $v_\omega - v_\omega'= 0$.
\end{proof}
Together with lemma \ref{innerBoundedFunctionals} this gives:
\begin{corollary} \label{RieszIsometry}
The map $C_\mathcal{H}:\mathcal{H}\to \tdual{\mathcal{H}}: v\mapsto \inner{v,\cdot}$ is a bijective anti-linear isometry.
\end{corollary}
\begin{corollary}
Every Hilbert space is reflexive.
\end{corollary}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Every bounded functional defined on a closed subspace of $\mathcal{H}$ can be extended to a functional on $\mathcal{H}$ with the same norm.
\end{corollary}
\begin{proof}
The functional on the closed subspace, say $K$, can be represented as $x\mapsto \inner{v,x}_K$ for some $v\in K$. The extended functional is then simply given by $x\mapsto \inner{v,x}_\mathcal{H}$.
\end{proof}

\begin{proposition}[Representation of sesquilinear forms] \label{sesquilinearRepresentation}
Let $\mathcal{H}_1,\mathcal{H}_2$ be Hilbert spaces over $\mathbb{F}$ and $h:\mathcal{H}_1,\mathcal{H}_2\to\mathbb{F}$ a bounded sesquilinear form. Then there exists a unique bounded operator $S:\mathcal{H}_1 \to \mathcal{H}_2$ such that
\[ h(x,y) = \inner{Sx,y}. \]
This operator has the property $\norm{S} = \norm{h}$.
\end{proposition}
\begin{proof}
For fixed $x$, $y\mapsto h(x,y)$ is a bounded linear functional, so by the Riesz representation theorem \ref{rieszRepresentation} this can be represented by a unique $v_x$. Let $S$ be the function $x\mapsto v_x$. Then $h(x,y) = \inner{Sx,y}$.

To prove this function $S$ is linear, take arbitrary $x_1,x_2\in \mathcal{H}_1;y\in \mathcal{H}_2$ and $\lambda \in \mathbb{F}$. Then
\begin{align*}
\inner{S(\lambda x_1+ x_2),y} &= h(\lambda x_1+ x_2, y) = \overline{\lambda} h(x_1,y)+h(v,y_2) \\
&= \overline{\lambda} \inner{Sx_1, y} + \inner{Sx_2, y} = \inner{\lambda Sx_1 + Sx_2,y},
\end{align*}
so $S$ is linear by lemma \ref{elementaryOrthogonality}.

The equality of norms follows from
\begin{align*}
\norm{h} = \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{|\inner{Sx,y}|}{\norm{x}\norm{y}} &\geq \sup_{\substack{x\neq 0 \\ Sx\neq 0}}\frac{|\inner{Sx,Sx}|}{\norm{x}\norm{Sx}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S} \\
&\leq \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{\norm{Sx}\norm{y}}{\norm{x}\norm{y}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S}
\end{align*}
where the second inequality is Cauchy-Schwarz.
\end{proof}

\section{Orthonormal bases}

Hamel basis / Schauder basis / Hilbert basis

Every Hilbert basis is Schauder basis if $V$ is separable.

Hamel basis too big in Banach space??

Necessity of completeness for existence of complete orthonormal system, i.e.\ orthonormal system $\{a_i\}_{i\in I}$ (so $a_i \cdot a_j = \delta_{ij}$) with
\[ v = \sum_{i\in I}(a_i \cdot v)a_i \]
for all $v$. This is equivalent with
\[ v \cdot w = \sum_{i\in I}(v\cdot a_i)(a_i \cdot w) \]
for all $v,w$.


\begin{theorem}[Riesz-Fischer]
Let $\{e_i\}_{i\in I}$ be an orthonormal basis of a Hilbert space $H$ and $\alpha: I\to \C$ a net. Then
\[ \sum_{i\in I}\alpha_i e_i \]
converges \textup{if and only if} $\sum_{i\in I}|\alpha_i|^2 < \infty$. 
\end{theorem}
\begin{proof}
If $\sum_{i\in I}\alpha_i e_i$ converges, then $\sum_{i\in I}|\alpha_i|^2$ is bounded by the Bessel inequality \ref{BesselInequality}.

By monotone convergence, $\sum_{i\in I}|\alpha_i|^2 < \infty$ is equivalent to saying the sum converges. By (ref TODO) $\alpha$ has finite support. So $\sum_{i\in I}\alpha_i e_i$ can be expressed as the series
\[ \sum_{k\in \N}\alpha_{i_k} e_{i_k}. \]
By completeness it is enough to show that $\seq{s_n} = \seq{\sum_{k=0}^n\alpha_{i_k} e_{i_k}}$ is Cauchy. Let $n < m$, then
\[ \norm{s_n - s_m}^2 = \norm{\sum_{k=m+1}^n\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n\norm{\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n |\alpha_{i_k}|^2 = \sum_{k=0}^n|\alpha_{i_k}|^2 -\sum_{k=0}^m|\alpha_{i_k}|^2.  \]
Since $\seq{\sum_{k=0}^n |\alpha_{i_k}|^2}$ is convergent, it is Cauchy and thus so is $\seq{s_n}$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $D$ be an orthonormal basis of $\mathcal{H}$. Then $\mathcal{H}$ is isometrically isomorphic to $\ell^2(D)$.
\end{corollary}
\begin{corollary}
Hilbert spaces whose orthonormal bases have the same cardinality are isometrically isomorphic.
\end{corollary}

??
\begin{lemma}
Let $(\Omega,\mathcal{A}, \mu)$ be a measure space. Then $L^2(\Omega, \mu)$ is separable \textup{if and only if} $\mu$ is $\sigma$-finite.
\end{lemma}

\begin{lemma}
Let $\{\phi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Omega, \mu)$ and $\{\psi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Lambda, \nu)$, then $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis of $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{lemma}
\begin{proof}
The set $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is orthonormal:
\[ \iint_{\Omega\times\Lambda} \phi_n(x)\psi_m(y)\overline{\phi_{n'}(x)\psi_{m'}(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega\phi_n(x)\overline{\phi_{n'}(x)}\diff{\mu(x)} \cdot \int_\Lambda\psi_m(y)\overline{\psi_{m'}(y)}\diff{\nu(y)} = \delta_{n,n'}\delta_{m,m'}, \]
using Fubini's theorem and the HÃ¶lder inequality (TODO refs).

To show $D = \{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis, we verify point 5. of \ref{totalONBParsevalEquivalence}: if $f\perp D$, then $f = 0$.

If $f\perp D$, then for all $m,n\in \N$
\[ 0 = \inner{f, \phi_n\psi_m} = \iint_{\Omega\times\Lambda}f(x,y)\overline{\phi_n(x)\psi_m(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega \left(\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \right)\overline{\phi_n(x)}\diff{\mu(x)}.  \]
Using point 5. of \ref{totalONBParsevalEquivalence} in $L^2(\Omega,\mu)$, we see that for all $m$ the function $x\mapsto\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)}$ is $0$ as an element of $L^2(\Omega, \mu)$, i.e.\ it is $0$ a.e. as a function of $x$. Let
\[ E_m = \setbuilder{x\in\Omega}{ \int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \neq 0} \]
and set $E = \bigcup_{m\in\N}E_m$.
Then
\[ \mu(E) =  \mu\left(\bigcup_{m\in \N}E_m\right) \leq \sum_{m\in\N}\mu(E_m) = 0. \]

For $x\notin E$, we have $\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} = 0$, so by the same logic $f(x,y) = 0$ for almost all $y$. 

Now $|f|^2$ is integrable and
\[ \iint_{\Omega\times \Lambda}|f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = \int_{\Omega\setminus E}\int_\Lambda |f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = 0, \]
so $f=0$ in $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{proof}


\section{Adjoints of operators}
\begin{definition}
Let $H,K$ be Hilbert spaces and $T: H\not\to K$ an operator. An \udef{adjoint} of $T$ is an operator $S: K\not\to H$ such that
\[ \inner{w,Tv}_K = \inner{S w,v}_H \quad \forall v\in \dom(T),\; \forall w\in \dom(S). \]
\end{definition}

\begin{lemma} \label{adjointRequirementSymmetric}
Let $H,K$ be Hilbert spaces, $T\in (H\not\to K)$ and $S\in(K\not\to H)$. Then $T$ is an adjoint of $S$ \textup{if and only if} $S$ is an adjoint of $T$.
\end{lemma}
\begin{proof}
The definition of adjoint is symmetric in $S$ and $T$.
\end{proof}

\subsection{The adjoint}
\subsubsection{The adjoint as a relation}
\begin{lemma}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) - S_2(x) \in \dom(T)^\perp$.

Conversely, let $S$ be an adjoint of $T$ and $x\in\dom(S)$. Then for all $v\in \dom(T)^\perp$ there exists an adjoint $S'$ such that $S'(x) = S(x) + v$.
\end{lemma}
\begin{proof}
For all $u\in \dom(T)$ we have
\[ \inner{S_1(x) - S_2(x), u}_H = \inner{S_1(x), u}_H - \inner{S_2(x), u}_H = \inner{x, Tu}_K - \inner{x, Tu}_K = 0. \]
So $(S_1(x) - S_2(x)) \in \dom(T)^\perp$.

For the converse, set $S' = S + \frac{\inner{x,\cdot}_K}{\inner{x,x}_K}v$. This is an adjoint: for all $a\in \dom(T), b\in \dom(S') = \dom(S)$ we have
\[  \inner{S' b,a}_H = \inner{Sb, a}_H + \frac{\inner{x,b}_K}{\inner{x,x}_K}\inner{v,a}_H = \inner{Sb, a}_H = \inner{b,Ta}_K. \]
\end{proof}
\begin{corollary} \label{agreementAdjoints}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) = S_2(x)$.
\end{corollary}
\begin{proof}
We have $\dom(T)^\perp = \overline{\dom(T)}^\perp = H^\perp = \{0\}$. So $S_1(x) - S_2(x) = 0$.
\end{proof}
\begin{corollary} \label{maximalAdjointIsOperator}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}} \]
is the graph of an operator \textup{if and only if} $T$ is densely defined.
\end{corollary}

\begin{definition}
Let $T: H\not\to K$ be an operator between Hilbert spaces. We define the adjoint $T^*$ as the \emph{relation} on $(H,K)$ with graph
\[ \graph(T^*) \defeq \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}}. \]
\end{definition}
Note that, by \ref{maximalAdjointIsOperator}, the adjoint is a function if and only if $T$ is densely defined.

\begin{lemma} \label{everywhereDefinedAdjointLemma}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. If $S$ is an adjoint of $T$ that is defined everywhere, then $T^* = S$.
\end{lemma}
\begin{corollary}
Let $H$ be a Hilbert space. Then $\id_H^* = \id_H$.
\end{corollary}

\begin{example}
Consider the left- and right-shift operators
\begin{align*}
&S_L: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto (x_{n+1})_{n\in\N} \\
&S_R: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto \left(\begin{cases}
x_{n-1} & (n\geq 1) \\ 0 &(n=0)
\end{cases}\right)_{n\in \N}.
\end{align*}
Then $S_L^* = S_R$ and $S_R^* = S_L$. To see this, take $\seq{x_n}, \seq{y_n}\in \ell^2(\N)$. Then
\[ \inner{S_L\seq{x_n}, \seq{y_n}} = \sum_{n\in\N}\overline{x_{n+1}}y_n = \overline{x_0}\cdot 0 + \sum_{n\in\N\setminus\{0\}}\overline{x_{n}}y_{n-1} = \inner{\seq{x_n}, S_R\seq{y_n}}. \]
Thus $S_L$ is an adjoint of $S_R$ and $S_R$ is an adjoint of $S_L$. We conclude with \ref{everywhereDefinedAdjointLemma}.
\end{example}

\begin{lemma} \label{adjointRelationLemma}
Let $T: H\not\to K$ be an operator between Hilbert spaces and $(x,y)\in K\times H$. Then $(x, y)\in T^*$ \textup{if and only if}
\[ \forall z\in\dom(T): \; \inner{x, T(z)} = \inner{y, z}. \]
\end{lemma}
\begin{proof}
$\boxed{\Rightarrow}$ If $(x, y)\in T^*$, then there exists an adjoint $f: K\not\to H$ such that $f(x) = y$. Then for all $z\in \dom(T)$ we have $\inner{x, T(z)} = \inner{f(x), z} = \inner{y, z}$.

$\boxed{\Leftarrow}$ The function defined by $f(x) = y$ and extended to $\Span\{x\}$ by linearity is an adjoint.
\end{proof}

\begin{proposition} \label{adjointDomain}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \dom(T^*) = \setbuilder{x\in K}{\text{$\dom(T)\to \F: u\mapsto \inner{x, Tu}$ is a bounded functional}}. \]
\end{proposition}
\begin{proof}
$\boxed{\subseteq}$ If $\omega_x: u\mapsto \inner{x, Tu}$ is bounded, then its domain can be extended by continuity to $\overline{\dom(T)}$, which is a Hilbert space. This extended functional has a Riesz vector $x^*$ such that $\omega_x = u\mapsto \inner{x^*, u}$. The linear operator with domain $\Span\{x\}$ that maps $x$ to $x^*$ is then an adjoint.

$\boxed{\supseteq}$ If $x\in\dom(T^*)$, then, using the Cauchy-Schwarz inequality,
\[ |\inner{x,Tu}| = |\inner{T^*x,u}| \leq \norm{T^*x}\;\norm{u}, \]
so the functional $u\mapsto \inner{x, Tu}$ is bounded.
\end{proof}
\begin{corollary}
The domain $\dom(T^*)$ is a vector space and in particular contains $0$.
\end{corollary}

\begin{proposition} \label{HilbertAdjointGaloisConnection}
Let $H, K$ be Hilbert spaces. Take $T\in (H\not\to K)$ and $S\in (K\not\to H)$. Then
\[ S \subseteq T^* \iff T\subseteq S^*. \]
Thus $(*,*)$ is an antitone Galois connection between $\sSet{(H\not\to K), \subseteq}$ and $\sSet{(K\not\to H), \subseteq}$.
\end{proposition}
\begin{proof}
We have $S \subseteq T^*$ iff $S$ is an adjoint of $T$ iff $T$ is an adjoint of $S$ (by \ref{adjointRequirementSymmetric}) iff $T\subseteq S^*$.
\end{proof}
\begin{corollary} \label{HilbertAdjointAntitone}
Let $S,T: H\not\to K$ be operators between Hilbert spaces such that $S\subseteq T$. Then $T^* \subseteq S^*$.
\end{corollary}
\subsubsection{Properties of the adjoint relation}

\begin{proposition}
Let $T$ be an operator between Hilbert spaces and $\lambda\in\C$. If $\lambda \neq 0$, then
\[ \begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*) = (\lambda T)^*. \]
\end{proposition}
Note that if $T^*$ is a function (i.e.\ if $T$ is densely defined), then $\begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*) = \overline{\lambda}T^*$. We write the former in the proposition, because we have not made this assumption.

If $\lambda = 0$ and $T: H\not\to K$, then
\[ \begin{pmatrix}
\id & 0 \\ 0 & 0
\end{pmatrix} \graph(T^*) = \big(0: \dom(T^*)\to H\big) \subseteq \big(0: K\to H\big) = (0 T)^*, \]
where the last equality is given by \ref{adjointBoundedEverywhereDefined}.
\begin{proof}
For the inclusion $\subseteq$, take $f$ to be an adjoint of $T$. It is enough to show that $\overline{\lambda}f$ is an adjoint of $\lambda T$. This follows from
\[ \inner{\overline{\lambda}f(w), v} = \lambda\inner{f(w), v} = \lambda\inner{w,Tv} = \inner{w,\lambda Tv} \qquad \forall w\in \dom(f), v\in \dom(T). \]

For the other inclusion, let $f$ be an adjoint of $\lambda T$. It is enough to show that $\overline{\lambda^{-1}}f$ is an adjoint of $T$, because then $f = \overline{\lambda}\cdot\overline{\lambda^{-1}}f \subseteq \begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*)$. Indeed
\[ \inner{\overline{\lambda^{-1}}f(w), v} = \lambda^{-1}\inner{f(w),v} = \inner{w,\lambda^{-1}\lambda Tv} = \inner{w,Tv} \quad \forall w\in \dom(f), v\in \dom(T). \]
\end{proof}

\begin{proposition} \label{adjointGraph}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\begin{align*}
\graph(T^*) &= \left( \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T) \right)^\perp 
=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp.
\end{align*}
If $T$ is densely defined, then $T^*$ is a closed operator.
\end{proposition}
\begin{proof}
We have
\[ \graph(T^*) = \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}}. \]
Take an adjoint $S$ and $(w, Sw)$ in $\graph(S)$. Then for all $v\in\dom(T)$:
\[ 0 = \inner{w, Tv}_K - \inner{Sw, v}_H = \inner{w, Tv}_K + \inner{Sw, -v}_H = \inner{(w, Sw), (Tv,-v)}_{K\oplus H}. \]
So $(Tv,-v) = \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix} (v,Tv) \in \graph(S)^\perp $.

The final equality follows from \ref{perpUnderIsometry}, using the fact that $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ is a surjective isometry.

If $T$ is densely defined, then $T^*$ is a function by \ref{maximalAdjointIsOperator}. It is closed by \ref{orthogonalComplementClosed}.
\end{proof}
\begin{corollary} \label{adjointDenselyDefinedClosable}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces.
Then
\begin{enumerate}
\item $\graph(T^{**}) = \overline{\graph(T)}$;
\item $T^*$ is densely defined \textup{if and only if} $T$ is closable;
\item If $T$ is closable, then $\overline{T} = T^{**}$.
\end{enumerate}
\end{corollary}
\begin{proof}
From the proposition we have
\begin{align*}
\graph(T^{**}) &=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T^*)^\perp 
=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\left(\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp\right)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}^2\graph(T)^{\perp\perp} = -\graph(T)^{\perp\perp}
= \overline{\graph(T)}.
\end{align*}
The right-hand side is the graph of an operator iff $T$ is closable and the left-hand side is the graph of an operator iff $T^*$ is densely defined, by \ref{maximalAdjointIsOperator}.

For a closable operator, the closure is defined by $\overline{\graph(T)} = \graph(\overline{T})$.
\end{proof}

\begin{proposition} \label{adjointBoundedEverywhereDefined}
Let $T: H\to K$ be a densely defined operator between Hilbert spaces. Then $\dom(T^*) = K$ \textup{if and only if} $T$ is bounded.
\end{proposition}
\begin{proof}
The direction $\Leftarrow$ is given by \ref{adjointDomain}.

For the other direction, note that $T^*$ is closed by \ref{adjointGraph}. Then $T^*$ is bounded by the closed graph theorem \ref{closedGraphTheorem}. We use the direction $\Leftarrow$ to see that $\dom(T^{**}) = H$. Similarly, $T^{**}$ is closed by \ref{adjointGraph} and bounded by the closed graph theorem \ref{closedGraphTheorem}. Thus $T\subseteq \overline{T} = T^{**}$ is bounded.
\end{proof}

An important application of this proposition is the Hellinger-Toeplitz theorem \ref{HellingerToeplitz}.

\begin{proposition} \label{adjointAlgebraicProperties}
Let $T,S$ be compatible operators between Hilbert spaces. Then
\begin{enumerate}
\item $S^* + T^* \subseteq (S+T)^*$;
\item $S^*T^* \subseteq (TS)^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Let $f$ be an adjoint of $S$ and $g$ an adjoint of $T$. It is enough to see that $f+g$ is an adjoint of $S+T$. Indeed $\forall w\in \dom(f + g), v\in \dom(S+T)$
\[ \inner{(f + g)(w), v} = \inner{f(w),v} + \inner{g(w), Tv} = \inner{w,Sv} + \inner{w,Tv} = \inner{w,(S+T)v}. \]

(2) Let $f$ be an adjoint of $T$ and $g$ an adjoint of $S$. It is enough to see that $gf$ is an adjoint of $TS$. Indeed
\[ \inner{g\circ f(w), v} = \inner{f(w), Sv} = \inner{w,TSv} \qquad \forall w\in \dom(g\circ f), v\in \dom(TS). \]
\end{proof}

\begin{example}
The inclusions in \ref{adjointAlgebraicProperties} are, in general, not equalities.
\begin{itemize}
\item If $S,T$ are densely defined, but $\dom(S+T) = \dom(S)\cap \dom(T)$ is not dense, then there can clearly not be an equality.
\item Let $T: H\to K$ be a densely defined unbounded operator. Then $\dom(T^*) \neq K$ by \ref{adjointBoundedEverywhereDefined}. Now we have
\[ T^* - T^* = \big(0: \dom(T^*) \to H\big) \subsetneq \big(0: K\to H\big) = \big(0: \dom(T) \to K\big)^* = (T-T)^*. \]
The penultimate equality follows from \ref{adjointBoundedEverywhereDefined}. In this case the domain of the sum is dense, but still there is no equality.
\end{itemize}
\end{example}

There exist various conditions that make the inclusions in \ref{adjointAlgebraicProperties} equalities.
\begin{proposition} \label{equalityAlgebraicPropertiesAdjoint}
Let $T,S$ be compatible operators between Hilbert spaces.
\begin{enumerate}
\item if $T$ is densely defined, $\dom(S) \subseteq \dom(T)$ and $\dom\big((S+T)^*\big) \subseteq \dom(T^*)$, then $S^* + T^* = (S+T)^*$;
\item if $T$ is densely defined, $\im(S)\subseteq \dom(T)$ and $\dom\big((TS)^*)\subseteq \dom(T^*)$, then $S^*T^* = (TS)^*$;
\item if $S$ is densely defined and $\im(S)$ has finite codimension, then $S^*T^* = (TS)^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) By \ref{adjointAlgebraicProperties}, we have
\[ (S+T)^* - T^* \subseteq (S+T-T)^* = S^*, \]
where the last equality is due to $\dom(S) \subseteq \dom(T)$. Now take $x,y$ such that $x\in \dom\big((S+T)^*\big)$. Then $T^*(x)$ exists and we have the implications
\begin{align*}
x(S+T)^*y \iff& x\big((S+T)^* - T^* + T^*\big)y \\
\iff& \exists z: \; x\big((S+T)^* - T^*\big)z \land (z+T^*(x) = y) \\
\implies& \exists z: \; x(S^*)z \land (z+T^*(x) = y) \\
\iff& x(S^* + T^*)y.
\end{align*}
Thus $(S+T)^* \subseteq S^* + T^*$.

(2) We need to prove $(TS)^* \subseteq S^*T^*$. Assume $(x,y)\in (TS)^*$. By \ref{adjointRelationLemma}, we have
\[ \forall z\in \dom(TS):\; \inner{x, TS(z)} = \inner{y, z}. \]
Because $\im(S)\subseteq \dom(T)$, we have $\dom(TS) = \dom(S)$. Also, by assumption, $x\in \dom(T^*)$. So we have
\[ \forall z\in \dom(S):\; \inner{x, TS(z)} = \inner{T^*(x), S(z)} = \inner{y, z}, \]
which means that $\big(T^*(x), y\big)\in S^*$, so $(x,y)\in S^*T^*$.

(3)
\end{proof}
\begin{corollary}
If $T$ is bounded and everywhere defined, then
\[ S^* + T^* = (S+T)^* \qquad\text{and}\qquad S^*T^* = (TS)^*. \]
\end{corollary}

\url{https://arxiv.org/pdf/1507.08418.pdf}
\url{https://link.springer.com/article/10.1007/s43036-020-00068-4}

\begin{lemma} \label{HilbertAdjointLemma}
Let $S,T\in\Bounded(H,K)$ and $\lambda \in \mathbb{F}$.
\begin{enumerate}
\item $(T^*)^* = T$;
\item $(S+T)^* = S^* + T^*$;
\item $(\lambda T)^* = \bar{\lambda}T^*$;
\item $\id_V^* = \id_V$.
\end{enumerate}
Let $T\in\Bounded(H_1,H_2), S\in\Bounded(H_2,H_3)$
\begin{enumerate}
\setcounter{enumi}{4}
\item $(ST)^* = T^*S^*$.
\end{enumerate}
\end{lemma}

\begin{note}
Useful exercise: The identities of \ref{HilbertAdjointLemma} can also be proven by elementary manipulations. For example, to prove (1), we take arbitrary $v\in H$ and $w\in K$, Then
\[ \inner{w,Tv} = \inner{T^*w,v} = \overline{\inner{v,T^*w}} = \overline{\inner{(T^*)^*v,w}} = \inner{w, (T^*)^*v}. \]
By lemma \ref{elementaryOrthogonality} we have $Tv = (T^*)^*v$ for all $v\in V$. 
\end{note}

\subsubsection{Adjoints of densely defined operators}
The adjoint of an operator is a function if and only the operator is densely defined.

\begin{proposition} \label{adjointRangeCriterion}
Let $S: K\not\to H$ and $T: H\not\to K$ be linear operators between Hilbert spaces. If
\[ \im(S\cap T^*) = H \qquad\text{and}\qquad \im(T\cap S^*) = K, \]
then $S$ and $T$ are densely defined with $S^* = T$ and $T^* = S$.
\end{proposition}
\begin{proof}
Notice that $S\cap T^*$ and $T\cap S^*$ are linear operators that are adjoints of each other.

We claim that they are densely defined: take $x\in \dom(S\cap T^*)^\perp$. Then there exists some $y\in H$ such that $x = (T\cap S^*)y$ because of surjectivity. Now for all $z\in \dom(S\cap T^*)$
\[ 0 = \inner{z,x} = \inner{z, (T\cap S^*)y} = \inner{(S\cap T^*)z, y}, \]
so $\inner{z',y} = 0$ for all $z'\in H$, by surjectivity. This means, by \ref{elementaryOrthogonality}, that $y=0$ and thus also $x = (T\cap S^*)y = 0$. We conclude that $\dom(S\cap T^*)^\perp = \{0\}$, meaning $(S\cap T^*)$ is densely defined. The argument for $(T\cap S^*)$ is similar.

It follows that $S$ and $T$ must be densely defined. We have, by \ref{kernelImageAdjoint},
\[ \ker(S) = \im(S^*)^\perp \subseteq \im(T\cap S^*)^\perp = \{0\}. \]
Similarly $\ker(T) = \ker(S^*) = \ker(T^*) = \{0\}$.

So we have $\ker(S) = \ker(T^*)$, $\im(S)\subseteq \im(S\cap T^*)$ and $\im(T^*)\subseteq \im(S\cap T^*)$. The equality $S = T^*$ follows from \ref{partialFunctionSubset}. The equality $T = S^*$ is similar.
\end{proof}


\begin{proposition} \label{kernelImageAdjoint}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \forall v\in K: \; (v,0)\in T^* \iff v\in \im(T)^\perp. \]
If $T^*$ is densely defined, this reduces to
\begin{enumerate}
\item $\ker(T^*) = \im(T)^\perp$;
\item $\ker(T) \subseteq \im(T^*)^\perp$;
\item if $T$ is closed, then $\ker(T) = \im(T^*)^\perp$
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Because $\dom(T)$ is dense in $H$, we have $\dom(T)^\perp = \{0\}$ by \ref{orthogonalComplementDenseSpace}. Take $v\in K$. We have the equivalences
\begin{align*}
v\in \im(T)^\perp &\iff \forall x \in\dom(T): \inner{v, T(x)} = 0 \\
&\iff \forall x \in\dom(T): \inner{v, T(x)} = \inner{v, 0} \\
&\iff (v,0)\in T^*,
\end{align*}
using \ref{adjointRelationLemma}.

Point (1) is a direct translation in the case that $T^*$ is a function.

For point (2) note that $T\subseteq T^{**}$ (by \ref{adjointDenselyDefinedClosable}) implies that $(v,0)\in T \implies (v,0)\in T^{**}$.

For point (3): in this case $\ker(T) = \ker(T^{**}) = \im(T^*)^\perp$.
\end{proof}
\begin{corollary}[Closed range theorem for Hilbert spaces]
Let $T$ be a closed, densely defined operator between Hilbert spaces. Then the following are equivalent:
\begin{enumerate}
\item $\im(T)$ is closed;
\item $\im(T^*)$ is closed;
\item $\im(T) = \ker(T^*)^\perp$;
\item $\im(T^*) = \ker(T)^\perp$.
\end{enumerate}
\end{corollary}
\begin{proof}
By the proposition and \ref{orthogonalComplementClosed}, we have $\overline{\im(T)} = \ker(T^*)^\perp$. This shows $(1) \Leftrightarrow (3)$ and $(2) \Leftrightarrow (4)$.

TODO equivalence $(1)\Leftrightarrow (2)$.
\end{proof}
TODO ref closed range theorem for Banach spaces. This is, e.g., the case when $T$ is bounded below, see \ref{boundedBelowClosedRange}.

\begin{proposition}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item $\im(T)$ is dense in $K$ \textup{if and only if} $T^*$ is injective;
\item if $T$ and $T^*$ are injective, then $(T^*)^{-1} = (T^{-1})^*$;
\item if $T$ is closable and $\overline{T}$ is injective, then $\overline{T}^{\,-1} = \overline{T^{-1}}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) This is immediate from \ref{kernelImageAdjoint} and \ref{injectivityKernelTriviality}:
\[ \text{$\im(T)$ is dense} \quad\iff\quad \{0\} = \im(T)^\perp = \ker(T^*). \]

(2) We have $\graph(T^{-1}) = \begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)$. Also note that $\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}$ and $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ commute. Then we compute using \ref{adjointGraph}:
\begin{align*}
\graph((T^*)^{-1}) &= \begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\left(\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)\right)^\perp = \graph((T^{-1})^*).
\end{align*}
The penultimate equality follows from \ref{perpUnderIsometry}, using the fact that $\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}$ is a surjective isometry.
\end{proof}

\subsubsection{Adjoints of bounded operators}
\begin{proposition}
Let $T: H\to K$ be a densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item if $T\in\Bounded(H,K)$, then $T^*\in\Bounded(K,H)$;
\item if $T^*\in\Bounded(K,H)$, then $T$ is bounded. If $T$ is closed, then $T$ is defined everywhere.
\end{enumerate}
Assume $T\in\Bounded(H,K)$. Then
\begin{enumerate} \setcounter{enumi}{2}
\item $\norm{T} = \norm{T^*}$;
\item $T^* = C_H^{-1}T^tC_K$, where $C_K$ is the Riesz isometry from \ref{RieszIsometry}.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Assume $T\in\Bounded(H,K)$. Then $u\mapsto \inner{x,Tu}$ is a bounded functional for all $x\in K$, so $\dom(T^*) = K$ by \ref{adjointDomain}. Also $T^*$ is closed by \ref{adjointGraph}, so it is bounded by the closed graph theorem \ref{closedGraphTheorem}.

(2) Assume $T^*\in\Bounded(K,H)$. By the previous argument $T \subseteq \overline{T} = T^{**}\in\Bounded(H,K)$.

(3) The function $(x,u)\mapsto \inner{x,Tu}$ is a bounded sesquilinear form. By proposition \ref{sesquilinearRepresentation}, $T^*$ must be the unique $S$ from the proposition, which has norm $\norm{T}$.

(4) Finally we note that $C_H^{-1}T^tC_K$ is an adjoint with domain $K$ and conclude by \ref{everywhereDefinedAdjointLemma}.
\end{proof}

\begin{lemma}
The adjoint defines a map $*:\Bounded(H,K)\to \Bounded(K,H)$ that is anti-linear and continuous in the weak and uniform operator topologies. It is continuous in the strong operator topology \textup{if and only if} finite dimensional.
\end{lemma}
\begin{proof}
By the proposition the adjoint map is anti-linear. It is also bounded with norm $1$. Then by corollary \ref{boundedAntiLinearMaps} it must be bounded.

TODO
\end{proof}

\begin{proposition}
Let $H,K$ be Hilbert spaces and $T:H\to K$ a bijective bounded linear operator with bounded inverse. Then $(T^*)^{-1}$ exists and
\[ (T^*)^{-1} = (T^{-1})^*. \]
\end{proposition}
\begin{proof}
We prove $(T^{-1})^*$ is both a left- and a right-inverse of $T^*$: $\forall x\in H, y\in K$
\begin{align*}
\inner{T^*(T^{-1})^*x,y} &= \inner{x,T^{-1}Ty} = \inner{x,y} \\
\inner{x,(T^{-1})^*T^*y} &= \inner{TT^{-1}x,y} = \inner{x,y}
\end{align*}
So, by lemma \ref{elementaryOrthogonality}, $T^*(T^{-1})^* = \id_H$ and $(T^{-1})^*T^* = \id_K$.
\end{proof}

\begin{proposition} \label{normOfSquare}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\[ \norm{T^*T}= \norm{T}^2 = \norm{TT^*}. \]
\end{proposition}
\begin{proof}
For $\norm{T^*T}= \norm{T}^2$ first observe that
\[ \norm{T^*T} \leq \norm{T^*}\cdot\norm{T} = \norm{T}^2. \]
Conversely, $\forall x\in H$:
\[ \norm{T(x)}^2 = \inner{Tx,Tx} = \inner{T^*Tx,x} \leq \norm{T^*Tx}\cdot \norm{x} \leq \norm{T^*T}\cdot\norm{x}^2. \]
The other equality follows by applying the first to $T^*$ and using $\norm{T^*}=\norm{T}$.
\end{proof}

\subsection{Normal operators}
\begin{definition}
A densely defined linear operator $T$ on a Hilbert space $H$ is \udef{normal} if it is closed and $TT^* = T^*T$.
\end{definition}
Self-adjoint and unitary operators are normal.

TODO 3.10 Self-Adjoint, Unitary and Normal Operators from Kreyszig.


\begin{proposition} \label{normalCriterion}
Let $T: H\not\to H$ be a densely defined operator. Then $T$ is normal \textup{if and only if} $\dom(T) = \dom(T^*)$ and $\forall x\in H: \norm{Tx} = \norm{T^*x}$.
\end{proposition}
\begin{proof}
First, assume $T$ normal. Then
\[ \norm{Tx}^2 = |\inner{Tx,Tx}| = |\inner{T^*Tx,x}| = |\inner{TT^*x,x}| = |\inner{T^*x,T^*x}| = \norm{T^*x}^2. \]

For the converse, we have $\inner{Tx, Ty} = \inner{T^*x, T^* y}$ for all $x,y\in H$ by polarisation. From this we have $\inner{T^*Tx, y} = \inner{TT^*x, y}$ and normality follows from \ref{equalityOfMapsInnerProductSpaces}.

TODO question of domain.
\url{https://www.math.drexel.edu/faculty/mjz55/wp-content/uploads/sites/8/2017/01/normalnotes.pdf}.
\end{proof}
\begin{corollary} \label{equalityKernelAdjointNormal}
If $T$ is a normal operator, then $\ker T = \ker T^*$.
\end{corollary}
\begin{proof}
We have $x\in\ker(T) \iff \norm{Tx} = 0 \iff \norm{T^*x} = 0 \iff x\in\ker(T^*)$. 
\end{proof}
\begin{corollary}
If $T$ is a normal operator then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item $\spec(T) = \apspec(T)$.
\end{enumerate} 
\end{corollary}
\begin{proof}
If $T$ is normal, then so is $\lambda\id-T$. Now $\lambda\in\rspec(T)$ iff $\ker(\lambda\id - T) = \{0\}$ and $\im(\lambda\id-T)^\perp \neq \{0\}$, but $\im(\lambda\id-T)^\perp = \ker(\lambda\id-T)^* = \ker(\lambda\id-T)$. By \ref{kernelImageAdjoint} and the previous corollary. This is a contradiction.

(2) then follows straight from (1).
\end{proof}

\begin{theorem} \label{closureNumericRangeConvexHullSpectrum}
The closure of the numerical range of a normal operator is the
convex hull of its spectrum.
\end{theorem}
\begin{proof}
Normal operators $T$ are by definition closed, so $\spec(T)\subseteq \overline{\NumRange(T)}$ by \ref{spectralInclusionNumericalRange}. TODO
\end{proof}

\begin{lemma} \label{normalSpectralRadiusEqualsNorm}
For normal elements the spectral radius equals the norm.
\end{lemma}

\begin{lemma}
A normal operator on a Hilbert space is invertible \textup{if and only if} it is bounded below.
\end{lemma}

\begin{proposition}
Let $T$ be a normal operator on a Hilbert space. If $\lambda$ is an isolated point of the spectrum, then $\lambda$ is an eigenvalue.
\end{proposition}
\begin{proof}
Because $\lambda$ is isolated, the function
\[ f: \spec(T)\to \C: x\mapsto \begin{cases}
1 & (x=\lambda) \\
0 & (x\neq \lambda)
\end{cases} \]
is continuous.

Set $P = f(T)$ by continuous functional calculus (TODo ref!!). This is a projector by (TODO ref).

For all $t\in \spec(T)$, we have $tf(t) = \lambda f(t)$. By functional calculus, this gives $TP = \lambda P$.
\end{proof}

\subsection{Symmetric and self-adjoint operators}
\begin{definition}
Let $A$ be an operator on a Hilbert space.
\begin{itemize}
\item If $A^* = A$, we say $A$ is \udef{self-adjoint}.
\item If $A^* = -A$, we say $A$ is \udef{skew-adjoint}.
\end{itemize}
We denote the set of self-adjoint operators on a Hilbert space $H$ by $\SelfAdjoints(H)$.
\end{definition}

\begin{lemma}
Every self-adjoint operator is normal.
\end{lemma}
\begin{proof}
By \ref{normalCriterion}.
\end{proof}

\begin{lemma} \label{selfAdjointClosed}
Every self-adjoint operator is closed.
\end{lemma}
\begin{proof}
For any self-adjoint operator $A$, we have $A = A^{**} = \overline{A}$. Alternatively, note that all normal operators are closed.
\end{proof}

\subsubsection{Domain related matters}
\begin{lemma} \label{symmetricOperatorAdjointInclusion}
Let $A$ be a densely defined operator on a Hilbert space. Then
\begin{enumerate}
\item $A$ is symmetric \textup{if and only if} $A\subseteq A^*$;
\item if $A$ is symmetric, then is $A$ closable and $\overline{A} = A^{**}$ is symmetric.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) $A$ is symmetric iff it is an adjoint of itself, iff $A\subseteq A^*$.

(2) From (1) we see that $A^*$ is densely defined, because the superset of a dense set is dense. Then $A$ is closable by \ref{adjointDenselyDefinedClosable}.

To show symmetry of $\overline{A}$, we have (using the properties implied by \ref{HilbertAdjointGaloisConnection}) $A^{**}\subseteq A^*$ from $A\subseteq A^*$ and thus
\[ \overline{A} = A^{**} \subseteq A^* = A^{***} = \overline{A}^*. \]
\end{proof}

A symmetric operator $A$ is self-adjoint if and only if $\dom(A) = \dom(A^*)$.

\begin{corollary}
A closed and densely defined symmetric operator $A$ is self-adjoint \textup{if and only if} $A^*$ is also symmetric.
\end{corollary}
\begin{proof}
If $A$ is self-adjoint, then $A^*$ is self-adjoint and thus symmetric,

If $A^*$ is symmetric, then $A\subseteq A^* \subseteq A^{**} = A$.
\end{proof}

\begin{example}
TODO example of closed symmetric operator that is not self-adjoint (see differential operator below)
\end{example}

\begin{theorem}[Hellinger-Toeplitz] \label{HellingerToeplitz}
Everywhere-defined symmetric operators are bounded.
\end{theorem}
\begin{proof}
Assume $A: H\to H$ an everywhere-defined symmetric operator. Then $\dom(A) = H$. Also $A\subseteq A^*$ by \ref{symmetricOperatorAdjointInclusion}. Thus $H = \dom(A) \subseteq \dom(A^*) \subseteq H$. So $\dom(A^*) = H$. By \ref{adjointBoundedEverywhereDefined}, $A$ is bounded. 
\end{proof}

\begin{proposition} \label{selfAdjointMaximal}
A self-adjoint operator cannot have a proper symmetric extension.
\end{proposition}
\begin{proof}
Assume $A$ self-adjoint and $A\subseteq B$ for some symmetric operator $B$. Then
\[ A \subseteq B \subseteq B^* \subseteq A^* = A, \]
so $A = B$. We have used \ref{symmetricOperatorAdjointInclusion} and \ref{HilbertAdjointAntitone}.
\end{proof}
\begin{corollary}
Let $A$ be a densely defined symmetric operator. If $\overline{A}$ is self-adjoint, then it is the unique self-adjoint extension of $A$.
\end{corollary}
Note that $\overline{A}$ is always an operator by \ref{symmetricOperatorAdjointInclusion}.
\begin{proof}
Let $B$ be a self-adjoint extension of $A$. Then $\overline{A} = A^{**}\subseteq B^{**} = B$, by \ref{HilbertAdjointAntitone}. This means that $B$ is symmetric extension of the self-adjoint operator $\overline{A}$, which, by the proposition, implies $B = \overline{A}$.
\end{proof}
In general it is possible for an unbounded,
symmetric operator to not have a self-adjoint extension or have multiple self-adjoint extensions, even if it is densely defined. (TODO example)

\begin{definition}
Let $A$ be a densely defined symmetric operator whose closure is self-adjoint. We call $A$
\begin{itemize}
\item \udef{essentially self-adjoint};
\item a \udef{core} for $A$.
\end{itemize}
\end{definition}

\begin{example}
Consider the operator
\[ A: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(A) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a) = 0 = f(b)}. \]
Then
\begin{align*}
\inner{g, Af} &= \int_{a}^b \overline{g(x)}i\od{f(x)}{x}\diff{x} \\
&= \overline{g(b)}f(b) - \overline{g(a)}f(a) - \int_{a}^b \Big(i \od{}{x}\overline{g(x)}\Big)f(x)\diff{x} \\
&= \int_a^b \overline{i \od{g(x)}{x}} f(x) \diff{x} = \inner{Ag, f}.
\end{align*}
So $A$ is symmetric and $\dom(A^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)}$. We cannot extend $\dom(A)$ while keeping $\dom(A^*)$ the same, because $A$ would no longer be symmetric due to boundary terms.

There are, however, multiple ways we can extend $A$ to a self-adjoint operator (in each case $\dom(A^*)$ must shrink).

Let $A_\alpha$, for $\alpha\in \R$, be the operator $A$ with domain
\[ \dom(A_\alpha) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(b) = e^{i\alpha}f(b)}. \]
We must have $\forall f\in \dom(A_\alpha)$ and $g\in\dom(A^*_\alpha)$ that
\[ \overline{g(b)}f(b) - \overline{g(a)}f(a) = f(a)\Big(e^{i\alpha}\overline{g(b)} - \overline{g(a)}\Big) = 0, \]
so we have $e^{-i\alpha}g(b) = g(a)$ and thus $g(b) = e^{i\alpha}g(a)$, which means $\dom(A_\alpha^*) = \dom(A_\alpha)$. So $A_\alpha$ is a self-adjoint extension of $A$ for all $\alpha\in \R$.

TODO: compare Aharonov-Bohm TODO show closure \url{https://math.stackexchange.com/questions/214218/uniform-convergence-of-derivatives-tao-14-2-7}.
\end{example}
Notice that the operator
\[ T: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(T) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)} \]
is not symmetric. In this case
\[  \dom(T^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a)=0=f(b)}, \]
so $\dom(T^*) \subseteq \dom(T)$.

\subsubsection{Spectrum and related criteria}
TODO: $iA$ dissipative!
\begin{lemma}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. If $\exists z \in \C\setminus\R: \; \im(A+z\id) = H$, then $A$ is densely defined.
\end{lemma}
\begin{proof}
Let $A+z\id$ be surjective and suppose, towards a contradiction that there exists an $y\perp \dom(A)$. Then $y = (A+z\id)x$ for some $x\in\dom(A)$ by surjectivity. Then
\[ 0 = \Im\inner{x,y} = \Im\inner{x, (A+z\id)x} = \cancel{\Im\inner{x,Ax}} + \Im \inner{x,zx} = \Im(z)\norm{x}^2. \]
By assumption, $\Im(z) \neq 0$, so $x=0$, meaning $y = (A+z\id)x = 0$ and thus $\dom(A)^\perp = \{0\}$.
\end{proof}

\begin{proposition} \label{symmetricPlusiBoundedBelow}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then $A + z\id_H$ is bounded below by $|\Im z|$ for all $z \in \C\setminus\R$.
\end{proposition}
\begin{proof}
We first calculate, $\forall x\in H$:
\[ \Im\inner{x, (A+ z\id_H)x} = \cancel{\Im\inner{x,Ax}} + \Im z\norm{x}^2. \]
Thus
\[ |\Im z|\;\norm{x}^2 = |\Im\inner{x, (A + z\id_H)x}| \leq |\inner{x, (A + z\id_H)x}| \leq \norm{x}\;\norm{(A + z\id_H)x}, \]
which means that $\norm{(A + z\id_H)x} \geq |\Im z|\;\norm{x}$, so $A + z\id_H$ is bounded below by $|\Im z|$.
\end{proof}
\begin{corollary} \label{approximateSpectrumSymmetricOperator}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then $\apspec(A) \subseteq \R$.
\end{corollary}
\begin{corollary}
The eigenvalues of a symmetric operator are real.
\end{corollary}
\begin{proof}
This is immediate using $\pspec(A)\subseteq \apspec(A)$. We can also give a direct calculation:

Assume there exists an $x\in \ker(\lambda\id_H - A)\setminus\{0\}$. Then $Ax = \lambda x$ and thus
\[ \lambda\norm{x}^2 = \lambda\inner{x,x} = \inner{x, \lambda x} = \inner{x,Ax} = \inner{Ax,x} = \inner{\lambda x, x} = \overline{\lambda}\inner{x,x} = \overline{\lambda}\norm{x}^2. \]
Because $\norm{x}^2 \neq 0$, we have $\lambda = \overline{\lambda}$, meaning $\lambda$ is real.
\end{proof}
\begin{corollary} \label{symmetricResolvent}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then for all $\lambda\in\C\setminus\R$, the resolvent $R_A(\lambda)$ well-defined and bounded by $\norm{R_A(\lambda)}\leq 1/|\Im \lambda|$.
\end{corollary}
Note this does not mean $\C\setminus\R\subseteq \res(A)$, as $\dom(R_A(\lambda))$ may not be all of $H$.
\begin{proof}
This is an application of \ref{boundedBelow}.
\end{proof}

\begin{proposition} \label{rangeSelfAdjointCriterion}
Let $A$ be a symmetric operator on a Hilbert space $H$. The following are equivalent:
\begin{enumerate}
\item $\forall z \in \C\setminus\R: \; \im(A+z\id) = H = \im(A+\overline{z}\id)$;
\item $\exists z \in \C: \; \im(A+z\id) = H = \im(A+\overline{z}\id)$;
\item $A$ is self-adjoint;
\item $\rspec(A) = \emptyset$;
\item $A$ is closed and $\forall z \in \C\setminus \R: \; \ker(A^*+z\id) = \{0\} = \ker(A^*+\overline{z}\id)$;
\item $A$ is closed and $\exists z \in \C\setminus \R: \; \ker(A^*+z\id) = \{0\} = \ker(A^*+\overline{z}\id)$.
\end{enumerate}
In this case $\spec(A) = \apspec(A)$.
\end{proposition}
Notice that in (2) we include $\R$ and in (7) we exclude $\R$.
\begin{proof}
$(1) \Rightarrow (2)$ Trivial.

$(2) \Rightarrow (3)$ From \ref{symmetricOperatorAdjointInclusion}, we have $A\subseteq A^*$ and thus $A+z\id = (A^* + z\id)\cap(A+z\id)$. From point (1) of \ref{equalityAlgebraicPropertiesAdjoint}, we have $A+z\id = (A+\overline{z}\id)^*\cap (A+z\id)$.

We then use \ref{adjointRangeCriterion} with $S = A+z\id$ and $T = A+\overline{z}\id$ to obtain $A^* + \overline{z}\id = (A+z\id)^* = A+\overline{z}\id$. Subtracting $\overline{z}\id$ from each side yields the result.

$(3) \Rightarrow (4)$ Because self-adjoint operators are normal, we can use \ref{equalityKernelAdjointNormal}.

$(4) \Rightarrow (1)$ We have $\spec(A) = \apspec(A)$. Because $\apspec\subseteq \R$, by \ref{approximateSpectrumSymmetricOperator}, we have that $A+z\id$ is surjective for all $\C\setminus\spec(A) = \C\setminus\apspec(A) \supseteq \C\setminus\R$.

$(1,3) \Rightarrow (5)$ The closedness of $A$ follows from its self-adjointness.

Pick arbitrary $z \in \C\setminus\R$. Using \ref{kernelImageAdjoint}, we have
\[ \ker(A^* + z\id) = \ker(A+\overline{z}\id)^* =\im(A+\overline{z}\id)^\perp = H^\perp = \{0\}, \]
and something similar for $\ker(A^* + \overline{z}\id)$.

$(5) \Rightarrow (6)$ Trivial.

$(6) \Rightarrow (2)$ Pick some $z\in\C\setminus \R$ for which the statement holds. We have
\[ \overline{\im(A+z\id)} = \im(A+z\id)^{\perp\perp} = \ker\big((A^*+\overline{z})\big)^\perp = \{0\}^\perp = H. \]
We now just need to show that $\im(A+z\id)$ is closed. This follows because $A+z\id$ is bounded below by \ref{symmetricPlusiBoundedBelow} and thus we can apply \ref{boundedBelowClosedRange}.
\end{proof}
\begin{corollary}
Let $A$ be a symmetric operator on a Hilbert space $H$. The following are equivalent:
\begin{enumerate}
\item $A$ is essentially self-adjoint;
\item $\exists z \in \C\setminus\R: \; \overline{\im(A+z\id)} = H = \overline{\im(A+\overline{z}\id)}$;
\item $\exists z \in \C\setminus\R: \; \ker(A^*+z\id) = \{0\} = \ker(A^*+\overline{z}\id)$.
\end{enumerate}
\end{corollary}
\begin{corollary}
Every surjective symmetric operator is self-adjoint.
\end{corollary}
\begin{proof}
Take $z=0$ in point (1).
\end{proof}

\begin{proposition}
Let $A$ be a closed symmetric operator. Then one of the following cases holds:
\begin{itemize}
\item $A$ is self-adjoint, in which case $\spec(A) \subseteq \R$;
\item $\spec(A) = \overline{\C^{\uparrow}}$;
\item $\spec(A) = \overline{\C^{\downarrow}}$;
\item $\spec(A) = \C$.
\end{itemize}
If $A$ is not densely-defined, then the last case holds.
\end{proposition}
We have denoted the closed upper half plane $\overline{\C^{\uparrow}}$ and the closed lower half plane $\overline{\C^{\downarrow}}$.
\begin{proof}
First assume $A$ self-adjoint, then $\spec(A)\subseteq \R$ by a combination of \ref{approximateSpectrumSymmetricOperator} and \ref{rangeSelfAdjointCriterion}.

Now note that if there exists a real $\lambda\in\R$ such that $\lambda \in \res(A)$, then in particular $\lambda\id -A$ is surjective, so $A$ is self-adjoint by \ref{rangeSelfAdjointCriterion}.

Now assume $A$ not self-adjoint and pick a $\lambda\in \C^{\uparrow}$. From \ref{rangeSelfAdjointCriterion} we must have either $\lambda\in\spec(A)$ or $\overline{\lambda}\in\spec(A)$ (or both).

If $\lambda\in \res(A)$, then $\C^\uparrow \subseteq \res(A)$ and if $\overline{\lambda}\in\res(A)$, then $\C^\downarrow \subseteq \res(A)$.

Indeed take some $\mu\in\C$.
By \ref{symmetricResolvent} we only need to check surjectivity of $\mu\id - A$. We calculate
\begin{align*}
(\mu\id - A)R_A(\lambda) &= (\mu\id -\lambda\id+\lambda\id - A)R_A(\lambda) \\
&= (\mu-\lambda)R_A(\lambda) + (\lambda\id-A)R_A(\lambda) \\
&= (\mu-\lambda)R_A(\lambda) + \id \\
&= \id - (\lambda-\mu)R_A(\lambda).
\end{align*}
Now, using \ref{symmetricResolvent}, we have
\[ \norm{(\lambda-\mu)R_A(\lambda)} \leq |\mu-\lambda| \, |\Im(\lambda)|^{-1}. \]
If $|\mu-\lambda| < |\Im(\lambda)|$, then $(\lambda-\mu)R_A(\lambda)$ is a contraction and we can apply the Neumann series formula \ref{NeumannSeries} to see that $(\mu\id - A)R_A(\lambda)$ is bijective. In particular $\mu\id - A$ is surjective.

We can iterate this construction to cover the whole of $\C^\uparrow$. The argument for $\overline{\lambda}$ is similar.
\end{proof}

\begin{example}
Spectrum half plane TODO \url{https://math.stackexchange.com/questions/893899/spectrum-of-symmetric-non-selfadjoint-operator-on-hilbert-space}

\url{https://math.stackexchange.com/questions/925097/spectrum-of-self-adjoint-operator-on-hilbert-space-real}
\end{example}

\begin{proposition}
Let $T$ be a densely defined operator on a Hilbert space $H$. Then
\begin{enumerate}
\item $T+T^*$ is symmetric;
\item $T^*T$ and $TT^*$ are symmetric
\item if $T$ is closed, then $T^*T$ and $TT^*$ are self-adjoint.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We use \ref{adjointAlgebraicProperties} to get
\[ T+T^* \subseteq T^{**} + T^* \subseteq (T+T^*)^*. \]
We conclude by \ref{symmetricOperatorAdjointInclusion}.

(2) We use \ref{adjointAlgebraicProperties} to get
\[ T^*T \subseteq T^*T^{**} \subseteq (T^*T)^* \qquad\text{and}\qquad TT^* \subseteq T^{**}T^* \subseteq (TT^*)^*, \]
which means that $T^*T$ and $TT^*$ are symmetric by \ref{symmetricOperatorAdjointInclusion}.

(3) Because $T^*$ is closed, $\graph(T^*)$ is closed in $H\oplus H$. Thus
\begin{align*}
H\oplus H &= \graph(T^*) \oplus \graph(T^*)^\perp \\
&= \graph(T^*) \oplus \left(\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph{T}\right)^{\perp\perp} \\
&= \graph(T^*) \oplus \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph{T}.
\end{align*}
The last equality holds because $\graph(T)$ is closed (and $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ is a homeomorphism).

Then for all $v\in H$, we can write
\[ \begin{pmatrix}
0 \\ v
\end{pmatrix} = \begin{pmatrix}
y \\ T^*y
\end{pmatrix} + \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
x \\ Tx
\end{pmatrix} = \begin{pmatrix}
y - Tx \\ T^*y + x
\end{pmatrix}. \]
So $y = Tx$ and $v = T^*y + x = T^*Tx + x = (T^*T +\id)x$, which means that $T^*T +\id$ is surjective. Thus $T^*T$ is self-adjoint by \ref{rangeSelfAdjointCriterion}.

We can show $TT^* + \id$ is surjective by writing
\[ \begin{pmatrix}
v \\ 0
\end{pmatrix} = \begin{pmatrix}
y \\ T^*y
\end{pmatrix} + \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
x \\ Tx
\end{pmatrix} = \begin{pmatrix}
y - Tx \\ T^*y + x
\end{pmatrix}, \]
so $x = -T^*y$ and $v = y - Tx = y+TT^*y = (TT^* + \id)y$.
\end{proof}

\begin{example}
Let $T$ be a densely defined operator. Then $T+T^*$ and $T^*T$ are in general not self-adjoint. Closedness of $T$ is enough to make $T^*T$ self-adjoint. This is not the case for $T+T^*$.
\begin{itemize}
\item If $T$ is not closed, then $T+T^* \subsetneq T^{**} + T^* \subseteq (T+T^*)^*$.
\item It is even not necessarily self-adjoint if $T$ is closed. Let $T$ be a closed, symmetric, but not self-adjoint operator, for example.
\end{itemize}
\end{example}

\begin{proposition}
Let $A$ be a self-adjoint operator on a Hilbert space. Then $A$ is positive \textup{if and only if} $\spec(A)\subseteq [0,\infty[$.
\end{proposition}
\begin{proof}
\ref{closureNumericRangeConvexHullSpectrum}
\end{proof}

\begin{proposition}
Let $A$ be a self-adjoint operator. Then
\begin{enumerate}
\item $\inf \sigma(A) = \inf\NumRange(A)$;
\item $\sup \sigma(A) = \sup\NumRange(A)$.
\end{enumerate}
\end{proposition}
\begin{proof}
\ref{closureNumericRangeConvexHullSpectrum}
\end{proof}

\begin{proposition}
Let $T$ be a densely defined self-adjoint operator. Then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item let $\lambda_1,\lambda_2 \in \pspec(T)$ and $\lambda_1\neq \lambda_2$, then 
\[ \Null(\lambda_1\id - T) \perp \Null(\lambda_2 \id - T). \]
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}


\begin{proposition}
Let $T$ be a symmetric operator on a Hilbert space $H$. Then
\begin{enumerate}
\item the eigenvalues of $T$ are real;
\item the eigenvectors corresponding to distinct eigenvalues are orthogonal.
\end{enumerate}
\end{proposition}
\begin{proof}
This is an application of \ref{eigenspaceOrthogonalAdjoint} and \ref{adjointSpectrumNoResidual}.
\end{proof}

\subsubsection{Compact self-adjoint operators}
\begin{proposition}
Every compact self-adjoint operator $L$ on a nontrivial Hilbert space has an eigenvalue $\lambda$ with $|\lambda| = \norm{L}$.
\end{proposition}

\begin{proposition}
Let $A$ be a compact self-adjoint operator. Then the only possible accumulation point of $\spec(A)$ is $0$.
\end{proposition}
TODO self-adjoint not necessary? See \ref{spectrumCompactOperator}?
\begin{proof}
Assume $\spec(A)$ is infinite. Then take $\seq{\lambda_n}\subset \spec(A)$. Any associated sequence $\seq{x_n}$ of eigenvectors is orthogonal. We can take it to be orthonormal. By \ref{limitCompactImageOrthonormalSequence} we have
\[ 0 = \lim_{n\to\infty} \norm{Ax_n}^2 = \lim_{n\to\infty}\inner{Ax_n,Ax_n} = \lim_{n\to\infty}\lambda_n^2\inner{x_n,x_n} = \lim_{n\to\infty}\lambda_n^2, \]
so $\seq{\lambda_n}$ converges to $0$.
\end{proof}

\begin{theorem}
Every spectral value $\lambda\neq 0$ of a compact self-adjoint linear
operator $A : H \to H$ is an eigenvalue of finite multiplicity that can only
accumulate at $\lambda = 0$. Conversely, a self-adjoint operator having these
properties is compact.
\end{theorem}
\begin{proof}
TODO See \ref{spectrumCompactOperator}
\end{proof}

\subsubsection{Self-adjoint extensions of symmetric operators}
\paragraph{Cayley transform}
Consider the MÃ¶bius transform
\[ \C\setminus\{\overline{\lambda}\} \to \C: x\mapsto \frac{x - \lambda}{x-\overline{\lambda}} \qquad \text{for some $\lambda\in\C\setminus\R$.} \]
This transform maps
\begin{itemize}
\item the real line to $\T\setminus\{1\}$;
\item the half-plane above / below the real line containing $\lambda$ to the interior of the unit disk;
\item the half plane containing $\overline{\lambda}$ to the exterior of the unit disk;
\item in particular $\lambda \mapsto 0$ and $\overline{\lambda} \mapsto \infty$.
\end{itemize}
Conventional choice: $\lambda = i$.

\paragraph{Defect indices}
Or deficiency(?)
\url{https://link-springer-com.ezproxy.ulb.ac.be/content/pdf/10.1007/978-94-007-4753-1.pdf}

Cfr. dilation theory through Cayley transform.

See also Conway.

\subsubsection{Positive self-adjoint extensions of symmetric operators}
\begin{theorem}[Friedrich's extension]
Let $A$ be a positive symmetric operator on a Hilbert space $H$. Then $A$ has a unique positive self-adjoint extension $\widetilde{A}$ with domain $\dom(\widetilde{A}) \subseteq \Closure_{\norm{\cdot}_{A+\id}}(\dom(A))$.
\end{theorem}
By \ref{energyNormTopology}, we have
\[ \dom(\widetilde{A}) \subseteq \Closure_{\norm{\cdot}_{A+\id}}(\dom(A)) \subseteq \Closure_{\norm{\cdot}}(\dom(A)). \]
\begin{proof}
Set $H_A \defeq \Closure_{\norm{\cdot}_{A+\id}}(\dom(A))$.

For \undline{existence}, we can construct the operator $\widetilde{A}$ as follows:
\begin{align*}
\dom(\widetilde{A}) &\defeq \setbuilder{x\in H_A}{\exists x'\in H:\forall y\in H_A:\; \inner{y,x}_{A+\id} = \inner{y,x'}} \\
\widetilde{A}x &\defeq x' - x.
\end{align*}
Now $\widetilde{A}$ is an extension of $A$, because for all $x\in \dom(A)$, we can take $x' = Ax + x$. So $\widetilde{A}x = Ax$.

But $\dom(\widetilde{A})$ may be larger than $\dom(A)$, because we can extended $\inner{y,x}_{A+\id}$ to be defined on all of $H_A$ by continuity.

By construction $\dom(\widetilde{A}) \subseteq \Closure_{\norm{\cdot}_{A+\id}}(\dom(A))$.

Now we claim $\im(\widetilde{A} + \id) = H$. Indeed for any $x'\in H$, the functional $H_A \to H_A: y\mapsto \inner{y,x'}$ is bounded. By Riesz representiation \ref{rieszRepresentation}, we can find an $x\in H_A$ such that $\inner{y,x}_{A+\id} = \inner{y,x'}$. Thus $(\widetilde{A} + \id)x = x'$.

By \ref{rangeSelfAdjointCriterion} we conclude that $\widetilde{A}$ is self-adjoint. 

For \undline{uniqueness}, assume there exists a second such extension $\widehat{A}$. For all $y\in \dom(A)$ and $x\in \dom(\widehat{A})$, we have
\[ \inner{y, (\widehat{A}+\id)x} = \inner{(\widehat{A}+\id)y, x} = \inner{(A+\id)y, x} = \overline{\inner{x, (A+\id)y}} = \overline{\inner{x, y}_{A+\id}} = \inner{y, x}_{A+\id}. \]
By continuity this holds for all $y\in H_A$. And thus by definition $\widetilde{A}x = \widehat{A}x$ for all $x\in\dom(\widetilde{A})$. Thus $\widetilde{A} \subseteq \widehat{A}$, but self-adjoint operators are maximal by \ref{selfAdjointMaximal}, so $\widetilde{A} = \widehat{A}$.
\end{proof}

\subsubsection{Bounded self-adjoint operators}
\begin{lemma}
Let $A, B\in\Bounded(H)$. Then
\begin{enumerate}
\item $A^*A, AA^*$ and $A+A^*$ are self-adjoint;
\item if $A,B$ are self-adjoint, then $AB$ is self-adjoint \textup{if and only if} $A,B$ commute.
\end{enumerate}
\end{lemma}
\begin{corollary}
Let $A\in\Bounded(H)$. Then there exist unique self-adjoint operators $S,T$ such that
\[ A = S+iT \qquad A^* = S-iT. \]
\end{corollary}
\begin{proof}
Indeed $S = (A+A^*)/2$ and $T = (A-A^*)/2i$ are self-adjoint.
\end{proof}
\begin{corollary}
The operator $A$ is normal \textup{if and only if} $S,T$ commute.
\end{corollary}
\begin{proof}
We calculate the commutator
\[ [S,T] = \left[\frac{A+A^*}{2}, \frac{A-A^*}{2i}\right] = \frac{A^*A - AA^*}{2i} = \frac{1}{2i}[A^*, A]. \]
\end{proof}

\begin{proposition}
The set of bounded self-adjoint operators forms an anti-lattice.
\end{proposition}
\begin{proof}
TODO + generalised to self-adjoint operators??
\end{proof}

\subsection{Orthogonal projections}
\url{https://planetmath.org/latticeofprojections}

\url{https://zfn.mpdl.mpg.de/data/Reihe_A/35/ZNA-1980-35a-0437.pdf}

We denote the set op projections on a Hilberts space $\mathcal{H}$ by $\Projections(\mathcal{H})$.

TODO: $\im(P) = \ker{P^*}^\perp$ shows that we need $P= P^*$ for orthogonality.

\begin{proposition}
Let $P$ be a bounded operator $P$ on a Hilbert space $\mathcal{H}$. Then the following are equivalent:
\begin{enumerate}
\item $P$ is an orthogonal projection onto a closed subspace of $\mathcal{H}$;
\item $P^2 = P$ and $P=P^*$;
\item $P^2 = P$ and $\norm{P}\in \{0,1\}$;
\item $P^2 = P$ and $\norm{P}\leq 1$;
\end{enumerate}
\end{proposition}
\begin{proof}
$\boxed{(1)\Rightarrow (2)}$  Suppose first that $P$ is the orthogonal projection operator onto a closed subspace $K$. Clearly $P^2 = P$. Let $x,y\in\mathcal{H}$ and write $x= x_1+x_2, y = y_1+y_2$ where $x_1,y_1\in K$ and $x_2,y_2\in K^\perp$. Then
\[ \inner{Px, y} = \inner{x_1, y_1+y_2} = \inner{x_1, y_1} + \inner{x_1,y_2} = \inner{x_1,y_1} = \inner{x_1+x_2, y_2} = \inner{x,Py}. \]
So $P = P^*$.

$\boxed{(2)\Rightarrow (3)}$ We calculate $\norm{P} = \norm{P^2} = \norm{P^*P} = \norm{P}^2$ using \ref{normOfSquare}. The solutions to this equation are $\{0,1\}$.

$\boxed{(3)\Rightarrow (4)}$ This is clear.

$\boxed{(4)\Rightarrow (1)}$ Define $K=\im P$, then $K$ is closed because $x\in K$ iff $Px=x$ and thus for any converging sequence $(x_n)_n\subset K$: $\lim x_n = \lim Px_n = P\left(\lim x_n\right)$, so the limit is in $K$.

We just need to show orthogonality: $Px \perp x- Px$. For this we use \ref{orthogonality}: for all $a\in\F$
\[ \norm{Px} = \norm{Px + aPx - aPx} = \norm{P(Px + a(x-Px))} \leq \norm{P}\cdot \norm{Px + a(x-Px)} \leq \norm{Px + a(x-Px)}. \]
We conclude $Px \perp x- Px$.
\end{proof}

\begin{proposition} \label{projectorOrthogonalComplement}
Let $\mathcal{H}$ be a Hilbert space and let $P$ be an orthogonal projector on a closed subspace $K$. Then $\id-P$ is the orthogonal projector on $K^\perp$.
\end{proposition}
\begin{proof}
Any $x\in \mathcal{H}$ can be uniquely decomposed as $x_1 + x_2\in K\oplus K^\perp$. If $Px = x_1$, then $(\id - P)x = x_1 +x_2 - x_1 = x_2$.
\end{proof}
\begin{corollary} \label{projectorsIn01}
The set of projectors $\Projections(\mathcal{H})$ is a subset of $[0,\id]$.
\end{corollary}
\begin{proof}
Let $P\in\Projections(\mathcal{H})$. Then $P\geq 0$ follows from $P = P^2 = P^*P$.
\end{proof}

\begin{proposition} \label{commutingProjectors}
Let $\mathcal{H}$ be a Hilbert space and $P,Q$ be projections. The following are equivalent:
\begin{enumerate}
\item $PQ = QP$;
\item $PQ$ is a projection;
\item $QP$ is a projection;
\item $P+Q-PQ$ is a projection;
\item $\im(PQP) = \im(P) \cap \im(Q)$;
\item $PQP = QP$;
\item $\mathcal{H} = \big(\im(P)\cap\im(Q)\big)\oplus \big(\im(P)\cap\im(Q)^\perp\big) \oplus \big(\im(P)^\perp\cap\im(Q)\big) \oplus \big(\im(P)^\perp\cap\im(Q)^\perp\big)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Points (1), (2), (3) are equivalent by the equation $(PQ)^* = Q^*P^* = QP$, and the fact that (1) implies $(PQ)^2 = PQPQ = PPQQ = PQ$.

(4) If $P,Q$ commute, then
\begin{align*}
(P+Q-PQ)^* &= P+Q-(PQ)^* = P+Q-Q^*P^* =P+Q-QP = P+Q-PQ \\
(P+Q-PQ)^2 &= P^2 + PQ -P^2Q + QP+Q^2 - QPQ - PQP -PQP +PQPQ \\
&= P + Q + 3PQ - 4PQ= P+Q-PQ.
\end{align*}
Assume (4), then $(P+Q-PQ)^* = P+Q-QP = P+Q-PQ$. This implies $PQ=QP$.

$\boxed{(1)\Rightarrow (5)}$ Clearly $\im(PQP) \subseteq \im(P) \cap \im(Q)$.
For the inverse inequality, take $x\in im(P)\cap\im(Q)$. Then $PQP(x) = PQ(x) = P(x) = x$, so $x\in\im(PQP)$.

$\boxed{(5)\Rightarrow (6)}$ We decompose $\mathcal{H} = \im(PQP) \oplus \ker(PQP)$ and show that the operators are the same on both parts. For all $x\in \mathcal{H}$ we have
\[ x\in \ker(PQP) \iff \inner{x,PQPx} = 0 \iff \inner{QPx,QPx} = 0 \iff \norm{QPx} = 0 \iff x\in\ker{QP}.  \]
Now let $x\in\im(PQP) = \im(P)\cap\im(Q)$. Then $QPx = Qx = x = PQPx$.

$\boxed{(6)\Rightarrow (3)}$ $PQP$ is always a projection.

$\boxed{(6)\Rightarrow (7)}$ Take some $x\in \mathcal{H}$. Then we can uniquely decompose $x = P(x) + (x-P(x)) = x_P + x_{P^\perp} \in \im(P)\oplus \im(P)^\perp$. We can then further decompose $x_P = x_{P,Q} + x_{P,Q^\perp}$ and $x_{P^\perp} = x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. In order to have the decomposition of the proposition, we need to show that $x_{P,Q},x_{P,Q^\perp}\in \im(P)$ and $x_{P^\perp, Q},x_{P^\perp, Q^\perp}\in\im(P)^\perp$.

First take $x_{P,Q} = QPx$. From (6) we have $P(QPx) = PQPx = QPx$, so $x_{P,Q}\in \im(P)$. For the others we have similar calculations (also using the identity $PQP = PQ$):
\begin{align*}
P(x_{P,Q^\perp}) &= P\big((\id-Q)P\big)x = Px - PQPx = Px - QPx = (\id-Q)Px = x_{P,Q^\perp} \\
(\id-P)(x_{P^\perp,Q}) &= (\id-P)\big(Q(\id-P)\big)x = (Q-QP-PQ+PQP)x = (Q-QP)x = Q(\id-P)x = x_{P^\perp,Q} \\
(\id-P)(x_{P^\perp,Q^\perp}) &= (\id-P)\big((\id-Q)(\id-P)\big)x = (\id-P-Q+QP-P+P+PQ-PQP)x \\
&= (\id-Q-P+QP)x = (\id-Q)(\id-P)x = x_{P^\perp,Q^\perp}.
\end{align*}
$\boxed{(7)\Rightarrow (1)}$ Take $x\in \mathcal{H}$ and decompose it as $x_{P,Q} + x_{P,Q^\perp} + x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. Then $PQx = P(x_{P,Q} + x_{P^\perp, Q}) = x_{P,Q}$ and $QP = Q(x_{P,Q} + x_{P, Q^\perp}) = x_{P,Q}$, so $PQ = QP$. 
\end{proof}

\begin{proposition} \label{perpendicularProjections} \label{subspaceProjections}
Let $P,Q$ be orthogonal projections onto subspaces $\im(P)$ and $\im(Q)$ of $\mathcal{H}$.
\begin{enumerate}
\item The following are equivalent to $\im(P) \perp \im(Q)$:
\begin{enumerate}
\item $QP = 0$;
\item $PQ = 0$;
\item $Q+P$ is an orthogonal projection.
\end{enumerate}
\item The following are equivalent to $\im(P) \subseteq \im(Q)$:
\begin{enumerate}
\item $QP = P$;
\item $PQ = P$;
\item $Q-P$ is an orthogonal projection;
\item $P\leq Q$;
\item $\norm{Px} \leq \norm{Qx}$ for all $x \in \mathcal{H}$.
\end{enumerate}
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \perp \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a, b)\Leftrightarrow (c)}$ We know $(P+Q)^* = P^*+Q^* =P+Q$ and we can write
\[ (P+Q)^2 = P^2 + Q^2 + PQ + QP = P+Q+ PQ+QP,  \]
So clearly (a) or (b) imply (c). Conversely, assume $PQ + QP = 0$, implying $PQ=-QP$. By left- and right-multiplication by $P$ this implies both
\[ PPQ = PQ = -PQP \qquad \text{and} \qquad PQP = -QPP = -QP. \]
So $PQ = -PQP = QP$, meaning $PQ = 1/2(PQ+QP) = 0$.

(2) We prove the following:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \subseteq \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a,b)\Rightarrow (c)}$ Obviously $(Q-P)^*= Q-P$. Also
\[ (Q-P)^2 = Q+P-PQ-QP= Q+P-2P = Q-P. \]

$\boxed{(c)\Rightarrow (a,b)}$ Now from
\[ Q-P = (Q-P)^2 = Q+P-PQ-QP \]
we obtain $2P = PQ+QP$. The result then follows if we can show that $PQ=QP$. This follows by multiplying the equality on the left and on the right by $P$ to obtain $QP = 2P-PQP$ and $PQ = 2P-PQP$, respectively. 

$\boxed{(c)\Rightarrow (d)}$ This follows because all projections are positive.

$\boxed{(d)\Rightarrow (a, b)}$ Assume, towards a contradiction, that $\im(P)\nsubseteq \im(Q)$. Then we can take $v\in\im(P)\setminus \im(Q)$. Then
\[ \inner{v,(Q-P)v} = \inner{v,Qv} - \inner{v,v} = \inner{Qv,Qv} - \inner{Qv,Qv} - \inner{v-Qv, v-Qv} = -\norm{v-Qv}^2. \]
Because $v\notin \im(Q)$, $\norm{v-Qv}$ is not zero and thus $Q-P$ is not positive.

$\boxed{(d)\Leftrightarrow (e)}$ By the equivalence
\[ \norm{Px} \leq \norm{Qx} \iff \inner{Px,Px} \leq \inner{Qx,Qx} \iff \inner{Px,x}\leq \inner{Qx,x} \iff \inner{(Q-P)x,x}\geq 0. \]
\end{proof}

We can generalise part 2(d) of the previous proposition to a slightly larger class of operators.
\begin{lemma} \label{comparisonSelfAdjointProjection}
Let $P\in \Projections(\mathcal{H})$ and $T \in [0,\id]$, then the following are equivalent:
\begin{enumerate}
\item $\im(T) \subseteq \im(P)$;
\item $T\leq P$.
\end{enumerate}
\end{lemma}
\begin{proof}
As $T$ is self-adjoint, we have $\norm{T} = \nr(T) \leq 1$ by \ref{normNumRadius}.

Assume (1) so that for all $x\in \mathcal{H}$ we get
\[ \inner{x,Tx} = \inner{x, PTx} = \inner{Px,PTx} \leq \norm{Px}^2\nr(T) \leq \norm{Px}^2 = \inner{Px,Px} = \inner{x,Px}. \]
So $\inner{x, (P-T)x}\geq 0$ and thus $T\leq P$.

Assume (2). The energy form associated with $T$ is a pre-inner product by \ref{positiveOperatorPositiveEnergyForm}. The Cauchy-Schwarz inequality \ref{CauchySchwarz} gives
\[ |\inner{v,Tw}|^2 \leq \inner{v,Tv}\inner{w,Tw} \leq \inner{v,Pv}\inner{w,Pw}. \]
So if $v\in\im(P)^\perp$, then $\inner{v,Tw} = 0$ for all $w\in \mathcal{H}$. So $\im(T)\perp \im(P)^\perp$, implying $\im(T)\subseteq \im(P)^{\perp\perp} = \im(P)$.
\end{proof}

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space. Let $\{P_i\}_{i\in I}$ be an arbitrary subset of $\Projections(\mathcal{H})$ and let $K_i = \im(P_i)$ for all $i\in I$. Then, as a subset of $[0,\id]$,
\begin{enumerate}
\item $\inf \{P_i\}_{i\in I} = P_M$ where $M = \bigcap_{i\in I}K_i$;
\item $\sup \{P_i\}_{i\in I} = P_N$ where $N = \bigcap\setbuilder{K \subseteq \mathcal{H}}{\text{$K$ is closed} \land \forall i\in I: K_i \subseteq K}$.
\end{enumerate}
The set of projections on $\mathcal{H}$ is thus a complete lattice as a subset of $[0,\id]$.

If $I$ is finite, then $N = \Span(\bigcup_{i\in I}K_i)$. TODO: always closure of this $N$????
\end{proposition}
In particular this means $\Projections(\mathcal{H})$ is a complete lattice as itself, with the same suprema and infima. It is not a lattice as a subset of $\SelfAdjoints(\mathcal{H})$ (TODO + example ??).
\begin{proof}
(1) By \ref{subspaceProjections} $P_M$ is a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. Let $T$ be a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. By \ref{comparisonSelfAdjointProjection} $\im(T)\subseteq K_i$ for all $i\in I$, so $\im(T)\subseteq M$ and thus $T\leq P$ again by \ref{comparisonSelfAdjointProjection}. This means $P$ is the greatest lower bound.

(2) The mapping $T\mapsto \id-T$ keeps $[0,\id]$ invariant and inverts the order. Then $\inf \{\id - P_i\}_{i\in I}$ is a projection due to the previous point and so $\sup \{P_i\}_{i\in I}$ is also a projection. The expression for $N$ is clear from \ref{subspaceProjections}.
\end{proof}

\begin{proposition}
Let $P,Q$ be idempotents such that $\norm{P-Q}<1$. Then $\im(P) \cong \im(Q)$.
\end{proposition}
\begin{proof}
Kato p.34 TODO
\end{proof}

\subsubsection{Sets of pairwise disjoint projections}
TODO!

\subsubsection{Derivatives of orthogonal projections}



\begin{proposition}
Let $\{P_i\}_{i\in I}$ be a set of pairwise disjoint orthogonal projectors which have derivatives and take $i\neq j$ in $I$. Then
\begin{enumerate}
\item $P_i'P_j = - P_iP_j'$;
\item if $\id \in \upset \{P_i\}_{i\in I}$, then
\[ P_iP_i' = \sum_{j\neq i}P'_iP_j \qquad\text{and}\qquad P_i'P_i = \sum_{j\neq i}P_jP_i'. \]
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have $P_iP_j = 0$, so $0 = P_i'P_j + P_iP_j'$.

(2) We calculate, using $\id = \sum_{j\in I}P_j$ and \ref{derivativeIdempotent}:
\[ P_iP_i' = P_iP_i'\left(\sum_{j\in I}P_j\right) = P_iP_i'P_i + \sum_{j\neq i}P_iP_i'P_j = 0 - \sum_{j\neq i}P_iP_iP_j' = -\sum_{j\neq i}P_iP_j' = \sum_{j\neq i}P_i'P_j. \]
\end{proof}
\begin{corollary}
Let $P_1, P_2$ be orthogonal projections such that $P_1 + P_2 = \id$. Then
\[ P_1P_1'= P_1'P_2 \qquad \text{and}\qquad P_1'P_1 = P_2P_1'. \]
\end{corollary}


\subsection{Isometries}
We recall that isometries are injective and continuous. On Hilbert spaces they are also closed. See \ref{isometryInjective}, \ref{isometryContinuous} and \ref{isometryClosed}.

\begin{proposition} \label{isometryCharacterisation}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\begin{enumerate}
\item $T$ is an isometry \textup{if and only if} $T^*T = \id_H$;
\item $T$ is unitary \textup{if and only if} $T^*T = \id_H$ and $TT^* = \id_K$, i.e.\ $T^{-1} = T^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) For all $v,w\in H$ we have
\[ \inner{Tv,Tw} = \inner{T^*Tv,w}. \]
The left-hand side is equal to $\inner{v,w}$ iff $T$ is an isometry. The right-hand side is equal to $\inner{v,w}$ iff $T^*T = \id_H$, by \ref{equalityOfMapsInnerProductSpaces}.

(2) If $T$ is invertible, it must have a left and right inverse. By lemma \ref{leftRightInverse} they must be the same.
\end{proof}
\begin{corollary}
An isometry $T\in\Bounded(H)$ is unitary \textup{if and only if} it is normal.
\end{corollary}

\begin{lemma} \label{isometryRangeProjection}
Let $T$ be an isometry between Hilbert spaces $H$ and $K$. Then $TT^*$ is an orthogonal projection.
\end{lemma}
\begin{proof}
Clearly $(TT^*)^* = TT^*$. Also $(TT^*)^2 = T(T^*T)T^* = T\id_HT^* = TT^*$.
\end{proof}


\subsubsection{Wandering spaces and unilateral shifts}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear map. Then $\mathcal{V}$ is called a \udef{wandering space} for $T$ if $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$.
\end{definition}

\begin{lemma} \label{WoldLemma1}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear isometry.
\begin{enumerate}
\item $\mathcal{V}$ is a wandering space for $T$ \textup{if and only if} $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$;
\item $T[\mathcal{H}]^\perp$ is a wandering subspace for $T$;
\item if $\mathcal{V}$ is a wandering space for $T$, then $T^n[\mathcal{V}] \cong \mathcal{V}$ for all $n\in N$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) The direction $\Rightarrow$ is clear. For the converse, assume $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$. We need to show that $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$. WLOG we may assume $p\leq q$. Take arbitrary $x\in T^p[\mathcal{V}]$ and $y\in T^q[\mathcal{V}]$. Then
\[ \inner{x,y} = \inner{T^p(u), T^q(v)} = \inner{u, T^{q-p}(v)} = 0 \]
because $\mathcal{V} \perp T^{q-p}[\mathcal{V}]$.

(2) For all $n\geq 1$ we have
\[ T^{n}\big[T[\mathcal{H}]^\perp\big] \subset T^{n}[\mathcal{H}] = T\big[T^{n-1}[\mathcal{H}]\big] \subset T[\mathcal{H}] \perp T[\mathcal{H}]^\perp. \]

(3) For all $n\in \N$ the operator $T^n$ is an isometry. It is injective by \ref{isometryInjective}, and thus maps its domain bijectively to its image.
\end{proof}

\begin{definition}
An isometry $T$ on a Hilbert space $\mathcal{H}$ is called a \udef{unilateral shift} if there is a closed subspace $\mathcal{V}\subseteq \mathcal{H}$ that is wandering for $T$ such that
\[ \mathcal{H} = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
We call the subspace $\mathcal{V}$ \udef{generating} for $T$ and $\dim(\mathcal{V})$ the \udef{multiplicity} of $T$.
\end{definition}

By \ref{WoldLemma1}, we see that any isometry $T:\mathcal{H}\to\mathcal{H}$ is a unilateral shift when restricted to $\bigoplus_{n=0}^\infty T^n\big[T[\mathcal{H}]^\perp\big]$.



\begin{lemma} \label{WoldLemma2}
Let $T$ be an isometry on $\mathcal{H}$. If $T$ is a unilateral shift, then it is generated by $T[\mathcal{H}]^\perp$.
\end{lemma}
\begin{proof}
Let $\mathcal{V}$ be the generating subspace of the unilateral shift $T$. We calculate
\[ T[\mathcal{H}] = T\left[\bigoplus_{n=0}^\infty T^n[\mathcal{V}]\right] = \bigoplus_{n=1}^\infty T^n[\mathcal{V}] = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \ominus \mathcal{V} = \mathcal{H}\ominus \mathcal{V} = \mathcal{V}^\perp, \]
so $\mathcal{V} = T[\mathcal{H}]^\perp$.
\end{proof}

A unilateral shift is determined up to unitary equivalence by its multiplicity:
\begin{lemma}
Let $T: \mathcal{H}\to\mathcal{H}$ and $T':\mathcal{H}'\to\mathcal{H}'$ be unilateral shifts generated by $\mathcal{V}$ and $\mathcal{V}'$ such that $\dim(\mathcal{V}) = \dim(\mathcal{V}')$. Then there exists an unitary $U:\mathcal{H}'\to\mathcal{H}$ such that
\[ T' = U^*TU \]
\end{lemma}
\begin{proof}
Choose an isometric isomorphism $u:\mathcal{V}'\to\mathcal{V}$. Then any $x\in\mathcal{H}'$ can be written as $x = \sum_{n=0}^\infty T^n(x_n)$. Then define
\[ Ux = \sum_{n=0}^\infty T^n(ux_n). \]
\end{proof}

\begin{theorem}[Wold decomposition]
Let $\mathcal{H}$ be a Hilbert space and $T\in\Bounded(\mathcal{H})$ an isometry. Then $\mathcal{H}$ decomposes into an orthogonal sum $\mathcal{H} = \mathcal{H}_0\oplus \mathcal{H}_1$such that $\mathcal{H}_0, \mathcal{H}_1$ reduce $T$ and
\[ T|_{\mathcal{H}_0}\;\text{is unitary} \quad\text{and}\quad T|_{\mathcal{H}_1}\;\text{is a unilateral shift}. \]
This decomposition is uniquely determined and given by
\[ \mathcal{H}_0 = \bigcap_{n=0}^\infty T^n[\mathcal{H}] \qquad\text{and}\qquad \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \qquad\text{where}\qquad \mathcal{V} = T[\mathcal{H}]^\perp. \]
\end{theorem}
\begin{proof}
The subspace $\mathcal{V} = T[\mathcal{H}]^\perp$ is wandering by \ref{WoldLemma1}. Then $T$ is a unilateral shift in the subspace
\[ \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
Now $v\in\mathcal{H}_0 = \mathcal{H}_1^\perp$ if and only if it is perpendicular to $\bigoplus_{i=0}^n T^i[\mathcal{V}]$ for all $n$ and we have
\begin{align*}
\bigoplus_{i=0}^n T^i[\mathcal{V}] &= \bigoplus_{i=0}^n T^i[\mathcal{H}\ominus T[\mathcal{H}]] = \bigoplus_{i=0}^n T^i[\mathcal{H}]\ominus T^{i+1}[\mathcal{H}] \\
&= (\mathcal{H}\ominus T[\mathcal{H}])\oplus(T[\mathcal{H}]\ominus T^2[\mathcal{H}])\oplus \ldots \oplus (T^n[\mathcal{H}]\ominus T^{n+1}[\mathcal{H}])  = \mathcal{H} \ominus T^{n+1}[\mathcal{H}] 
\end{align*}
using \ref{perpUnderIsometry} and \ref{cancellationOminus}, which is applicable because $T^i[\mathcal{V}]$ is closed by \ref{isometryClosed}. So $\mathcal{H}_0\subseteq T^n[\mathcal{H}]$ for all $n$.

Finally $T|_{\mathcal{H}_0}$ is unitary because it is an isometry and surjective on $\mathcal{H}_0$.
\end{proof}

\subsubsection{Left and right shifts on $\ell^2$}
\begin{definition}
Consider the space $\ell^2(\N)$ with o.n. basis $\seq{e_i}$. Then
\begin{itemize}
\item the \udef{right shift operator} $S_r$ is the operator that maps $e_i \mapsto e_{i+1}$;
\item the \udef{left shift operator} $S_l$ is the operator that maps $e_i \mapsto \begin{cases}
e_{i-1} & i \geq 1 \\ 0 & i = 0
\end{cases}$.
\end{itemize}
\end{definition}

\begin{lemma}
$S_r$ is a unilateral shift
\end{lemma}

\begin{proposition}
$S_r = S^*_l$ (also converse?)
\end{proposition}

\subsubsection{Partial isometries}
\begin{definition}
An operator $T\in \Lin(H, H')$ is called a \udef{partial isometry} if there is a closed subspace $K\subseteq H$ such that
\begin{itemize}
\item $T|_K$ is an isometry;
\item $T|_{K^\perp} = 0$.
\end{itemize}
\end{definition}

Clearly every partial isometry is bounded.

\begin{lemma}
An operator $T\in \Lin(H, H')$ is a partial isometry \textup{if and only if} $T|_{\ker(T)^\perp}$ is an isometry.
\end{lemma}

\begin{proposition} \label{partialIsometryEquivalences}
Let $T\in \Bounded(H,H')$. The following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry;
\item $T^*TT^* = T^*$;
\item $TT^*T = T$;
\item $TT^*: H' \to H'$ is a projection;
\item $T^*T: H \to H$ is a projection;
\item $T^*$ is a partial isometry.
\end{enumerate}
Moreover,
\begin{enumerate}
\item $T^*T$ is the projection onto $\ker(T)^\perp$;
\item $\im(T)$ is closed and $TT^*$ is the projection onto $\im(T)$.
\end{enumerate}
\end{proposition}
\begin{proof}

$\boxed{(1)\Rightarrow (2)}$ By \ref{elementaryOrthogonality} it is enough to show that $\inner{T^*TT^*x,y} = \inner{T^*x,y}$ for all $x\in H', y\in H$. Take such $x,y$. We decompose $y = y_1\oplus y_2 \ker(T)\oplus \ker(T)^\perp$. Then
\[ \inner{T^*TT^*x, y_1} = \inner{TT^*x, Ty} = 0 = \inner{x,Ty_1} = \inner{T^*x, y_1} \]
and
\[ \inner{T^*TT^*x, y_2} = \inner{TT^*x, Ty_2} = \inner{T^*x,y_2}, \]
where we have used the fact that both $y_2$ and $T^*x$ are elements of $\ker(T)^\perp = \overline{\im(T^*)}$, and $T$ is an isometry on this space. In conclusion, we have
\[ \inner{T^*TT^*x,y} = \inner{T^*TT^*x,y_1} + \inner{T^*TT^*x,y_2} = \inner{T^*x,y_1} + \inner{T^*x,y_2} = \inner{T^*x,y} \]
for all $x\in H', y\in H$, so $T^*TT^* = T^*$.

$\boxed{(2) \Leftrightarrow (3)}$ By taking adjoints: $(TT^*T)^* = T^*TT^*$.

$\boxed{(2) \Rightarrow (4,5)}$ Clearly $T^*T$ and $TT^*$ are self-adjoint. We just need to show idempotency:
\[ (T^*T)^2 = (T^*T)(T^*T) = (T^*TT^*)T = T^*T \qquad (TT^*)^2 = (TT^*)(TT^*) = T(T^*TT^*) = TT^*. \]

$\boxed{(4) \Rightarrow (1)}$ Assume $TT^*$ a projection. Let $v\in \ker(T)^\perp = \overline{\im(T^*)}$. Then there exists a sequence $\seq{v_n}\in H^{\prime\N}$ such that $\lim_{n\to\infty}T^*v_n = v$. Then
\begin{align*}
\norm{Tv}^2 &= \lim_{n\to\infty}\norm{TT^*v_n}^2 = \lim_{n\to\infty}\inner{TT^*v_n,TT^*v_n} \\
&= \lim_{n\to\infty}\inner{(TT^*)^2v_n,v_n} = \lim_{n\to\infty}\inner{TT^*v_n,v_n} \\
&= \lim_{n\to\infty}\inner{T^*v_n,T^*v_n} = \lim_{n\to\infty}\norm{T^*v_n}^2 = \norm{v}^2,
\end{align*}
so $T$ is a partial isometry.

$\boxed{(5,6)}$ Applying the proposition to $T^*$ instead of $T$ yields the equivalences with $T=TT^*T$, and thus with the rest of the statements.

TODO + $\im(T^*) = \ker(T)^\perp$ means support and range are exchanged between $T$ and $T^*$.
\end{proof}

\begin{definition}
Let $T$ be a partial isometry. We call
\begin{itemize}
\item $T^*T$ the \udef{support projection} or \udef{initial projection} of $T$;
\item $TT^*$ the \udef{range projection} or \udef{final projection} of $T$.
\end{itemize}
\end{definition}

\begin{proposition}
Let $H,H'$ be Hilbert spaces with $K\subseteq H$ and $L\subseteq H'$ closed subspaces. Then the following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry with support $K$ and range $L$;
\item $(T,T^*)$ is a Galois connection between $\sSet{H, \perp_K}$ and $\sSet{H', \perp_L}$.
\end{enumerate}
Here $\perp_K$ is defined by
\[ x \perp_K y \quad\defequiv\quad P_K(x)\perp P_{K}(y). \]
\end{proposition}
\begin{proof}
The direction $(2) \Rightarrow (1)$ is immediate from \ref{partialIsometryEquivalences}, because $T,T^*$ are generalised inverses.

For the other direction, we first prove $T$ preserves the relational structure. Take arbitrary $x= x_1+x_2$ and $y=y_1+y_2$ in $K\oplus K^\perp$ such that $x\perp_K y$. Then
\[ \inner{T(x), T(y)} = \inner{T(x_1), T(y_1)} = \inner{x_1, y_1} = 0. \]
So $T(x)\perp T(y)$ and, because $T(x), T(y) \in L$, we have $T(x)\perp_L T(y)$. The argument for $T^*$ is similar.

For the Galois condition, we need to show that $T^*T(x)\perp_K y \implies x\perp_K y$. Indeed
\begin{align*}
T^*T(x)\perp_K y &\iff T^*T(x_1)\perp y_1 \\
&\iff 0= \inner{T^*T(x_1), y_1} = \inner{T(x_1), T(y_1)} = \inner{x_1,y_1} \\
&\iff P_K(x)\perp P_K(y).
\end{align*}
\end{proof}
\begin{corollary}
Let $T: H\to H'$ be a partial isometry with support $K$ and range $L$. Then
\[ T(x) \perp P_L(y) \iff P_K(x) \perp T^*(y) \]
for all $x\in H, y\in H'$.
\end{corollary}
\begin{proof}
This is the Galois identity \ref{GaloisIdentity}, although the direct proof is also very simple.
\end{proof}

\subsubsection{Unitaries}
\paragraph{Bilateral shifts}


\section{Dirac notation}
\url{https://core.ac.uk/download/pdf/25263496.pdf}
\url{https://michael-herbst.com/talks/2014.07.22_Mathematical_Concept_Dirac_Notation.pdf}
\url{http://galaxy.cs.lamar.edu/~rafaelm/webdis.pdf}
\url{https://plato.stanford.edu/entries/qt-nvd/}
\url{file:///C:/Users/user/Downloads/Abdus%20Salam,%20E.P.%20Wigner%20(Ed.)%20-%20Aspects%20of%20Quantum%20Theory%20-%20Dedicated%20to%20Dirac%E2%80%99s%2070th%20Birthday-Cambridge%20University%20Press%20(1972).pdf}
\url{https://aip.scitation.org/doi/pdf/10.1063/1.1705001}

\begin{lemma}
\begin{enumerate}
\item $T\ketbra{\varphi}{\psi} = \ketbra{T\varphi}{\psi} = \ketbra{\varphi}{\psi}T = \ketbra{\varphi}{T^*\psi}$;
\item $\ketbra{\varphi}{\psi}\ketbra{\xi}{\eta} = \inner{\psi, \xi}\ketbra{\varphi}{\eta}$;
\item $(\ketbra{\varphi}{\psi})^* = \ketbra{\psi}{\varphi}$.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $H$ be a Hilbert space and $\seq{e_i}_{i\in I}$ a basis for $H$. Then
\[ \id_H = \sum_{i\in I}\ketbra{e_i}{e_i} \qquad\text{in the strong limit.} \]
\end{lemma}
\begin{proof}
TODO!!
\end{proof}
\begin{lemma} \label{operatorBraketExpansion}
Let $H$ be a Hilbert space, $\seq{e_i}_{i\in I}$ a basis for $H$ and $T$ an operator on $H$. Then
\[ T = \sum_{i,j\in I}\braket[T]{e_i}{e_j}\; \ketbra{e_i}{e_j}. \]
in the strong limit.
\end{lemma}
\begin{proof}
TODO!! Tannery.
\end{proof}

\section{Hilbert space ideals}

\subsection{Finite-rank operators}
Remember that finite-rank operators are bounded by definition (this is not automatic, cfr. \ref{continuousMapCriterion}).

\begin{proposition}[Finite rank singular value decomposition] \label{finiteRankSingularValues}
Let $V$ be an inner product space and $T\in\Hom(V)$. Then $T$ is a finite-rank operator \textup{if and only if} $T$ can be written in the form
\[ T = \sum_{i=1}^N \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^N$ and $(w_i)_{i=1}^N$ are finite sets of vectors and $(\lambda_i)_{i=1}^N$ are positive (non-zero) numbers.

The numbers $(\lambda_i)_{i=1}^N$ in this decomposition are uniquely determined by the operator.
\end{proposition}
The numbers $(\lambda_i)_{i=1}^N$ are called the \udef{singular values} of the operator.
\begin{proof}
Because $\im(T)$ is finite-dimensional, we can find an orthonormal basis $(v_i)_{i=1}^N$ for it. Then we can write
\begin{align*}
Tx &= \sum_{i=1}^N \ket{v_i}\braket{v_i}{Tx} = \sum_{i=1}^N \ket{v_i}\braket{T^*v_i}{Tx} = \sum_{i=1}^N \ket{v_i}\braket{\lambda_i w_i}{Tx}  = \sum_{i=1}^N \lambda_i\ket{v_i}\braket{w_i}{Tx}
\end{align*}
where $\lambda_i = \norm{T^*v_i}$ and $w_i = \frac{T^*v_i}{\lambda_i}$.

We just need to show that the $\lambda_i$ are independent of the chosen basis $(v_i)_{i=1}^N$. TODO!!!!
\end{proof}
\begin{corollary}
Every finite-rank operator on a Hilbert space is a finite sum of rank-1 operators.
\end{corollary}

\begin{lemma}
Let $H$ be Hilbert space. The set of finite rank operators on $H$ is a $*$-ideal in $H$.
\end{lemma}

\subsection{Compact operators}

\url{https://math.stackexchange.com/questions/4198074/space-of-compact-operators-is-the-only-proper-closed-two-sided-ideal-of-the-spac}

\begin{proposition}
Let $T\in\Bounded(H)$. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item $T^*$ is compact;
\item there exists a sequence $(T_n)_{n\in\N}$ of finite rank operators such that $\norm{T-T_n}\to 0$.
\end{enumerate}
\end{proposition}
This is false in Banach spaces. (TODO Enflo, approximation property, goose problem)
\begin{proof}
TODO
\end{proof}
\begin{corollary}[Canonical expansion]
Any compact operator $T$ on a Hilbert space $\mathcal{H}$ can be written in the form
\[ T = \sum_{i=1}^\infty \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^\infty$ and $(w_i)_{i=1}^\infty$ are orthonormal sets and $(\lambda_i)_{i=1}^\infty$ is a monotonically decreasing sequence of positive numbers with $\lim_{i\to\infty}\lambda_i = 0$.
\end{corollary}
As in \ref{finiteRankSingularValues} for finite-rank operators we call $(\lambda_i)_{i=1}^\infty$ the \udef{singular values} of $T$. They are uniquely determined by the operator.
\begin{proof}
TODO (one way is with polar decomposition and spectral theorem. Are there others?)
\end{proof}
Compare with \ref{operatorBraketExpansion}.

\begin{lemma}
Let $H$ be a Hilbert space. Then the set of compact operators on $H$, $\Compact(H)$ is a two-sided $*$-ideal of $H$. 
\end{lemma}

\begin{proposition}
Let $H$ be a Hilbert space with orthonormal basis $(e_i)_{i\in I}$. If $T\in\Bounded(H)$ and
\[ \sum_{i\in I}\norm{Te_i}^2  < \infty, \]
then $T$ is a compact operator. + Converse??
\end{proposition}
\begin{proof}
TODO + weaken $T\in\Bounded(H)$?
\end{proof}
\begin{corollary}
An integral operator defined by a square integrable kernel $K\in L^2(A\times A, \mu)$ is compact.
\end{corollary}

\begin{proposition}
Let $T$ be an operator on a Hilbert space. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item for all sequences $\seq{x_n}$, weak convergence $x_n \overset{w}{\to} x$ implies the strong convergence $Ax_n \to Ax$;
\item for any two weakly convergent sequences $x_n\overset{w}{\to} x$ and $y_n\overset{w}{\to} y$ the energy form is continuous in both arguments:
\[ \lim_{n\to\infty}\inner{x_n,y_n}_T = \lim_{n\to\infty}\inner{x_n,Ty_n} = \inner{x,Ty} = \inner{x,y}_T. \]
\end{enumerate} 
\end{proposition}

\begin{lemma}
Let $H$ be a Hilbert space and $P\in\Projections(H)$. If $P$ is compact, then $P$ has finite rank.
\end{lemma}

\subsection{Positive operators}

\subsubsection{Polar decomposition}
\begin{proposition}
Let $H$ be a Hilbert space and $T\in \Bounded(H)$. There exists a unique partial isometry $V$ such that $T = V|T|$ and $\ker(V) = \supp(T)$.
\end{proposition}
TODO: should this be $\ker(V) = \ker(T)$??
\begin{proof}
TODO
\end{proof}
\begin{lemma}
There is only one positive operator $A$ such that $T = VA$ for some partial isometry.
\end{lemma}
\begin{proof}
TODO uniqueness positive squared root.
\end{proof}

\url{https://encyclopediaofmath.org/wiki/Polar_decomposition}

\subsection{Trace-class operators}
TODO Simon


\begin{proposition} \label{traceCommutatorCompactSA}
Let $H$ be a Hilbert space and $A,B\in\Lin(X)$ such that $B$ is compact self-adjoint, then $\Tr[A,B] = 0$.
\end{proposition}
\begin{proof}
Let $\seq{e_n}$ be an orthonormal basis of eigenvectors of $B$, with corresponding real eigenvalues $\lambda_n$. This exists by the spectral theorem (TODO ref). Then
\begin{align*}
\Tr[A,B] &= \sum_n\inner{e_n, [A,B]e_n} \\
&= \sum_n\inner{e_n, ABe_n} - \inner{e_n, BAe_n} \\
&= \sum_n\inner{e_n, ABe_n} - \inner{Be_n, Ae_n} \\
&= \sum_n\lambda_n\inner{e_n, Ae_n} - \lambda_n\inner{e_n, Ae_n} \\
&= 0.
\end{align*}
\end{proof}
\begin{corollary}
Let $H$ be a Hilbert space and $A,B\in\Lin(X)$ such that $A$ is self-adjoint and $B$ compact. If $[A,B]$ is trace-class, then $\Tr[A,B] = 0$.
\end{corollary}
\begin{proof}
If $[A,B]$ is trace-class, then $-[A,B]^* = [A,B^*]$ is also 
\end{proof}


\section{Dilation theory}
\subsection{Dilations, $N$-dilations and power dilations}
\begin{definition}
Let $\mathcal{H} \subseteq \mathcal{H}'$ be Hilbert spaces and let $P_\mathcal{H}$ be the projection on $\mathcal{H}$. If a pair of linear maps $S: \mathcal{H}'\to\mathcal{H}'$ and $T: \mathcal{H}\to \mathcal{H}$ satisfy the relation
\[ T = P_\mathcal{H} S |_\mathcal{H} \]
then $T$ is called a \udef{compression} of $S$ and $S$ a \udef{dilation} of $T$. This is abbreviated $T\prec U$.

\begin{itemize}
\item Let $N\in\N$. If $T^k = P_\mathcal{H} S^k |_\mathcal{H}$ for all $k\leq N$, then $S$ is called an \udef{$N$-dilation}.
\item If this holds for all $k\in\N$, then $S$ is called a \udef{power dilation}.
\item If $T^* = P_\mathcal{H} S^* |_\mathcal{H}$, we call TODO??
\end{itemize}
We call $\mathcal{H}'$ \udef{minimal} if the only reducing subspace for $S$ that contains $\mathcal{H}$ is $\mathcal{H}'$.
\end{definition}

If $S$ is a dilation of $T$, then we clearly have $T = P_\mathcal{H} S P_\mathcal{H}|_\mathcal{H}$.

\begin{lemma}
Let $S:\mathcal{H}'\to\mathcal{H}'$ be an $N$-dilation of $T: \mathcal{H}\to \mathcal{H}$ and $p$ a polynomial of degree at most $N$. Then
\[ p(T) = P_\mathcal{H}p(S)|_\mathcal{H}. \]
\end{lemma}

Let $\mathcal{H}$ be a Hilbert space. We call $T\in\Bounded(\mathcal{H})$ a \udef{contraction} if $\norm{T}\leq 1$.
\begin{proposition} \label{dilationOfContraction}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
From $\norm{T}\leq 1$ (and the fact that $T^*T$ is normal), we have that $\vec{1}-T^*T\geq 0$ by spectral mapping. We can define $D_T = \sqrt{\vec{1}-T^*T}$. Then
\[ U = \begin{pmatrix}
T & D_{T^*} \\ D_T & -T^*
\end{pmatrix} \]
is a dilation of $T$ and it is unitary:
\begin{align*}
UU^* &= \begin{pmatrix}
TT^* + D_{T^*}^2 & TD_T^* - D_{T^*}T \\
D_TT^* - T^*D_{T^*}^* & D^2_{T} + T^*T
\end{pmatrix} = \begin{pmatrix}
\vec{1} & TD_T - D_{T^*}T \\
D_TT^* - T^*D_{T^*} & \vec{1}
\end{pmatrix} \\
U^*U &= \begin{pmatrix}
T^*T + D_{T}^2 & T^*D_{T^*} - D_{T}^*T^* \\
D_{T^*}^*T - TD_{T} & D^2_{T^*} + TT^*
\end{pmatrix} = \begin{pmatrix}
\vec{1} & T^*D_{T^*} - D_{T}T^* \\
D_{T^*}T - TD_{T} & \vec{1}.
\end{pmatrix}
\end{align*}
We have used that $D_T$ is self-adjoint for all contractions $T$. We just need to show that $TD_T = D_{T^*}T$. Clearly we have
\[ T(D_T)^2 = T(\vec{1} - T^*T) = T - TT^*T = (\vec{1} - TT^*)T = (D_{T^*}T)^2T. \]
By functional-like calculus (TODO!!) we have $TD_T = D_{T^*}T$.
\end{proof}
The operator $D_T$ in the previous proof is sometimes called the \udef{defect operator} of $T$. It measures in some sense how far $T$ is from being a unitary operator. If $T$ is unitary, then $D_T = 0 = D_{T^*}$. If $T$ is an isometry, then $D_T = 0$ (by \ref{isometryRangeProjection}) and $D_{T^*}$ is a projector ($TT^*$ is a projector by \ref{isometryCharacterisation}, so $\vec{1} - TT^*$ is too by \ref{projectorOrthogonalComplement} and $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$).

\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every isometry $T$ on $\mathcal{H}$ has a unitary power dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
Consider the unitary dilation of \ref{dilationOfContraction}. When $T$ is an isometry this reduces to
\[ U = \begin{pmatrix}
T & D_{T^*} \\ 0 & -T^*
\end{pmatrix} = \begin{pmatrix}
T & \vec{1}-TT^* \\ 0 & -T^*
\end{pmatrix}, \]
where we have used that $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$ is a projector.

Now for all $n\in\N$ we have $U^n = \begin{pmatrix}
T^n & * \\ 0 & (-T^*)^n
\end{pmatrix}$, so in particular $P_\mathcal{H}U^n|_\mathcal{H} = T^n$, meaning $U$ is a power dilation of $T$. 
\end{proof}

\begin{lemma}
Let $T$ a contraction on a Hilbert space $\mathcal{H}$. Then $V_T: \mathcal{H} \to \mathcal{H}\oplus\mathcal{H}: x\mapsto (Tx, D_Tx)$ is an isometry.
\end{lemma}
\begin{proof}
For all $x\in \mathcal{H}$ we have
\[ \norm{V_Tx} = \sqrt{\norm{Tx}^2 + \norm{D_Tx}^2} = \sqrt{\inner{Tx,Tx} + \inner{D_Tx,D_Tx}} = \sqrt{\inner{T^*Tx,x} + \inner{D_T^2x,x}} = \sqrt{\inner{x,x}} = \norm{x}. \]
\end{proof}

\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\}^N \subseteq \mathcal{H}^{N+1}$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary $N$-dilation $U$ on $\mathcal{H}^{N+1}$.
\end{proposition}
\begin{proof}
Let $U'$ be a unitary dilation of $T$ on $\mathcal{H}^2$. Let $C_1 = U'_{-,1}$ and $C_2 = U'_{-,2}$ denote the columns. Then
\[ U = \begin{pmatrix}
C_1 & \mathbb{0}^{2\times N-1} & C_2 \\
\mathbb{0}^{N-1\times 1} & \mathbb{1}^{N-1\times N-1} & \mathbb{0}^{N-1\times 1}
\end{pmatrix} \]
is unitary by
\[ \begin{pmatrix}
C_1^* & \mathbb{0} \\
\mathbb{0} & \mathbb{1} \\
C_2^* & \mathbb{0}
\end{pmatrix}\begin{pmatrix}
C_1 & \mathbb{0} & C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0}
\end{pmatrix} = \begin{pmatrix}
C_1^*C_1 & \mathbb{0} & C_1^*C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0} \\
C_2^*C_1 & \mathbb{0} & C_2^*C_2
\end{pmatrix} = \mathbb{1}^{N+1\times N+1}. \]
We just need to show that the (1,1)-component of $U^k$ is $T^k$ for all $k\in 1:N$. In order to perform the multiplication, we rewrite $U$ such that the row and column partitions are the same, i.e.\ $(2|(N-3)|2)\times (2|(N-3)|2)$:
\[ U = \begin{pmatrix}
\begin{bmatrix}
T & 0 \\ D_T & 0
\end{bmatrix} & \mathbb{0} & \begin{bmatrix}
0 & D_{T^*} \\ 0 & -T^*
\end{bmatrix} \\
\begin{bmatrix}
0 & 1 \\ \mathbb{0} & \mathbb{0}
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 0 \\ \mathbb{1} & \mathbb{0}
\end{bmatrix} & \mathbb{0} \\
\begin{bmatrix}
0 & 0 \\ 0 & 0
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 1 \\ \mathbb{0} & 0
\end{bmatrix} & \begin{bmatrix}
0 & 0 \\ 1 & 0
\end{bmatrix}
\end{pmatrix} \]
TODO
\end{proof}

\begin{proposition}[von Neumann's inequality]
Let $T$ be a contraction on some Hilbert space $\mathcal{H}$. Then, for every polynomial $p\in\C[z]$,
\[ \norm{p(T)}\leq \sup_{|z|=1}|p(z)|. \]
\end{proposition}
\begin{proof}
Suppose the degree of $p$ is $N$. Let $U$ be a unitary $N$-dilation of $T$. Then
\[ \norm{p(T)} = \norm{P_\mathcal{H}p(U)|_\mathcal{H}}\leq \norm{p(U)} = \sup_{z\in\sigma(U)}|p(z)| \leq \sup_{|z|=1}|p(z)| \]
since the spectrum of $U$ is contained in the unit circle.
\end{proof}

\begin{theorem}[Sz.-Nagy's dilation theorem]
Let $\mathcal{H} \subseteq \ell^2(\N)\otimes\mathcal{H}$ be Hilbert spaces. Every contraction on $\mathcal{H}$ has a unitary power dilation on $\ell^2(\N)\otimes\mathcal{H}$.
\end{theorem}




\section{Constructions}
\subsection{Direct sum}
\subsection{Tensor product}
\url{https://web.ma.utexas.edu/mp_arc/c/14/14-2.pdf}

\section{Spectral properties of operators on Hilbert spaces}
\begin{proposition}
Let $T \in \Bounded(H)$ for some Hilbert space $H$. Then
\begin{enumerate}
\item $\spec(T) \neq \emptyset$;
\item $\rho(T) = \overline{\rho(T^*)}$, where the bar denotes complex conjugation.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Let $x,y\in H$ and define
\[ f(\lambda) = \inner{x,R_\lambda(T)y}. \]
If $\spec(T) = \emptyset$, then $f$ is an entire function. Now
\[ \norm{R_\lambda(T)} \leq \frac{1}{|\lambda| - \norm{T}} \to 0 \quad\text{as}\quad |\lambda| \to \infty. \]
By Liouville's theorem (TODO ref) we must have $f\equiv 0$. Because the $x,y$ we arbitrary we must have $R_\lambda(T)y = 0$ for all $y\in H$, such that $R_\lambda(T)$ is not injective, which is impossible as it is an inver\begin{proposition}
Let $K$ be a compact operator on a Banach space. Then
\[ \spec(K)\setminus\{0\} = \pspec(K)\setminus\{0\}. \]
\end{proposition}
\begin{proof}
For all $\lambda\neq 0$, we have that $\lambda\id - K$ is Fredholm with index zero (and thus bounded). Then by the Fredholm alternative \ref{FredholmAlternative} $\lambda\id - K$ is either bijective or neither injective nor surjective, meaning $\lambda$ is either in $\rho(T)$ or in $\pspec(T)$. 
\end{proof}

\begin{proposition} \label{spectrumCompactOperator}
Let $K$ be a compact operator on a Banach space $X$. Then
\begin{enumerate}
\item for all $\lambda\in\spec(K)\setminus\{0\}$ there exists a least $m$ such that $\ker(\lambda\id- K)^m = \ker(\lambda\id- K)^{m+1}$. This space is finite dimensional and reducing for $K$;
\item for $\alpha > 0$ the number of eigenvalues $\lambda$ such that $|\lambda|\geq \alpha$ is finite;
\item $0$ is the only accumulation point; if $X$ is infinite dimensional, then $0\in\spec(K)$;
\item $\spec(K)$ is at most countably infinite;
\item every $\lambda \in \spec(K)\setminus \{0\}$ is a pole of the resolvent $R_K$.
\end{enumerate}
\end{proposition}
\begin{proof}
\url{https://en.wikipedia.org/wiki/Spectral_theory_of_compact_operators}
\end{proof}
TODO: if $K$ is a self-adjoint compact operator on a Hilbert space $H$, then $H$ has an orthonormal basis of eigenvectors of $K$.
se.

(2) Take $\lambda\in\rho(T)$. Then
\[ ((\lambda\id - A)^{-1})^* = (\overline{\lambda}\id - A^*)^{-1} \]
so $\overline{\lambda}\in\rho(T^*)$ iff $((\lambda\id - A)^{-1})^*$ is bounded iff $(\lambda\id - A)^{-1}$ is bounded iff $\lambda\in \rho(T)$.
\end{proof}

\begin{lemma} \label{eigenspaceOrthogonalAdjoint}
Let $L$ be a densely defined operator on a Hilbert space $H$. Take $\lambda\in \pspec(L)$ and $\mu\in\pspec(L^*)$. If $\lambda \neq \overline{\mu}$, then
\[ \ker(\lambda\id - L)\perp \ker(\mu\id - L^*). \]
\end{lemma}
\begin{proof}
Take non-zero eigenvectors $x,y$ such that $Ax = \lambda x$ and $A^*y = \mu y$. Then
\[ \lambda \inner{y,x} = \inner{y,\lambda x} = \inner{y, Ax} = \inner{A^*y,x} = \inner{\mu y,x} = \overline{\mu}\inner{y,x}. \]
So we have $(\lambda - \overline{\mu})\inner{y,x} = 0$.
\end{proof}

\begin{proposition} \label{adjointSpectrumNoResidual}
Let $L$ be a densely defined operator on a Hilbert space $H$. Then the following are equivalent:
\begin{enumerate}
\item the residual spectrum of $L$ is empty;
\item $\overline{\pspec(L^*)} \subseteq \pspec(L)$;
\end{enumerate}
as are the following:
\begin{enumerate}
\item the residual spectrum of $L^*$ is empty;
\item $\pspec(L) \subseteq \overline{\pspec(L^*)}$.
\end{enumerate}
In particular all these statements hold if $L$ is normal.
\end{proposition}
\begin{proof}
Consider, for all $x\in \dom(L), y\in\dom(L^*)$, the equality
\[ \inner{(\lambda\id-L)x,y} = \inner{x,(\overline{\lambda}\id-L^*)y}. \]
We can make the following inferences:
\begin{itemize}
\item If $\lambda\in \overline{\pspec(L^*)}$, then the equality holds in particular for all eigenvectors $y$. This implies $\inner{(\lambda\id-L)x,y} = 0$. By \ref{perpToDenseSet} $\im(\lambda\id-L)$ may then not be dense, so it cannot be injective because the residual spectrum of $L$ is empty.
\item Assume $\lambda\id-L$ injective and take  $y\perp \im(\lambda\id-L)$. Then by the equality $\inner{x, (\overline{\lambda}\id - L^*)y} = 0$ for all $x\in\dom(L)$, which is dense. So $(\overline{\lambda}\id - L^*)y = 0$ by \ref{perpToDenseSet}. Now $\lambda\notin \pspec(L)$, so $\overline{\lambda}\notin \pspec(L^*)$. Thus $y = 0$ and $\im(\lambda\id-L)^\perp = \{0\}$, meaning $\im(\lambda\id-L)$ is dense.
\end{itemize}
The arguments for the second set of statements are similar.

If $L$ is normal, then $\ker(\lambda \id - L) = \ker{\overline{\lambda}\id -L^*}$ by \ref{equalityKernelAdjointNormal}, so $\pspec(L) = \overline{\pspec(L^*)}$.
\end{proof}

\begin{proposition}
Let $T$ be a closed, densly defined operator on a Hilbert space.
\begin{enumerate}
\item If $\lambda\in\rho(T)$, then $\overline{\lambda}\in\rho(T^*)$.
\item If $\lambda\in\rspec(T)$, then $\overline{\lambda}\in\pspec(T^*)$.
\item If $\lambda\in\pspec(T)$, then $\overline{\lambda}\in\rspec(T^*)\cup\pspec(T^*)$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO Compare with \ref{adjointSpectrumNoResidual}. CLosure necessary?
\end{proof}


\begin{proposition}
Let $T$ be a unitary operator. Then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item $\spec(T) \subset \setbuilder{\lambda\in\C}{|\lambda| = 1}$.
\end{enumerate}
\end{proposition}
TODO: move to more general place??

\begin{lemma}
The eigenvalues of a bounded dissipative linear operator
lie in the half-plane $\Im\lambda \geq 0$.
\end{lemma}


\subsubsection{Residual spectrum}
\begin{proposition}
Let $L$ be a densely defined linear operator on a Hilbert space. If $\lambda$ is in the residual spectrum of $L$ with deficiency $m$, then $\overline{\lambda}$ is in the point spectrum of $L^*$ with multiplicity $m$.
\end{proposition}
\begin{proof}
By \ref{kernelImageAdjoint} we have
\[ \im(\lambda \id - L)^\perp = \ker(\lambda\id - L)^* = \ker(\overline{\lambda}\id - L^*). \]
\end{proof}

\subsection{Rayleigh quotient}
\begin{lemma}
Let $L$ be an operator on a Hilbert space. If $x$ is an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \lambda. \]
\end{lemma}
\begin{proof}
Let $x$ be an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \frac{\inner{x,Lx}}{\inner{x,x}} = \lambda \frac{\inner{x,x}}{\inner{x,x}} = \lambda. \]
\end{proof}

\begin{proposition}
If $U$ is unitary, then $\spec(U)\subset \mathbb{T}$.
\end{proposition}

\chapter{Types of operators}
\section{Fredholm operators}
TODO: Calkin algebra: study of properties invariant under compact pertubation.
\begin{definition}
An operator $T\in\Bounded(X,Y)$ between Banach spaces is called a \udef{Fredholm operator} if $T$ has a finite-dimensional kernel and cokernel.

The \udef{Fredholm index} of $T$ is defined as
\[ \Index T \defeq \dim\ker T - \dim\coker T.  \]

We denote the space of Fredholm operators from $X$ to $Y$ as $\Fred(X,Y)$. If $X=Y$, we write $\Fred(X)$.
\end{definition}

\begin{example}
\begin{enumerate}
\item If $X=Y$ is finite-dimensional, then all operators are Fredholm with index $0$.
\item The left shift $S_l:\ell^2(\N)\to\ell^2(\N): (x_n)_n\mapsto (x_{n+1})_n$ has index $1$.
\item The right shift $S_r = S_l^*$ has index $-1$.
\end{enumerate}
\end{example}

\begin{lemma}
A Fredholm operator has closed range.
\end{lemma}

\begin{lemma}
Let $T\in\Bounded(H)$ be a bounded operator on a Hilbert space. Then $\dim\coker T = \dim\ker T^*$.
\end{lemma}
\begin{proof}
TODO (is it correct?) $\ker(T^*) = \im(T)^\perp$.
\end{proof}


\begin{proposition}
Let $S,T\in\Fred(X)$, $\lambda\in\F$ and $K\in\Compact(X)$. Then
\begin{enumerate}
\item $\Index(ST) = \Index(S)+\Index(T)$;
\item $\Index(T+K) = \Index(T)$;
\item $\Index(\lambda T) = \Index(T)$, if $\lambda \neq 0$;
\item $\Index(T) = 0$ \textup{if and only if} $T=K'+L$ for some compact $K'$ and invertible $L$.
\end{enumerate}
Let $T\in\Fred(H)$ for some Hilbert space $H$. Then
\begin{enumerate} \setcounter{enumi}{4}
\item $\Index(T^*) = -\Index(T)$.
\end{enumerate}
\end{proposition}
TODO: integrate with corollary??

\begin{lemma}
Let the commutative diagram
\[ \begin{tikzcd}
0 \rar & X \dar{T} \rar & Y \dar{S} \rar & Z \dar{R} \rar & 0 \\
0 \rar & X \rar & Y \rar & Z \rar & 0
\end{tikzcd} \]
have short exact rows. If any two of $T,S,R$ are Fredholm, then so is the third and
\[ \Index S = \Index T + \Index R. \]
\end{lemma}
\begin{proof}
TODO snake lemma to obtain long exact
\[ 0\to \ker T \to \ker S\to \ker R \to \coker T \to \coker S \to \coker R \to 0. \]
\end{proof}
\begin{corollary} \mbox{}
\begin{enumerate} 
\item Let $T\in\Fred(X)$ and $S\in\Fred(Y)$ be Fredholm, then so is $T\oplus S$ with
\[ \Index(T\oplus S) = \Index(T)+\Index(S). \]
\item Let $T\in\Fred(X,Y)$ and $S\in\Fred(Y,Z)$ be Fredholm, then so is $ST$ with
\[ \Index(ST) = \Index(T)+\Index(S). \]
\item Let $K\in\Compact(X)$ be compact, then $\id_X+K$ is Fredholm with
\[ \Index(\id_X+K) = 0. \]
\end{enumerate}
\end{corollary}


\begin{lemma}[Fredholm alternative] \label{FredholmAlternative}
Let $T$ be a Fredholm operator of index zero. Then either $T$ is bijective, or it is neither injective nor surjective.
\end{lemma}
\begin{proof}
The operator $T$ is injective iff $\dim\ker(T) = 0$ and surjective iff $\dim\coker(T) = 0$.
\end{proof}

\subsection{Moore-Penrose pseudoinverse}

\section{Integral operators and transforms}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. Then an \udef{integral operator} or \udef{integral transform} is a map of the form
\[ T: U\subset (\Omega\to\C) \to (\Omega\to\C): f \mapsto \int_\Omega K(x,y)f(y) \diff{\mu(y)} \]
where $K\in (\Omega\times \Omega \to \C)$ is the \udef{kernel} or \udef{nucleus} of $T$.

The kernel is called
\begin{itemize}
\item \udef{symmetric} if $K(x,y) = \overline{K(y,x)}$;
\item \udef{Volterra} if $\Omega = \R$ and $K(x,y) = 0$ for $y>x$;
\item \udef{convolutional} if $\Omega$ is a group and $K(x,y) = F(x-y)$ for some function $F$;
\item \udef{Hilbert-Schmidt} if $K\in L^2(\Omega\times \Omega)$, i.e.\
\[ \int_{\Omega\times \Omega}|K(x,y)|^2\diff{x}\diff{y} < \infty; \]
\item \udef{singular} if $K(x,y)$ is unbounded on $\Omega\times \Omega$.
\end{itemize}
\end{definition}

\begin{lemma}
Hilbert-Schmidt integral operators are compact operators on $L^2(\Omega\times \Omega)$.
\end{lemma}
\begin{proof}
A Hilbert-Schmidt integral operator $T$ maps $L^2(\Omega)$ to $L^2(\Omega)$ functions:
\begin{align*}
\norm{Tu}^2_{L^2} &= \int_\Omega \left|\int_{\Omega} K(x,y)u(y)\diff{\mu(y)}\right|^2\diff{\mu(x)} \\
&\leq \int_\Omega \left(\int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg)\diff{\mu(x)} \\
&= \left(\int_\Omega \int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\diff{\mu(x)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg) < \infty
\end{align*}
where we have used the Cauchy-Schwarz inequality. This also immediately shows Hilbert-Schmidt integral operators are bounded.

TODO Compact
\end{proof}

\begin{proposition}
Let $T$ be an integral operator with kernel $K(x,y)$, then $T^*$ is the integral operator with kernel $\overline{K(y,x)}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A$ be a Borel set and $K:A\times A\to \C$ a measurable function such that the integral operator with kernel $K$ is bounded. Then the adjoint of the integral operator is again an integral operator with kernel $K^*(x,y) = \overline{K(y,x)}$.
\end{proposition}

\begin{proposition}
Let $T$ be a Volterra integral operator. Then $\spec(T) = \cspec(T) = \{0\}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsection{Integral equations}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. An \udef{integral equation} is an equation containing an unknown function on $\Omega$ and an integral over $\Omega$.

An integral equation is 
\begin{itemize}
\item \udef{of the first kind} if it is of the form
\[ \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function and $u$ is the unknown function;
\item \udef{of the second kind} if it is of the form
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function, $\lambda$ is a scalar and $u$ is the unknown function.
\end{itemize}
\end{definition}

\begin{proposition}
Let
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x)\]
be an integral equation of the second kind. This integral equation has a unique solution $u$ if
\[ |\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}. \]
\end{proposition}
\begin{proof}
Let the map $T$ be defined by
\[ T(u) = x\mapsto \frac{1}{\lambda}\left(\int_\Omega K(x,y)u(y)\diff{\mu(y)} + f(x)\right) \]
so that solutions of the integral equation are exactly the fixed points of $T$. Then
\[ \norm{Tu-Tv}_\infty = \sup_{x\in\Omega} \frac{1}{|\lambda|} \left|\int_\Omega K(x,y)(u(y)- v(y))\diff{\mu(y)}\right| \leq \frac{1}{|\lambda|} \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)} \cdot \norm{u-v}_\infty. \]
So $T$ is a contraction if $|\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}$. The result follows from \ref{contractionFixedPoint}.
\end{proof}

\section{Convolution operators}






\chapter{Fourier transforms}

\section{Types of Fourier transform}

\subsection{Discrete Fourier transform}
\begin{definition}
Then $N$-dimensional \udef{discrete Fourier transform} (DFT) is the linear transformation $\C^N \to \C^N$ defined by the matrix $DFT_N$ with components
\[ [DFT_N]_{j,k} = \frac{1}{\sqrt{N}}\omega_N^{(j-1)(k-1)}, \]
where $\omega_N$ is the $N^\text{th}$ root of unity.
\end{definition}

\begin{lemma} \mbox{}
\begin{enumerate}
\item The $DFT_N$ matrix is the Vandermonde matrix of the roots of unity, up to the normalisation factor $1/\sqrt{N}$.
\item The $DFT_N$ matrix is unitary.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Just an observation.

(2) We calculate
\[ [DFT_N\cdot DFT_N]_{j,l} = \frac{1}{N}\sum_{k=1}^N\omega_N^{jk}\overline{\omega_N}^{kl} = \frac{1}{N}\sum_{k=1}^N\omega_N^{k(j-l)} = \delta_{j,l}. \]
\end{proof}


\chapter{$C^*$-algebras}
\section{$*$-algebras}
\begin{definition}
A \udef{$*$-algebra} is a $*$-r(i)ng $(A,+,\cdot, *)$, with involution $*$, that is an associative algebra over a commutative $*$-ring $(R,+,\cdot, ')$, with involution $'$,
such that
\[ \forall r\in R, x\in A: \quad (rx)^* = r'x^*. \]

A \udef{complex $*$-algebra} is a $*$-algebra where the $*$-ring $R$ is $\C$ with complex conjugation as the involution $'$.

A \udef{real $*$-algebra} is a $*$-algebra where the $*$-ring $R$ is $\R$ with the identity map as the involution $'$.

If a $*$-algebra is also a Banach algebra and for all elements $\norm{x^*} =\norm{x}$, then it is called a \udef{Banach-$*$-algebra}.
\end{definition}
TODO: drop condition $\norm{x^*} =\norm{x}$? Not required for $C^*$ (already implied).

\begin{lemma}
Let $A$ be a $*$-algebra. The unitisation $A^\dagger = A\oplus \F$ can also be seen as a $*$-algebra with the involution defined by
\[ (a, \lambda)^* = (a^*, \overline{\lambda}) \qquad \forall a\in A, \lambda\in \F.\]
\end{lemma}
\begin{lemma} \label{elementaryStarLemma}
Let $A$ be a unital $*$-algebra. Then
\begin{enumerate}
\item $\vec{1}^* = \vec{1}$;
\item if $x$ is invertible, then $x^*$ is invertible with $(x^*)^{-1} = (x^{-1})^*$;
\item $\sigma(x^*) = \setbuilder{\overline{\lambda}}{\lambda \in \sigma(x)}$.
\end{enumerate}
\end{lemma}
\begin{proof}
Take some $x\in A$.
\begin{enumerate}
\item $\vec{1}^* x = (x^*\cdot \vec{1})^* = x^{**} = x$.
Similarly $x\vec{1}^* = x$.
\item $x^*\cdot (x^{-1})^* = (x^{-1}x)^* = \vec{1}^* = \vec{1}$. Similarly $(x^{-1})^*\cdot x^* = \vec{1}$.
\end{enumerate}
\end{proof}

\begin{proposition} \label{smallestBanach*Algebra}
Let $A$ be a Banach-$*$-algebra and $S\subset A$ a subset. Then
\[ \mathcal{B}^*(S) \defeq \mathcal{B}(S\cup S^*) \]
is the smallest Banach-$*$-subalgebra in $A$ that contains $S$, where $\mathcal{B}$ is defined as in \ref{smallestBanachAlgebra} and $S^* = \setbuilder{s^*\in A}{s\in S}$.
\end{proposition}

\begin{definition}
Let $A$ be a $*$-algebra and $x\in A$. We say that $x$ is
\begin{enumerate}
\item \udef{normal}, if $x^*x = xx^*$;
\item \udef{self-adjoint}, if $x=x^*$;
\item \udef{unitary}, if $x^*x = xx^* = \vec{1}$ (assuming $A$ unital);
\item a \udef{projection}, if $x=x^*=x^2$.
\end{enumerate}
The set of all
\begin{enumerate}
\item normal elements in $A$ is denoted $\Normals(A)$;
\item self-adjoint elements in $A$ is denoted $\SelfAdjoints(A)$;
\item unitaries in $A$ is denoted $\Unitaries(A)$;
\item projections in $A$ is denoted $\Projections(A)$.
\end{enumerate}

\end{definition}
\begin{lemma}
We have the following implications:
\[ \text{projection} \Rightarrow \text{self-adjoint} \Rightarrow \text{normal} \Leftarrow \text{unitary}. \]
\end{lemma}

\begin{lemma} \label{orthogonalProjection}
Let $A$ be a unital $*$-algebra and $p\in\Projections(A)$. Then $\vec{1}-p$ is a projection.
\end{lemma}
\begin{proof}
We simply calculate
\[ (\vec{1}-p)^2 = (\vec{1}-p)(\vec{1}-p) = \vec{1} - p -p + p = \vec{1}-p = (\vec{1}-p)^*. \]
\end{proof}

\begin{lemma} \label{realImaginaryParts}
Let $A$ be a $*$-algebra and $x\in A$. Then there are unique self-adjoint elements $x_1,x_2\in A$ such that $x = x_1+i\cdot x_2$. They are given by
\[ x_1 = \frac{x+x^*}{2} \qquad \text{and} \qquad x_2 = \frac{x-x^*}{2i}. \]
\end{lemma}
We call $x_1$ and $x_2$ the \udef{real part} and \udef{imaginary part} of $x$, respectively.

\subsection{$*$-homomorphisms}
\begin{definition}
Let $A,B$ be $*$-algebras. A \udef{$*$-homomorphism} is a linear, multiplicative, $*$-preserving map $\Psi: A \to B$.

If $A,B$ are unital and $\Psi(\vec{1}_A) = \vec{1}_B$, then we say $\Psi$ is \udef{unital}.
\end{definition}
\begin{lemma}
Let $A$ be a $*$-algebra, then $*$-homomorphisms map
\begin{enumerate}
\item normal elements to normal elements;
\item self-adjoints to self-adjoints;
\item projections to projections;
\item unitaries to unitaries, if the $*$-homomorphism is unital.
\end{enumerate}
\end{lemma}

\subsection{$*$-matrix algebras}
TODO define matrix algebra.
\begin{definition}
Let $A$ be a $*$-algebra. Then the matrix algebra $A^{n\times n}$ is considered a $*$-algebra with the star operation given defined by
\[ [a^*]_{i,j} \defeq [a]_{j,i}^*. \]
for all components of $a\in A^{n\times n}$.
\end{definition}
Notice that the $*$-operation acts as the element-wise $*$-operation composed with the transpose.
\section{$C^*$-algebras}
\begin{definition}
A (complex) \udef{$C^*$-algebra} is a complex Banach-$*$-algebra $A$ such that
\[\forall x\in A: \quad \norm{x^*x} = \norm{x}^2.\]
This identity is known as the \udef{$C^*$-identity}, the \udef{$C^*$-property}, the \udef{$C^*$-condition} or the \udef{$C^*$-axiom}.
\end{definition}

\begin{definition}
A \udef{real} $C^*$-algebra is a real Banach-$*$-algebra such that 
\end{definition}

\begin{example}
TODO: Concrete $C^*$-algebras.

$\mathcal{C}(X)$ for some compact $X$ (need Hausdorff?). TODO: norm well defined (i.e.\ bounded) and for $g\in \mathcal{C}(X)$, $\sigma(g) = g[X]$.
\end{example}

\begin{proposition}
The $C^*$-identity is equivalent to
\[\forall x\in A: \quad \norm{x^*x} = \norm{x^*}\cdot\norm{x}.\]
\end{proposition}
\begin{proof}
TODO. Highly non-trivial. TODO: move later.
\end{proof}


\begin{lemma} \label{C*identityEquivalent}
The $C^*$-identity is equivalent to
\[\forall x\in A: \quad \norm{x^*x} \geq \norm{x}^2.\]
\end{lemma}
\begin{proof}
Let $x\in A$, then $\norm{x}^2 \leq \norm{x^*x} \leq \norm{x}\cdot\norm{x^*}$ and so $\norm{x}\leq \norm{x^*}$. By replacing $x$ with $x^*$ and using $x^{**}=x$ we also get $\norm{x}\geq \norm{x^*}$. Then $\norm{x^*x}\leq \norm{x^*}\cdot\norm{x} = \norm{x}^2$. Together with the original inequality this implies the $C^*$-identity.
\end{proof}

\begin{definition}
Let $A$ be a $C^*$-algebra and $D$ a subset of $A$. The $C^*$-algebra \udef{generated} by $D$, $C^*(D)$, is the smallest $C^*$-subalgebra of $A$ containing $D$.
TODO refine def.
\end{definition}
\begin{lemma}
$C^*(\vec{1},a)$ is commutative.
\end{lemma}

\begin{lemma} \label{consequencesC*}
Let $A$ be a $C^*$-algebra. The $C^*$-identity implies
\begin{enumerate}
\item  $\norm{\vec{1}} = 1$.
\item the involution $*$ is isometric: $\norm{x^*} = \norm{x}$;
\item the involution $*$ is continuous.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $A$ be a $C^*$-algebra. Then the sets $\Normals(A), \SelfAdjoints(A), \Unitaries(A)$ and $\Projections(A)$ are closed in $A$. 
\end{lemma}
\begin{proof}
This follows from the continuity of the multiplication and the involution $*$.
\end{proof}

\begin{proposition} \label{normNormalElement}
Let $A$ be a $C^*$-algebra and $x\in A$ a normal element. Then $r(x) = \norm{x}$.
\end{proposition}
\begin{proof}
We compute
\[ \norm{x}^4 = \norm{x^*x}^2 = \norm{x^*xx^*x} = \norm{(x^*)^2x^2} = \norm{x^2}^2 \]
where we have repeatedly applied the $C^*$-identity and used normality once. We conclude that $\norm{x^2} = \norm{x}^2$. Inductively we obtain $\norm{x^{(2^n)}} = \norm{x}^{2^n}$. By the spectral radius formula, \ref{spectralRadiusFormula}, we get
\[ \spr(x) = \lim_{n\to\infty}\norm{x^{(2^n)}}^{1/2^n} = \lim_{n\to\infty}\norm{x} = \norm{x}. \]
\end{proof}
\begin{corollary} \label{atMostOneNorm}
Let $A$ be a $*$-algebra. There exists at most one norm on $A$ turning it into a $C^*$-algebra. If there is such a norm, it is given by $\norm{x} = \sqrt{\spr(x^*x)}$.
\end{corollary}
\begin{proof}
By the $C^*$-identity $\norm{x} = \sqrt{\norm{x^*x}}$ and $x^*x$ is normal, so we can apply the proposition.
\end{proof}
It is important to note that the spectral radius is a purely algebraic property and is independent of the norm.

\subsection{Approximate units and ideals}
\begin{proposition}
Let $A$ be a $C^*$-algebra and $J \subseteq A$ a two-sided $*$-ideal. Then there exists a net $(e_\lambda)_{\lambda\in\Lambda}$ in $J$ that that is an increasing approximate unit of $\overline{J}$.
\end{proposition}


\subsection{$C^*$-homomorphisms}
TODO drop unital
\begin{proposition}
Let $A,B$ be unital $C^*$-algebras and $\Psi: A\to B$ a unital $*$-homomorphism. Then $\Psi$ is bounded (and thus continuous) with $\norm{\Psi} = 1$.
\end{proposition}
\begin{proof}
Because $x^*x$ and $\Psi(x^*x)$ are normal, we calculate using \ref{normNormalElement}
\[ \norm{x}^2 = \norm{x^*x} = \spr(x^*x) \geq \spr(\Psi(x^*x)) = \norm{\Psi(x^*x)} = \norm{\Psi(x)^*\Psi(x)} = \norm{\Psi(x)}^2, \]
where the inequality follows from an application of lemma \ref{spectrumOfImage} to $x^*x$. Hence $\norm{\Psi}\leq 1$. Equality follows from $\Psi(\vec{1}) = 1$.
\end{proof}
\begin{corollary}
The kernel of $\Psi$ is a closed $*$-ideal.
\end{corollary}
\begin{proof}
TODO ref.
\end{proof}

\begin{lemma}
A surjective $*$-homomorphism from a unital $C^*$-algebra is unital.
\end{lemma}
\begin{proof}
Let $\Psi: A \to B$ be a surjective $*$-homomorphism with $A$ unital. For all $a\in A$:
\[ \Psi(a)\Psi(\vec{1}) = \Psi(a \vec{1}) = \Psi(a) \qquad \text{and} \qquad \Psi(\vec{1})\Psi(a) = \Psi(\vec{1} a) = \Psi(a). \]
As all elements of $B$ are of the form $\Psi(a)$, $\Psi(\vec{1})$ is a multiplicative identity for $B$.
\end{proof}

\subsubsection{Lifts}
TODO: move to $*$-algebra homomorphisms?
\begin{proposition}
Let $\Psi: A \to B$ be a surjective $*$-homomorphism between $C^*$-algebras. Then
\begin{enumerate}
\item every self-adjoint element $b\in B$ has a self-adjoint lift $a\in A$, such that $\norm{a} = \norm{b}$;
\item every positive element $b\in B$ has a positive lift $a\in A$, such that $\norm{a} = \norm{b}$;
\item every element $b\in B$ has a lift $a\in A$ such that $\norm{a} = \norm{b}$.
\end{enumerate}
\end{proposition}
In general normal elements, unitaries and projections do not lift to normal elements, unitaries and projections, unless $\Psi$ is injective.
\begin{lemma} \label{injectiveLifts}
Let $\Psi: A \to B$ be an injective $*$-homomorphism between $C^*$-algebras.
\begin{enumerate}
\item if $\Psi(a)$ is a normal element, then $a$ is a normal element;
\item if $\Psi(p)$ is a projection, then $p$ is a projection;
\item if $\Psi(u)$ is a unitary element, then $u$ is a unitary element.
\end{enumerate}
\end{lemma}
\begin{proof}
If $\Psi(a)\Psi(a)^* = \Psi(a)^*\Psi(a)$, then $\Psi(a^*a) = \Psi(aa^*)$ and $a^*a = aa^*$ by injectivity.

The other conditions are verified similarly.
\end{proof}

\section{Direct sums of $C^*$-algebras}
TODO!

\subsection{Unitisation of $C^*$-algebras}
For Banach-$*$-algebras we may have a choice of norms to put on the unitisation. For $C^*$-algebras there is exactly one.

TODO: of course $C^*$-algebras use supremum norms. They are fundamentally operators after all!
\begin{proposition}
Let $A$ be a $C^*$-algebra. Then there exists a unique norm on $A^\dagger$ that turns it into a $C^*$-algebra: the operator norm
\[ \norm{(a,\lambda)} \defeq \sup\setbuilder{\norm{ax + \lambda x}}{x\in A \land \norm{x}\leq 1} .\]
\end{proposition}
\begin{proof}
There is at most one such norm, by \ref{atMostOneNorm}. Because the operator norm is a suitable norm for $A^\dagger$ by \ref{normsOfUnitisation}, we just need to verify the $C^*$-identity:
\begin{align*}
\norm{(a,\lambda)^*(a,\lambda)} &= \sup_{\norm{x}\leq 1} \norm{(a,\lambda)^*(a,\lambda)x} \\
&\geq \sup_{\norm{x}\leq 1} \norm{x^*}\cdot\norm{(a,\lambda)^*(a,\lambda)x} \geq \sup_{\norm{x}\leq 1} \norm{x^*(a,\lambda)^*(a,\lambda)x} \\
&= \sup_{\norm{x}\leq 1} \norm{((a,\lambda)x)^*(a,\lambda)x} = \sup_{\norm{x}\leq 1} \norm{(a,\lambda)x}^2 = \norm{(a,\lambda)}^2,
\end{align*}
where we have used the $C^*$-identity in $A$ because $(a,\lambda)x = ax + \lambda x \in A$.
\end{proof}

\section{Functionals and spectrum}
\begin{lemma} \label{normSelfAdjoint}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a self-adjoint element. Then
\[ \forall t\in\R: \quad \norm{x+it}^2 = \norm{x^2+t^2} \leq \norm{x}^2 + t^2. \]
\end{lemma}

\begin{proposition}
Let $A$ be a unital $C^*$-algebra and $\varphi: A\to \C$ a linear functional satisfying $\norm{\varphi} = \varphi(\vec{1})$. If $a\in A$ is self-adjoint, then $\varphi(a) \in \R$.
\end{proposition}
\begin{proof}
We may assume $\varphi \neq 0$ and $\varphi(\vec{1}) = 1$. Using \ref{normSelfAdjoint}, we calculate
\[ |\varphi(x)+it|^2 = |\varphi(x+it)|^2 \leq \norm{x+it}^2 \leq \norm{x}^2 + t^2. \]
By \ref{boundedThenReal} this means $\varphi(x)\in\R$.
\end{proof}
TODO: alternate proof in Fillmore using exponential map.
\begin{corollary} \label{selfAdjointSpectrumReal}
Let $x\in A$ be self-adjoint. Then $\sigma(x) \subseteq \R$.
\end{corollary}
\begin{proof}
By \ref{commutativeSameSpectrum}, and the fact that $x$ is self-adjoint (TODO ref) we may assume $A$ commutative. Let $\lambda \in \sigma(x)$. By \ref{spectrumFromSpectrum} there is a $\varphi\in\hat{A}$ such that $\lambda = \varphi(x)$. By \ref{charactersUnital}, $\norm{\varphi} = \varphi(\vec{1}) = 1$. Then by the proposition $\lambda = \varphi(x) \in \R$.
\end{proof}
\begin{corollary}
Let $\varphi:A\to \C$ be a \emph{linear} functional satisfying $\norm{\varphi} = \varphi(\vec{1})$. Then $\varphi$ is $*$-preserving, i.e.\ for all $x\in A$
\[\varphi(x^*) = \overline{\varphi(x)}. \] 
\end{corollary}
\begin{proof}
By \ref{realImaginaryParts} we can write $x= x_1+ix_2$. Then $\varphi(x^*) = \varphi(x_1-ix_2) = \varphi(x_1) - i \varphi(x_2)$. By the proposition $\varphi(x_1), \varphi(x_2)\in \R$.
\end{proof}
\begin{corollary} \label{characters*Preserving}
Every character on $A$ is $*$-preserving.
\end{corollary}

\begin{proposition}
Let $A$ be a unital $C^*$-algebra, $B\subseteq A$ a unital $C^*$-subalgebra and $x\in B$. Then $x$ is invertible in $B$ \textup{if and only if} $x$ is invertible in $A$.
\end{proposition}
\begin{proof}
If an inverse exists in $B$, said inverse will also be in $A$.

Conversely, suppose $x$ not invertible in $B$. Then either $x^*x$ or $xx^*$ is not invertible in $B$ by \ref{elementaryStarLemma} and \ref{productInvertibility}. Let $y$ be one of the two that is not invertible. Because $y$ is self-adjoint, $\sigma_B(y)\subset \R$, by \ref{selfAdjointSpectrumReal}. Thus $(y_n) = (y+\frac{i}{n})$ is a sequence of invertibles converging to $y$. By \ref{openSetInvertibles}, $\norm{y_n - y}\geq \norm{y_n^{-1}}^{-1}$ must hold for all $n$, otherwise $y$ would be invertible. Thus $\norm{y_n^{-1}}^{-1}$ must converge to zero and $\norm{y_n^{-1}}$ must diverge (TODO ref). Since the inversion map is continuous on $\GL(A)$, by \ref{inverseMapContinuous}, it follows that $y$ cannot be invertible in $A$. Since $x$ is invertible if and only if $x^*$ is invertible, by \ref{elementaryStarLemma}, $y$ being non-invertible implies $x$ is not invertible, by \ref{productInvertibility}.
\end{proof}
\begin{corollary} \label{spectrumIndependentOfSurroundingAlgebra}
Let $A$ be a $C^*$-algebra, $B\subseteq A$ a $C^*$-subalgebra and $x\in B$.

The spectrum of $x$ is independent of the surrounding algebra:
\[ \sigma_B(x) = \sigma_A(x). \]
\end{corollary}
\begin{proof}
Apply the proposition to $\tilde{A}$ and $\tilde{B}$. Note that if $A,B$ are non-unital, the unit of $B^\dagger$ is the same as that of $A^\dagger$.
\end{proof}
This does not hold in general for Banach-$*$-algebras!

\begin{proposition} \label{generatedAlgebraSpectrumHomeomorphism}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a normal element. Then the map
\[ \hat{x}|_{\widehat{C^*(x,\vec{1})}}: \widehat{C^*(x,\vec{1})}\to \sigma(x): \varphi \mapsto \varphi(x) \]
is a homeomorphism.
\end{proposition}
\begin{proof}
By \ref{spectrumIndependentOfSurroundingAlgebra} the map is independent of the surrounding algebra (TODO: $C^*(x,\vec{1})$ also independent?). So without WLOG we take $A = C^*(x,\vec{1})$. Because $x$ is normal, $C^*(x,\vec{1})$ is commutative (TODO:ref). By \ref{spectrumFromSpectrum} the map is surjective and well-defined, in that it maps into the codomain $\sigma(x)$. It is also continuous by \ref{weak*continuousFunctional}.

To show injectivity, suppose $\varphi(x) = \psi(x)$ for some $\varphi,\psi\in\hat{A}$. Because (TODO ref)
\[ A = \overline{\Span}\setbuilder{x^n(x^*)^m}{n,m\geq 0} \]
and characters are continuous homomorphisms, \ref{charactersUnital}, we see that $\varphi = \psi$.

Finally we need to show the map is open. Because $\sigma(x)$ is compact, this follows from \ref{compactToHausdorffHomeomorphism}.
\end{proof}

\section{Commutative $C^*$-algebras}
\url{https://math.stackexchange.com/questions/4401500/how-to-see-that-pure-states-are-multiplicative-linear-functionals-in-a-commutati?rq=1} TODO

\begin{theorem}[Stone-Weierstrass] \label{StoneWeierstrass}
Let $X$ be a compact Hausdorff space. Let $A\subseteq \mathcal{C}(X)$ be a unital $*$-subalgebra. Suppose that $A$ separates points, i.e.\ for all $x\neq y$ in $X$ there exists $f\in A$ with $f(x) \neq f(y)$. Then $A$ is dense in $\mathcal{X}$ with respect to $\norm{\cdot}_\infty$.
\end{theorem}

\subsection{The Gelfand-Naimark theorem}
TODO: non-unital case!
\begin{theorem}[Gelfand-Naimark] \label{GelfandNaimarkCommutative}
Let $A$ be a unital commutative $C^*$-algebra. Then the Gelfand transform
\[ \wedge: A\to\mathcal{C}(\hat{A}): x\mapsto \hat{x} \]
is an isometric $*$-isomorphism.
\end{theorem}
Here we view $\mathcal{C}(\hat{A})$ as a $C^*$-algebra with involution $f^*(\varphi) = \overline{f(\varphi)}$ for all $\varphi\in\hat{A}$. TODO: more in exercises.
\begin{proof}
We already know the Gelfand transform is a homomorphism by \ref{GelfandTransformHomomorphism}.

We first prove it is a $*$-homomorphism: $\forall x\in A: \hat{x}^* = (x^*)^\wedge$. To that end, take some $\varphi\in\hat{A}$. Then
\[ (\hat{x})^*(\varphi) = \overline{\hat{x}(\varphi)} = \overline{\varphi(x)} = \varphi(x^*) = (x^*)^\wedge(\varphi) \]
where the third equality is due to \ref{characters*Preserving}.

For isometry, notice that every element is normal due to commutativity. By \ref{normNormalElement} and \ref{GelfandTransformHomomorphism}, we have
\[ \norm{x} = \spr(x) = \norm{\hat{x}}. \]

Then we just need to show the Gelfand transform is bijective (TODO ref). Injectivity follows from isometry, \ref{isometryInjective}. For surjectivity we want to apply the Stone-Weierstrass theorem \ref{StoneWeierstrass}. To show the image of $A$ separates points, take $\varphi \neq \psi$ in $\hat{A}$. Then $\varphi(x) \neq \psi(x)$ for some $x\in A$, meaning $\hat{x}(\varphi) \neq \hat{x}(\psi)$. 

By the Stone-Weierstrass theorem \ref{StoneWeierstrass} the image of $A$ is dense in $\mathcal{\hat{A}}$. By \ref{isometryClosed} the image of an isometry from a complete space is closed. Thus the image of $A$ is all of $\mathcal{\hat{A}}$.
\end{proof}
\begin{corollary}
Every commutative $C^*$-algebra is isomorphic to $C(X)$ for some compact Hausdorff space $X$.

This space $X$ is unique up to isomorphism.
\end{corollary}
\begin{proof}
The first part follows directly from the Gelfand-Naimark theorem. For the second part, we know that $\widehat{C(X)}\cong X$ by TODO ref. And if $X \cong Y$, then $C(X) \cong C(Y)$?TODO ref?
\end{proof}

\begin{proposition}
If $A$ (commutative) generated by one element $a$, then $A$ is isomorphic to the $C^*$-algebra of continuous functions on the spectrum of $a$ which vanish at $0$.
\end{proposition}

\begin{proposition}
Any injective $*$-homomorphism of $C^*$-algebras is an isometry.
\end{proposition}
\begin{proof}
Let $\Psi:A\to B$ be an injective $*$-homomorphism of $C^*$-algebras. It is enough to show $\norm{\Psi(a)} = \norm{a}$ for self-adjoint $a\in A$. Then for arbitrary $x\in A$, we have
\[ \norm{\Psi(x)} = \sqrt{\norm{\Psi(x)^*\Psi(x)}} = \sqrt{\norm{\Psi(x^*x)}} = \sqrt{\norm{x^*x}} = \norm{x}. \]
We can assume $A,B$ unital by passing to $\Psi^\dagger$ using \ref{DaggerMorphismProperties}.

We can also assume $A, B$ are commutative by restricting $\Psi$ to $\Psi': C^*(a,\vec{1}) \to C^*(\Psi(a),\vec{1})$.

By the Gelfand-Naimark theorem \ref{GelfandNaimarkCommutative} we can suppose we have is a unital injection $\Psi: \mathcal{C}(X)\to \mathcal{C}(Y)$ for some compact Hausdorff spaces $X,Y$. There is then (TODO ref) some continuous surjection $\alpha: Y\to X$ such that $\forall f\in\mathcal{C}(X): \Psi(f) = f\circ \alpha$. Then $\norm{\Psi(f)} = \norm{f}$ because both functions have the same range.
\end{proof}

TODO non-unital Gelfand-Naimark!

\section{Continuous functional calculus}

TODO: relocate.
\begin{lemma} \label{WeierstrassApproximation}
Let $D\subseteq \C$. Then
\[ \mathcal{C}(D) = C^*(I_D, \vec{1}_D). \]
\end{lemma}
\begin{proof}
TODO ref to Stone-Weierstrass.
\end{proof}

\begin{theorem}[Continuous functional calculus]
Let $A$ be a unital $C^*$-algebra and $x\in A$ a normal element. There exists a unique $*$-homomorphism
\[ \Phi_x: \mathcal{C}(\sigma(x))\to A: f\mapsto f(x) \]
such that $\id_{\sigma(x)}(x) = x$ and $\vec{1}_{\sigma(x)}(x) = \vec{1}_A$. Moreover
\[ \im\Phi_x = C^*(x,\vec{1}). \]
\end{theorem}
\begin{proof}
Unicity from \ref{WeierstrassApproximation} TODO.

For existence, let $B = C^*(x,\vec{1})$, which is commutative because $x$ is normal. Then $\hat{x}: \hat{B}\to \sigma(x): \varphi\mapsto \varphi(x)$ is a homeomorphism by \ref{generatedAlgebraSpectrumHomeomorphism} and $\hat{x}^t: \mathcal{C}(\sigma(x))\to \mathcal{C}(\hat{B})$ is an isometric isomorphism (TODO!). The we have the isometric $*$-homomorphism
\[ \Phi_x = \iota\circ\wedge^{-1}\circ\hat{x}^t: \begin{tikzcd} \mathcal{C}(\sigma(x)) \ar[r, "\hat{x}^t"] & \mathcal{C}(\hat{B}) \ar[r, "\wedge^{-1}"] & B \ar[r , hook, "\iota"] & A \end{tikzcd} \]
where the inverse Gelfand transform $\wedge^{-1}$ exists and is isometric by the Gelfand-Naimark theorem \ref{GelfandNaimarkCommutative}, because $B$ is commutative.

We verify
\[ \id_{\sigma(x)}(x) = \Phi_x(\id_{\sigma(x)}) = (\iota\circ\wedge^{-1}\circ\hat{x}^t)(\id_{\sigma(x)}) = (\iota\circ\wedge^{-1})(\id_{\sigma(x)}\circ\hat{x}) = (\iota\circ\wedge^{-1})(\hat{x}) = \iota(x) = x \]
using the definition of the transpose, cancellation of $I_{\sigma}(x)$ and inverse of Gelfand transform. We also verify
\[ \vec{1}_{\sigma(x)}(x) = \Phi_x(\vec{1}_{\sigma(x)}) = (\iota\circ\wedge^{-1}\circ\hat{x}^t)(\vec{1}_{\sigma(x)}) = (\iota\circ\wedge^{-1})(\vec{1}_{\sigma(x)}\circ\hat{x}) = (\iota\circ\wedge^{-1})(\vec{1}_{\hat{B}}) = \iota(\vec{1}) = \vec{1} \]
using the fact that the Gelfand transform of $\vec{1}$ is $\varphi\mapsto \varphi(\vec{1})$, which is $\vec{1}_{\hat{B}}$ by \ref{charactersUnital}.
\end{proof}
For polynomials in $z,\overline{z}$, this functional calculus works as expected, because it is a $*$-homomorphism.

In fact we can apply functional calculus to any continuous defined on a superset of the spectrum: restricting to the spectrum still yields a continuous function by \ref{continuousConstructions}.

\begin{proposition} \label{commutativityFunctionalCalculus}
Let $A$ be a unital $C^*$-algebra and $x\in A$ be a normal element. Let $f$ be a continuous function on the spectrum of $x$ and let $y\in A$ commute with $x$. Then $f(x)$ commutes with $y$.
\end{proposition}
TODO proof + restructure + define joint functional calculus? (but we still need case where $y$ is not necessarily normal)


\begin{lemma}
Let $X$ be a compact space and view $\mathcal{C}(X)$ as a unital commutative $C^*$-algebra. Fix $g\in\mathcal{C}(X)$. The functional calculus is then given simply by composition:
\[ \mathcal{C}(g[X]) \to \mathcal{C}(X): f\mapsto f(g) = f\circ g. \]
\end{lemma}
\begin{proof}
Composition is a unital $*$-homomorphism with the right properties. The claim follows from uniqueness of the functional calculus.
\end{proof}

\begin{proposition} \label{propertiesContinuousFunctionalCalculus}
Let $A$ be a unital $C^*$-algebra and $x\in A$ be a normal element with functional calculus $\Phi_x$.
\begin{enumerate}
\item If $B$ is a unital $C^*$-algebra and $\Psi: A\to B$ a unital $*$-homomorphism, then
\[ \Psi\circ\Phi_x = \Phi_{\Psi(x)}, \]
which means
\[ \forall f\in \mathcal{C}(\sigma(x)):\quad \Psi(f(x)) = f(\Psi(x)). \]
\item For any $f\in\mathcal{C}(\sigma(x))$:
\[ \sigma(f(x)) = f(\sigma(x)). \]
This is the spectral mapping theorem.
\item For any $f\in\mathcal{C}(\sigma(x))$ and $g\in\mathcal{C}(\sigma(f(x)))$:
\[ (g\circ f)(x) = \Phi_x(g\circ f) = \Phi_{f(x)}(g) = g(f(x)). \]
\end{enumerate}
\end{proposition}
\begin{proof}
\hspace{1em}
\begin{enumerate}
\item The claim is well-defined because $\sigma(\Psi(x))\subseteq \sigma(x)$, by \ref{spectrumOfImage}. It is easy to check both sides are unital $*$-homomorphisms from $\mathcal{C}(\sigma(x))$ to $B$, sending the identity function to $\Psi(x)$. The claim then follows from uniqueness of the functional calculus.
\item Let $B = C^*(x,\vec{1})$, which is commutative because $x$ is normal. We calculate
\[ \sigma(f(x)) = \setbuilder{\varphi(f(x))}{\varphi\in\hat{B}} = \setbuilder{f(\varphi(x))}{\varphi\in\hat{B}} = f(\sigma(x)). \]
using \ref{spectrumFromSpectrum} and the previous point.
\item For all $\varphi\in\hat{B}$:
\[ \varphi((g\circ f)(x)) = g(f(\varphi(x))) = g(\varphi(f(x)) = \varphi(g(f(x))) \]
Using the first point. Hence $(g\circ f)(x) = g(f(x))$ by TODO ref.
\end{enumerate}
\end{proof}

\begin{proposition} \label{continuityContinuousFunctionalCalculus}
Let $K\subset \R$ be non-empty and compact; $f:K\to \C$ a continuous function; $A$ a unital $C^*$-algebra and $\Omega_K$ the set of self-adjoint elements in $A$ with spectrum contained in $K$. The function
\[ f: \Omega_K\subset A \to A: a\mapsto f(a) \]
is continuous.
\end{proposition}
\begin{proof}
The map $A\to A$ given by $a\mapsto a^n$ is continuous for every $n\geq 0$, because multiplication is continuous. Then every polynomial $f$ induces a continuous map $A\to A$.
Then $\epsilon/3$ by Stone-Weierstrass.
\end{proof}
TODO: also for non-compact $K$ and $K\subseteq \C$. Then $\Omega_K$ set of normal elements.

\begin{proposition}
Let $A$ be a unital $C^*$-algebra, $x\in A$ a normal element and $f\in \cont(\sigma(x))$. Then
\[ \norm{f(x)} = \sup_{\lambda\in \sigma(x)}|f(\lambda)|. \]
\end{proposition}
\begin{proof}
We calculate
\[ \norm{f(x)} = \spr(f(x)) = \sup |\sigma(f(x))| = \sup |f(\sigma(x))| = \sup_{\lambda\in \sigma(x)}|f(\lambda)|. \]
\end{proof}
\begin{corollary}
Let $x$ be an normal element in a unital $C^*$-algebra and $\lambda_0 \in \rho(x)$. Then $d(\lambda_0, \sigma(x)) = \norm{R_x(\lambda_0)}^{-1}$.
\end{corollary}
\begin{proof}
We calculate
\[ \norm{R_x(\lambda_0)} = \norm{\frac{1}{x-\lambda_0\cdot \vec{1}}} = \sup_{\lambda\in \sigma(x)}\left|\frac{1}{\lambda - \lambda_0}\right| = \frac{1}{\inf_{\lambda\in \sigma(x)}|\lambda- \lambda_0|} = \frac{1}{d(\lambda_0, \sigma(x))}. \]
\end{proof}
TODO: for general closed operators $d(\lambda_0, \sigma(x)) \geq \norm{R_x(\lambda_0)}^{-1}$??

\begin{lemma}
If two polynomials in $z,\overline{z}$ agree on the spectrum of a normal element, they give an equation the element obeys.
\end{lemma}
The proof is the unicity of the functional calculus.
\begin{corollary} \label{propertiesFromSpectrum}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a normal element. Then
\begin{enumerate}
\item $x$ is self-adjoint \textup{if and only if} $\sigma(x)\subseteq \R$;
\item $x$ is unitary \textup{if and only if} $\sigma(x)\subseteq \mathbb{T}$;
\item $x$ is a projection \textup{if and only if} $\sigma(x)\subseteq \{0,1\}$.
\end{enumerate}
\end{corollary}
\begin{proof}
\hspace{1em} TODO spectral mapping
\begin{enumerate}
\item By \ref{selfAdjointSpectrumReal} we have that self-adjoint implies real spectrum. The converse follows from the lemma applied to $z = \overline{z}$.
\item Assume $x$ unitary. By the $C^*$-identity $\norm{x} = \sqrt{\norm{x^*x}} = 
\sqrt{\norm{\vec{1}}} = 1$, by \ref{consequencesC*}. By \ref{normNormalElement}, $\spr(x) = \norm{x} = 1$. Also $\norm{x^{-1}}^{-1} = 1$. So $\sigma(x)\subseteq \mathbb{T}$ by \ref{openSetInvertibles}. The converse follows from the lemma applied to $1 = z\overline{z} = \overline{z}z$.
\item Assume $x$ a projection. Then $x-\lambda$ has an inverse given by $-\lambda^{-1} + (1 - \lambda)^{-1}\lambda^{-1}x$ if $\lambda \notin \{0,1\}$:
\[ (x-\lambda)\left( - \frac{1}{\lambda} + \frac{x}{(1-\lambda)\lambda} \right) = \vec{1} - \frac{x}{\lambda} + \frac{x-x\lambda}{(1-\lambda)\lambda} =  \vec{1} - \frac{x}{\lambda} + \frac{x}{\lambda} = \vec{1}. \]
The converse follows from the lemma applied to $z = \overline{z} = z^2$.
\end{enumerate}
\end{proof}



\begin{proposition}
Let $A$ be a unital $C^*$-algebra. Then every element in $A$ can be written as a linear combination of at most four unitaries.
\end{proposition}
\begin{proof}
Let $x\in A$. By \ref{realImaginaryParts} we can write $x=x_1+ix_2$ for some self-adjoint $x_1,x_2$.
TODO
\end{proof}

\subsection{Constructions}
\subsubsection{Pseudoinverse}
\begin{definition}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a normal element. Then the \udef{pseudo-inverse} of $x$ is 
\[ x^+ \defeq f(x) \qquad\text{where}\qquad f: y \mapsto \begin{cases}
y^{-1} & (y \neq 0) \\ 0 & (y = 0).
\end{cases} \]
\end{definition}

\section{Positivity}
\url{https://link.springer.com/content/pdf/10.1023/A:1009717500980.pdf}
\subsection{Positive elements}
\begin{definition}
Let $A$ be a $C^*$-algebra. An element $a\in A$ is \udef{positive} if it is normal and $\sigma(a) \subset [0,+\infty[$. We write $a\geq 0$.

The set of all positive elements of $A$ is the \udef{positive cone} of $A$
\[ A^+ \defeq \setbuilder{a\in A}{a\geq 0}. \]
\end{definition}
By \ref{propertiesFromSpectrum} every projection is positive and every positive element is in fact self-adjoint.

By \ref{normalSpectralRadiusEqualsNorm} we have for all positive $a\in A$:
\[ \norm{a} = \sup\sigma(a) = \max\sigma(a). \]

\begin{proposition}
Let $A$ be a unital $C^*$-algebra and $a\in A$ a self-adjoint element. Then
\[ a\in A^+ \qquad\iff\qquad \exists b\in \SelfAdjoints(A):\quad a = b^2. \]
If we further require $b$ to be positive, then it is unique.
\end{proposition}
\begin{proof}
Let $a\in A^+$. Then define $b = \sqrt{a}$ by spectral calculus. Then we have
\[ b^2 = (\Phi_a(\sqrt{\mbox{\;\;}}))^2 = \Phi_a(\sqrt{\mbox{\;\;}})\cdot \Phi_a(\sqrt{\mbox{\;\;}}) = \Phi_a(\sqrt{\mbox{\;\;}}^2) = \Phi_a(I_{\sigma(a)}) = a. \]
Converse by spectral mapping \ref{propertiesContinuousFunctionalCalculus}.
\end{proof}

\begin{lemma} \label{positivityDistanceToNorm}
Let $A$ be a unital $C^*$-algebra and $a\in A$ self-adjoint. Then the following are equivalent:
\begin{enumerate}
\item $a$ is positive;
\item $\norm{\frac{1}{2}\vec{1} - a / \norm{a}} \leq \frac{1}{2}$;
\item $\norm{r\vec{1} - a / \norm{a}} \leq r$ for all $r\geq 1/2$.
\item $\norm{r\vec{1} - a / \norm{a}} \leq r$ for some $r\geq 1/2$.
\end{enumerate}
\end{lemma}
\begin{proof}
The proof is cyclic:

$(1.\Rightarrow 2.)$ Assume $a$ positive. Then $\sigma(a) \subseteq [0,\norm{a}]$. By spectral mapping, \ref{propertiesContinuousFunctionalCalculus}, we have $\sigma(\frac{1}{2}\vec{1} - a / \norm{a}) \subseteq [-1/2, 1/2]$ and thus $\norm{\frac{1}{2}\vec{1} - a / \norm{a}} \leq \frac{1}{2}$, by \ref{normNormalElement}.

$(2.\Rightarrow 3.)$ Write $r = 1/2 + r'$, so $r'\geq 0$. Then
\[ \norm{r\vec{1} - \frac{a}{\norm{a}}} = \norm{\frac{1}{2}\vec{1} + r'\vec{1} - \frac{a}{\norm{a}}} \leq \norm{r'\vec{1}}+ \norm{\frac{1}{2}\vec{1} - \frac{a}{\norm{a}}} \leq r' + \frac{1}{2} = r. \]

$(3.\Rightarrow 4.)$ Clear.

$(4.\Rightarrow 1.)$ By \ref{normNormalElement}, $\sigma(r\vec{1} - a / \norm{a}) \subseteq [-r, r]$. By spectral mapping, this means $\sigma(a) \subseteq [0, 2r\norm{a}]$ and thus $a$ is positive.
\end{proof}

\begin{proposition}\label{existenceSquareRoot}
Let $A$ be a $C^*$-algebra and $a\in A$. Then $a$ is positive \textup{if and only if} $a = b^*b$ for some $b\in A$.
\end{proposition}
TODO: link with $\sqrt{a}$?
\begin{corollary}
Let $A$ be a concrete $C^*$-algebra of bounded operators on some Hilbert space $\mathcal{H}$ and $a\in A$. Then $a$ is positive as an element of the $C^*$ algebra \textup{if and only if} $a$ is positive as an operator on $\mathcal{H}$. for all $x\in \mathcal{H}: \inner{x,ax}\geq 0$.
\end{corollary}
\begin{proof}
If $a$ is positive, then $a = b^*b$ and thus
\[ \forall x\in \mathcal{H}: \inner{x,ax} = \inner{x, b^*bx} = \inner{bx,bx} = \norm{bx}^2 \geq 0, \]
meaning $a$ is a positive operator.

Conversely, the spectrum is contained in the closure of the numerical range (TODO ref), which is a subset of $[0,\infty[$.
\end{proof}
Consequently if $\inner{x,ax}\geq 0$ for all $x\in\mathcal{H}$, then $a$ is self-adjoint.

\subsection{Partial order on self-adjoint elements}
\begin{proposition}
Let $A$ be a $C^*$-algebra. The set $A^+$ is a salient pointed convex cone.
\end{proposition}
\begin{proof}
That $A^+$ is a cone follows from spectral mapping (\ref{propertiesContinuousFunctionalCalculus}), as does the salience of $A^+$.

For convexity, we verify additive closure (see \ref{convexityAdditiveClosure}) Take $a,b\in A$ and set $p= \frac{\norm{a}+\norm{b}}{\norm{a+b}} \geq 1 \geq 1/2$. Then
\[ \norm{p\vec{1} - \frac{a+b}{\norm{a+b}}} \leq \frac{1}{\norm{a+b}}\left(\norm{\norm{a}-a} + \norm{\norm{b}-b}\right) \leq \frac{\norm{a}+\norm{b}}{\norm{a+b}} = p \]
using the triangle inequality and point 3. of \ref{positivityDistanceToNorm} with $r=1$.
\end{proof}
By \ref{positiveCone}, we have:
\begin{corollary}
The relation $\leq$ defined by
\[ a\leq b \qquad\iff\qquad b-a\in A^+ \]
is a vector partial order on $A$.
\end{corollary}
TODO in general $\sSet{A, \leq}$ is not a Riesz space. (See e.g.\ the absolute value)

\begin{lemma}
If $\alpha,\beta\in\R$ and $\alpha\leq \beta$, then $\alpha\vec{1}\leq \beta\vec{1}$.
\end{lemma}

\begin{lemma}
If $0\leq a \leq b$ and $a$ invertible, then $b$ invertible and $0\leq a^{-1}\leq b^{-1}$.
\end{lemma}

\begin{lemma}
Let $A$ be a unital $C^*$-algebra and $v$ an arbitrary element $v\in A$. If $v^*v$ is positive, then $v=0$.
\end{lemma}

\begin{proposition}
Let $A$ be a unital $C^*$-algebra and $a\in A$ a self-adjoint element. Then $a\leq \norm{a}\cdot\vec{1}$.
\end{proposition}
\begin{proof}
This follows from
\[ \sigma(\norm{a}-a) = \setbuilder{\norm{a}-\lambda}{\lambda\in\sigma(a)} \]
using the fact that the spectrum of $a$ is real, \ref{selfAdjointSpectrumReal}.
\end{proof}
\begin{corollary}
If $0\leq a \leq b$, then $\norm{a} \leq \norm{b}$.
\end{corollary}
\begin{proof}
TODO
\end{proof}

\subsubsection{Lattice properties of self-adjoint operators}
\url{https://www.ams.org/journals/proc/1951-002-03/S0002-9939-1951-0042064-2/S0002-9939-1951-0042064-2.pdf}

\begin{proposition}
Let $A$ be a $C^*$-algebra. The real vector space of self-adjoint operators $\SelfAdjoints(A)$ is a Riesz space \textup{if and only if} $A$ is commutative.
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{corollary} \label{positiveNegativeParts}
Let $A$ be a unital $C^*$-algebra, and $a\in A$ a self-adjoint element. Then there exists a unique decomposition $a=a^+ - a^-$ where $a^+,a^-\in A^+$ and $a^+a^- = 0$.
\end{corollary}
\begin{proof}
TODO
\end{proof}
The corollary \ref{positiveNegativeParts} can also be proved using functional calculus, by setting $a^+ = f^+(a)$ and $a^- = f^-(a)$ where
\[ f^+: x\mapsto \begin{cases}
x & (x\geq 0)\\ 0 & (x < 0)
\end{cases} \qquad\text{and}\qquad f^-: x\mapsto \begin{cases}
0 & (x\geq 0)\\ -x & (x < 0)
\end{cases} \]

\begin{corollary}[Cartesian decomposition]
Let $A$ be a $C^*$-algebra and $a\in A$. Then we can decompose $a$ as
\[ (p_1-p_2) + i(p_3-p_4) \]
where $p_1,p_2,p_3,p_4$ are positive.
\end{corollary}
\begin{proof}
By \ref{realImaginaryParts}.
\end{proof}
Thus $\Span_\C(A^+) = A$.

\begin{proposition}
The set of all bounded self-adjoint operators on a Hilbert space is an anti-lattice.
\end{proposition}

\subsubsection{Operator monotonicity}
Delicate!
\begin{proposition}
Assume $0\leq a\leq b$. Then
\begin{enumerate}
\item $\sqrt{a}\leq \sqrt{b}$;
\item $0\leq ab$ if $a,b$ commute.
\end{enumerate}
\end{proposition}
It is not true that $a^2\leq b^2$ or that $ab$ is positive in general!

\subsection{Absolute value}
\begin{definition}
Let $A$ be a unital $C^*$-algebra. For each $a\in A$, the \udef{absolute value} of $a$ is $|a| = (a^*a)^{1/2}$.
\end{definition}
The square root is well defined using functional calculus on the self-adjoint element $a^*a$.

\begin{lemma} \label{propertiesAbsoluteValue}
Let $A$ be a unital $C^*$-algebra.
\begin{enumerate}
\item Let $u\in\Unitaries$, then $|u| = 1$.
\item Let $a\in A$, then $|a|$ is positive and thus self-adjoint.
\item The map $a\mapsto |a|$ is continuous.
\end{enumerate}
\end{lemma}
\begin{proof}
For the first point, $|u| = (u^*u)^{1/2} = \vec{1}^{1/2} = \vec{1}$.

The second point follows from spectral mapping \ref{propertiesContinuousFunctionalCalculus} using the fact that $z\mapsto \overline{z}z$ has positive image in $\C$.

For the third point, $a\mapsto a^*a$ is continuous by \ref{multiplicationContinuous} and \ref{consequencesC*}. Then $a\mapsto |a|$ is continuous by \ref{continuityContinuousFunctionalCalculus}.
\end{proof}

\begin{lemma}
Let $T$ be a bounded operator on a Hilbert space $\mathcal{H}$. Then $|T|$ is the only positive operator $A$ in $\Bounded(\mathcal{H})$ such that $\norm{Ax} = \norm{Tx}$ for all $x\in\mathcal{H}$.
\end{lemma}
\begin{proof}
We have for all $x\in\mathcal{H}$,
\[ \inner{Ax,Ax} = \inner{Tx,Tx} \implies \inner{(A^*A-T^*T)x,x}=0, \]
which implies $T^*T = A^*A$. Now $A$ is positive, so $A^*A = A^2$ and taking the squared root give $A = \sqrt{T^*T} = |T|$.
\end{proof}

We do \emph{not}, in general, have a triangle inequality $|a+b| \leq |a| + |b|$.
\begin{example}
Consider the $C^*$ algebra $\C^{2\times 2}$ with
\[ A = \begin{pmatrix}
1 & 1 \\ 1 & 1
\end{pmatrix} = \frac{1}{2}\begin{pmatrix}
1 & 1 \\ -1 & 1
\end{pmatrix}\begin{pmatrix}
0 & 0 \\ 0 & 2
\end{pmatrix}\begin{pmatrix}
1 & -1 \\ 1 & 1
\end{pmatrix} = |A| \qquad\text{and}\qquad B = \begin{pmatrix}
0 & 0 \\ 0 & -2
\end{pmatrix} \]
Then
\[ A + B = \begin{pmatrix}
1 & 1 \\ 1 & -1
\end{pmatrix} = \begin{pmatrix}
1-\sqrt{2} & 1=\sqrt{2} \\ 1 & 1
\end{pmatrix}\begin{pmatrix}
-\sqrt{2} & 0 \\ 0 & \sqrt{2}
\end{pmatrix}\begin{pmatrix}
1-\sqrt{2} & 1=\sqrt{2} \\ 1 & 1
\end{pmatrix}^{-1} \]
So
\[ |A+B| = \begin{pmatrix}
\sqrt{2} & 0 \\ 0 & \sqrt{2}
\end{pmatrix} \qquad\text{and}\qquad |A| + |B| = \begin{pmatrix}
1 & 1 \\ 1 & 3
\end{pmatrix}. \]
Finally $|A| + |B| - |A+B| = \begin{pmatrix}
1 - \sqrt{2} & 1 \\ 1 & 3-\sqrt{2}
\end{pmatrix}$ is not positive:
\[ \begin{pmatrix}
1 & 0
\end{pmatrix}\begin{pmatrix}
1 - \sqrt{2} & 1 \\ 1 & 3-\sqrt{2}
\end{pmatrix}\begin{pmatrix}
1 \\ 0
\end{pmatrix} = 1-\sqrt{2} < 0. \]
\end{example}

\subsubsection{Polar decomposition}
\begin{proposition}[Polar decomposition]
Let $A$ be a unital $C^*$-algebra and $a\in \GL(A)$ an invertible element. Then there exists a unique decomposition
\[ a = u(a) |a| \]
such that $u(a)$ is unitary. The map $u: \GL(A) \to \Unitaries(A)$ is continuous.
\end{proposition}
\begin{proof}
If $a$ is invertible, then so are $a^*$ and $|a|$ by spectral mapping \ref{propertiesContinuousFunctionalCalculus} ($0\notin \sigma(a^*)$ and $0\notin \sigma(|a|)$). Put $u(a) = a|a|^{-1}$. Clearly $a = u(a)|a|$ and $u(a)$ is unitary because it is invertible and
\[ u(a)^*u(a) = |a|^{-1}a^*a|a|^{-1} = |a|^{-1}|a|^2|a|^{-1} = \vec{1}. \]

The continuity of u: $a\mapsto a|a|^{-1}$ follows from the continuity of multiplication, \ref{multiplicationContinuous}, the continuity of the absolute value, \ref{propertiesAbsoluteValue} and the continuity of the inverse, \ref{inverseMapContinuous}.
\end{proof}
TODO: polar decomposition for non-invertible elements. Then $u$ is a partial isometry.

\subsection{Positive maps}
\begin{definition}
Let $A,B$ be $C^*$-algebras. Then $f:A\to B$ is a \udef{positive map} if
\[ \forall x\in A: \quad x\geq 0 \implies f(x)\geq 0. \]
\end{definition}
By \ref{existenceSquareRoot}, this is equivalent to the condition that $f(x^*x)\geq 0$ for all $x\in A$.

TODO: \url{https://www-m5.ma.tum.de/foswiki/pub/M5/CQC/Masterarbeit.pdf}
\url{https://iopscience.iop.org/article/10.1088/0305-4470/34/29/308}

\subsubsection{Positive functionals and states}
\begin{definition}
Let $A$ be a $C^*$-algebra. A linear functional $\rho$ on $A$ is positive, written $\rho\geq 0$, if
\[ \forall x\in A: \quad x\geq 0 \implies \rho(x)\geq 0. \]
\end{definition}
\begin{definition}
A \udef{state} on a $C^*$-algebra $A$ is a positive linear functional of norm $1$. The set $\mathcal{S}(A)$ of all states on $A$ is called the \udef{state space} of $A$.
\end{definition}
Not necessarily multiplicative!

\begin{example}
Let $A$ be a concrete $C^*$-algebra of operators acting non-degenerately on $\mathcal{H}$ and $\xi \in \mathcal{H}$. Define
\[ \rho_\xi: A \to \C: x\mapsto \inner{\xi, x\xi}, \]
then $\rho_\xi$ is a positive linear functional on $A$ of norm $\norm{\xi}^2$, so  $\rho_\xi$ is a state if $\norm{\xi} =1$. Such a state is called a \udef{vector state} of $A$.
\end{example}

\begin{proposition}
Let $A$ be a $C^*$-algebra and $\rho$ a positive linear functional on $A$. Then
\[ A\times A \to \C: (a,b)\mapsto \rho(a^*b) \]
is a positive Hermitian form.
\end{proposition}
\begin{corollary}
This form obeys the Cauchy-Schwarz inequality, \ref{CauchySchwarz}:
\[ \forall a,b\in A:\quad |\rho(a^*b)|^2 \leq \rho(a^*a)\rho(b^*b). \]
Or
\[ \forall a,b\in A:\quad |\rho(ab)|^2 \leq \rho(aa^*)\rho(b^*b). \]
\end{corollary}

\begin{proposition}
Let $\omega$ be a linear functional over a $C^*$-algebra $A$. The following are equivalent:
\begin{enumerate}
\item $\omega$ is positive;
\item $\omega$ is continuous and $\norm{\omega} = \lim_\lambda \omega(e_\lambda^2)$ for some approximate unit $\{e_\lambda\}$.
\end{enumerate}
\end{proposition}

\begin{proposition}
Let $\omega$ be a positive linear functional over a $C^*$-algebra $A$ and $a,b\in A$, then
\begin{enumerate}
\item $\omega(a^*) = \overline{\omega(a)}$;
\item $|\omega(a)|^2 \leq \omega(a^*a)\norm{\omega}$;
\item $|\omega(a^*ba)| \leq \omega(a^*a)\norm{b}$;
\item $\norm{\omega} = \sup\setbuilder{\omega(a^*a)}{\norm{a} = 1}$
\item for any approximate unit $\{e_\lambda\}$, $\norm{\omega} = \lim_\lambda\omega(e_\lambda^2)$
\end{enumerate}
\end{proposition}
\begin{proof}
By \ref{realImaginaryParts} and \ref{positiveNegativeParts} we can write $a\in A$ as
\[ a = x_{1,+} - x_{1,-} + i(x_{2,+} - x_{2,-}).\]
Where $x_{1,+}, x_{1,-}, x_{2,+}, x_{2,-}$ are self-adjoint. Then
\[ \rho(a^*) = \rho(x_{1,+} - x_{1,-} - i(x_{2,+} - x_{2,-})) = \rho(x_{1,+}) - \rho(x_{1,-}) - i(\rho(x_{2,+}) - \rho(x_{2,-})) = \overline{\rho(a)}. \]
Where the last equality follows because $\rho$ takes real values on self-adjoint elements. (TODO!)
\end{proof}
\begin{corollary}
Let $\omega_1$ and $\omega_2$ be positive linear functionals over a $C^*$-algebra $A$. Then $\omega_1+\omega_2$ is a positive linear functional and
\[ \norm{\omega_1+\omega_2} = \norm{\omega_1} + \norm{\omega_2}. \]
Thus the state space is a convex subset of the dual of $A$.
\end{corollary}
\begin{corollary}
Let $X$ be a compact Hausdorff space. Let $\omega$ be a positive linear functional on $\cont(X)$. Then $\omega$ is continuous and $\norm{\omega} = 1$.
\end{corollary}
TODO: move up for Riesz-Markov?

\begin{proposition}
If $A$ is commutative, the pure states are exactly the characters.
\end{proposition}

\subsection{Comparison of projectors}
Lattice
\subsection{General comparison theory}

\section{Matrix $C^*$-algebras}
\begin{lemma}
Let $A$ be a $C^*$-algebra. There exists a norm that makes $A^{n\times n}$ a $C^*$-algebra. TODO expression
\end{lemma}
\subsection{Completely positive maps}
\begin{definition}
Let $A,B$ be $C^*$-algebras and $f:A\to B$ a linear function. We call $f$ \udef{completely positive} if for all $n\in\N$ the pointwise extension of $f$ in $(A^{n\times n}\to B^{n\times n})$ is positive.
\end{definition}

\subsection{Completely bounded maps}
\begin{definition}
Let $A,B$ be $C^*$-algebras and $f:A\to B$ a linear function. We call $f$ \udef{completely bounded} if for all $n\in\N$ the pointwise extension of $f$ in $(A^{n\times n}\to B^{n\times n})$ is bounded.
\end{definition}


\chapter{Representations and states}
\section{Representations}
\begin{definition}
A \udef{representation} of a $C^*$-algebra $A$ on a Hilbert space $\mathcal{H}$ is a $*$-homomorphism $\pi: A \to \Bounded(\mathcal{H})$.

A \udef{subrepresentation} of $\pi$ is the restriction of $\pi$ to a closed invariant subspace of $\mathcal{H}$.

We say a representation $\pi: A \to \Bounded(\mathcal{H})$ is
\begin{enumerate}
\item \udef{faithful} if it is injective;
\item \udef{non-degenerate} if $\overline{\pi(A)\mathcal{H}} = \mathcal{H}$;
\item \udef{cyclic} w.r.t. a unit vector $\xi\in\mathcal{H}$ if $\overline{\pi(A)\xi} = \mathcal{H}$.
\end{enumerate}
\end{definition}

\begin{lemma}
Let $A$ be a $C^*$-algebra and $\pi: A \to \Bounded(\mathcal{H})$ a representation of $A$. Then $\pi$ is a faithful representation of $A/\ker\pi$.
\end{lemma}

\begin{proposition}
Let $\pi:A\to\mathcal{H}$ be a representation of a $C^*$-algebra, then $\pi$ being faithful is equivalent to any of the following:
\begin{enumerate}
\item $\ker \pi = \{0\}$;
\item $\norm{\pi(a)} = \norm{a}$ for all $a\in A$;
\item $\pi(a) > 0$ for all $a>0$.
\end{enumerate}
\end{proposition}

\begin{lemma}
Let $\pi: A\to\mathcal{H}$ be a representation and $P_1$ be a projector with closed range $\mathcal{H}_1$. Then $\pi|_{\mathcal{H}_1}$ is a subrepresentation \textup{if and only if}
\[ \forall a\in A: \quad \pi(a)P_1 = P_1\pi(a).  \]
\end{lemma}
\begin{proof}
Assume $P_1\pi(a) = \pi(a)P_1$ for all $a\in A$. Then multiplying by $P_1$ gives
\[ P_1\pi(a)P_1 = \pi(a)P_1 \quad \forall a\in A \]
which expresses invariance. Conversely, assume this invariance condition. Then
\[ \pi(a)P_1 = P_1\pi(a)P_1 = (P_1\pi(a)P_1)^{**} = (P_1\pi(a^*)P_1)^{*} = (\pi(a^*)P_1)^* = P_1\pi(a). \]
\end{proof}

\begin{lemma} \label{nonDegeneracyAlgebraRepresentation}
Let $\pi:A\to \mathcal{H}$ be a representation of a $C^*$-algebra $A$. Define
\[ \mathcal{H}_0 \defeq \setbuilder{x\in\mathcal{H}}{\forall a\in A: \pi(a)x = 0}. \]
Then $\pi$ is non-degenerate \textup{if and only if} $\mathcal{H}_0 = \{0\}$.
\end{lemma}
\begin{proof}
It is enough to prove that
\[ (\pi(A)\mathcal{H})^\perp = \mathcal{H}_0. \]
This implies $\overline{\pi(A)\mathcal{H}} = \mathcal{H}_0^\perp$ by \ref{doubleComplementClosure}. The claim then follows from \ref{OrthogonalComplementProperties}.

The proof then rests on the equality
\[ \forall x,y\in \mathcal{H}, a\in A: \quad \inner{x,\pi(a)y} = \inner{\pi(a^*)x,y}. \]
An $x\in\mathcal{H}$ is an element of $(\pi(A)\mathcal{H})^\perp$ iff the left side is zero for all $y\in\mathcal{H},a\in A$. An $x\in\mathcal{H}$ is an element of $\mathcal{H}_0$ iff $\pi(a^*)x = 0$ for all $a^*\in A$. This is equivalent to saying the right side is zero for all $y\in\mathcal{H},a\in A$ by the non-degeneracy of the inner product \ref{nonDegeneracyInnerProduct}.
\end{proof}

\begin{proposition}
Let $\pi:A\to \mathcal{H}$ be a non-degenerate representation. Then $\pi$ is the direct sum of a family of cyclic representations.
\end{proposition}

\begin{definition}
Two representations $\pi,\rho$ of $A$ on Hilbert spaces $\mathcal{X}$ and $\mathcal{Y}$ respectively are \udef{(unitarily) equivalent} if there is a unitary operator $U\in\Bounded(\mathcal{X}, \mathcal{Y})$ such that
\[ \forall x\in A: \quad U\pi(x)U^* = \rho(x). \]
\end{definition}

\subsection{Irreducible representations}
TODO connect to more general definitions.
\begin{definition}
A set $D$ of bounded operators on a Hilbert space $\mathcal{H}$ is called \udef{algebraically irreducible} if the only subspaces of $\mathcal{H}$ invariant under the action of $D$ are the trivial subspaces $\{0\}$ and $\mathcal{H}$.

The set $D$ is called \udef{topologically irreducible} if the only closed invariant subspaces of $\mathcal{H}$ are the trivial subspaces.

A representation $\pi: A\to\mathcal{H}$ is called irreducible if $\pi[A]$ is irreducible.
\end{definition}
\begin{proposition}
For $C^*$-algebras the notions of algebraic and topological irreducibility are equivalent.
\end{proposition}

\begin{proposition}
Let $D$ be a self-adjoint set of bounded operators on the Hilbert space $\mathcal{H}$. The following are equivalent:
\begin{enumerate}
\item $D$ is irreducible;
\item the commutant $\comm{D}$ consists of multiples of the identity operator;
\item every non-zero vector $x\in\mathcal{H}$ is cyclic for $D$ in $\mathcal{H}$, or $D =0$ and $\mathcal{H} = \C$.
\end{enumerate}
\end{proposition}

\section{The GNS construction}
\begin{lemma}
Let $\omega$ be a state on a $C^*$-algebra $A$. Then the map
\[ A\times A \to \C: (x,y)\mapsto \omega(x^*y) \]
is a pre-inner product on $A$.
\end{lemma}

\begin{lemma}
Let $\omega$ be a state on a $C^*$-algebra $A$. Then
\[ N_\omega \defeq \setbuilder{x\in A}{\omega(x^*x) = 0} \]
is a closed left ideal in $A$.
\end{lemma}
\begin{proof}
TODO: left ideal criterion?

Let $x,y\in N_\omega$, Then
\[ \omega((x+y)^*(x+y)) = \omega(x^*x) + \omega(y^*x) + \omega(x^*y) + \omega(y^*y) = 0 \]
where $\omega(y^*x), \omega(x^*y)$ are zero by corollary \ref{preInnerProductCSBZero} to the Cauchy-Schwarz inequality.
\end{proof}

\begin{lemma}
Let $\omega$ be a state on a $C^*$-algebra $A$. Then $H^0_\omega = A/N_\omega$ is an inner product space with inner product
\[ \inner{[x], [y]}_\omega = \omega(x^*y) \]
where $[x] = x + N_\omega$ for $x\in A$.

We also define $\mathcal{H}_\omega$ as the Hilbert space completion of $H^0_\omega$.
\end{lemma}
\begin{proof}
We verify this inner product is well-defined: take $x,x'\in [x]$ and $y,y'\in[y]$. Then $(x'-x) \in N_\omega$ and $(y'-y) \in N_\omega$. Again using corollary \ref{preInnerProductCSBZero} to the Cauchy-Schwarz inequality, we see
\[ \omega(x^*y) = \omega(x^*y) + \omega(x^*(y'-y)) = \omega(x^*y') = \omega(x^*y') + \omega((x'-x)^*y') = \omega(x^{\prime *}y'). \]
\end{proof}

\begin{lemma}
Let $\omega$ be a state on a $C^*$-algebra $A$ and write $[x] = x+N_\omega$. Then for any increasing approximate unit $(e_n)_n$,
\[ \xi_\omega = \lim_{n\to\infty}[e_n] \]
is a unit vector in $\mathcal{H}_\omega$ which does not depend on the choice of approximate unit.
\end{lemma}

\begin{theorem}[Gelfand-Naimark-Segal]
Let $\omega$ be a state on a $C^*$-algebra $A$. The map $\pi_\omega: A\to \Bounded(H^0_\omega)$ defined by
\[ \pi_\omega(x) : H^0_\omega \to H^0_\omega: [y] \mapsto [xy] \]
is a well-defined representation of $A$ on $H^0_\omega$. This can be extended to a representation of $A$ on $\mathcal{H}_\omega$. Also
\begin{enumerate}
\item $\pi_\omega$ is cyclic w.r.t. $\xi_\omega$;
\item for all $a\in A:\; \omega(a) = \inner{\xi_\omega, \pi_\omega(a)\xi_\omega}_\omega.$
\end{enumerate}
This is the unique representation with these properties, up to unitary equivalence.
\end{theorem}
\begin{corollary}
Let $\omega$ be a state over the $C^*$-algebra $A$ and $\tau: A\to A$ a $*$-automorphism that leaves $\omega$ invariant:
\[ \forall a\in A: \quad \omega(\tau(a)) = \omega(a). \]
Then there exists a unique unitary operator $U$ on $\mathcal{H}_\omega$ such that
\[ \forall a\in A: \quad U\pi_\omega(a)U^* = \pi_\omega(\tau(a)) \]
and $U\xi_\omega = \xi_\omega$.
\end{corollary}

\begin{definition}
Let $\omega: A\to \C$ be a state.
We call the cyclic representation $(\mathcal{H}_\omega,\pi_\omega,\xi_\omega)$ constructed in the GNS theorem is called the \udef{canonical cyclic representation} of $A$ associated with $\omega$.
\end{definition}

\begin{lemma}
Let $\omega$ be a state over a $C^*$-algebra $A$ and $(\mathcal{H}_\omega,\pi_\omega,\xi_\omega)$ the associated cyclic representation. There is a bijective correspondence 
\[ \omega_T(a) = \inner{T\xi_\omega, \pi_\omega(a)\xi_\omega} \]
between positive functionals $\omega_T$ over $A$ majorised by $\omega$ and positive operators $T$ in the commutant $\comm{\pi_\omega}$ with $\norm{T}\leq 1$.
\end{lemma}

\begin{proposition}
Let $\omega$ be a state over a $C^*$-algebra $A$ and $(\mathcal{H}_\omega,\pi_\omega,\xi_\omega)$ the associated cyclic representation. The following are equivalent:
\begin{enumerate}
\item $(\mathcal{H}_\omega,\pi_\omega)$;
\item $\omega$ is a pure state;
\item $\omega$ is an extremal point of the state space $\mathcal{S}(A)$.
\end{enumerate}
\end{proposition}

\begin{theorem}
Let $A$ be a $C^*$-algebra. Then $A$ is isomorphic to a norm-closed self-adjoint algebra of bounded operators on a Hilbert space.
\end{theorem}


\section{Multiplier algebras}
\subsection{Essential ideals}
TODO move to section about ideals
\begin{definition}
Let $J$ be an ideal of a $C^*$-algebra $A$. Then $J$ is called \udef{essential} if for all $a\in A$ we have that $aJ = \{0\}$ implies $a=0$.
\end{definition}

\begin{lemma} \label{C*idealSquared}
Let $A$ be a $C^*$-algebra and $I\subset A$ an ideal. Then $I^2 = I$.
\end{lemma}
\begin{proof}
Let $a\in I^+$. Then $a = (a^{1/2})^2\in I^2$. As $I$ and $I^2$ are $C^*$-algebras, they are spanned by their positive elements. So $I^2 
\subset I \subset I^2$.
\end{proof}
\begin{lemma} \label{productC*ideals}
Let $A$ be a $C^*$-algebra and $I,J\subset A$ ideals. Then $IJ = I\cap J$.
\end{lemma}
\begin{proof}
We calculate $I\cap J = (I\cap J)^2 \subset IJ \subset I\cap J$ using \ref{C*idealSquared}.
\end{proof}

\begin{proposition}
Let $J$ be an ideal of a $C^*$-algebra $A$. Then the following are equivalent:
\begin{enumerate}
\item $J$ is essential;
\item $\forall a\in A:\;Ja = \{0\}$ implies $a=0$;
\item every other non-zero ideal in $A$ has a non-zero intersection with $J$.
\end{enumerate}
\end{proposition}
\begin{proof}
Assume (3) and let $a\in A$ such that $aJ=\{0\}$. Let $I= \overline{AaA}$ be the ideal generated by $a$.
\end{proof}

\subsection{Multiplier algebras}
\begin{proposition}
Let $A$ be a $C^*$-algebra and $\pi_1, \pi_2$ faithful, non-degenerate representations. Then the idealisers $I(\pi_1[A]), I(\pi_2[A])$ of $\pi_1[A]$ and $\pi_2[A]$ are isomorphic to each other.
\end{proposition}
\begin{proof}
We need to show
\[ I(\pi_1[A]) = \setbuilder{T\in \Bounded(\mathcal{H}_1)}{T\pi_1[A] \cup \pi_1[A]T \subseteq \pi_1[A]} \cong \setbuilder{T\in \Bounded(\mathcal{H}_2)}{T\pi_2[A] \cup \pi_2[A]T \subseteq \pi_2[A]} = I(\pi_2[A]). \]

We first show $I(\pi[A])$ contains $\pi[A]$ as an essential ideal. That it contains $A$ as an ideal is obvious. Non-degeneracy of the representation means that the only $x\in\mathcal{H}$ that is mapped to $0$ by all $\pi[A]$ is $0$, by \ref{nonDegeneracyAlgebraRepresentation}. 

The idealiser is the largest subalgebra of $\Bounded(\mathcal{H})$ that contains $A$ as an ideal. The ideal is necessarily 
\end{proof}

\begin{definition}
Let $A$ be a $C^*$-algebra. The \udef{multiplier algebra} $M(A)$ of $A$ is the largest $C^*$-algebra that contains $A$ as an essential ideal.
\end{definition}

The multiplier algebra is the non-commutative analogue of StoneâÄech compactification: if $A$ is commutative, then $A\cong C(X)$ and
\[ M(A) \cong C_b(X) \cong C(\beta(X)), \]
where $\beta(X)$ denotes the StoneâÄech compactification of $X$.

\begin{lemma}
If $A$ is a unital $C^*$-algebra, then $M(A) = A$.
\end{lemma}
If we view the $C^*$-algebra $A$ as a Hilbert $A$-module, then $M(A)$ is the set of adjointable operators on $A$.

\begin{proposition}
Let $A$ be a $C^*$-algebra and $\pi_1, \pi_2$ faithful, non-degenerate representations. Then the idealisers $I(\pi_1[A]), I(\pi_2[A])$ of $\pi_1[A]$ and $\pi_2[A]$ are isomorphic to each other and to the multiplier algebra $M(A)$.


$M(A)$ can be realised as the idealiser
\[ M(A) \cong I(\pi[A]) = \setbuilder{T\in \Bounded(\mathcal{H})}{T\pi[A] \cup \pi[A]T \subseteq \pi[A]} \]
of $A$ in $\Bounded(\mathcal{H})$.
\end{proposition}
\begin{proof}
We need to show
\[ I(\pi_1[A]) = \setbuilder{T\in \Bounded(\mathcal{H}_1)}{T\pi_1[A] \cup \pi_1[A]T \subseteq \pi_1[A]} \cong \setbuilder{T\in \Bounded(\mathcal{H}_2)}{T\pi_2[A] \cup \pi_2[A]T \subseteq \pi_2[A]} = I(\pi_2[A]). \]

We first show $I(\pi[A])$ contains $\pi[A]$ as an essential ideal. That it contains $A$ as an ideal is obvious. Non-degeneracy of the representation means that the only $x\in\mathcal{H}$ that is mapped to $0$ by all $\pi[A]$ is $0$, by \ref{nonDegeneracyAlgebraRepresentation}. 

The idealiser is the largest subalgebra of $\Bounded(\mathcal{H})$ that contains $A$ as an ideal. The ideal is necessarily 
\end{proof}

For example, let $\mathcal{H}$ be a Hilbert space. Then $M(\mathcal{K}(\mathcal{H})) = \Bounded(\mathcal{H})$, where $\mathcal{K}(\mathcal{H})$ is the algebra of compact operators on $\mathcal{H}$.

We write $\mathcal{U}M(A)$ to mean the unitary elements of the multiplier algebra.

Let $\pi: A\to M(B)$ be a $*$-homomorphism. If $\overline{\Span}(\pi(A)B) = B$, then $\pi$ can be uniquely extended to $\overline{\pi}: M(A) \to M(B)$. 

\section{Universal $C^*$-algebras}
\begin{definition}
Let $\mathcal{X}$ be a non-empty set. We formally write $\mathcal{X}^* = \setbuilder{x^*}{x\in \mathcal{X}}$ and view it as a set disjoint from $\mathcal{X}$. A noncommutative-$*$-polynomial with variables in $\mathcal{X}$ is a formal expression of the form
\[ \sum_{k=1}^m\lambda_k x_{k,1}x_{k,2}\ldots x_{k,n_k} \]
where $m, n_k\in \N$, $x_{k,n}\in \mathcal{X}\cup\mathcal{X}^*$ and $\lambda_k\in \C$.

a \udef{polynomal relation} $\mathcal{R}$ on $\mathcal{X}$ is a collection of formal statements of the form
\[ \norm{p_j(\mathcal{X})}\leq r_j \]
indexed by some index set $J$ where $r_j \in\R^{\geq 0}$ and $p_j$ is a noncommutative-$*$-polynomial with variables in $\mathcal{X}$.
\end{definition}

\begin{definition}
Let $\mathcal{X}$ be a non-empty set and $\mathcal{R}$ a set of polynomial relations on $\mathcal{X}$.

A \udef{representation} of $(\mathcal{X}\;|\;\mathcal{R})$ is a $C^*$-algebra $A$ together with a map $\pi:\mathcal{X}\to A$ such that $\mathcal{R}$ becomes true in the image of $\pi$.

A representation $\pi_u:\mathcal{X}\to B$ of $(\mathcal{X}\;|\;\mathcal{R})$ is called \udef{universal} if for any other representation $\pi:\mathcal{X}\to A$ of $(\mathcal{X}\;|\;\mathcal{R})$, there exists a unique $*$-homomorphism $\varphi: B\to A$ such that $\varphi\circ \pi_u = \pi$.

In this case we call $B$ the \udef{universal $C^*$-algebra} generated by $(\mathcal{X}\;|\;\mathcal{R})$ and write $B= C^*(\mathcal{X}\;|\;\mathcal{R})$.
\end{definition}

\begin{definition}
A polynomial relation $\mathcal{R}$ on $\mathcal{X}$ is said to be \udef{bounded}, if for every $x\in \mathcal{X}$, we have
\[ \sup\setbuilder{\norm{\pi(x)}}{\pi:\mathcal{X}\to A\;\text{is a representation of}\;(\mathcal{X}\;|\;\mathcal{R})} < \infty. \] 
\end{definition}

\begin{example}
\begin{itemize}
\item The relation $(\mathcal{X}\;|\;\mathcal{R}) = (\{a\}\;|\;\{\norm{a-a^*}\leq 0\})$ is not bounded.
\item The relation $(\mathcal{X}\;|\;\mathcal{R}) = (\{x,y\}\;|\;\{\norm{\vec{1}-x^*x-y^*y}\leq 0\})$ is bounded: writing $x$ for $\pi(x)$, the spectrum of $x^*x = 1-y^*y$ is positive and bounded by $1$ according to the spectral mapping theorem \ref{propertiesContinuousFunctionalCalculus}. Now $\norm{\pi(x)} = \sqrt{\spr(\pi(x)^*\pi(x))} \leq 1$ by \ref{normNormalElement} and so
\[ \sup\setbuilder{\norm{\pi(x)}}{\pi:\mathcal{X}\to A\;\text{is a representation of}\;(\mathcal{X}\;|\;\mathcal{R})} \leq 1 < \infty. \]
\item The relation $(\mathcal{X}\;|\;\mathcal{R}) = (\mathcal{X}\;|\;\bigcup_{x\in\mathcal{X}}\{\norm{x-x^*}\leq 0,\norm{x-x^2}\leq 0 \})$ is bounded. It gives rise to a universal $C^*$-algebra generated by projections.
\end{itemize}
\end{example}

\begin{proposition}
Let $\mathcal{X}$ be a non-empty set and $\mathcal{R}$ a polynomial relation on $\mathcal{X}$. Then $(\mathcal{X}\;|\; \mathcal{R})$ is bounded if and only if $C^*(\mathcal{X}\;|\;\mathcal{R})$ exists.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\section{Direct limits}
\subsection{AF}
\subsection{UHF}
\subsection{Stable algebras}
\url{http://web.math.ku.dk/~rordam/manus/encyc.pdf}

\section{Tensor products}
\subsection{Algebraic tensor product}
\subsection{Spatial tensor product}

\chapter{Von Neumann Algebras}
TODO: definitions of SOT and WOT!
\begin{definition}
A concrete $C^*$-algebra $A\subseteq \Bounded(\mathcal{H})$ is a \udef{von Neumann algebra} if it is closed in the SOT.
\end{definition}

\section{von Neumann bicommutant theorem}
\begin{proposition}
Let $S\subset \Bounded(\mathcal{H})$ be a set for some Hilbert space $\mathcal{H}$. Then
\begin{enumerate}
\item $\comm{S}$ is a Banach algebra;
\item $\comm{S}$ is a $C^*$-algebra if $S = S^*$;
\item $\comm{S}\subseteq \Bounded(\mathcal{H})$ is WOT-closed.
\end{enumerate}
\end{proposition}

\chapter{Harmonic analysis}
\section{Functions on locally compact groups}

\begin{proposition}
Let $G$ be a locally compact group. Then any function $f\in \cont_c(G)$ is uniformly continuous.
\end{proposition}

\begin{proposition} \label{integralCompactSupportContinuous}
Let $G$ be a locally compact group, $\mu$ a Borel measure on $G$ and $f\in \cont_c(G)$.
Then the function $g\mapsto \int_G f(xg)\diff{\mu(x)}$ is continuous on $G$.
\end{proposition}

\section{Haar integration}

\begin{lemma}
Let $G$ be a topological group and $\mathcal{B}$ the Borel $\sigma$-algebra of $G$. Then $x\cdot A$ and $A\cdot x$ are measurable for all $A\in \mathcal{B}$ and $x\in G$.
\end{lemma}

\begin{definition}
Let $G$ be a group. A measure $\mu$ on $G$ such that $\mu(x\cdot A) = \mu(A)$ for all measureable sets $A$ and $x\in G$,
is called \udef{left-invariant}.
\end{definition}

\begin{theorem}[Haar measure]
Let $G$ be a locally compact group. There exists a non-zero left-invariant Radon measure $\mu$ $G$.
It is uniquely determined up to scalar multiples.
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{definition}
This measure is called the \udef{Haar measure} of $G$. Let $\mu$ be the Haar measure of $G$. We abbreviate $L^p(G,\mu)$ by $L^p(G)$.
\end{definition}

\begin{lemma}
Let $\mu$ be the Haar measure on a locally compact group $G$. For any non-empty open $U\subseteq G$ and compact $K\subseteq G$ there exists an $n\in \N$ such that $\mu(K)\leq n\mu(U)$.
\end{lemma}
\begin{proof}
Because $\bigcup_{x\in G} x\cdot U = G$, the family $\setbuilder{x\cdot U}{x\in G}$ is an open cover of $G$. For any compact $K\subseteq G$, $\setbuilder{x\cdot U \cap K}{x\in G}$ is an open cover of $K$, which has a finite subcover $\setbuilder{x_i\cdot U \cap K}{x_1,\ldots, x_n \in G}$ by \ref{topologyCompactnessOpenCover}. Then
\[ \mu(K) = \mu\left(\bigcup_{i=1}^n x_i\cdot U \cap K\right) \leq \sum_{i=1}^n \mu(x_i\cdot U \cap K) \leq \sum_{i=1}^n \mu(x_i\cdot U) = \sum_{i=1}^n \mu(U) = n\mu(U). \]
\end{proof}
\begin{corollary} \label{HaarConsequences}
Let $\mu$ be the Haar measure on a locally compact group $G$.
\begin{enumerate}
\item Let $U\subseteq G$ be an open set, then $\mu(U) =0 \iff U = \emptyset$.
\item Let $f: G\to \R^{\geq 0}$ be a positive continuous function, then $\int_G f \diff{\mu} = 0 \iff f= 0$.
\item Every compact set has finite measure.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) If $U = \emptyset$, then $\mu(U) = 0$ by definition. Assume, towards a contradiction, $\mu(U) = 0$ but $U \neq \emptyset$. Then for any compact $K\subseteq G$ we have $\mu(K) \leq n\mu(U) = 0$ by the lemma.

By weak inner regularity all open sets are null sets. By outer regularity all sets are null sets and thus $\mu$ is the zero measure, which is not the Haar measure.

(2) By \ref{functionPropertiesFromIntegral} the open set $f^\preimf\big(\,]0, \infty[\,\big)$ has measure zero and so must be empty by (1).

(3) By local finiteness, there exists an open set $U$ of finite measure. Take $K$ a compact set. Then $\mu(K) \leq n\mu(U)$, which is finite.
\end{proof}

\begin{proposition}
Let G be a locally compact group. The following are equivalent:
\begin{enumerate}
\item there exist $x\in G$ such that $\{x\}$ has non-zero Haar measure;
\item the counting measure is a Haar measure;
\item $G$ is a discrete group.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ Let $\mu$ be a Haar measure and let $\{x\}$ have non-zero Haar measure. Then $\mu(\{x\})^{-1}\mu$ is also a Haar measure. For all $y\in G$ we have
\[ mu(\{x\})^{-1}\mu(\{y\}) = mu(\{x\})^{-1}\mu(xy^{-1}\cdot\{y\}) = mu(\{x\})^{-1}\mu(\{x\}) = 1, \]
so $\mu(\{x\})^{-1}\mu$ is the counting measure by \ref{countingMeasureCriterion}.

$(2) \Rightarrow (3)$ Every point (in particular $1$) has a compact neighbourhood. By \ref{HaarConsequences} this compact neighbourhood has finite measure and thus is a finite set. The neighbourhood contains an open set of finite measure. TODO use Hausdorff!

$(3) \Rightarrow (1)$ If $G$ is discrete, then $\{x\}$ is open and has non-vanishing Haar measure by \ref{HaarConsequences}.
\end{proof}

\begin{proposition}
Let $G$ be a locally compact group. Then $G$ has finite Haar measure \textup{if and only if} $G$ is compact.
\end{proposition}
\begin{proof}
If $G$ is compact, then it has finite Haar measure by \ref{HaarConsequences}.

Now assume $G$ has finite Haar measure. Let $U$ be a compact neighbourhood of $1$. By \ref{HaarConsequences}, $\mu(U) \neq 0$. Consider the set
\[ \setbuilder{x_1\cdot U\cup \ldots \cup x_n\cdot U}{n\in \N, \forall i < j \leq n: x_i\cdot U \perp x_j\cdot U} \]
as a subset of the poset $\sSet{\powerset(G), \subseteq}$. Every chain in this set is finite as $n = \mu(U)^{-1}\mu(x_1\cdot U\cup \ldots \cup x_n\cdot U) \leq \mu(G)$. Take a maximal chain with maximum $K = y_1\cdot U\cup \ldots \cup y_n\cdot U$. Now $K$ is compact (TODO ref) and for all $z\in G$ we have $K\mesh z\cdot K$, because otherwise the chain would not have been maximal. Thus $G = K\cdot K^{-1}$, which is a compact set by (TODO ref).
\end{proof}

\subsection{The modular function}
\begin{definition}
Let $G$ be a locally compact group and $\mu$ a Haar measure on $G$. Then $\mu_x$ defined by
\[ \mu_x(A) \defeq \mu(A \cdot x) \]
is also a Haar measure. There exists a unique function $\Delta: G \to \R^{> 0}$ such that
\[ \mu_x = \Delta(x)\mu. \]
This function is called the \udef{modular function} of $G$.

If $\Delta = \underline{1}$, then $G$ is called \udef{unimodular}.
\end{definition}
The modular function is independent of the choice of Haar measure. Clearly every commutative group is unimodular.

\begin{lemma} \label{rightShiftHaarIntegral}
Let $G$ be a locally compact group, $f\in L^1(G)$ and $y\in G$. Then
\begin{enumerate}
\item $\int_G f(yx)\diff{x} = \int_G f(x)\diff{x}$;
\item $\int_G f(xy)\diff{x} = \Delta(y^{-1})\int_G f(x)\diff{x}$.
\end{enumerate}
\end{lemma}
\begin{proof}
TODO 
\end{proof}

\begin{proposition}
Let $G$ be a locally compact group. The modular function $\Delta: G \to \sSet{\R^{> 0},\cdot}$ is a continuous group homomorphism.
\end{proposition}
\begin{proof}
Let $\mu$ be a Haar measure.

To show $\Delta$ is a group homomorphism, take $x,y\in G$. Let $A$ be a measurable set with finite and non-zero measure. Then
\[ \Delta(xy)\mu(A) = \mu(A \cdot xy) = \mu\big((A\cdot x)\cdot y\big) = \Delta(y)\mu(A\cdot x) = \Delta(y)\Delta(x)\mu(A). \]

For continuity, take some $f\in\cont_c(G)$ with non-zero integral (TODO show existence). Set $c = \int_G f\diff{\mu} \neq 0$. Then by \ref{rightShiftHaarIntegral} we have
\[ \Delta(y) = c^{-1}\int_G f(xy^{-1})\diff{x} = \int_G f\circ \rho_{y^{-1}}(x)\diff{x}, \]
which is continuous by \ref{integralCompactSupportContinuous} because $f\circ \rho_{y^{-1}} \in \cont_c(G)$ (TODO ref).
\end{proof}
\begin{corollary}
Every compact group is unimodular.
\end{corollary}
\begin{proof}
The image $f^{\imf}[G]$ is a compact subgroup of $\sSet{\R^{> 0},\cdot}$ by \ref{compactConstructions}. The only compact subgroup of $\sSet{\R^{> 0},\cdot}$ is $\{1\}$ (TODO ref).
\end{proof}

\subsection{The character group}
\begin{definition}
Let $A$ be a locally compact abelian group. A \udef{character} of $A$ is a continuous group homomorphism $\chi: A\to \T$.

The set of all characters forms a group under pointwise multiplication, called the \udef{character group}. It is denoted $\widehat{A}$.
\end{definition}

\subsection{Convolution}
\begin{definition}
Let $G$ be a locally compact group that is a countable union of compact subsets and $f,g\in L^1(G)$. The \udef{convolution} $f*g$ is defined by
\begin{align*}
(f*g)(x) &= \int_Gf(y)g(y^{-1}x)\diff{\mu(y)} \\
&= \int_Gf(xz)g(z^{-1})\diff{\mu(z)}.
\end{align*}
\end{definition}

\begin{lemma}
Let $G$ be a locally compact group that is a countable union of compact subsets and $f,g\in L^1(G)$.
The convolution $f*g$ is defined a.e., is a function in $L^1(G)$ and satisfies
\[ \norm{f*g}_1 \leq \norm{f}_1\norm{g}_1. \]
\end{lemma}
\begin{proof}
We calculate,
\begin{align*}
\int_G\int_G |f(y)g(y^{-1}x)| \diff{\mu(y)}\diff{\mu(x)} &= \int_G\int_G |f(y)g(y^{-1}x)| \diff{\mu(x)}\diff{\mu(y)} \\
&= \int_G |f(y)| \int_G |g(y^{-1}x)| \diff{\mu(x)}\diff{\mu(y)} \\
&= \int_G |f(y)| \int_G |g(x)| \diff{\mu(x)}\diff{\mu(y)} \\
&= \int_G |f(y)| \;\norm{g}_1\diff{\mu(y)} \\
&= \norm{f}_1\norm{g}_1 <\infty,
\end{align*}
where we have used Tonelli's theorem \ref{tonellisTheorem} and left-translation-invariance. By \ref{functionPropertiesFromIntegral}, we have that $\int_G |f(y)g(y^{-1}x)| \diff{\mu(y)}<\infty$ a.e.\ and thus $(f*g)(x) = \int_G f(y)g(y^{-1}x) \diff{\mu(y)}$ is defined a.e.

Using (TODO ref inequality int abs value), we have
\begin{align*}
\norm{f*g}_1 = \int_G|f*g|\diff{\mu} &= \int_G\left|\int_G f(y)g(y^{-1}x) \diff{\mu(y)}\right|\diff{\mu(x)} \\
&\leq \int_G\int_G |f(y)g(y^{-1}x)| \diff{\mu(y)}\diff{\mu(x)} \\
&= \norm{f}_1\norm{g}_1 <\infty.
\end{align*}
This shows $f*g\in L^1(G)$ and the inequality.
\end{proof}

\begin{proposition}
Let $G$ be a locally compact group that is a countable union of compact subsets. Then $L^1(G)$ is a Banach algebra under the convolution product.
\end{proposition}
\begin{proof}
We need to show associativity and distributivity. TODO.
\end{proof}

\subsubsection{Convolution inequalities}
TODO Minkowski and Young.

\subsection{The Fourier transform}
\begin{definition}
Let $A$ be a locally compact abelian group and $f\in L^1(A)$. The \udef{Fourier transform} of $f$ is the function
\[ \hat{f}: \widehat{A} \to \C: \chi \mapsto \hat{f}(\chi) = \int_A f(x)\overline{\chi(x)}\diff{x}. \]
\end{definition}

\section{Group $C^*$-algebras}
\subsection{Discrete groups}
\begin{definition}
Let $G$ be a finite group and $R$ a r(i)ng. The \udef{group ring} $RG$ is the set of functions $(G\to R)$ with pointwise addition and the convolution product
\[ (x\star y)(g) = \sum_h x(h)y(h^{-1}g) = \sum_{g=hk}x(h)y(k) \]
for all $x,y\in RG$ and $g\in G$. 
\end{definition}
The a group ring can be seen as a free module generated by $G$. (TODO: this as definition?)

The group algebra $\C G$ has an involution:
\[ x^*(g) = \overline{x(g^{-1})} \qquad \text{for all $x\in \C G$}. \]
And it admits a norm making it a $C^*$-algebra.

\subsection{Locally compact Hausdorff groups}
For topological groups we are not restricted to finite sums.

\begin{definition}
Let $G$ be a locally compact group and $f,g\in L^1(G)$. The \udef{convolution product} $f\star g$ of $f$ and $g$ is the partial function defined by
\[ (f\star g)(x) = \int_Gf(y)g(y^{-1}x)\diff{y}, \]
whenever the integral exists.
\end{definition}

\begin{proposition}
Let $G$ be a locally compact group and $f,g\in L^1(G)$. Then convolution makes $L^1(G)$ a Banach-$*$-algebra.
\end{proposition}
We need to show:
\begin{enumerate}
\item $f\star g$ exists a.e.;
\item $\norm{f\star g}_1 = \int_G |f\star g|\diff{\mu}< \infty$;
\item $\norm{f \star g}_1 \leq \norm{f}_1\norm{g}_1$;
\item the convolution is a bilinear and associative.
\end{enumerate}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
The algebra $L^1(G)$ is commutative \textup{if and only if} $G$ is a commutative group.
\end{proposition}

\begin{lemma}
Let $G$ be a locally compact Hausdorff group. Convolution is a bilinear operation that maps $C_c(G)\times C_c(G)\to C_c(G)$ defined by
\[ (f\star g)(t) \defeq \int_Gf(s)g(s^{-1}t)\diff\mu(s). \]
\end{lemma}
\begin{proof}
Continuity follows from the dominated convergence theorem. Also
\[ \operatorname{supp}(f\star g)\subseteq \operatorname{supp}(f)\cdot\operatorname{supp}(g) \]
where $\cdot$ is the group multiplication.
\end{proof}
\begin{lemma}
The algebra $C_c(G)$ has an involutive anti-linear anti-automorphism
\[ *: f \mapsto f^* = (s\mapsto \overline{f(s^{-1})}\Delta(s^{-1})) \]
where $\Delta$ is the modular function on $G$. This means $C_c(G)$ is a $*$-algebra with $*$ as involution.
\end{lemma}


\section{$C^*$-dynamical systems}
\begin{definition}
A \udef{$C^*$-dynamical system} is a triple $(G,\alpha, A)$ consisting of a locally  
compact group $G$, a $C^*$-algebra $A$ and a homomorphism $\alpha$ of $G$ into $\Aut(A)$, such that $g \mapsto a_g(a)$ is continuous for all $a \in A$. 
\end{definition}

\subsection{Covariant homomorphisms and representations}
\begin{definition}
Let $(G,\alpha, A)$ be a $C^*$-dynamical system. A \udef{covariant homomorphism} into the multiplier algebra $M(D)$ of some $C^*$-algebra $D$ is a pair $(\rho, U)$ where
\begin{itemize}
\item $\rho: A\to M(D)$ is a $*$-homomorphism and
\item $U: G\to \Unitaries M(D)$ is a strictly continuous homomorphism between groups
\end{itemize}
satisfying
\[ \rho(\alpha_g(a))= U_g\rho(a)U_{g}^* \qquad \text{for all $g\in G$.} \]
We say $(\rho, U)$ is non-degenerate if $\rho$ is.
\end{definition}

\subsubsection{Integrated forms}
\begin{definition}
Given a covariant homomorphism $(\rho, U)$ on a $C^*$-dynamical system $(G,\alpha, A)$ into $M(D)$ we can parcel these two functions into one function $C_c(G,A)\to M(D)$, called the \udef{integrated form}
\[ (\rho \rtimes U) (f)  \defeq \int_G\rho(f(r))U_r \diff\mu(r) \]
where $\mu$ is the left Haar measure.
\end{definition}
\begin{lemma}
Let $(\rho, U)$ be a covariant homomorphism. Then $\rho \rtimes U$ is a $*$-homomorphism.
\end{lemma}

\subsubsection{Induced covariant morphisms}
Given a $*$-homomorphism $\rho: A\to M(D)$ we can extend it naturally to a covariant homomorphism.
\begin{definition}
Let $(G, \alpha, A)$ be a $C^*$-dynamical system and $\rho: A\to M(D)$ a $*$-homomorphism. Then the \udef{covariant homomorphism induced from $\rho$} $\Ind \rho$ is the covariant homomorphism $(\widetilde{\rho}, 1\otimes \lambda)$ of $(G, \alpha, A)$ into $M(D\otimes\Compact(L^2(G))$ where
\begin{itemize}
\item $\lambda: G\to \Unitaries(L^2(G))$ is the left regular representation of $G$ given by $(\lambda_s\xi)(t) =\xi(s^{-1}t)$;
\item $\rho$ is the composition
\[ \begin{tikzcd}
A \rar{\widetilde{\alpha}} & C_b(G,A) \ar[r,hook] & M(A\otimes C_0(G)) \rar{\rho\otimes M} & M(D\otimes \Compact(L^2(G)))
\end{tikzcd} \]
where $\widetilde{\alpha}: A\to C_b(G,A)$ is defined by $\widetilde{\alpha}(a)(s)= \alpha_{s^{-1}}(a)$ and 
\[ M: C_0(G)\to \Bounded(L^2(G)) = M(\Compact(L^2(G))) \]
denotes the representation by multiplication operators.
\end{itemize}
\end{definition}
The \udef{regular representation} of $(G, \alpha, A)$ is $\Lambda^G_A \defeq \Ind(\id_A)$.

\begin{lemma}
Let $\rho: A\to M(D)$ be a $*$-homomorphism. Then
\[ \Ind\rho = () \]
\end{lemma}

\subsubsection{Covariant representations}
\begin{definition}
A \udef{(covariant) representation} of a $C^*$-dynamical system $(G,\alpha, A)$ on a Hilbert space $\mathcal{H}$ is a covariant homomorphism $(\pi, U)$ into $M(\Compact(\mathcal{H})) = \Bounded(\mathcal{H})$.
\end{definition}

\begin{definition}
Covariant representations $(\pi, U)$ on $\mathcal{H}$ and $(\pi', U')$ on $\mathcal{H}'$ are \udef{unitarily equivalent} if there is a unitary operator $W: \mathcal{H}\to \mathcal{H}'$ such that
\[ \pi'(a) = W\pi(a)W^* \qquad \text{and} \qquad U_g' =  WU_gW^* \]
for all $a\in A,g\in G$.
\end{definition}

Suppose $(\pi,U)$ and $(\rho, V)$ are covariant representations on $\mathcal{H}$ and $\mathcal{V}$ respectively. Their direct sum $(\pi, U) \oplus (\rho, V )$ is the covariant representation $(\pi \oplus \rho, U \oplus V )$ on $\mathcal{H} \oplus \mathcal{V}$ given by $(\pi \oplus \rho)(a) \defeq \pi(a)\oplus \rho(a)$ and $(U \oplus V)_s \defeq U_s \oplus V_s$.
\subsection{Crossed products}
The crossed product $A \rtimes_\alpha G$ will be defined as the completion of $C_c(G,A)$, viewed as a $*$-algebra in a certain way, with respect to a certain norm.

First the algebra: the set of functions $G\to A$ with compact support naturally comes equipped with scalar multiplication and vectorial addition. We define the multiplication as
\[ f\star g: G \to \C: x\mapsto \int_G f(s)\alpha_s(g(s^{-1}x))\diff\mu(s) \]
and the involution $*$ by
\[ f^*: x\mapsto \Delta(x^{-1})\alpha_x(f(x^{-1})^*). \]
Notice the appearance of $\alpha$ in the definitions.

Next we define a norm. This will be done using integrated forms.
Let $(\pi, U)$ be a covariant representation of a $C^*$-dynamical system $(A,G,\alpha)$ on $\mathcal{H}$. Then
\[ (\pi \rtimes U) (f)  \defeq \int_G\pi(f(r))U_r \diff\mu(r)\]
defines a $*$-representation of the $*$-algebra $C_c(G,A)$ on $\mathcal{H}$, called the \udef{integrated form}.

Then we can define a norm, called the \udef{universal norm}, on $C_c(G,A)$ by
\[ \norm{f} \defeq \sup\setbuilder{\norm{(\pi\rtimes U)(f)}}{(\pi, U)\;\text{is a covariant representation of} \; (A,G,\alpha)} \]
The supremum\footnote{One may worry we are taking the supremum over a class and not a set (the covariant representations do not form a set). Luckily the class is a subclass of the real numbers and thus a set.} is finite because $\norm{f} \leq \norm{f}_1$.

The completion of $C_c(G,A)$ with respect to the universal norm is called the \udef{crossed product} $A \rtimes_\alpha G$.

In fact, when evaluating the supremum for the universal norm, we do not need to consider all representations of $(A,G,\alpha)$: Let $(\pi, U)$ be a covariant representation of $(A,G,\alpha)$ on $\mathcal{H}$. Let 
\[ \mathcal{E} \defeq \overline{\Span}\setbuilder{\pi(a)h}{a\in A; h\in \mathcal{H}} \]
be the \udef{essential subspace} of $\pi$. We call the corresponding subrepresentation $\operatorname{ess} \pi$. Because
\[ U_s h =  \pi(\alpha_{s^{-1}}(a))U_s\pi(a) h \qquad \forall a\in A, h\in\mathcal{H}, \]
it is clear $\mathcal{E}$ is invariant under $U$ as well. Call $U'$ the restriction of $U$ to $\mathcal{E}$.

Then $\norm{(\operatorname{ess}\pi\rtimes U')(f)} = \norm{(\pi\rtimes U)(f)}$ and so
\[ \norm{f} =\sup\setbuilder{\norm{(\pi\rtimes U)(f)}}{(\pi, U)\;\text{is a non-degenerate covariant representation of} \; (A,G,\alpha)} \]

\begin{proposition}
If $(A,G,\alpha)$ is a dynamical system, then the map sending
a covariant pair $(\pi, U)$ to its integrated form $\pi\rtimes U$ is a one-to-one correspondence
between non-degenerate covariant representations of $(A, G, \alpha)$ and non-degenerate
representations of $A\rtimes_\alpha G$. This correspondence preserves direct sums, irreducibility
and equivalence.
\end{proposition}

\subsubsection{Universal property}

In general the crossed product $A\rtimes_\alpha G$ does not contain a copy of either $A$ or $G$. The multiplier algebra $M(A\rtimes_\alpha G)$ does however: There exist injective homomorphisms
\[ i_A: A\to M(A\rtimes_\alpha G) \qquad i_G: G\to \mathcal{U}M(A\rtimes_\alpha G) \]
satisfying
\begin{enumerate}
\item $i_A(\alpha_r(a)) = i_G(r)i_A(a)i_G(r)^*$ for all $a\in A, r\in G$;
\item $A \rtimes_\alpha G = \overline{\Span}\setbuilder{i_A(a)\int_Gf(s)i_G(s)\diff\mu(s)}{a\in A, f\in C_c(G)}$;
\item if $(\pi, U)$ is a covariant representation of $(G,A,\alpha)$, then
\[ \pi = (\pi\rtimes U)\circ i_A \qquad \text{and} \qquad U = (\pi \rtimes U)\circ i_G. \]
\end{enumerate}
This is a universal property. Suppose another $C^*$-algebra $B$ and maps
\[ j_A: A\to M(B) \qquad \text{and} \qquad j_G: G\to \mathcal{U}M(B) \]
satisfy these conditions, then there exists an isomorphism $\Psi: A\rtimes_\alpha G \to B$ such that
\[ \Psi \circ i_A = j_A \qquad \text{and} \qquad \Psi\circ i_G = j_G. \]

The existence of such homomorphisms $i_A,i_G$ is proved by explicitly giving them.
They are defined by
\begin{align*}
(i_A(a)f)(t) &= af(t) & (i_G(s)f)(t) &= \alpha_s(f(s^{-1}t) \\
(fi_A(a))(t) &= f(t)\alpha_t(a) & (fi_G(s))(t) &= f(t^{-1}s)\Delta(s^{-1})
\end{align*}
for all $f\in C_c(G,A), a\in A, t\in G$.

We can use the existence of these embeddings to prove the proposition. We need to show the existence of an inverse of the map from non-degenerate covariant representations to integrated forms.

Let $\omega$ be a non-degenerate covariant representation of $A\rtimes_\alpha G$. Because it is non-degenerate, we can extend it uniquely to a representation of $M(A\rtimes_\alpha G)$. Using $i_A, i_G$ we can restrict this representation to a representation of $A$ and $G$. Together they form a covariant representation and one can check it is exactly the original representation $\omega$.

\subsubsection{Induced representations}