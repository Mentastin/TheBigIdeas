\chapter{Inner product spaces}

\begin{definition}
An \udef{inner product} on a vector space $V$ is a function
\[ \inner{\cdot,\cdot}: V\times V \to \mathbb{F}  \]
that has the following properties:
\begin{itemize}[leftmargin=4.5cm]
\item[\textbf{Linearity}] in the \emph{second}\footnote{Some authors take linearity in the first component.} component
\[\inner{v,\lambda_1 w_1 + \lambda_2 w_2} = \lambda_1\inner{v,w_1} + \lambda_2\inner{v,w_2},\]
where $\lambda_1,\lambda_2 \in \mathbb{F}$ and $v,w_1,w_2\in V$.
\item[\textbf{Conjugate symmetry}\footnote{This is for $\mathbb{F} = \C$. For $\mathbb{F} = \R$ this reduces to normal symmetry $\inner{v,w} = \inner{w,v}$.}] $\inner{v,w} = \overline{\inner{w,v}}$ for all $v,w\in V$.
\item[\textbf{Positivity}\footnote{By conjugate symmetry we know that $\inner{v,v}$ is a real number, so this condition makes sense.}] $\inner{v,v} \geq 0$ for all $v\in V$.
\item[\textbf{Definiteness}]$\inner{v,v} = 0$ if and only if $v= 0$.
\end{itemize}
An \udef{inner product space} or \udef{pre-Hilbert space} $(\mathbb{F}, V,+,\inner{\cdot,\cdot})$ is a vector space $(\mathbb{F}, V,+)$ together with an inner product $\inner{\cdot,\cdot}$ on $V$.

\begin{itemize}
\item A real finite-dimensional inner product space is called a \udef{Euclidean space}.
\item A \udef{Hilbert space} is an inner product space that is complete as a metric space.
\end{itemize}
\end{definition}

A finite-dimensional inner product space is automatically a Hilbert space by proposition \ref{finiteDimComplete}.


\section{Inner product spaces}
\begin{lemma}
An inner product over a complex vector space $V$ is anti-linear in the first component.
\end{lemma}

\begin{lemma} \label{nonDegeneracyInnerProduct}
Definiteness implies the inner product on $V$ is non-degenerate:
\[ [\forall u\in V:\inner{u,v} = 0] \implies v = 0. \]
\end{lemma}
The converse is not true.

There are some generalised notions of inner product:
\begin{definition}
Let $V$ be a complex vector space.
\begin{enumerate}
\item A \udef{sesquilinear form} is a function $V\times V\to \C$ that is linear in the second component and anti-linear in the first.
\item A \udef{Hermitian form} is a conjugate symmetric sesquilinear form.
\item A \udef{pre-inner product} is a positive Hermitian form, i.e.\ an inner product without the requirement of definiteness.
\end{enumerate}
\end{definition}

\begin{example}
\begin{enumerate}
\item The \udef{standard inner product} on $\R^n$ is given by
\[ \inner{a,b} = \inner{\begin{bmatrix}
a_1 \\ \vdots \\ a_n
\end{bmatrix},\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix}} = \begin{bmatrix}
a_1 & \hdots & a_n
\end{bmatrix}\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix} = a^\transp b \]
This is also known as the \udef{dot product} $a\cdot b$.
\item The \udef{standard inner product} on $\C^n$ is given by
\[ \inner{a,b} = \inner{\begin{bmatrix}
a_1 \\ \vdots \\ a_n
\end{bmatrix},\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix}} = \begin{bmatrix}
\bar{a}_1 & \hdots & \bar{a}_n
\end{bmatrix}\begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix} = \bar{a}^\transp b \]
\item The \udef{Frobenius inner product} on $\C^{m\times n}$ is given by
\[ \inner{A,B}_F =  \Tr(\overline{A}^\transp B) = \overline{\vectorisation_C(A)}^\transp \vectorisation_C(B)\]
\item On the vector space $\mathcal{C}[a,b]$ of continuous real functions on $[a,b]$, we can take the inner product
\[ \inner{f,g} = \int_a^b f(x)\cdot g(x) \diff{x}. \]
\end{enumerate}
\end{example}

\begin{definition}
Two vectors $u,v \in V$ are \udef{orthogonal} if $\inner{u,v} =0$. This is denoted $u\perp v$.
\end{definition}
\begin{lemma} \label{elementaryOrthogonality}
Let $V$ be an inner product space.
\begin{enumerate}
\item $0$ is the only vector orthogonal to itself.
\item $0$ is orthogonal to all $v\in V$;
\item Let $x,y\in V$. If, for all $v\in V$, $\inner{v,x} = \inner{v,y}$, then $x=y$.
\end{enumerate}\end{lemma}
\begin{proof}
The first is a consequence of definiteness, the second a consequence of linearity: $\inner{v,0} = \inner{v,0\cdot0} = 0\inner{v,0} = 0$.

The third is also a consequence of linearity: assume $\forall v\in V: \inner{v,x} = \inner{v,y}$, then $\inner{v,x-y}=0$ and $x-y$ is orthogonal to all $v\in V$ and in particular to $0$. Thus $x-y$ must be zero.
\end{proof}

\begin{proposition}
Every inner product gives rise to a norm, defined by
\[ \norm{\cdot} = \sqrt{\inner{\cdot,\cdot}}. \]
\end{proposition}
\begin{proof}
The only non-trivial part is the triangle inequality. This will be proved later using the Cauchy-Schwarz inequality.
\end{proof}


\begin{lemma}
Let $V$ be an inner product space. Then
\[ \norm{v+w}^2 = \norm{v}^2+\norm{w}^2+2\Re\inner{v,w} \]
\end{lemma}
\begin{lemma} \label{orthogonalDecomposition}
Let $V$ be a pre-inner product space and $v,w\in V$, with $w\neq 0$. We can decompose $v$ as a multiple of $w$ and a vector $u$ orthogonal to $w$:
\[ v = cw+u = \left(\frac{\inner{v,w}}{\norm{w}^2}\right)w + \left( v- \frac{\inner{w, v}w}{\norm{w}^2} \right). \]
\end{lemma}
\begin{proof}
The only thing to check is $\inner{w, v- \frac{\inner{w,v}w}{\norm{w}^2}} = 0$, which is immediate:
\[ \inner{w, v- \frac{\inner{w,v}w}{\norm{w}^2}} = \inner{w,v} - \frac{\inner{w,v}}{\norm{w}^2}\inner{w,w} = \inner{w,v} - \inner{w,v} = 0. \]
\end{proof}

\subsection{Pythagoras and Cauchy-Schwarz}
\begin{theorem}[Pythagorean theorem] \label{Pythagoras}
Suppose $u\perp v$. Then $\norm{u+v}^2 = \norm{u}^2 + \norm{v}^2$.
\end{theorem}
\begin{proof}
\[ \norm{u+v}^2 = \inner{u+v,u+v} = \inner{u,u}+ \inner{u,v} + \inner{v,u} + \inner{v,v} = \norm{u}^2 + \norm{v}^2. \]
\end{proof}

\begin{theorem}[Cauchy-Schwarz-Bunyakovsky inequality.] \label{CauchySchwarz}
Let $V$ be a vector space with a pre-inner product $\inner{\cdot,\cdot}$. Let $v,w\in V$. Then
\[ |\inner{v,w}|^2\leq \inner{v,v}\cdot\inner{w,w}. \]
Suppose $\inner{\cdot,\cdot}$ is definite (i.e.\ an inner product), then
this is an equality \textup{if and only if} $v$ and $w$ are scalar multiples.
\end{theorem}
This result is also known as the Cauchy-Schwarz inequality, or the CSB inequality.
\begin{proof}
Take the decomposition from lemma \ref{orthogonalDecomposition} and apply the Pythagorean theorem to obtain
\[ \norm{v}^2 = \frac{|\inner{v,w}|^2}{\norm{w}^4}\norm{w}^2 + \norm{u}^2 \geq \frac{|\inner{v,w}|^2}{\norm{w}^2}. \]
We have equality if and only if $\norm{u} = 0$. If $V$ is an inner product space, then this implies $u = 0$, so $v = \frac{\inner{w,v}}{\norm{w}^2}w$ and thus $v$ is a scalar multiple of $w$.
\end{proof}
The following is an alternate proof that does not show the claim about scalar multiples.
\begin{proof}[Alternate proof]
Consider 
\[ \inner{v-\lambda w, v-\lambda w} = \inner{v,v}-\lambda\inner{v,w}-\overline{\lambda}\inner{w,v} + |\lambda|^2 \inner{w,w} \geq 0. \]
Suppose $\inner{v,w}=re^{i\theta}$ (if $\mathbb{F} = \R$, then $\theta=0$ or $\theta = \pi$). The inequality must still hold for all $\lambda$ of the form $te^{-i\theta}$ for some $t\in \R$. The inequality thus becomes
\[ 0\leq \inner{v,v}-te^{-i\theta}re^{i\theta}-te^{i\theta}re^{-i\theta} + t^2 \inner{w,w} = \inner{v,v}-2rt + t^2 \inner{w,w}. \]
On the right we have a quadratic formula in $t$. This may never be negative and the discriminant may therefore not be positive. Calculating the discriminant gives $(2r)^2 - 4\inner{v,v}\inner{w,w}$. Thus
\[ 0\geq r^2 - \inner{v,v}\inner{w,w} = |\inner{v,w}|^2 - \inner{v,v}\inner{w,w}. \]
\end{proof}
\begin{corollary} \label{normVectorSupUnitvectors}
Let $V$ be an inner product space and $v\in V$. Then
\begin{align*}
\norm{v} &= \sup_{\norm{w} = 1}|\inner{v,w}| \\
&= \sup_{\norm{w} \leq 1}|\inner{v,w}|.
\end{align*}
\end{corollary}
\begin{proof}
We prove 
\[ \norm{v} \leq \sup_{\norm{w} = 1}|\inner{v,w}| \leq \sup_{\norm{w} \leq 1}|\inner{v,w}| \leq \norm{v}. \]
If $v = 0$, then the equalities hold. If $v\neq 0$, then $\norm{v} \neq 0$ by definiteness.

For the first inequality, take $w = \frac{v}{\norm{v}}$. Then
\[ |\inner{v,w}| = |\inner{v,\frac{v}{\norm{v}}}| = \frac{|\inner{v,v}|}{\norm{v}^2} = \frac{\norm{v}^2}{\norm{v}} = \norm{v}. \]

The second inequality is straightforward.

For the third inequality, take some vector $w$ with $\norm{w}\leq 1$. Then the CSB inequality gives
\[ |\inner{v,w}|^2 \leq \norm{v}^2\norm{w}^2 \leq \norm{v}^2, \]
so $|\inner{v,w}| \leq \norm{v}$.
\end{proof}
\begin{corollary} \label{innerBoundedFunctionals}
Let $V$ be an inner product space. The functions
\[\inner{v,\cdot}: V\to \mathbb{F}: x\mapsto \inner{v,x} \]
are bounded linear functionals for all $v\in V$.
\end{corollary}
\begin{corollary} \label{preInnerProductCSBZero}
Let $V$ be a vector space with a pre-inner product $\inner{\cdot,\cdot}$. Then
\[ \inner{x,x}=0\lor\inner{y,y}=0 \quad\implies\quad \inner{x,y} = 0. \]
\end{corollary}
\begin{definition}
The Cauchy-Schwarz inequality allows us to define the \udef{angle} $\theta$ between two vectors $v,w$ by
\[ \cos\theta = \frac{\inner{v,w}}{\norm{v}\norm{w}}.\]
\end{definition}
\begin{lemma}
If $v\perp w$, then the angle between them is $\pi/2 + k\pi$.
\end{lemma}

TODO CS special case of HÃ¶lder inequality.

\begin{theorem}[Triangle inequality]
Let $v,w\in V$. Then
\[ \norm{v+w} \leq \norm{v}+\norm{w} \]
This inequality is an equality if and only if one of $u,v$ is a nonnegative multiple of the other. Also
\begin{enumerate}
\item $\big|\norm{v}-\norm{w}\big|\leq \norm{v-w}$;
\item $\big|\norm{v}-\norm{w}\big| \leq \norm{v+w} \leq \norm{v}+\norm{w}$.
\end{enumerate}
\end{theorem}
\begin{proof}
We calculate
\begin{align*}
\norm{v+w}^2 &= \norm{v}^2+\norm{w}^2+2\Re\inner{v,w} \\
&\leq \norm{v}^2+\norm{w}^2+2|\inner{v,w}| \\
&\leq \norm{v}^2+\norm{w}^2+2\norm{v}\norm{w} \\
&= (\norm{v}+\norm{w})^2.
\end{align*}
The other inequalities are the reverse triangle inequalities \ref{reverseTriangleInequality}.
\end{proof}

\subsection{Parallelogram law and polarisation}
\begin{theorem}[Parallelogram law] \label{parallelogramLaw}
Let $V$ be an inner product space and $v,w\in V$. Then
\[ \norm{v+w}^2 + \norm{v-w}^2 = 2 (\norm{v}^2+\norm{w}^2). \]
\end{theorem}
\begin{proof}
We calculate
\begin{align*}
\norm{v+w}^2 + \norm{v-w}^2 = \inner{v+w, v+w}+\inner{v-w,v-w} = 2(\norm{v}^2 + \norm{w}^2).
\end{align*}
\end{proof}
\begin{corollary}[Appolonius' identity] \label{AppoloniusIdentity}
Let $V$ be an inner product space and $x,y,z\in V$. Then
\[ \norm{z-x}^2 + \norm{z-y}^2 = \frac{1}{2}\norm{x-y}^2 + 2\norm*{z-\frac{1}{2}(x+y)}^2. \]
\end{corollary}
\begin{proof}
Apply the parallelogram law to $u = \frac{1}{2}(z-x)$ and $v = \frac{1}{2}(z-y)$.
\end{proof}

\begin{proposition}[Ptolemy's inequality] \label{PtolemyInequality}
Let $V$ be an inner product space. Then the norm satisfies $\forall u,v,w\in V$
\[ \norm{u-v}\;\norm{w} + \norm{v-w}\;\norm{u} \geq \norm{u-w}\;\norm{v}. \]
\end{proposition}

Polarisation identities allow us to recover the inner product from the norm.
\begin{theorem}[Polarisation identities] \label{polarisationIdentities}
\mbox{}
\begin{enumerate}
\item For real inner product spaces, $\mathbb{F} = \R$:
\begin{align*}
\inner{v,w} &= \frac{1}{2}(\norm{v+w}^2 - \norm{v}^2-\norm{w}^2) \\
&= \frac{1}{2}(\norm{v}^2 + \norm{w}^2-\norm{v-w}^2) \\
&= \frac{1}{4}(\norm{v+w}^2 - \norm{v-w}^2) = \frac{1}{4}\sum_{k=0}^1 (-1)^k\norm{v+(-1)^k w}^2.
\end{align*}
\item For complex inner product spaces, $\mathbb{F} = \C$:
\[ \inner{x,y} = \frac{1}{4}\sum_{k=0}^3 i^k\norm{i^k x+y}^2. \]
\item For sesquilinear forms, $\mathbb{F} = \C$:
\[ S(x,y) = \frac{1}{4}\sum_{k=0}^3 i^k S(i^k x+y, i^k x+y). \]
\end{enumerate}
\end{theorem}
\begin{proof}
We prove (3):
\begin{align*}
\sum_{k=0}^3 i^k S(i^k x+y, i^k x+y) &= \sum_{k=0}^3 i^ki^k(-i)^k S(x, x) + i^k(-i)^kS(x,y) + i^ki^kS(y,x) + i^kS(y,y) \\
&= \sum_{k=0}^3 i^k S(x, x) + S(x,y) + (-1)^kS(y,x) + i^kS(y,y) \\
&= \sum_{k=0}^3 S(x,y) \\
&= 4 S(x,y),
\end{align*}
where we have used that $\sum_{k=0}^3 i^k = 0$ and $\sum_{k=0}^3 (-1)^k = 0$.
\end{proof}

\begin{proposition} \label{HermitianRealQuadratic}
Let $V$ be a \emph{complex} vector space and $S: V\times V\to \C$ a sesquilinear form. Then
\begin{enumerate}
\item $S$ is Hermitian \textup{if and only if} $S(v,v)$ is real for all $v\in V$;
\item $S = \constant{0}$ \textup{if and only if} $S(v,v = 0)$ for all $v\in V$.
\end{enumerate}
In particular, all positive sesquilinear forms are pre-inner products.
\end{proposition}
Note that we do not have analogous results in real vector spaces.
\begin{proof}
(1) The direction $\boxed{\Rightarrow}$ is immediate: conjugate symmetry gives $S(v,v) = \overline{S(v,v)}$.

For the other direction, assume $S(v,v)$ is real for all $v\in V$ and consider arbitrary $u,v\in V$. Then
\[ \begin{cases}
S(u+iv, u+iv) = S(u,u) + S(v,v) + i\Big(S(u,v) - S(v,u)\Big) \\
S(u+v, u+v) = S(u,u) + S(v,v) + \Big(S(u,v) + S(v,u)\Big).
\end{cases} \]
Taking the imaginary part gives
\[ \begin{cases}
0 = \Im S(u+iv, u+iv) = \cancel{\Im S(u,u)} + \cancel{\Im S(v,v)} + \Re\Big(S(u,v) - S(v,u)\Big) \\
0 = \Im S(u+v, u+v) = \cancel{\Im S(u,u)} + \cancel{\Im S(v,v)} + \Im\Big(S(u,v) - S(v,u)\Big).
\end{cases} \]
Thus $\Re S(u,v) = \Re S(v,u)$ and $\Im S(u,v) = - \Im S(v,u)$, which means $S(u, v) = \overline{S(v,u)}$.

(2) The direction $\Rightarrow$ is clear. The converse follows from the polarisation identity \ref{polarisationIdentities}. Take $u,v\in V$. Then
\[ S(u,v) = \frac{1}{4}\sum_{k=0}^3 i^k S(i^k u+v, i^k u+v) = 0. \]
\end{proof}
\begin{proof}[Alternate proof of (1)]
We can also prove the direction $\Leftarrow$ by a direct calculation using the polarisation identity \ref{polarisationIdentities}:
\begin{align*}
\overline{S(u,v)} &= \frac{1}{4}\sum^3_{k=0}(-i)^kS(u+i^kv,u+i^kv) & &\text{Using the fact that $S(u+i^kv,u+i^kv)$ is real} \\
&= \frac{1}{4}\sum^3_{k=0}(-i)^kS\Big((i^k)(v+(-i)^ku),(i^k)(v+(-i)^ku)\Big) & &\text{Using $i^k(-i)^k=1$}\\
&= \frac{1}{4}\sum^3_{k=0}(-i)^k\cancel{(i^k)}\cancel{(-i^k)}S(v+(-i)^ku,v+(-i)^ku) & &\text{Using (conjugate) linearity}\\
&= \frac{1}{4}\sum^3_{k=0}i^kS(v+i^ku,v+i^ku) & &\text{Substituting $k\to k+2$}\\
&= S(v,u).
\end{align*}
\end{proof}
Not all norms on vector spaces can be obtained from an inner product. If a norm can be obtained from an inner product, we can use polarisation to recover the inner product. If a norm cannot be obtained from an inner product, the putative inner product suggested by polarisation will turn out not to be an inner product.
\begin{theorem}[Jordan-von Neumann] \label{JordanVonNeumann}
Let $\sSet{V,\norm{\cdot}}$ be a real or complex normed space. The following are equivalent:
\begin{enumerate}
\item the polarisation yields an inner product;
\item the parallelogram law holds;
\item Appolonius' identity holds;
\item Ptolemy's inequality holds.
\end{enumerate}
\end{theorem}
TODO! (For other fields??)
\begin{proof}
If polarisation yields an inner product, then we have an inner product space and thus the parallelogram law and Ptolemy's inequality hold by \ref{parallelogramLaw} and \ref{PtolemyInequality}.

The polarisation identities immediately imply
\begin{itemize}
\item Conjugate symmetry:
\[ \inner{x,y} = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ y}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{x+ i^{-k}y}^2 = \frac{1}{4}\sum_{k'=0}^3\overline{i^{k'}}\norm{i^{k'}y+ x}^2 = \overline{\inner{y,x}}. \]
\item Positivity and definiteness:
\[ \inner{x,x} = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ x}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{(i^k+1)x}^2 = \frac{\norm{x}^2}{4}\sum_{k=0}^3 i^k\cdot |1+i^k|^2 = \norm{x}^2 \]
\end{itemize}
Now assume Appolonius' identity, \ref{AppoloniusIdentity}, holds. We need to show linearity in second component.
We can calculate
\begin{align*}
\inner{x,y_1} + \inner{x,y_2} &= \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ y_1}^2 + \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x+ y_2}^2 = \frac{1}{4}\sum_{k=0}^3i^k\Big(\norm{i^k x+ y_1}^2\frac{1}{4} + \norm{i^k x+ y_2}^2\Big) \\
&= \frac{1}{4}\sum_{k=0}^3i^k\left(\frac{1}{2}\norm{y_1-y_2}^2 + 2\norm{i^k+\frac{y_1+y_2}{2}}\right) \\
&= 2\frac{1}{4}\sum_{k=0}^3i^k\left(\norm{i^k+\frac{y_1+y_2}{2}}\right) \\
&= 2\inner{x,\frac{y_1+ y_2}{2}}.
\end{align*}
Setting $y_2 = 0$ and $y_1 = 2y$, this yields $\inner{x,2y} = 2\inner{x,y}$, which also means that
\[ \inner{x,y_1+y_2} = 2\inner{x,\frac{y_1+ y_2}{2}} = \inner{x,y_1} + \inner{x,y_2}. \]
By induction, we can prove that this putative inner product is linear for all positive rational scalars. By continuity this result extends to all positive scalars.

Finally we check
\begin{align*}
\inner{x,-y} &= \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x - y}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^{k-2} x + y}^2 = -\frac{1}{4}\sum_{k'=0}^3i^{k'}\norm{i^{k'} x + y}^2 = -\inner{x,y} \\
\inner{x,iy} &= \frac{1}{4}\sum_{k=0}^3i^k\norm{i^k x + iy}^2 = \frac{1}{4}\sum_{k=0}^3i^k\norm{i^{k-1} x + y}^2 = i\frac{1}{4}\sum_{k'=0}^3i^{k'}\norm{i^{k'} x + y}^2 = i\inner{x,y}.
\end{align*}
TODO Ptolemy inequality.
\end{proof}
\begin{corollary}
The space $l^p$ is an inner product space \textup{if and only if} $p=2$.
\end{corollary}
\begin{proof}
The inner product on $l^2$ is defined by $\inner{x_n, y_n} = \sum_{n=1}^\infty \overline{x_n}y_n$.

If $p\neq 2$ we can find a counterexample to the parallelogram law: let $x=(1,1,0,0,\ldots)\in l^p$ and $y = (1,-1,0,0,\ldots)\in l^p$. Then
\[ \norm{x}_p = \norm{y}_p = 2^{1/p} \qquad \text{and} \qquad \norm{x+y} = \norm{x-y} = 2 \]
and the parallelogram law is then not valid if $p\neq 2$.
\end{proof}

\subsection{Topology of inner product spaces}
\begin{lemma}[Continuity of inner product]
Let $V$ be an inner product space. Then the inner product is a continuous function $V\times V \to \mathbb{F}$.
\end{lemma}
\begin{proof}
We show that if $x_n \to x$ and $y_n \to y$, then $\inner{x_n,y_n}\to \inner{x,y}$. By the triangle and Cauchy-Schwarz inequalities
\begin{align*}
|\inner{x_n,y_n}-\inner{x,y}| &= |\inner{x_n,y_n}-\inner{x_n,y}+\inner{x_n,y} - \inner{x,y}| \\
&\leq |\inner{x_n, y_n-y}| + |\inner{x_n-x, y}| \\
&\leq \norm{x_n}\norm{y_n-y} + \norm{x_n-x}\norm{y}.
\end{align*}
Because the right-hand side converges to $0$, the left-hand side must too.
\end{proof}

\subsubsection{Completion}
\begin{lemma}
Let $V$ be an inner product space. The inner product on $V$ can be extended to its completion by continuity. The completion is a Hilbert space.
\end{lemma}
\begin{proof}
\ref{uniformlyContinuousExtensionToCompletion}

The completion of $V$ exists by \ref{existenceMetricCompletion} and the 
\end{proof}

\section{Orthogonal and orthonormal sets of vectors}
\subsection{Orthogonal complements}
\begin{definition}
Let $A$ be a subset of an inner product space $V$. The \udef{orthogonal complement} $A^\perp$ of $A$ is the set of vectors in $V$ that are orthogonal to every vector in $A$:
\[ A^\perp = \{ v\in V\;|\; \inner{v,a}=0\; \forall a\in A \}. \]
\end{definition}

\begin{proposition} \label{OrthogonalComplementProperties}
Let $A,B$ be \emph{subsets} of an inner product space $V$.
\begin{enumerate}
\item $A^\perp$ is a subspace of $V$;
\item $A^\perp = \Span(A)^\perp$;
\item $\{0\}^\perp = V$;
\item $V^\perp = \{0\}$;
\item $A\cap A^\perp \subset \{0\}$;
\item If $A\subset B$, then $B^\perp \subset A^\perp$.
\end{enumerate}
\end{proposition}

We can also consider the orthogonal complement of a subspace with respect to another subspace, not the full space.
\begin{definition}
Let $A\subseteq B$ be subsets of an inner product space $V$. The \udef{orthogonal complement} of $A$ with respect to $B$ is the set of vectors in $B$ that are orthogonal to every vector in $A$:
\[ B\ominus A = \{ b\in B\;|\; \inner{b,a}=0\; \forall a\in A \}. \]
\end{definition}

\begin{lemma} \label{ominusSubspace}
Let $V$ be an inner product space and $A\subseteq B \subseteq V$ subsets. Then $B\ominus A = B\cap A^\perp$.
\end{lemma}
\begin{proof}
Take $v\in B\ominus A$. This is equivalent to $v\in B \land \forall u\in A: \inner{v,u} =0$ and thus equivalent to $v\in B \land v\in A^\perp$.
\end{proof}

\begin{proposition} \label{perpUnderIsometry}
Let $V$ be an inner product space, let $A\subseteq B\subseteq V$ be subsets and $T:V\to V$ an isometry. Then
\begin{enumerate}
\item if $A\perp B$, then $T[A]\perp T[B]$;
\item $T[B\ominus A] = T[B]\ominus T[A]$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) If $\inner{a,b}=0$ for all $a\in A, b\in B$, then $\inner{T(a), T(b)} =0$, meaning $T[A]\perp T[B]$.

(2) Take $v\in T[B\ominus A]$. This is equivalent to the existence of $x\in B$ such that $T(x) = v$ and $\inner{x,y}=0$ for all $y\in A$. By isometry $\inner{x,y}=0 \iff \inner{T(x), T(y)}=0$ for all $y\in A$. So, equivalently, $v\in T[B]\ominus T[A]$. 
\end{proof}

\begin{proposition} \label{orthogonalComplementClosed}
Let $A$ be a \emph{subset} of an inner product space $V$. Then $A^\perp$ is closed and $\overline{A}^\perp = A^\perp$. This can be rephrased as
\[ \overline{A}^\perp = \overline{A^\perp} = A^\perp. \]
Also
\[\overline{A} \subset (A^\perp)^\perp. \]
\end{proposition}
\begin{proof}
Let $x\in \overline{A^\perp}$. Then there exists a sequence $(x_i)$ in $A^\perp$ that converges to $x$. For all $a\in A$, the functional $\inner{a,\cdot}:y\mapsto \inner{a,y}$ is bounded (by Cauchy-Schwarz). Thus all these functionals are continuous. Applying any one to the sequence $x_i$ gives a sequence of zeros. Thus $\inner{a,x} = 0$ for all $a\in A$. Thus $x\in A^\perp$ and hence $A^\perp \supset \overline{A^\perp}$ meaning $A^\perp$ is closed.

Now $\overline{A}\supset A$, so $\overline{A}^\perp \subset A^\perp$. For the other inclusion, take an $x\in A^\perp$. Take an arbitrary $y\in \overline{A}$. Then there exists a sequence $(y_i)$ in $A$ that converges to $y$. Apply the bounded functional $\inner{x,\cdot}$ to the sequence $(y_i)$, yielding a sequence of zeros. Thus $\inner{x,y}=0$. Thus $x\in \overline{U}^\perp$.

Finally let $a\in \overline{A}$. Take a sequence $a_i\to a$. Take an arbitrary element $x\in A^\perp$. As before $\inner{x,a} = \lim_i\inner{x,a_i} = 0$. So $a\in (A^\perp)^\perp$.
\end{proof}
\begin{corollary} \label{orthogonalComplementDenseSpace}
Let $A$ be a subset of $V$. If $\Span(A)$ is dense in $V$, then $A^\perp = \{0\}$. 
\end{corollary}
\begin{proof}
\[ A^\perp = \Span(A)^\perp = \overline{\Span(A)}^\perp = V^\perp = \{0\}. \]
\end{proof}
\begin{corollary} \label{perpToDenseSet}
Let $x\in V$. If there exists a dense set $S$ such that $x\perp y$ for all $y\in S$, then $x=0$.
\end{corollary}
\begin{proof}
If such an $S$ exists, then $x\in S^\perp = \overline{S}^\perp = V^\perp = \{0\}$.
\end{proof}
\begin{proposition}
Let $U$ be a finite-dimensional subspace of an inner product space $V$.
\begin{enumerate}
\item $V=U\oplus U^\perp$;
\item $U = (U^\perp)^\perp$.
\end{enumerate}
\end{proposition}
Notice that $V$ may be infinite dimensional!
\begin{proof}
We start with the first point. The sum $U + U^\perp$ is definitely direct, $U\oplus U^\perp$, by proposition \ref{OrthogonalComplementProperties} and the criterion for a direct sum, proposition \ref{directSumCriterion}. Clearly $U\oplus U^\perp\subseteq V$, so we just need to show that $V \subseteq U\oplus U^\perp$.

To that end, take a vector $v\in V$. Let $\{e_i\}_{i=1}^n$ be an orthonormal basis of $U$. We can write
\[ v = \left(v - \sum_{i=1}^n\inner{v,e_i}e_i\right) + \left(\sum_{i=1}^n\inner{v,e_i}e_i\right). \]
The first part is an element of $U^\perp$, the second of $U$, so $v\in U\oplus U^\perp$.

For the second point: any finite-dimensional subspace $U$ is automatically closed, so $U = \overline{U} \subset (U^\perp)^\perp$, by proposition \ref{orthogonalComplementClosed}. For the other inclusion, take $v\in (U^\perp)^\perp$. By the first point, we can write $v = v_1 + v_2$ where $v_1\in U$ and $v_2\in U^\perp$. Because $v\in (U^\perp)^\perp$ and $v_2\in U^\perp$, we must have
\[ 0 = \inner{v_2, v} = \inner{v_2, v_1+v_2} = \inner{v_2, v_1} + \inner{v_2,v_2} = \norm{v_2}. \]
So $v=v_1\in U$.
\end{proof}

TODO all projection results for projection onto finite dim? See proposition before Bessel inequality. In fact better: projection onto summand of direct sum! Put under decompositions.

\begin{proposition} \label{linearDeMorgan}
Let $W_1,W_2$ be subspaces of an inner product space $V$. Then
\[ (W_1+W_2)^\perp = W_1^\perp \cap W_2^\perp. \]
\end{proposition}
\begin{proof}
For a vector $v\in V$,
\[  v\in (W_1+W_2)^\perp \implies \forall x\in W_1\cup W_2: \inner{v,x} = 0 \implies v\in W_1^\perp \cap W_2^\perp \]
and
\begin{align*}
v\in W_1^\perp \cap W_2^\perp &\implies \forall x\in W_1, y\in W_2: \inner{v,x} = 0 = \inner{v,y} \\
&\implies \forall x\in W_1, y\in W_2:\inner{v, x+y} = 0 \implies v\in (W_1+W_2)^\perp.
\end{align*}
\end{proof}

A result dual to proposition \ref{linearDeMorgan} holds for finite-dimensional spaces:
\begin{proposition}
Let $W_1,W_2$ be subspaces a finite-dimensional space $V$. Then
\[ (W_1\cap W_2)^\perp = W_1^\perp + W_2^\perp. \]
\end{proposition}
\begin{proof}
We start by applying proposition \ref{linearDeMorgan} to $W_1^\perp$ and $W_2^\perp$:
\[ (W_1^\perp+W_2^\perp)^\perp = (W_1^\perp)^\perp \cap (W_2^\perp)^\perp = W_1 \cap W_2. \]
Taking the orthogonal complement of both sides gives the result. In infinite dimensions $(W_1^\perp+W_2^\perp)$ is not necessarily closed. 
\end{proof}

\subsection{Orthogonal sets and sequences}
\begin{definition}
\begin{itemize}
\item A set of vectors $D$ is called \udef{orthogonal} if for any two vectors $v,w\in D$, $v\perp w$ \textup{if and only if} $v\neq w$.
\item A set of vectors $D$ is called \udef{orthonormal} if for any two vectors $v,w\in D$,
\[ \inner{v,w} = \begin{cases}
0 & (v\neq w) \\ 1 & (v=w)
\end{cases}. \]
\end{itemize}
In particular an orthonormal set is an orthogonal set of unit vectors.
\end{definition}

\begin{lemma} \label{orthogonalLinearlyIndependent}
Every orthogonal set of vectors is linearly independent.
\end{lemma}
\begin{lemma}
Every subset of an orthogonal (resp. orthonormal) set is orthogonal (resp. orthonormal).
\end{lemma}

\begin{theorem}[Gram-Schmidt procedure]
Every finite set of linearly independent vectors $D = \{v_1,\ldots, v_n\}$ can be transformed into an orthonormal set $D' = \{e_1,\ldots,e_n\}$ with the same number of vectors such that the spans are the same: $\Span(D') = \Span(D)$.
\end{theorem}
\begin{proof}
The procedure goes as follows:
\begin{align*}
e_1 &= \frac{v_1}{\norm{v_1}} \\
e_2 &= \frac{v_2 - \inner{e_1,v_2}e_1}{\norm{v_2 - \inner{e_1,v_2}e_1}} \\
&\hdots \\
e_j &= \frac{v_j - \inner{e_1,v_j}e_1- \ldots - \inner{e_{j-1},v_j}e_{j-1}}{\norm{v_2 - \inner{e_1,v_2}e_1- \ldots - \inner{e_{j-1},v_j}e_{j-1}}} \\
&\hdots
\end{align*}
\end{proof}

If we only need an orthogonal set $\{y_1,\ldots,y_n\}$, not an orthonormal one, we can use the procedure
\[ y_{k+1} = v_{k+1} - \sum_{i=1}^k \frac{\inner{v_{k+1}, y_i}}{\inner{y_i,y_i}}y_i. \]

\begin{lemma} \label{orthogonality}
Let $(\mathbb{F}, V,+,\inner{\cdot,\cdot})$ be an inner product space. Then
\[ \inner{v,w}=0 \qquad \iff \qquad \forall a\in\mathbb{F}:\;\norm{v}\leq\norm{v+aw}.  \]
\end{lemma}
\begin{proof}
The implication $\Rightarrow$ is a consequence of the Pythagorean theorem. For the other implication, assume $\forall a\in\mathbb{F}:\;\norm{v}\leq\norm{v+aw}$. Then
\[ \norm{v}^2 \leq \norm{v-aw}^2 = \norm{v}^2 - 2\Re\inner{v,aw} + \norm{aw}^2 \]
which implies $2\Re\inner{v,aw} \leq a^2\norm{w}^2$. Let $\inner{v,w} = re^{i\theta}$. (If $\mathbb{F} = \R$, then $\theta=0$.) Then in particular the inequality holds for all $a=te^{i\theta}$ with $t\in\R$. This yields
\[ 2\Re(te^{-i\theta}re^{i\theta}) \leq t^2\norm{w}^2 \qquad \text{or}\qquad 2rt\leq t^2\norm{w}^2. \]
Letting $t\geq 0$, we can divide out a $t$: $2r\leq t\norm{w}^2$. Then letting $t\to 0$ gives $r=0$ and thus $\inner{v,w}=0$.
\end{proof}

\begin{proposition}
Let $V$ be an inner product space and $D = \{e_1,\ldots, e_n\}$ a finite orthonormal set of vectors. Then $\forall v\in V$
\[ \inf_{c_i\in\mathbb{F}}\norm{v-\sum_{i=1}^nc_ie_i} = \norm{v-\sum_{i=1}^n\inner{e_i,v}e_i} \]
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
\norm{v-\sum_{i=1}^nc_ie_i}^2 &= \inner{v-\sum_{i=1}^nc_ie_i,v-\sum_{j=1}^nc_je_j} \\
&= \norm{v} - \sum_{j=1}^n c_j\inner{v,e_j} - \sum_{i=1}^n\bar{c}_i\inner{e_i,v} + \sum_{i,j=1}^n\bar{c}_ic_j\inner{e_i,e_j} \\
&= \norm{v} - 2\Re\left(\sum_{i=1}^nc_i\overline{\inner{e_i,v}}\right) + \sum_{i=1}^n|c_i|^2 \\
&= \sum_{i=1}^n\left(|c_i|^2 - 2\Re\left(\sum_{i=1}^nc_i\overline{\inner{e_i,v}}\right) + |\inner{e_i,v}|^2\right) +\norm{v} - \sum_{i=1}^n|\inner{e_i;v}|^2 \\
&= \sum_{i=1}^n|c_i - \inner{e_i,v}|^2 +\norm{v} - \sum_{i=1}^n|\inner{e_i,v}|^2.
\end{align*}
This is clearly minimised when $c_i = \inner{e_i,v}$.
\end{proof}
\begin{corollary}
Let $v\in\Span(D)$, then $v = \sum_{i=1}^n \inner{e_i,v}e_i$.
\end{corollary}
We call the numbers $\inner{e_i,v}$ the \udef{Fourier coefficients} of $v$ w.r.t. $D$.
\begin{proof}
In this case $\inf_{c_i\in\mathbb{F}}\norm{v-\sum_{i=1}^nc_ie_i} = 0$.
\end{proof}
\begin{corollary}[Bessel inequality] \label{BesselInequality}
Let $\{e_i\}_{i\in I}$ be an orthonormal family and $v\in V$, then
\[ \sum_{i\in I}|\inner{e_i,v}|^2 = \sup \left\{\sum_{\substack{i\in I' \subset I\\ I' \;\text{finite}}} |\inner{e_i,v}|^2 \right\} \leq \norm{v}^2. \]
\end{corollary}
\begin{proof}
In the previous proof,
\[ 0 \leq \norm{v-\sum_{i=1}^nc_ie_i}^2 = \sum_{i=1}^n|c_i - \inner{e_i,v}|^2 +\norm{v} - \sum_{i=1}^n|\inner{e_i,v}|^2 = \norm{v} - \sum_{i=1}^n|\inner{e_i,v}|^2. \]
Where we have set $c_i = \inner{e_i,v}$. Thus the supremum must also be $\leq \norm{v}$.
\end{proof}
\begin{corollary}
For any $v\in V$, $\inner{e_i,v} = 0$ except for countably many $i\in I$. \label{countableComponents}
\end{corollary}
\begin{proof}
Immediate from \ref{finiteSumsAreCountable}.
\end{proof}
TODO: link with metric topology being sequential?

\begin{corollary}[Riemann-Lebesgue lemma] \label{RiemannLebesgueLemma}
For any sequence $\seq{e_i}_{i\in J \subset I}$, we have
\[ \lim_{i\in J} \inner{e_i,v} = 0. \]
\end{corollary}

\begin{corollary}
We can also obtain the Cauchy-Schwarz inequality from the Bessel inequality.
\end{corollary}
\begin{proof}
Let $x,y\in V$. Then $\{x/\norm{x}\}$ is an orthonormal set. Applying the Bessel inequality for $y$ gives $\norm{y}^2 \geq |\inner{x/\norm{x}, y}|^2 \implies |\inner{x,y}|^2\leq \norm{x}^2\norm{y}^2 \implies |\inner{x,y}| \leq \norm{x}\;\norm{y}$.
\end{proof}

\subsection{Orthonormal bases}
\begin{definition}
Let $D$ be an orthonormal set of vectors in an inner product space $V$, then $D$ is said to be
\begin{enumerate}
\item \udef{maximal}, if it is a maximal element in the set of orthonormal sets ordered by inclusion;
\item \udef{total}, if the smallest closed subspace that includes $D$ is $V$ (i.e.\ $\Span(D)$ is dense in $V$);
\item an \udef{orthonormal basis} (o.n. basis) or a \udef{Hilbert basis} if any vector in $V$ can be written as a (possibly infinite) linear combination of elements of $D$.
\end{enumerate}
\end{definition}
\begin{note}
Hilbert bases are in general not Hamel bases.  e.g\, take $\R^\mathbb{N}$. Then 
\begin{align*}
(1,0,0,&\ldots), \\
(0,1,0,&\ldots), \\
(0,0,1,&\ldots), \\
&\ldots
\end{align*}
is an orthonormal basis, but not a Hamel basis (consider $(1,1,1,\ldots)$).
\end{note}

\begin{proposition} \label{exitenceMaximalOrthonormalSet}
\begin{itemize}
\item Every vector inner product space has a maximal orthonormal set.
\item Every orthonormal set can be extended to a maximal orthonormal set.
\end{itemize}
\end{proposition}
\begin{proof}
The first statement follows easily from the second. The second statement is proved using Zorn's lemma. Let $S$ be an orthonormal set. Define
\[ \mathcal{A} = \{ D\subset V \;|\; S\subset D \; \text{and $D$ is orthonormal} \} \]
ordered by inclusion. It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union which is still orthonormal. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. This is by definition an orthonormal basis.

In the finite-dimensional case this can also be proved using the Gram-Schmidt procedure.
\end{proof}

\begin{proposition}
If $V$ is finite-dimensional, then the notions of maximal orthonormal set, total orthonormal set and orthonormal set coincide. Such an orthonormal set is also a (Hamel) basis of $V$.
\end{proposition}
\begin{proof}
Corollaries of Gram-Schmidt.
\end{proof}

\begin{lemma} \label{characterisationMaximalOrthonormalSet}
Let $V$ be an inner product space and $D$ an orthonormal set. Then
\begin{enumerate}
\item $D$ is maximal \textup{if and only if} $D^\perp = \{0\}$;
\item if $D$ is an orthonormal basis, then $D$ is maximal.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) All possible vectors with which to extend $D$ are elements of $D^\perp$. 

(2) Assume $D$ an o.n. basis. Then $D^\perp = (\Span(D))^\perp = V^\perp = \{0\}$, using \ref{OrthogonalComplementProperties} and \ref{orthogonalComplementDenseSpace}.
\end{proof}
There are maximal orthonormal families that are not bases.
\begin{example}
Consider the space $l^2(\N)$ and take the subspace $X$ generated by the family of elements
\[ \left( \sum_{n=1}^\infty n^{-1}e_n, e_2,e_3,e_4,\ldots \right) \]
with the inner product induced by the inner product of $l^2$. In this space $F=\{e_2,e_3,\ldots\}$ is orthonormal and maximal, but not an orthonormal basis.
\end{example}

Maximal orthonormal families are easy to construct, but do not have the nice properties of orthonormal bases (see below). We would really like the concepts of orthonormal basis and maximal orthonormal family to coincide. We will see they coincide exactly in Hilbert spaces (see \ref{criterionHilbertSpace}).

\begin{proposition} \label{totalONBParsevalEquivalence} \label{plancherel}
Let $V$ be an inner product space and $D = \{e_i\}_{i\in I}$ an orthonormal set. The following are equivalent:
\begin{enumerate}
\item $D$ is an orthonormal basis of $V$;
\item $D$ is total in $V$;
\item for all $v,w\in V$,
\[ \inner{v,w} = \sum_{i\in I}\inner{v,e_i}\inner{e_i,w}; \]
\item \textup{(Parseval's identity)} for all $v\in V$,
\[ \norm{v}^2 = \sum_{i\in I}|\inner{e_i,v}|^2; \]
\item for all $v\in V$: if $v\perp D$, then $v=0$;
\item \textup{(Plancherel formula)} for all $v\in V$,
\[ v = \sum_{i\in I}\inner{e_{i},v}e_{i}. \]
\end{enumerate}
\end{proposition}
\begin{proof}
We proceed round-robin-style.
\begin{itemize}[leftmargin=2cm]
\item[$\boxed{(1) \Rightarrow (2)}$] Assume $D$ an o.n. basis. Then there exists a net of partial sums converging to any element $v\in V$. Each of these partial sums is a finite linear combination of elements in $D$ and thus this net is a net in $\Span(D)$. This means $v\in\overline{\Span(D)}$.
\item[$\boxed{(2) \Rightarrow (3)}$] Fix $v,w\in V$. Because $V$ is a metric spaces and thus sequential, we can find sequences $(v_j)_{j\in J}$ and $(w_k)_{k\in K}$ in $\Span(D)$ converging to $v$ and $w$. Now the linear maps $u\mapsto \overline{\inner{u, e_i}}$ and $u\mapsto \inner{e_i, u}$ are bounded by Cauchy-Schwarz and thus continuous by theorem \ref{boundedLinearMaps} (TODO corollary CSB). Then we can calculate, using the fact that each $v_j$ and $w_k$ is a finite linear combination of $e_i$,
\begin{align*}
\inner{v,w} &= \inner{\lim_{j}v_j, \lim_k w_k} = \lim_{j}\lim_{k}\inner{v_j,w_k} \\
&= \lim_{j}\lim_{k}\inner{\sum_{i=1}^{N_{j}}\inner{e_i,v_j}e_i,\sum_{i'=1}^{N_k}\inner{e_{i'},w_k}e_{i'}} \\
&= \lim_{j}\lim_{k}\sum_{i=1}^{N_{j}}\sum_{i'=1}^{N_k}\inner{v_j,e_i}\inner{e_{i'},w_k}\inner{e_i,e_{i'}} = \lim_{j}\lim_{k}\sum_{i=1}^{N_{j}}\sum_{i'=1}^{N_k}\inner{v_j,e_i}\inner{e_{i'},w_k}\delta_{i,i'} \\
&= \lim_{j}\lim_{k}\sum_{i=1}^{\min\{N_{j},N_{k}\}}\inner{v_j,e_i}\inner{e_i,w_k} \\
&= \lim_{j}\lim_{k}\sum_{i\in I}\inner{v_j,e_i}\inner{e_i,w_k} \\
&= \sum_{i\in I}\lim_{j}\lim_{k}\inner{v_j,e_i}\inner{e_i,w_k} \\
&= \sum_{i\in I}\inner{v,e_i}\inner{e_i,w}.
\end{align*}
For the interchange of the limits and the summation in the penultimate equality we can use Tannery's theorem, \ref{tannery}. Indeed $|\inner{e_i,w_k}|$ is bounded by $\norm{w_k}$ by the Bessel inequality. By the continuity of the norm we have $\lim_k \norm{w_k} = \norm{w}$, so the sequence $\norm{w_k}$ is bounded.
\item[$\boxed{(3) \Rightarrow (4)}$] Set $v=w$.
\item[$\boxed{(4) \Rightarrow (5)}$] If $v\perp D$, then
\[ \norm{v}^2 = \sum_{i\in I}|\inner{e_i,v}|^2 = 0 \qquad\text{which implies $v=0$.} \]
\item[$\boxed{(5) \Rightarrow (6)}$] The vector $v-\sum_{i\in I}\inner{e_i,v}e_i$ is perpendicular to $D$:
\[ \forall e_j\in D: \quad \inner{e_j, v-\sum_{i\in I}\inner{e_i,v}e_i} = \inner{e_j, v}-\sum_{i\in I}\inner{e_i,v}\inner{e_j,e_i} = \inner{e_j, v} - \inner{e_j, v} = 0. \]
So $v-\sum_{i\in I}\inner{e_i,v}e_i = 0$ and the Plancherel formula holds.
\item[$\boxed{(6) \Rightarrow (1)}$] By definition of o.n. basis.
\end{itemize}
\end{proof}

\begin{lemma}
Let $V$ be an inner product space. If $D$ is an orthonormal basis of $V$, then it is also an orthonormal basis of $\overline{V}$, the completion of $V$.
\end{lemma}
\begin{proof}
Let $D$ be an o.n. basis. By \ref{totalONBParsevalEquivalence} $\Span(D)$ is dense in $V$, meaning it is also dense in $\overline{V}$, by \ref{denseSubsetOfDenseSubspaceIsDense}. Thus $D$ is total in $\overline{V}$ and an o.n. basis by \ref{totalONBParsevalEquivalence}.
\end{proof}

\subsubsection{Cardinality and separable inner product spaces}
\url{https://arxiv.org/pdf/1606.03869.pdf}
\begin{definition}
An inner product space is \udef{separable} if it is separable as a metric space, i.e.\ it admits a countable dense subset.
\end{definition}

\begin{proposition}
Given a vector space $V$, any two maximal orthonormal sets have the same cardinality.
\end{proposition}
\begin{proof}
Take $D = \{e_i\}_{i\in I}$ and $D' = \{f_j\}_{j\in J}$ maximal orthonormal sets.
\end{proof}

\begin{proposition}
An inner product space is separable \textup{if and only if} it admits an orthonormal basis with at most countably many vectors.
\end{proposition}
\begin{proof}
TODO infinite-dimensional analog of the Gram-Schmidt process
\end{proof}
\begin{corollary}
Any separable inner product space has an orthonormal basis.
\end{corollary}

\begin{proposition}
Not every inner product space has an orthonormal basis.
\end{proposition}
\begin{proof}
\url{https://en.wikipedia.org/wiki/Inner_product_space#Orthonormal_sequences}
\url{https://groups.google.com/g/sci.math.research/c/1SA_3h1whQo?pli=1}
\url{https://www.angelfire.com/journal/mathematics/innerproduct.pdf}
\url{https://arxiv.org/pdf/1009.1441.pdf}
\end{proof}


\section{Maps on inner product spaces}

\begin{lemma} \label{equalityOfMapsInnerProductSpaces}
Let $V$ be an inner product space and $S,T\in\Hom(V)$. Then $S=T$ \textup{if and only if}
\[ \forall v,w\in V: \inner{Tv,w} = \inner{Sv,w}. \]
\end{lemma}
\begin{proof}
The direction $\boxed{\Rightarrow}$ is obvious. For the other direction, use
\[ 0 = \inner{Tv,w} - \inner{Sv,w} = \inner{(T-S)v,w} \]
for all $v,w$. In particular set $w$ equal to $(T-S)v$. Then $\norm{(T-S)v} = 0$ for all $v\in V$. By the definiteness of the norm we have $(T-S)v = 0$, meaning $Tv = Sv$.
\end{proof}

\subsection{Bounded operators}
\begin{lemma} \label{operatorNormInnerProduct}
Let $T\in\Bounded(V,W)$, then
\begin{align*}
\norm{T} &= \sup_{w\in \im(T),v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \\
&= \sup\setbuilder{|\inner{w,Tv}|}{w\in \im(T)\;\land\; v\in\dom{T}\;\land\; \norm{w} = 1 = \norm{v}} \\
&= \sup_{w\in W,v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \\
&= \sup\setbuilder{|\inner{w,Tv}|}{w\in W\;\land\; v\in\dom{T}\;\land\; \norm{w} = 1 = \norm{v}}.
\end{align*}
\end{lemma}
TODO: compare \ref{normVectorSupUnitvectors}.
\begin{proof}
We prove
\[ \norm{T} \leq \sup_{w\in \im(T),v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \leq \sup_{w\in W,v \in \dom(T)} \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \leq \norm{T}. \]
The first two inequalities follow from the characterisation \ref{operatorNorm}
\[ \norm{T} = \sup_{v \in \dom(T)} \frac{\norm{Tv}}{\norm{v}} = \sup_{v \in \dom(T)} \frac{\inner{Tv,Tv}}{\norm{Tv}\,\norm{v}} \]
and the inclusions
\begin{align*}
\setbuilder{\frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}}}{v\in\dom(T), w = Tv} &\subseteq \setbuilder{\frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}}}{v\in\dom(T), w\in\im(T)}\\
&\qquad\quad\subseteq \setbuilder{\frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}}}{v\in\dom(T), w\in V}.
\end{align*}
The last equality follows from the Cauchy-Schwarz inequality \ref{CauchySchwarz}:
\[ \frac{|\inner{w,Tv}|}{\norm{w}\,\norm{v}} \leq \frac{\norm{w}\,\norm{Tv}}{\norm{w}\,\norm{v}} = \frac{\norm{Tv}}{\norm{v}} \leq \frac{\norm{T}\,\norm{v}}{\norm{v}} = \norm{T} \]
for all $v\in\dom(T), w\in V$. 
\end{proof}

\subsection{Isometries}
\begin{lemma}
Let $V,W$ be inner product spaces. Let $f:V\to W$ be a function. Then $f$ preserves the metric (i.e.\ is an isometry) \textup{if and only if} $f$ also preserves the inner product:
\[ \forall x,y \in V: \quad \inner{f(x),f(y)}_W = \inner{x,y}_V. \]
\end{lemma}
The proof is a simple application of the polarisation identities.

\begin{definition}
Let $V,W$ be an inner product spaces. A linear map $U\in\Hom(V,W)$ is called \udef{unitary} if it is an isometry and invertible.

Unitary operators on real vector spaces are also called \udef{orthogonal operators}.
\end{definition}
Because every isometry is injective (see lemma \ref{isometryLemma}), it is enough for a linear map to be isometric and surjective to be unitary.

\begin{lemma}
Every unitary map is bounded and has norm $1$.
\end{lemma}
\begin{proof}
Let $U: V\to W$ be a unitary map between inner product spaces. Then $\forall v\in V: \norm{U(v)} = \norm{v}$.
\end{proof}

Unitary operators transform orthonormal bases to orthonormal bases:
\begin{proposition}
Let $T\in \Hom(V,W)$ with $V,W$ inner product spaces and let $V$ have an orthonormal basis $\{e_i\}_{i\in I}$. Then $T$ is unitary \textup{if and only if} $\{Te_i\}_{i\in I}$ is an orthonormal basis of $W$.
\end{proposition}
\begin{proof}
Assume $T$ unitary. The family $\{Te_i\}_{i\in I}$ is certainly orthonormal, by preservation of the inner product. Now let $w\in W$ and so $T^{-1}w\in V$. By the Plancherel formula, proposition \ref{plancherel}, we can write
\[ T^{-1}w = \sum_{n=1}^\infty \inner{e_{i_n},T^{-1}w}e_{i_n} = \lim_{N\to\infty}\sum_{n=1}^N \inner{e_{i_n},T^{-1}w}e_{i_n} \]
and so
\[ w = TT^{-1}w = T\lim_{N\to\infty}\sum_{n=1}^N \inner{e_{i_n},T^{-1}w}e_{i_n} = \lim_{N\to\infty}\sum_{n=1}^N \inner{e_{i_n}T^{-1}w}Te_{i_n} \]
because $T$ is bounded and thus continuous, by theorem \ref{boundedLinearMaps}.
Thus $\{Te_i\}_{i\in I}$ is an orthonormal basis of $W$.

Conversely, assume $\{Te_i\}_{i\in I}$ is an orthonormal basis of $W$. We first prove $T$ is bounded, which is a simple application of Parseval's identity, proposition \ref{totalONBParsevalEquivalence}:
\[ \norm{Tv}^2 = \sum_{i\in I}|\inner{Te_i,Tv}|^2 = \sum_{i\in I}|\inner{e_i,v}|^2 = \norm{v}^2. \]
The rest of the proof is again an application of the Plancherel formula.
\end{proof}

\begin{lemma}
Let $U$ be a unitary map. If $\lambda$ is an eigenvalue of $U$, then $|\lambda| = 1$.
\end{lemma}
\begin{proof}
Let $v$ be an eigenvector associated to the eigenvalue $\lambda$. Then
\[ \inner{v,v} = \inner{L(v),L(v)} = \inner{\lambda v, \lambda v} = \lambda^2\inner{v,v},  \]
so $\lambda^2 = 1$.
\end{proof}

\subsection{Symmetric operators}
\begin{definition}
Let $(\mathbb{F},V,+,\inner{\cdot,\cdot})$ be an inner product space. A linear operator $L$ is called \udef{symmetric} if, $\forall v,w\in \dom(L)$
\[ \inner{L(v),w} = \inner{v,L(w)}. \]
\end{definition}

\begin{proposition}
Let $V$ be an inner product space and $L$ a symmetric operator on $V$. Then eigenvectors of $L$ associated to different eigenvalues are orthogonal.
\end{proposition}
\begin{proof}
Let $v,w$ be eigenvectors of $L$ with eigenvalues $\lambda, \mu$ such that $\lambda \neq \mu$. Then
\[ \lambda\inner{v,w} = \inner{\lambda v,w}=\inner{L(v),w} = \inner{v,L(w)} = \inner{v,\mu w} = \mu \inner{v,w} \]
and consequently $\inner{v,w} =0$.
\end{proof}

\subsection{Impact on subspaces}
\subsubsection{Invariant and reducing subspaces}
\begin{definition}
Let $V$ be an inner product space and $T$ a linear operator on $V$.
\begin{itemize}
\item A subspace $U\subseteq V$ is said to be \udef{invariant} under $T$ if $T[U] \subset U$.
\item A subspace $U\subseteq V$ is said to be \udef{reducing} for $T$ if both $U$ and $U^\perp$ are invariant under $T$.
\end{itemize}
\end{definition}

\section{Energy forms}
\begin{definition}
Let $T$ be an operator on an inner product space $V$. The \udef{energy form} of $T$ is the map
\[ \inner{\cdot, \cdot}_T: \dom(T)\times \dom(T) \to \F: (x,y) \mapsto \inner{x,Ty}. \]
We also define the associated quadratic form
\[ Q_T: \dom(T)\to \F: x\mapsto \inner{x,x}_T = \inner{x,Tx}. \]
\end{definition}
Energy forms are clearly sesquilinear.

\begin{lemma} \label{quadraticFormInverseOperator}
Let $T$ be an invertible operator. Then
\[ Q_{T^{-1}}(x) = \overline{Q_T(T^{-1}(x))}. \]
\end{lemma}
\begin{proof}
For all $x\in V$
\[ Q_{T^{-1}}(x) = \inner{x,T^{-1}x} = \inner{TT^{-1}x, T^{-1}x} = \overline{\inner{T^{-1}x, T(T^{-1}x)}} = \overline{Q_T(T^{-1}(x))}. \]
\end{proof}

\begin{lemma} \label{sameEnergyFormSameOperator}
Two operators $T_1,T_2\in \Lin(V)$ have the same energy form \textup{if and only if} $T_1 = T_2$.
\end{lemma}
\begin{proof}
This is a consequence of \ref{elementaryOrthogonality}.
\end{proof}

\begin{lemma} \label{energyFormHermitianSymmetric}
Let $V$ be a \emph{complex} inner product space and $T$ an operator on $V$. Then the following are equivalent:
\begin{enumerate}
\item $T$ is symmetric;
\item $\inner{\cdot,\cdot}_T$ is Hermitian;
\item $Q_T$ is real-valued.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Leftrightarrow (2)$ For all $x,y\in \dom(T)$ we have $\inner{x,Ty} = \inner{Tx, y}$ if and only if $\inner{x,Ty} = \overline{\inner{y, Tx}}$.

$(2) \Leftrightarrow (3)$ Given by \ref{HermitianRealQuadratic}.
\end{proof}

So the energy form associated to a symmetric operator is Hermitian. We typically would like our energy forms to be pre-inner products. This is exactly the case for positive operators.

\subsection{Positive operators}
\begin{definition}
Let $T$ be an operator on an inner product space $V$. Then $T$ is called \udef{positive} if the associated energy form is positive: for all $x\in V$
\[ Q_T(x) = \inner{x,x}_T = \inner{x,Tx} \geq 0. \]
We write $A \geq 0$. We also say
\begin{itemize}
\item $A$ is \udef{strictly positive}, denoted $A > 0$, if $Q_T(u) > 0$;
\item $A$ is \udef{negative} if $-A$ is positive;
\item $A$ is \udef{positive definite}, \udef{strongly positive} or \udef{coercive} if there exists a constant $k>0$ such that
\[ Q_T(x) \geq k\norm{x}^2 > 0. \]
\end{itemize}
\end{definition}

\begin{lemma} \label{positiveOperatorSymmetric}
If $T$ is a positive operator on a complex inner product space, then $T$ is symmetric.
\end{lemma}
\begin{proof}
Follows immediately from \ref{energyFormHermitianSymmetric}.
\end{proof}

On a real inner product space there may exist positive operators that are not symmetric.
\begin{example}
Let $V= \R^2$ and $T: \R^2 \to\R^2: (x,y)\mapsto (y,-x)$, so $T$ has matrix representation $\begin{pmatrix}
0 & 1 \\ -1 & 0
\end{pmatrix}$. Then
\[ \forall (x,y)\in V: \quad Q_T\big((x,y)\big) = \inner{(x,y), (y,-x)} = xy -xy = 0 \geq 0, \]
so $T$ is positive, but $T$ is clearly not symmetric.
\end{example}

\begin{lemma}
Let $A$ be an bounded operator on a Hilbert space $H$. Then $A^*A$ and $AA^*$ are positive. Also $A^*A$ is strictly positive \textup{if and only if} $A$ is injective.
\end{lemma}
\begin{proof}
For all $x\in H$:
\[ \inner{A^*Ax,x} = \inner{Ax,Ax} = \norm{Ax}^2 \geq 0 \qquad \inner{AA^*x,x} = \inner{A^*x,A^*x} = \norm{A^*x}^2 \geq 0. \]
If $A$ is injective, then its kernel is $\{0\}$ and thus $\norm{Ax}^2 > 0$ for all $x\in H\setminus\{0\}$.
\end{proof}

\begin{lemma}
Let $T$ be an invertible operator on an inner product space $V$. Then $Q_T[V] = Q_{T^{-1}}[V]$.
\end{lemma}
\begin{proof}
Immediate from \ref{quadraticFormInverseOperator}.
\end{proof}
\begin{corollary}
Let $T$ be an invertible operator. Then $T$ is positive (definite) \textup{if and only if} $T^{-1}$ is positive (definite).
\end{corollary}

\begin{lemma} \label{positiveOperatorPositiveEnergyForm}
Let $T$ be an operator. The energy form $\inner{\cdot,\cdot}_T$ is positive (and thus a pre-inner product) \textup{if and only if} $T$ is a positive operator.
\end{lemma}

\subsubsection{Energy norm}
\begin{definition}
Let $T$ be a positive operator. Then
\[ \norm{\cdot}_{\inner{}_T}: \dom(T) \to \interval[co]{0,+\infty}: x\mapsto \norm{x}_{\inner{}_T} = \sqrt{\inner{x,x}_T} = \sqrt{Q_T(x)} \]
is the \udef{energy norm} associated to $T$.
\end{definition}

\begin{lemma}
The energy norm of a positive operator determines a pseudometric topology.
\end{lemma}

\begin{definition}
The topology generated by the energy norm is called the \udef{energy topology} and convergence in the energy topology is called \udef{convergence in energy}.
\end{definition}

\begin{proposition}
Let $T$ be a positive operator. Then
\begin{enumerate}
\item the energy topology is coarser than the norm topology;
\item the topologies are the same on $\dom(T)$ if $T$ is positive definite.
\end{enumerate}
\end{proposition}

\subsubsection{The partial order on operators}
\begin{definition}
We define an \udef{operator partial order} by
\[ A\leq B \qquad\iff\qquad B-A \geq 0. \]
\end{definition}
TODO: restrict to bounded operators??

\begin{lemma}
The operator partial order is a partial order on the set of operators on an inner product space.
\end{lemma}

\subsubsection{Induced topology}
We consider the topology induced by an energy norm $\norm{\cdot}_{\inner{}_T}$.

\begin{proposition} \label{energyNormTopology}
Let $V$ be an inner product space and $T$ a positive operator on $V$. Then
\begin{enumerate}
\item if $T$ is bounded, then $\norm{\cdot}_{\inner{}_T}$ is bounded by $\norm{\cdot}$;
\item $\norm{\cdot}$ is bounded by $\norm{\cdot}_{\inner{}_{T+\id}}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) If $T$ is bounded, then $\forall v\in V$
\[ \norm{v}_{\inner{}_T} = \sqrt{\inner{v,Tv}} = \sqrt{|\inner{v,Tv}|} \leq \sqrt{\norm{v}^2\norm{T}} = \sqrt{\norm{T}}\norm{v}, \]
where we have used the Cauchy-Schwarz inequality \ref{CauchySchwarz}.

(2) For all $v\in V$ we have
\[ \norm{v} \leq \norm{v} + \norm{v}_{\inner{}_T} = \inner{v,v} + \inner{v,Tv} = \inner{v,(T+\id)v} = \norm{v}_{\inner{}_{T+\id}}. \]
\end{proof}
\begin{corollary}
Let $V$ be an inner product space and $T$ a positive operator on $V$. Then
\begin{enumerate}
\item if $T$ is bounded, then the topology induced by $\norm{\cdot}$ is finer than the topology induced by $\norm{\cdot}_{\inner{}_T}$;
\item the topology on $\dom(T)$ induced by $\norm{\cdot}_{\inner{}_{T+\id}}$ is finer than the topology induced by $\norm{\cdot}$.
\end{enumerate}
\end{corollary}
\begin{proof}
This follows straight from \ref{normComparison}.
\end{proof}
\begin{corollary}
Let $H$ be a Hilbert space and $T$ a positive operator on $H$. Then $\inner{\cdot,\cdot}_{T+\id}$ is an inner product on $\dom(T)$.
\end{corollary}
\begin{proof}
We have that $\inner{\cdot,\cdot}_{T+\id}$ is a pre-inner product by \ref{positiveOperatorPositiveEnergyForm}. We just need to check definiteness. This follows because $\norm{\cdot} \leq \norm{\cdot}_{\inner{}_{T+\id}}$ and $\norm{\cdot}$ is definite.
\end{proof}




\ref{existenceMetricCompletion}

$H_T \defeq \Closure_{\inner{}_{T+\id}}\big(\dom(T)\big)$ is a Hilbert space with inner product
\[ \inner{x,y}_{H_T} \defeq \lim_{n\to \infty} \inner{x_n,y_n}_{T+\id} \qquad \text{for any $\seq{x_n} \overset{\inner{}_{T+\id}}{\longrightarrow} x$ and $\seq{y_n} \overset{\inner{}_{T+\id}}{\longrightarrow} y$.} \]


\subsection{Dissipative operators}
\begin{definition}
Let $T$ be an operator on $H$. Then $T$ is \udef{dissipative} if, for all $x\in\dom(T)$
\[ \Im \inner{x,Tx} \geq 0. \]
\end{definition}

\subsection{Rayleigh quotient}
\begin{definition}
Let $T$ be a linear operator on an inner product space $V$. The \udef{Rayleigh quotient} for $T$ is 
\[ J_T: \dom(T)\setminus\{0\}\to \F: u\mapsto \frac{Q(u)}{\norm{u}^2} = \frac{\inner{u,Tu}}{\norm{u}^2}. \]
We may also write just $J$ if the intended operator $T$ is clear.
\end{definition}

\begin{lemma}
Let $T\in\Lin(V)$ be a linear operator and $J_T$ the associated Rayleigh quotient. Then for all $u\in V$:
\[ J_T(u) = J_T\left(\frac{u}{\norm{u}}\right). \]
\end{lemma}

\subsubsection{Numerical range}
\url{https://users.math.msu.edu/users/shapiro/pubvit/downloads/numrangenotes/numrange_notes.pdf}

\url{https://pskoufra.info.yorku.ca/files/2016/07/Numerical-Range.pdf}

\url{http://www.math.wm.edu/~ckli/nrnote}

\url{https://link-springer-com.ezproxy.ulb.ac.be/content/pdf/10.1007%2F978-3-319-01448-7.pdf}

\url{https://projecteuclid.org/journalArticle/Download?urlId=10.1307%2Fmmj%2F1028997958}

\begin{definition}
Let $T$ be a linear operator on an inner product space $V$ and $J_T$ the Rayleigh quotient of $T$. The range $\NumRange(T) \defeq \im(J_T)$ is known as the \udef{numerical range}.
\end{definition}

The numerical range of $T$ can equivalently be defined as the image of the unit sphere under the quadratic form associated to $T$.

\begin{lemma}
Let $T$ be a linear operator on an inner product space $V$ and $J_T$ the Rayleigh quotient of $T$. Then
\begin{align*}
\NumRange(T) &= J_T[\setbuilder{u\in V}{\norm{u} = 1} \cap \dom(T)] \\
&= Q_T[\setbuilder{u\in V}{\norm{u} = 1}\cap \dom(T)].
\end{align*}
\end{lemma}

\begin{lemma}
Let $V$ be an inner product space over a field $\F$, $\lambda,\mu\in \F$ and $T$ an operator on $V$. Then
\[ W(\lambda T + \mu) = \lambda W(T) + \mu. \]
\end{lemma}

\begin{theorem}[Toeplitz-Hausdorff theorem]
Let $V$ be an inner product space and $T$ an operator on $V$. Then $W(T)$ is convex.
\end{theorem}
\begin{proof}
TODO \url{https://www.ams.org/journals/proc/1970-025-01/S0002-9939-1970-0262849-9/S0002-9939-1970-0262849-9.pdf}

\url{https://www.cambridge.org/core/services/aop-cambridge-core/content/view/BA251EBB1E1DE08DBD3D84964F65938B/S0008439500058197a.pdf/the-toeplitz-hausdorff-theorem-explained.pdf}
\end{proof}

\begin{proposition}
Let $V$ be an inner product space and $T$ an operator on $V$. If $V$ is finite dimensional, then $W(T)$ is compact.
\end{proposition}
\begin{proof}
Heine-Borel. TODO.
\end{proof}

\begin{lemma}
Let $V$ be an inner product space and $T$ a bounded symmetric operator on $V$. Then
\begin{enumerate}
\item the directional derivative $\partial_v(J_T(u))$ exists if $u\neq 0$ and is equal to (TODO remove and place in proof?)
\[ \partial_v(J_T)|_u = \frac{\inner{u,u}\Big( \inner{v,Tu} + \inner{u,Tv} \Big) - \inner{u,Tu}\Big(\inner{u,v}+\inner{v,u}\Big)}{\inner{u,u}^2}; \]
\item $u\in V\setminus \{ 0 \}$ is a critical point of $J_T$ \textup{if and only if} $u$ is an eigenvector of $T$ with corresponding eigenvalue $\lambda = J_T(u)$.
\end{enumerate}
\end{lemma}
\begin{proof}
TODO: critical point in $\C$ v $\R$?? (For symmetric operators $J$ is real valued)
\ref{derivativeBilinearFunction}
\end{proof}

\subsubsection{Numerical radius}
\begin{definition}
Let $T$ be a linear operator on an inner product space $V$. Then
\[ \nr(T) \defeq \sup_{u\in \dom(T)\setminus\{0\}} |J_T(u)| \]
is the \udef{numerical radius}.
\end{definition}
If $Q_T$ is the quadratic form associated to an operator $T$, we have
\[ |Q_T(u)| \leq \norm{u}^2\nr(T). \]

\begin{lemma}
Let $T$ be a linear operator on an inner product space $V$ and $J_T$ the Rayleigh quotient of $T$. Then
\begin{align*}
\nr(T) &= \sup_{\substack{u\in \dom(T) \setminus\{0\}\\ \norm{u} = 1}} |J_T(u)| \\
&= \sup_{\substack{u\in \dom(T) \setminus\{0\}\\ \norm{u} = 1}} |Q_T(u)|.
\end{align*}
\end{lemma}

\begin{proposition} \label{normNumRadius}
Let $T$ be an operator on an inner product space $V$.
\begin{enumerate}
\item If $T$ is bounded, then $\forall u\in \dom(T)\setminus\{0\}$
\[ |J_T(u)| \leq \nr(T) \leq \norm{T}. \]
\item If $T$ is symmetric, then $T$ is bounded with $\norm{T} = \nr(T)$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) The first claim follows simply from the Cauchy-Schwarz inequality \ref{CauchySchwarz}
\[ |J(u)| \leq \frac{\norm{u}\,\norm{Tu}}{\norm{u}^2} = \frac{\norm{Tu}}{\norm{u}} \leq \frac{\norm{T}\norm{u}}{\norm{u}} = \norm{T}. \]

(2) For the second claim we need to also show the inverse inequality. By \ref{operatorNormInnerProduct} it is enough to show that $|\inner{w,Tv}| \leq \nr(T)$ for all $v\in \dom(T)$ and $w\in\im(T)$ with $\norm{v} = 1 = \norm{w}$.

Take arbitrary unit vectors $v,w\in V$ and let $\theta$ be such that $|\inner{w,Tv}| = e^{i\theta}\inner{w,Tv}$. Then $\inner{e^{-i\theta}w,Tv}$ is real, so, viewing it as a sesquilinear form, the imaginary parts of the polarisation identity \ref{polarisationIdentities} cancel:
\begin{align*}
\inner{e^{-i\theta}w,Tv} &= \frac{1}{4}\sum_{k=0}^3i^k \inner{(i^ke^{-i\theta}w + v), T((i^ke^{-i\theta}w + Tv))} \\
&= \frac{1}{4}\Big( \inner{v+e^{-i\theta}w, T(v+e^{-i\theta}w)} - \inner{v-e^{-i\theta}w, T(v-e^{-i\theta}w)} \Big),
\end{align*}
where we have used that the quadratic form is real by \ref{energyFormHermitianSymmetric}.

Thus
\begin{align*}
|\inner{w,Tv}| &= |\inner{e^{-i\theta}w,Tv}| \\
&= \frac{1}{4}\Big(\inner{v+e^{-i\theta}w, T(v+e^{-i\theta}w)} - \inner{v-e^{-i\theta}w, T(v-e^{-i\theta}w)} \Big) \\
&\leq \frac{1}{4}\Big( |\inner{v+e^{-i\theta}w, T(v+e^{-i\theta}w)}| + |\inner{v-e^{-i\theta}w, T(v-e^{-i\theta}w)}| \Big) \\
&\leq \frac{1}{4}\nr(T)\Big( \norm{v+e^{-i\theta}w}^2 + \norm{v-e^{-i\theta}w}^2 \Big) \\
&= \frac{1}{4}\nr(T)\Big( 2\norm{v}^2 + 2\norm{w}^2 \Big) = \nr(T),
\end{align*}
where we have used the fact that $v,w$ are unit vectors and the parallelogram law \ref{parallelogramLaw}.
\end{proof}
\begin{corollary}
If $T$ is a symmetric operator; it is bounded iff $J_T$ is bounded above and below:
\[ \forall u\in\dom(T): \; k \leq J_T(u) \leq K \]
for some $k,K\in \R$.
\end{corollary}
\begin{corollary}
If $T$ is symmetric and bounded, then
\[ \norm{T} = \sup_{\norm{u}\leq 1} |\inner{u,Tu}|. \]
\end{corollary}






\chapter{Hilbert spaces}

\section{Tools to study operators}
\subsection{Numerical range}
\begin{lemma}
Let $T$ be an operator on a Hilbert space. If $\lambda\in\cspec(T)$, then there exists a Weyl sequence for $\lambda$. 
\end{lemma}
\begin{proof}
If $\lambda\in\cspec(T)$, then $R_T(\lambda)$ is densely defined.
\end{proof}

\begin{proposition}[Spectral inclusion property of numerical range] \label{spectralInclusionNumericalRange}
Let $T$ be an operator on a Hilbert space. Then
\begin{enumerate}
\item $\pspec(T)\subseteq \NumRange(T)$;
\item $\rspec(T) \subseteq \NumRange(T)$;
\item $\apspec(T) \subseteq \overline{\NumRange(T)}$.
\end{enumerate}
In particular
\begin{enumerate} \setcounter{enumi}{3}
\item $\cspec(T) \subseteq \overline{\NumRange(T)}$;
\item if $T$ is closed, then $\spec(T) \subseteq \overline{\NumRange(T)}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Let $\lambda \in \pspec(T)$. Then $Tx = \lambda x$ for some unit vector $x$ and so
\[ \inner{x,Tx} = \inner{x,\lambda x} = \lambda \inner{x,x} = \lambda\norm{x}^2 = \lambda, \]
which means that $\lambda \in \NumRange(A)$.

(2) Let $\lambda \in \rspec(T)$. Then there exists a unit vector $x \in \im(\lambda\id - T)^\perp$, so
\[ 0 = \inner{x,(\lambda\id-T)x} = \lambda\inner{x}^2 - \inner{x,Tx} = \lambda - \inner{x,Tx}, \]
which means that $\lambda \in \NumRange(A)$.

(3) Let $\lambda \in \apspec(T)$. Then there exists a Weyl sequence $\seq{e_n}$ for $\lambda$ by \ref{WeylSequence}. Then
\[ \norm{(\lambda\id - T)e_n} = \norm{e_n}\;\norm{(\lambda\id - T)e_n} \geq |\inner{e_n, (\lambda\id - T)e_n}| = |\lambda - \inner{e_n,Te_n}| \to 0. \]
Thus $\lambda\in\overline{\NumRange(T)}$.

Finally we have $\cspec(T)\subseteq\apspec(T)$ by \ref{approximateSpectrum} and so $\cspec(T)\subseteq\overline{\NumRange(T)}$.

(4) We have $\cspec(T) \subseteq \apspec(T) \subseteq \overline{\NumRange(T)}$ by \ref{approximateSpectrum}.

(5) Immediate from \ref{spectrumDecomposition}.
\end{proof}

\subsection{Graph norm and inner product}
Let $T: H\not\to H'$ be a linear map between Hilbert spaces. Then $\graph(T)\subseteq H\oplus H'$. If we want to $H\oplus H'$ to be a Hilbert space, we set
\[ \inner{x\oplus x', y\oplus y'}_{H\oplus H'} = \inner{x,y}_H + \inner{x',y'}_{H'}, \]
then $\norm{x\oplus y}_{H\oplus H'} = \sqrt{\norm{x}_H^2 + \norm{y}_{H'}}$.

We define, for $x,y\in \dom(T)$,
\[ \inner{x,y}_{\graph(T)} = \inner{x,y}_H + \inner{Tx,Ty}_{H'}. \]

\section{Projectors and minimisation problems}
Every subspace is a convex, non-empty subset.
\begin{theorem}[Hilbert projection theorem]
Let $\mathcal{H}$ be a Hilbert space, $K$ a closed, convex, non-empty subset of $\mathcal{H}$.
\begin{enumerate}
\item There exists a unique element of $K$ of least norm. i.e.\ there exists a unique $k_0\in K$ such that
\[ \norm{k_0} = \inf\setbuilder{\norm{k}}{k\in K}. \]
i.e.\ $\min\setbuilder{\norm{k}}{k\in K}$ exists.
\item For any $h\in\mathcal{H}$ there exists a unique point $k_0$ in $K$ such that
\[ \norm{h-k_0} = \inf\{\norm{h-k}\;|\; k\in K\}. \]
We use this to define the distance $d(h,K) \defeq \norm{h-k_0}$.
\item If $K$ is a (closed) subspace, then $k_0$ is also the unique point in $K$ such that $(h-k_0)\perp K$.
\end{enumerate}
\end{theorem}
The idea for the first part of the proof is to take a sequence $\seq{\norm{k_i}}\to \inf\setbuilder{\norm{k}}{k\in K}$. By the parallelogram law $\seq{k_i}$ is Cauchy and by completeness it has a limit $k_0$.
\begin{proof}
(1) We can find a sequence $\seq{k_i}$ in $K$ such that $\norm{k_i}$ converges to $d = \inf\setbuilder{\norm{k}}{k\in K}$ by \ref{sequenceToSupInf}. By the parallelogram law
\begin{align*}
\norm{k_i-k_j}^2 &= 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4\norm{\frac{1}{2}(k_i+k_j)}^2 \\
&\leq 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4d^2
\end{align*}
the sequence $\seq{k_i}$ is Cauchy. So it converges to some $k_0$ in $K$ because $K$ is a closed subset of a complete space.

To prove uniqueness, take another $k_0'\in K$ such that $\norm{k_0'}=d$. By convexity $\tfrac{1}{2}(k_0 +k_0')\in K$, hence
\[ d\leq \norm{\tfrac{1}{2}(k_0+k_0')}\leq \tfrac{1}{2}(\norm{k_0}+\norm{k_0'}) = d. \]
So $\norm{\tfrac{1}{2}(k_0+k_0')} = d$. The parallelogram law gives
\[ d^2 = \norm{\frac{k_0+k_0'}{2}}^2 = d^2- \norm{\frac{k_0-k_0'}{2}}^2; \]
hence $\norm{k_0 - k_0'}^2 = 0$ and thus $h_0=k_0$.

(2) The element $k_0$ considered in point 1. is the point closest to a particular choice for $h$, namely $h=0$. For other $h$ consider the set $K-h$, which is again closed and convex.

(3) For all $k\in K$ and $a\in \mathbb{F}$, we have
\[ \norm{h-k_0}\leq \norm{h-k_0+ak} \]
and thus, by lemma \ref{orthogonality}, $(h-k_0)\perp k$, meaning $(h-k_0)\perp K$.

For the converse (i.e.\ uniqueness), suppose $f_0\in K$ such that $(h-f_0)\perp K$. Then for all $f\in K$ we have $(h-f_0)\perp (f_0 -f)$ so that
\begin{align*}
\norm{h-f}^2 &= \norm{(h-f_0) + (f_0-f)}^2 \\
&= \norm{h-f_0}^2 + \norm{f_0 - f}^2 \geq \norm{h-f_0}^2.
\end{align*}
So $\norm{h-f_0}=\inf\{\norm{h-k}\;|\; k\in K\} = d(h,K)$ and thus $f_0=k_0$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed vector subspace. Then $\mathcal{H} = K^\perp \oplus K$.
\end{corollary}
\begin{proof}
We need to prove every vector $x\in \mathcal{H}$ has a unique decomposition of the form
\[ x = y+z \qquad y\in K,\; z\in K^\perp. \]

Such a decomposition exists: we can take $y=k_0$ and $z = x-k_0$. We have already proved uniqueness. We can also give another argument for uniqueness: assume another such decomposition $x=y'+z'$. Then $y-y'= z-z'$ where the left side is in $K$ and the right in $K^\perp$. The only element in $K\cap K^\perp$ is $0$, so $y=y'$ and $z=z'$.
\end{proof}
The ability to make such decompositions in general is unique to Hilbert spaces, see theorem \ref{criterionHilbertSpace}.

\subsection{Orthogonal projection and decomposition}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. Given a subspace $K$ and an element $x \in \mathcal{H}$, we call the unique element $y\in K$ of the decomposition $K\oplus K^\perp$ the \udef{orthogonal projection} of $x$ on $K$. It is denoted $P_K(x)$. This defines a function $P_K:\mathcal{H}\to K$ called the \udef{orthogonal projection} on $K$.
\end{definition}

\begin{proposition}
Let $P$ be the orthogonal projection on a closed subspace $K$. Then
\begin{enumerate}
\item $P$ is a linear operator on $\mathcal{H}$;
\item $\norm{Px}\leq \norm{x}$ for all $x\in\mathcal{H}$;
\item $P^2 = P$;
\item $\ker P = K^\perp$ and $\im P = K$;
\item $\id_\mathcal{H} - P$ is the orthogonal projection of $\mathcal{H}$ onto $K^\perp$.
\end{enumerate}
\end{proposition}
\begin{proof}
These are mostly direct results of the decomposition. In particular 5. follows if we know $K^\perp$ is closed, which it is by proposition \ref{orthogonalComplementClosed}.
\end{proof}
\begin{corollary} \label{HilbertClosedSpaceOrthogonalDecomposition}
Let $\mathcal{H}$ ba a Hilbert space and $K$ a closed subspace, then $\mathcal{H} = K\oplus K^\perp$.
\end{corollary}
\begin{proof}
Let $P$ be the orthogonal projection on $K$. Then by \ref{directSumKernelImageIdempotent}
\[ \mathcal{H} = \im P \oplus \ker P = K\oplus K^\perp. \]
\end{proof}
\begin{corollary} \label{doubleComplementClosure}
Let $\mathcal{H}$ be a Hilbert space.
\begin{enumerate}
\item If $K$ is a subspace, then $(K^\perp)^\perp = \overline{K}$ is the closure of $K$.
\item If $A$ is a subset, then $(A^\perp)^\perp$ is the closed linear span of $A$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Assume $K$ is closed. Then using $0=(I-P_K)x\;\; \Leftrightarrow \;\; x=P_Kx$, we see
\[ (K^\perp)^\perp = \ker(I-P_K) = \im P_K = K. \]
Then, if $K$ is not closed, $(K^\perp)^\perp = (\overline{K}^\perp)^\perp = \overline{K}$, by proposition \ref{orthogonalComplementClosed}.

(2) Using \ref{OrthogonalComplementProperties} we calculate $(A^\perp)^\perp = (\Span(A)^\perp)^\perp = \overline{\Span(A)}$.
\end{proof}
\begin{corollary} \label{denseZeroComplement}
Let $A$ be a subset of a Hilbert space $\mathcal{H}$. Then $\Span(A)$ is dense in $\mathcal{H}$ \textup{if and only if} $A^\perp = \{0\}$.
\end{corollary}
\begin{proof}
The subspace $\Span(A)$ is dense in $\mathcal{H}$ iff $\overline{\Span(A)} = \mathcal{H}$ iff $(\Span(A)^\perp)^\perp = (A^\perp)^\perp = \mathcal{H}$ iff $A^\perp = \{0\}$.

In the last step we have used that $A^\perp$ is closed so that $((A^\perp)^\perp)^\perp = \overline{A^\perp} = A^\perp$, see \ref{orthogonalComplementClosed}.
\end{proof}

\subsubsection{Existence of orthonormal bases}
\begin{corollary}
Let $D$ be an orthonormal subset of a Hilbert space $\mathcal{H}$, then $D$ is an ortonormal basis \textup{if and only if} it is maximal.
\end{corollary}
\begin{proof}
This is a restatement of the previous corollary in the language of \ref{characterisationMaximalOrthonormalSet}.
\end{proof}
\begin{corollary}
Every Hilbert space has an orthonormal basis.
\end{corollary}
\begin{proof}
Every inner product space has a maximal orthonormal set by \ref{exitenceMaximalOrthonormalSet}. This maximal orthonormal set is an orthonormal set by the proposition.
\end{proof}
\begin{corollary} \label{HilbertOnBasisMaximal}
An orthonormal subset of a Hilbert space is an orthonormal basis \textup{if and only if} it is maximal.
\end{corollary}

\begin{lemma} \label{sumExpansionOrthogonalProjector}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed subspace. Let $\{e_i\}_{i\in I}$ be an orthonormal basis of $K$. Then
\[ P_K(x) = \sum_{i\in I} \inner{e_i,x}e_i. \]
\end{lemma}
\begin{proof}
We can extend $\{e_i\}_{i\in I}$ to an orthonormal basis $\{e_i\}_{i\in J}$ of $\mathcal{H}$. Then
\[ x = \sum_{i\in J}\inner{e_i,x}e_i = \sum_{i\in I}\inner{e_i,x}e_i + \sum_{i\notin I}\inner{e_i,x}e_i, \]
which is clearly a decomposition in $K\oplus K^\perp$. This is unique, so we have found $P_K(x)$.
\end{proof}

\subsubsection{When are inner product spaces complete?}
Notice that some of the results obtained for Hilbert spaces have one direction that is generally true for inner product spaces: in any inner product space we have
\begin{itemize}
\item $\overline{K}\subset (K^\perp)^\perp$;
\item $\Span(A)$ dense in $\mathcal{H}$ implies $A^\perp = \{0\}$;
\item if $D$ is an orthonormal basis, then it is maximal.
\end{itemize}
See \ref{orthogonalComplementClosed}, \ref{orthogonalComplementDenseSpace} and \ref{characterisationMaximalOrthonormalSet}.

The converses are only true for Hilbert spaces.
\begin{theorem} \label{criterionHilbertSpace}
Let $H$ be an inner product space. If any of the following hold, $H$ is a Hilbert space:
\begin{enumerate}
\item For any orthonormal set $D$,
\[ \text{$D$ is maximal} \quad\implies\quad \text{$D$ is an orthonormal basis.} \]
\item For any subset $A$, $A^\perp = \{0\}$ implies $\Span(A)$ is dense in $H$.
\item For any subspace $K$, we have $(K^\perp)^\perp = \overline{K}$.
\item For all closed subspaces $K$ we can decompose $H = K\oplus K^\perp$.
\end{enumerate}
\end{theorem}
\begin{proof}
We prove the first statement implies $H$ is a Hilbert space. The other three imply the first and thus that $H$ is a Hilbert space.
\begin{enumerate}
\item We prove the contrapositive: assume $H$ is not complete, we wish to show that 1. does not hold, i.e.\ there exists a maximal orthonormal subset of $H$ that is not an orthonormal basis.

Let $\mathcal{H}$ be the completion of $H$ and take a unit vector $v\in \mathcal{H}\setminus H$. Now working in the completion, we have the decomposition $\Span\{v\}\oplus \Span\{v\}^\perp$. Consider the subspace $\Span\{v\} + H = \Span\{v\}\oplus(H\cap \Span\{v\}^\perp)$. We can extend $\{v\}$ to a maximal orthonormal set $\{v\}\cup D$ by \ref{exitenceMaximalOrthonormalSet}.

We claim $D$ is the orthonormal set we want:

Firstly it is maximal.
Assume, towards a contradiction, that $D$ is not maximal in $H$, so there exists an orthonormal set $D'\supsetneq D$. Take $w\in D'\setminus D$ and let $w'$ be the normalisation of $w - \inner{v,w}v$. Then $w' \perp v$ and $w' \perp D$, so $\{v\}\cup D\cup\{w'\}$ is an orthonormal set in $\Span\{v\} + H$, which contradicts the maximality of $\{v\}\cup D$.

Secondly it cannot be total. Indeed if $\Closure_H(\Span(D)) = H\cap\overline{\Span(D)}$ were equal to $H$, then $H \subseteq \overline{\Span(D)}$ and thus $\mathcal{H} = \overline{H} \subseteq \overline{\Span(D)} \subseteq \mathcal{H}$, meaning $\overline{\Span(D)} = \mathcal{H}$. But $v\notin \overline{\Span(D)}$, so $\overline{\Span(D)} \neq \mathcal{H}$.

\item 2. clearly implies 1. We can also adapt the proof above to show 2. implies $H$ is a Hilbert space:
Assume $H$ is not complete and let $\mathcal{H}$ be the completion of $H$. There exists a $v\in \mathcal{H}\setminus H$. All orthogonal complements are taken in the completion.
The set
\[ U \defeq H\cap\{v\}^\perp \]
is not dense in $\mathcal{H}$ for the same reason $D$ was not total above. We claim that the orthogonal complement of $U$ in $H$ is $\{0\}$:
\[ U^\perp\cap H = \{0\}. \]
First we claim $U$ is dense in $\{v\}^\perp$: take a $w\in \{v\}^\perp$ and let $(x_n)_{n\in\N}\subseteq H$ converge to $w$ (this is possible because $w\in\mathcal{H}$ and $H$ is dense in $\mathcal{H}$). Fix some $x\in H$ such that $\inner{x,v}\neq 0$, then we have the following sequence in $U$ that converges to $w$:
\[ n\mapsto x_n - \inner{x_n,v}\frac{x}{\inner{x,v}}. \]
Then because $U$ is dense in $\{v\}^\perp$,
\[ U^\perp\cap H = \overline{U}^\perp\cap H = (\{v\}^\perp)^\perp \cap H = \Span\{v\}\cap H = \{0\}. \]
\item Assume 3. Let $D$ be a maximal orthonormal set. Then
\[ \overline{\Span(D)} = (\Span(D)^\perp)^\perp = (D^\perp)^\perp = \{0\}^\perp = H, \]
so $D$ is an orthonormal basis.
\item Assume 4. Let $D$ be a maximal orthonormal set. Then $D^\perp$ is a closed subspace, so
\[ H  = D^\perp \oplus (D^\perp)^\perp = \{0\} \oplus (D^\perp)^\perp = (\Span(D)^\perp)^\perp = \overline{\Span(D)}. \]
\end{enumerate}
\end{proof}

\subsubsection{Orthogonal decomposition}
\begin{theorem}
 A Banach space such all of its closed subspaces are complemented is isomorphic to a Hilbert space.
\end{theorem}
\begin{proof}
TODO Lindestrauss and Tzafriri in 1971. Only real??
\end{proof}

\begin{proposition} \label{directSumOrthogonalClosed}
Let $\mathcal{H}$ be a Hilbert space and let $\{V_i\}_{i\in I}$ be a family of closed, (pairwise) orthogonal subspaces. Then
\[ \bigoplus_{i\in I}V_i \qquad \text{is a closed subspace of $\mathcal{H}$.} \]
\end{proposition}
\begin{proof}
Let $(v_n)$ be a Cauchy sequence in $\bigoplus_{i\in I}V_i$ which converges to $w$. Let $v_{i,n}$ be the component of $v_n$ in $V_i$. By orthogonality we have
\[ \norm{v_n-v_m}^2 = \sum_{i\in I}\norm{v_{i,n}-v_{i,m}}^2. \]
Then
\[ \norm{v_{i,n}-v_{i,m}} \leq \norm{v_n-v_m} \]
which implies $(v_{i,n})_n$ is a Cauchy sequence in the closed space $V_i$ which therefore converges to $w_i\in V_i$. Now there are only a finite number of $i$ for which there exist non-zero $v_{i,n}$ (TODO proof!!!!). So then
\[ \lim_n v_n = \lim_n \sum_{i\in I}v_{i,n} = \sum_{i\in I}w_i \in \bigoplus_{i\in I}V_i \]
where the interchange of limits and last equality follow because the sums are finite.
\end{proof}

\begin{lemma} \label{cancellationOminus}
Let $\mathcal{H}$ be a Hilbert space and $A\supseteq B \supseteq C$ subspaces with $B$ closed. Then
\[ (A\ominus B)\oplus (B\ominus C) = A\ominus C.\]
\end{lemma}
\begin{proof}
Take $v\in(A\ominus B)\oplus (B\ominus C)$. Then either $\{v\}\perp C$ or $\{v\}\perp B$, but this implies $\{v\}\perp C$, so $v\in A\ominus C$.

Take $v\in A\ominus C$. We can uniquely write $v = v_1 + v_2 \in (A\ominus B)\oplus B = A$. We just need to show that $v_2\in B\ominus C$. Indeed assume $\inner{c,v_2}\neq 0$ for some $c\in C$. Then
\[ \inner{c, v} = \inner{c, v_1+v_2} = \inner{c,v_1}+\inner{c,v_2} = \inner{c,v_2} \neq 0, \]
so $v\notin A\ominus C$, a contradiction.
\end{proof}

\subsection{Projection and minimisation in finite-dimensional spaces}

\begin{lemma}
Let $K$ be a subspace of $\F^n$ spanned by the orthonormal basis $\{\vec{u}_i\}_{i=1}^k$. Then
\[ P_K = QQ^* \qquad\text{where}\qquad Q = \begin{bmatrix}
\vec{u}_1 & \vec{u}_2 & \hdots & \vec{u}_k
\end{bmatrix}. \]
\end{lemma}
\begin{proof}
$P_K(\vec{x}) = \sum_{i=1}^k\vec{u}_i\inner{\vec{u}_i,\vec{x}} = \sum_{i=1}^k\vec{u}_i \vec{u}_i^*\vec{x} = \left(\sum_{i=1}^k\vec{u}_i \vec{u}_i^*\right)\vec{x} = QQ^*\vec{x}$.
\end{proof}
\begin{corollary}
For any matrix $A$ with QR factorisation $A=QR$, we have
\[ P_{\Col(A)} = QQ^*. \]
\end{corollary}
In general $P_{\Col(A)} = A(A^*A)^{-1}A^*$.

\begin{proposition}[Normal equations]
Let $\{\vec{v}_i\}_{i=1}^k$ be linearly independent set of vectors in $\F^n$. Set $K = \Span\{\vec{v}_i\}_{i=1}^k$. Then for all $\vec{x}\in \F^n$
\[ P_K(\vec{x}) = \sum_{i=1}^k c_i \vec{v}_i, \]
where $\begin{bmatrix}
c_1 & c_2 & \hdots & c_k
\end{bmatrix}^\transp$ is the solution of
\[ \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix}\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{x}} \\ \inner{\vec{v}_2,\vec{x}} \\ \vdots \\ \inner{\vec{v}_k,\vec{x}}
\end{bmatrix}. \]
This system of linear equations is consistent, yielding a unique solution.
\end{proposition}
The equations in this proposition are known as \udef{normal equations} and the matrix
\[ G(\vec{v}_1, \ldots, \vec{v}_k) \defeq \begin{bmatrix}
\vec{v}_1^* \\ \vec{v}_2^* \\ \vdots \\ \vec{v}_k^*
\end{bmatrix}\begin{bmatrix}
\vec{v}_1 & \vec{v}_2 & \hdots & \vec{v}_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix} \]
is known as the \udef{Gram matrix} or \udef{Grammian}.
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A\in\F^{m\times n}$, $\vec{b}\in\F^m$ and $\vec{x}_0\in\F^n$. Then
\[ \min_{\vec{x}\in\F^n}\norm{\vec{b}-A \vec{x}} = \norm{\vec{b} - A \vec{x}_0} \]
if and only if
\[ A^*A \vec{x}_0 = A^* \vec{b}. \]
\end{proposition}
We regard $\vec{x}_0$ as the ``best approximate solution'' to the (not necessarily consistent) system $A \vec{x} = \vec{b}$.
\begin{proof}
TODO
\end{proof}

\section{Orthonormal bases}

Hamel basis / Schauder basis / Hilbert basis

Every Hilbert basis is Schauder basis if $V$ is separable.

Hamel basis too big in Banach space??

Necessity of completeness for existence of complete orthonormal system, i.e.\ orthonormal system $\{a_i\}_{i\in I}$ (so $a_i \cdot a_j = \delta_{ij}$) with
\[ v = \sum_{i\in I}(a_i \cdot v)a_i \]
for all $v$. This is equivalent with
\[ v \cdot w = \sum_{i\in I}(v\cdot a_i)(a_i \cdot w) \]
for all $v,w$.


\begin{theorem}[Riesz-Fischer]
Let $\{e_i\}_{i\in I}$ be an orthonormal basis of a Hilbert space $H$ and $\alpha: I\to \C$ a net. Then
\[ \sum_{i\in I}\alpha_i e_i \]
converges \textup{if and only if} $\sum_{i\in I}|\alpha_i|^2 < \infty$. 
\end{theorem}
\begin{proof}
If $\sum_{i\in I}\alpha_i e_i$ converges, then $\sum_{i\in I}|\alpha_i|^2$ is bounded by the Bessel inequality \ref{BesselInequality}.

By monotone convergence, $\sum_{i\in I}|\alpha_i|^2 < \infty$ is equivalent to saying the sum converges. By (ref TODO) $\alpha$ has finite support. So $\sum_{i\in I}\alpha_i e_i$ can be expressed as the series
\[ \sum_{k\in \N}\alpha_{i_k} e_{i_k}. \]
By completeness it is enough to show that $\seq{s_n} = \seq{\sum_{k=0}^n\alpha_{i_k} e_{i_k}}$ is Cauchy. Let $n < m$, then
\[ \norm{s_n - s_m}^2 = \norm{\sum_{k=m+1}^n\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n\norm{\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n |\alpha_{i_k}|^2 = \sum_{k=0}^n|\alpha_{i_k}|^2 -\sum_{k=0}^m|\alpha_{i_k}|^2.  \]
Since $\seq{\sum_{k=0}^n |\alpha_{i_k}|^2}$ is convergent, it is Cauchy and thus so is $\seq{s_n}$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $D$ be an orthonormal basis of $\mathcal{H}$. Then $\mathcal{H}$ is isometrically isomorphic to $\ell^2(D)$.
\end{corollary}
\begin{corollary}
Hilbert spaces whose orthonormal bases have the same cardinality are isometrically isomorphic.
\end{corollary}

??
\begin{lemma}
Let $(\Omega,\mathcal{A}, \mu)$ be a measure space. Then $L^2(\Omega, \mu)$ is separable \textup{if and only if} $\mu$ is $\sigma$-finite.
\end{lemma}

\begin{lemma}
Let $\{\phi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Omega, \mu)$ and $\{\psi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Lambda, \nu)$, then $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis of $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{lemma}
\begin{proof}
The set $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is orthonormal:
\[ \iint_{\Omega\times\Lambda} \phi_n(x)\psi_m(y)\overline{\phi_{n'}(x)\psi_{m'}(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega\phi_n(x)\overline{\phi_{n'}(x)}\diff{\mu(x)} \cdot \int_\Lambda\psi_m(y)\overline{\psi_{m'}(y)}\diff{\nu(y)} = \delta_{n,n'}\delta_{m,m'}, \]
using Fubini's theorem and the HÃ¶lder inequality (TODO refs).

To show $D = \{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis, we verify point 5. of \ref{totalONBParsevalEquivalence}: if $f\perp D$, then $f = 0$.

If $f\perp D$, then for all $m,n\in \N$
\[ 0 = \inner{f, \phi_n\psi_m} = \iint_{\Omega\times\Lambda}f(x,y)\overline{\phi_n(x)\psi_m(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega \left(\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \right)\overline{\phi_n(x)}\diff{\mu(x)}.  \]
Using point 5. of \ref{totalONBParsevalEquivalence} in $L^2(\Omega,\mu)$, we see that for all $m$ the function $x\mapsto\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)}$ is $0$ as an element of $L^2(\Omega, \mu)$, i.e.\ it is $0$ a.e. as a function of $x$. Let
\[ E_m = \setbuilder{x\in\Omega}{ \int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \neq 0} \]
and set $E = \bigcup_{m\in\N}E_m$.
Then
\[ \mu(E) =  \mu\left(\bigcup_{m\in \N}E_m\right) \leq \sum_{m\in\N}\mu(E_m) = 0. \]

For $x\notin E$, we have $\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} = 0$, so by the same logic $f(x,y) = 0$ for almost all $y$. 

Now $|f|^2$ is integrable and
\[ \iint_{\Omega\times \Lambda}|f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = \int_{\Omega\setminus E}\int_\Lambda |f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = 0, \]
so $f=0$ in $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{proof}

\section{Self-duality of Hilbert spaces}
\subsection{Riesz representation}
\begin{theorem}[Riesz-FrÃ©chet representation theorem] \label{rieszRepresentation}
Let $\mathcal{H}$ be a Hilbert space. For every continuous linear functional $\omega\in \dual{\mathcal{H}}$, there exists a unique $v_\omega\in\mathcal{H}$ such that
\[ \omega(x) = \inner{v_\omega, x} \qquad \forall x\in\mathcal{H}. \]
Moreover, $\norm{v_\omega}_\mathcal{H} = \norm{\omega}_{\dual{\mathcal{H}}}$.
\end{theorem}  

The idea of the proof is as follows: consider $\mathcal{H} \cong \ker\omega \oplus \im\omega$. So we can find a subspace $U\subseteq \mathcal{H}$ such that $\mathcal{H} = \ker\omega\oplus U$. Clearly $\dim U = \dim\im\omega = \dim\F = 1$. Between $1$-dimensional spaces there can only be one linear map, up to rescaling. This map is given by $x\mapsto \inner{v,x}$ for some $v\in U$, where the scaling determines the $v$. So we choose $v$ such that $\omega|_U = x\mapsto \inner{v,x}$.

Now we want extend this form of $\omega|_U$ to the whole of $\mathcal{H}$. This works exactly if $v\in(\ker\omega)^\perp$. So we need $U=(\ker\omega)^\perp$ which is true if and only if $\mathcal{H} = \ker\omega\oplus U = \ker\omega\oplus (\ker\omega)^\perp$, which only works in general if $\ker\omega$ is closed and $\mathcal{H}$ is a Hilbert space. Now $\ker\omega$ is closed if and only if it is continuous, by \ref{continuousMapCriterion}.

With this idea we give a full proof:
\begin{proof}
If $\ker\omega = \mathcal{H}$, we can take $v_\omega = 0$.

Assume $\ker\omega\neq \mathcal{H}$, then $(\ker\omega)^\perp\neq \{0\}$ by \ref{denseZeroComplement}, because $\ker\omega$ is closed (\ref{continuousMapCriterion}). So we can take a non-zero $u\in (\ker\omega)^\perp$. We can choose it such that $\omega(u) = 1$, by rescaling. Now let $h\in\mathcal{H}$. We can write $h = (h - \omega(h)u)+\omega(h)u\in\ker\omega\oplus (\ker\omega)^\perp$, because $\omega(h - \omega(h)u) = 0$. So
\[ 0 = \inner{u,h - \omega(h)u} = \inner{u,h} - \omega(u)\norm{u}^2. \]
If $v_\omega = \norm{u}^{-2}u$, then $\omega(h) = \inner{v_\omega, h}$ for all $h\in\mathcal{H}$.

For uniqueness: assume we can find two vectors $v_\omega,v_\omega'$ such that for all $h\in\mathcal{H}$ we have $\omega(h) = \inner{v_\omega, h} = \inner{v_\omega', h}$. Then $v_\omega - v_\omega'\perp \mathcal{H}$, so $v_\omega - v_\omega'= 0$.
\end{proof}
Together with lemma \ref{innerBoundedFunctionals} this gives:
\begin{corollary} \label{RieszIsometry}
Let $\mathcal{H}$ be a Hilbert space, $x, y\in \mathcal{H}$ and $f,g\in\dual{\mathcal{H}}$. Then
\begin{enumerate}
\item the map $C_\mathcal{H}:\mathcal{H}\to \dual{\mathcal{H}}: v\mapsto \inner{v,\cdot}$ is a bijective anti-linear isometry;
\item $C_\mathcal{H}(x)(y) = \inner{x,y}$ and $\inner{C_\mathcal{H}^{-1}(f), x} = f(x)$;
\item $\dual{\mathcal{H}}$ is a Hilbert space with $\inner{f,g}_{\dual{H}} = \inner{C_\mathcal{H}^{-1}(g), C_\mathcal{H}^{-1}(f)}$;
\item $C_{\dual{\mathcal{H}}}\circ C_{\mathcal{H}} = \evalMap$;
\item $\mathcal{H}$ is reflexive.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Restatement of the theorem.

(2) The first equation is a restatement of the definition of $C_\mathcal{H}$. For the second, we use the first and calculate
\[ \inner{C_\mathcal{H}^{-1}(f), x} = C_\mathcal{H}\big(C_\mathcal{H}^{-1}(f)\big)(x) = \big((C_\mathcal{H}\circ C_\mathcal{H}^{-1})(f)\big)(x) = f(x). \]

(3) Since $C_\mathcal{H}$ is an isometry, the parallelogram law holds in $\im(C_{\mathcal{H}}) = \dual{H}$. Then the Jordan-von Neumann theorem \ref{JordanVonNeumann} implies that the norm of $\dual{H}$ comes from an inner product. The inner product can be recovered from the norm by the polarisation identity \ref{polarisationIdentities}.

We can then calculate
\begin{align*}
\inner{f,g}_{\dual{\mathcal{H}}} &= \frac{1}{4}\sum_{k=0}^3 i^k\norm{i^kf+ g}^2_{\dual{\mathcal{H}}} \\
&= \frac{1}{4}\sum_{k=0}^3 i^k\norm{i^k(C_\mathcal{H}\circ C_\mathcal{H}^{-1})(f)+ (C_\mathcal{H}\circ C_\mathcal{H}^{-1})(g)}^2_{\dual{\mathcal{H}}} \\
&= \frac{1}{4}\sum_{k=0}^3 i^k\norm{C_\mathcal{H}\Big((-i)^kC_\mathcal{H}^{-1}(f)+ C_\mathcal{H}^{-1}(g)\Big)}^2_{\dual{\mathcal{H}}} \\
&= \frac{1}{4}\sum_{k=0}^3 i^k\norm{(-i)^kC_\mathcal{H}^{-1}(f)+ C_\mathcal{H}^{-1}(g)}^2_{\mathcal{H}} \\
&= \frac{1}{4}\sum_{k=0}^3 i^k\norm{i^{k+2}C_\mathcal{H}^{-1}(f)+ C_\mathcal{H}^{-1}(g)}^2_{\mathcal{H}} \\
&= \frac{1}{4}\sum_{k=0}^3 i^{k-2}\norm{i^{k}C_\mathcal{H}^{-1}(f)+ C_\mathcal{H}^{-1}(g)}^2_{\mathcal{H}} \\
&= \frac{1}{4}\sum_{k=0}^3 (-i)^{k}\norm{i^{k}C_\mathcal{H}^{-1}(f)+ C_\mathcal{H}^{-1}(g)}^2_{\mathcal{H}} \\
&= \overline{\frac{1}{4}\sum_{k=0}^3 i^{k}\norm{i^{k}C_\mathcal{H}^{-1}(f)+ C_\mathcal{H}^{-1}(g)}^2_{\mathcal{H}}} \\
&= \overline{\inner{C_\mathcal{H}^{-1}(f), C_\mathcal{H}^{-1}(g)}_{\mathcal{H}}} \\
&= \inner{C_\mathcal{H}^{-1}(g), C_\mathcal{H}^{-1}(f)}_{\mathcal{H}}.
\end{align*}

(4) We use (2) and (3) to calculate, for arbitrary $x\in \mathcal{H}$ and $f\in\dual{\mathcal{H}}$,
\[ \big((C_{\dual{\mathcal{H}}}\circ C_\mathcal{H})(x)\big)(f) = \inner{C_\mathcal{H}(x), f}_{\dual{\mathcal{H}}} = \inner{C^{-1}_{\mathcal{H}}(f), x} = f(x). \]
Thus $(C_{\dual{\mathcal{H}}}\circ C_\mathcal{H})(x) = \evalMap_x|_{\dual{\mathcal{H}}}$ and $C_{\dual{\mathcal{H}}}\circ C_\mathcal{H} = \evalMap$.

(5) Let $H$ be a Hilbert space. By \ref{normedReflexiveIFFSemireflexive}, it is enough to show that $\evalMap: H\to \bidual{H}$ is surjective.

Since both $C_{\dual{\mathcal{H}}}$ and $C_\mathcal{H}$ are surjective and $\evalMap = C_{\dual{\mathcal{H}}}\circ C_\mathcal{H}$, we have that $\evalMap$ is surjective.
\end{proof}
\begin{corollary}
Every bounded functional defined on a closed subspace of $\mathcal{H}$ can be extended to a functional on $\mathcal{H}$ with the same norm.
\end{corollary}
\begin{proof}
The functional on the closed subspace, say $K$, can be represented as $x\mapsto \inner{v,x}_K$ for some $v\in K$. The extended functional is then simply given by $x\mapsto \inner{v,x}_\mathcal{H}$.
\end{proof}

\begin{proposition}[Representation of sesquilinear forms] \label{sesquilinearRepresentation}
Let $\mathcal{H}_1,\mathcal{H}_2$ be Hilbert spaces over $\mathbb{F}$ and $h:\mathcal{H}_1,\mathcal{H}_2\to\mathbb{F}$ a bounded sesquilinear form. Then there exists a unique bounded operator $S:\mathcal{H}_1 \to \mathcal{H}_2$ such that
\[ h(x,y) = \inner{Sx,y}. \]
This operator has the property $\norm{S} = \norm{h}$.
\end{proposition}
\begin{proof}
For fixed $x$, $y\mapsto h(x,y)$ is a bounded linear functional, so by the Riesz representation theorem \ref{rieszRepresentation} this can be represented by a unique $v_x$. Let $S$ be the function $x\mapsto v_x$. Then $h(x,y) = \inner{Sx,y}$.

To prove this function $S$ is linear, take arbitrary $x_1,x_2\in \mathcal{H}_1;y\in \mathcal{H}_2$ and $\lambda \in \mathbb{F}$. Then
\begin{align*}
\inner{S(\lambda x_1+ x_2),y} &= h(\lambda x_1+ x_2, y) = \overline{\lambda} h(x_1,y)+h(v,y_2) \\
&= \overline{\lambda} \inner{Sx_1, y} + \inner{Sx_2, y} = \inner{\lambda Sx_1 + Sx_2,y},
\end{align*}
so $S$ is linear by lemma \ref{elementaryOrthogonality}.

The equality of norms follows from
\begin{align*}
\norm{h} = \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{|\inner{Sx,y}|}{\norm{x}\norm{y}} &\geq \sup_{\substack{x\neq 0 \\ Sx\neq 0}}\frac{|\inner{Sx,Sx}|}{\norm{x}\norm{Sx}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S} \\
&\leq \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{\norm{Sx}\norm{y}}{\norm{x}\norm{y}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S}
\end{align*}
where the second inequality is Cauchy-Schwarz.
\end{proof}

\subsection{Bras and kets}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. We define the
\begin{itemize}
\item \udef{bra} function $\bra{\cdot}: \mathcal{H}\to \dual{\mathcal{H}}: x\mapsto \inner{x, \cdot}$;
\item \udef{ket} function $\ket{\cdot}: \mathcal{H}\to \mathcal{H}: x\mapsto x$.
\end{itemize}
Since $\dual{\mathcal{H}} = \im(\bra{\cdot})$ and $\mathcal{H} = \im(\ket{\cdot})$, we can define the standard pairing
\[ \dual{\mathcal{H}}\times\mathcal{H} \to \C: \big(\bra{x}, \ket{y}\big) \mapsto \inner{x,y}. \]
\end{definition}
We have $\ket{\cdot} = \id_\mathcal{H}$.

\begin{lemma} \label{dualitySetHilbertSpace}
Let $\mathcal{H}$ be a Hilbert space and $x\in \mathcal{H}$. Then $\dualitySet(x) = \{\bra{x}\}$.
\end{lemma}
\begin{proof}
We have $\bra{x}\in \dualitySet(x)$ because
\[ \norm{\bra{x}}^2 = \inner[\big]{\bra{x}, \bra{x}}_{\dual{H}} = \inner{x,x} = \norm{x}^2. \]
Now take $\bra{y}\in \dualitySet(x)$. Then $\inner{y,y} = \inner{y,x} = \inner{x,x}$, so $|\inner{x,y}|^2 = \inner{x} \inner{y}$. By \ref{CauchySchwarz} this implies that $x = \lambda y$ for some $\lambda\in\F$. Thus $\inner{y,y} = \inner{y,\lambda y} = \lambda \inner{y,y}$ and so $\lambda = 1$.
\end{proof}

\subsection{Hilbert space adjoints}

\subsection{Weak convergence of vectors}
\begin{lemma}
Let $\mathcal{H}$ be a Hilbert space. The weak convergence $\sSet{\mathcal{H}, \sigma(\dual{\mathcal{H}}, \mathcal{H})}$ on $\mathcal{H}$ is the initial convergence w.r.t.
\[ \setbuilder{\inner{x,-}: \mathcal{H}\to \F}{x\in\mathcal{H}}. \]
\end{lemma}
\begin{proof}
By Riesz representation \ref{rieszRepresentation} every $f\in \dual{H}$ is of the form $\inner{v_f, -}$ for some $v_f\in \mathcal{H}$. Conversely, $\inner{x,-}\in \dual{\mathcal{H}}$, for all $x\in \mathcal{H}$.
\end{proof}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. We call the weak convergence $\sSet{\mathcal{H}, \sigma(\dual{\mathcal{H}}, \mathcal{H})}$ on $\mathcal{H}$ simply the \udef{weak convergence} and abbreviate $\sigma(\dual{\mathcal{H}}, \mathcal{H})$ by $\sigma$.
\end{definition}

\begin{example}
Let $\seq{e_n}$ be an orthonormal basis. Then the Riemann-Lebesgue lemma, \ref{RiemannLebesgueLemma}, gives $e_n \overset{\sigma}{\longrightarrow} 0$, even though clearly $e_n \not\to 0$.
\end{example}

So norm convergence is strictly stronger than weak convergence.

\begin{proposition} \label{weakHilbertSpaceConvergence}
Let $\mathcal{H}$ be a Hilbert space, $x\in \mathcal{H}$ and $\seq{x_n}\subseteq \mathcal{H}$. Then
\begin{enumerate}
\item $x_n \overset{\sigma}{\longrightarrow} x$ implies $\norm{x} \leq \liminf \norm{x_n}$;
\item $x_n \longrightarrow x$ \textup{if and only if} $x_n \overset{\sigma}{\longrightarrow} x$ and $\norm{x_n}\to \norm{x}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have, using the CSB inequality \ref{CauchySchwarz},
\[ \norm{x}^2 = |\inner{x,x}| = \lim_n |\inner{x,x_n}| = \liminf_n|\inner{x,x_n}| \leq \norm{x}\liminf_n\norm{x_n}. \]
We use that for convergent sequences, $\lim = \liminf$.

(2) The direction $\Rightarrow$ is clear, using the continuity of the norm.

For the converse, we have
\[ \norm{x-x_n}^2 = \norm{x}-2\Re(\inner{x,x_n}) + \norm{x_n} \to 0, \]
because $\norm{x_n} \to \norm{x}$ and $\inner{x,x_n} \to \inner{x,x} = \norm{x}^2$.
\end{proof}


\subsection{Strong and weak convergence of operators}
\begin{lemma}
Let $V_1,V_2$ be inner product spaces. Then the weak operator topology on $\Bounded(H_1,H_2)$ is the initial topology w.r.t.\ $\big\{\inner{y, (-)x}: T\mapsto \inner{y,Tx}\big\}_{x\in H_1, y\in H_2}$.
\end{lemma}

TODO introduce shifts earlier. \label{adjointMapNotSOTContinuous}
\begin{example}
Consider the left and right shifts $S_l$ and $S_r$ on $\ell^2(\N)$. Then
\begin{itemize}
\item $S_l^n \overset{SOT}{\longrightarrow} 0$, but $S_l^n \overset{norm}{\not\longrightarrow} 0$;
\item $S_r^n \overset{WOT}{\longrightarrow} 0$, but $S_r^n \overset{SOT}{\not\longrightarrow} 0$.
\end{itemize}
\end{example}

\begin{example}
The adjoint map is not continuous w.r.t. the strong operator convergence.
\begin{itemize}
\item Consider the left and right shifts $S_l$ and $S_r$ on $\ell^2(\N)$. Then $S_l^n \overset{SOT}{\longrightarrow} 0$, but $S_r^n = (S_l^*)^n \overset{SOT}{\not\longrightarrow} 0$.
\item In any infinite-dimensional Hilbert space, take some orthonormal set $\{e_n\}_{n\in \N}$. And consider the sequence of operators $\seq{\ketbra{e_1}{e_n}}$. This converges strongly to $0$, indeed
\[ \lim_n\norm{\ketbra{e_1}{e_n}\ket{x}} = \lim_n\norm{\ket{e_1}\inner{e_n, x}} = \norm{\ket{e_1}}\lim_n\inner{e_n, x} = \lim_n\inner{e_n, x} = 0, \]
for all vectors $x$ by the Bessel inequality \ref{BesselInequality}. The sequence of adjoints $\seq{\big(\ketbra{e_1}{e_n}\big)^*} = \seq{\ketbra{e_n}{e_1}}$ does not converge strongly to $0$:
\[ \lim_n\norm{\ketbra{e_n}{e_1}\ket{x}} = \lim_n\norm{\ket{e_n}\inner{e_1, x}} = \lim_n\norm{\ket{e_n}}\inner{e_1, x} = \lim_n\inner{e_1, x} = \inner{e_1, x} \neq 0. \]
\end{itemize}
\end{example}

\url{https://math.stackexchange.com/questions/1054288/the-set-of-all-normal-operators-on-a-hilbert-space-is-not-strongly-closed}


Note normal operators not $SOT$-closed!
\begin{proposition}
If $\seq{A_n}$ is a sequence of normal operators that converges to a normal operator $A$ in the strong operator topology, then $A_n^* \overset{SOT}{\longrightarrow} A^*$.
\end{proposition}

\begin{proposition}
Let $\seq{A_n}$ be a sequence of bounded operators on a Hilbert space and $A\in\Bounded(\mathcal{H})$. Then
\begin{enumerate}
\item if $A_n \overset{WOT}{\longrightarrow} A$, then $\norm{A}\leq \liminf\norm{A_n}$;
\item if $A_nx \longrightarrow Ax$ for all $x$ in a dense subset of $\mathcal{A}$ and $\seq{A_n}$ is a bounded sequence, then $A_n \overset{SOT}{\longrightarrow} A$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) For all unit vectors $x$ we have $A_nx \overset{\sigma}{\longrightarrow} Ax$, so, by \ref{weakHilbertSpaceConvergence}, $\norm{Ax} \leq \liminf_n\norm{A_nx}$. Then
\begin{align*}
\norm{A} = \sup_{\norm{x}=1}\norm{Ax} &\leq \sup_{\norm{x}=1}\liminf_n\norm{A_nx} \\
&= \sup_{\norm{x}=1}\sup_{k\in \N}\inf_{n\geq k}\norm{A_nx} \\
&\leq \sup_{k\in \N}\inf_{n\geq k}\sup_{\norm{x}=1}\norm{A_nx} = \liminf{n}\norm{A_n}.
\end{align*}

(2) TODO!!
\end{proof}


\section{Adjoints of operators}
\begin{definition}
Let $H,K$ be Hilbert spaces and $T: H\not\to K$ an operator. An \udef{adjoint} of $T$ is an operator $S: K\not\to H$ such that
\[ \inner{w,Tv}_K = \inner{S w,v}_H \quad \forall v\in \dom(T),\; \forall w\in \dom(S). \]
\end{definition}

\begin{lemma} \label{adjointRequirementSymmetric}
Let $H,K$ be Hilbert spaces, $T\in (H\not\to K)$ and $S\in(K\not\to H)$. Then $T$ is an adjoint of $S$ \textup{if and only if} $S$ is an adjoint of $T$.
\end{lemma}
\begin{proof}
The definition of adjoint is symmetric in $S$ and $T$.
\end{proof}

\subsection{The adjoint}
\subsubsection{The adjoint as a relation}
\begin{lemma}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) - S_2(x) \in \dom(T)^\perp$.

Conversely, let $S$ be an adjoint of $T$ and $x\in\dom(S)$. Then for all $v\in \dom(T)^\perp$ there exists an adjoint $S'$ such that $S'(x) = S(x) + v$.
\end{lemma}
\begin{proof}
For all $u\in \dom(T)$ we have
\[ \inner{S_1(x) - S_2(x), u}_H = \inner{S_1(x), u}_H - \inner{S_2(x), u}_H = \inner{x, Tu}_K - \inner{x, Tu}_K = 0. \]
So $(S_1(x) - S_2(x)) \in \dom(T)^\perp$.

For the converse, set $S' = S + \frac{\inner{x,\cdot}_K}{\inner{x,x}_K}v$. This is an adjoint: for all $a\in \dom(T), b\in \dom(S') = \dom(S)$ we have
\[  \inner{S' b,a}_H = \inner{Sb, a}_H + \frac{\inner{x,b}_K}{\inner{x,x}_K}\inner{v,a}_H = \inner{Sb, a}_H = \inner{b,Ta}_K. \]
\end{proof}
\begin{corollary} \label{agreementAdjoints}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) = S_2(x)$.
\end{corollary}
\begin{proof}
We have $\dom(T)^\perp = \overline{\dom(T)}^\perp = H^\perp = \{0\}$. So $S_1(x) - S_2(x) = 0$.
\end{proof}
\begin{corollary} \label{maximalAdjointIsOperator}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}} \]
is the graph of an operator \textup{if and only if} $T$ is densely defined.
\end{corollary}

\begin{definition}
Let $T: H\not\to K$ be an operator between Hilbert spaces. We define the adjoint $T^*$ as the \emph{relation} on $(H,K)$ with graph
\[ \graph(T^*) \defeq \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}}. \]
\end{definition}
Note that, by \ref{maximalAdjointIsOperator}, the adjoint is a function if and only if $T$ is densely defined.

\begin{lemma} \label{everywhereDefinedAdjointLemma}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. If $S$ is an adjoint of $T$ that is defined everywhere, then $T^* = S$.
\end{lemma}
\begin{corollary}
Let $H$ be a Hilbert space. Then $\id_H^* = \id_H$.
\end{corollary}

\begin{example}
Consider the left- and right-shift operators
\begin{align*}
&S_L: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto (x_{n+1})_{n\in\N} \\
&S_R: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto \left(\begin{cases}
x_{n-1} & (n\geq 1) \\ 0 &(n=0)
\end{cases}\right)_{n\in \N}.
\end{align*}
Then $S_L^* = S_R$ and $S_R^* = S_L$. To see this, take $\seq{x_n}, \seq{y_n}\in \ell^2(\N)$. Then
\[ \inner{S_L\seq{x_n}, \seq{y_n}} = \sum_{n\in\N}\overline{x_{n+1}}y_n = \overline{x_0}\cdot 0 + \sum_{n\in\N\setminus\{0\}}\overline{x_{n}}y_{n-1} = \inner{\seq{x_n}, S_R\seq{y_n}}. \]
Thus $S_L$ is an adjoint of $S_R$ and $S_R$ is an adjoint of $S_L$. We conclude with \ref{everywhereDefinedAdjointLemma}.
\end{example}

\begin{lemma} \label{adjointRelationLemma}
Let $T: H\not\to K$ be an operator between Hilbert spaces and $(x,y)\in K\times H$. Then $(x, y)\in T^*$ \textup{if and only if}
\[ \forall z\in\dom(T): \; \inner{x, T(z)} = \inner{y, z}. \]
\end{lemma}
\begin{proof}
$\boxed{\Rightarrow}$ If $(x, y)\in T^*$, then there exists an adjoint $f: K\not\to H$ such that $f(x) = y$. Then for all $z\in \dom(T)$ we have $\inner{x, T(z)} = \inner{f(x), z} = \inner{y, z}$.

$\boxed{\Leftarrow}$ The function defined by $f(x) = y$ and extended to $\Span\{x\}$ by linearity is an adjoint.
\end{proof}

\begin{proposition} \label{adjointDomain}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \dom(T^*) = \setbuilder{x\in K}{\text{$\dom(T)\to \F: u\mapsto \inner{x, Tu}$ is a bounded functional}}. \]
\end{proposition}
\begin{proof}
$\boxed{\subseteq}$ If $\omega_x: u\mapsto \inner{x, Tu}$ is bounded, then its domain can be extended by continuity to $\overline{\dom(T)}$, which is a Hilbert space. This extended functional has a Riesz vector $x^*$ such that $\omega_x = u\mapsto \inner{x^*, u}$. The linear operator with domain $\Span\{x\}$ that maps $x$ to $x^*$ is then an adjoint.

$\boxed{\supseteq}$ If $x\in\dom(T^*)$, then, using the Cauchy-Schwarz inequality,
\[ |\inner{x,Tu}| = |\inner{T^*x,u}| \leq \norm{T^*x}\;\norm{u}, \]
so the functional $u\mapsto \inner{x, Tu}$ is bounded.
\end{proof}
\begin{corollary}
The domain $\dom(T^*)$ is a vector space and in particular contains $0$.
\end{corollary}

\begin{proposition} \label{HilbertAdjointGaloisConnection}
Let $H, K$ be Hilbert spaces. Take $T\in (H\not\to K)$ and $S\in (K\not\to H)$. Then
\[ S \subseteq T^* \iff T\subseteq S^*. \]
Thus $(*,*)$ is an antitone Galois connection between $\sSet{(H\not\to K), \subseteq}$ and $\sSet{(K\not\to H), \subseteq}$.
\end{proposition}
\begin{proof}
We have $S \subseteq T^*$ iff $S$ is an adjoint of $T$ iff $T$ is an adjoint of $S$ (by \ref{adjointRequirementSymmetric}) iff $T\subseteq S^*$.
\end{proof}
\begin{corollary} \label{HilbertAdjointAntitone}
Let $S,T: H\not\to K$ be operators between Hilbert spaces such that $S\subseteq T$. Then $T^* \subseteq S^*$.
\end{corollary}
\subsubsection{Properties of the adjoint relation}

\begin{proposition} \label{adjointScalarMultiple}
Let $T$ be an operator between Hilbert spaces and $\lambda\in\C$. If $\lambda \neq 0$, then
\[ \begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*) = (\lambda T)^*. \]
\end{proposition}
Note that if $T^*$ is a function (i.e.\ if $T$ is densely defined), then $\begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*) = \overline{\lambda}T^*$. We write the former in the proposition, because we have not made this assumption.

If $\lambda = 0$ and $T: H\not\to K$, then
\[ \begin{pmatrix}
\id & 0 \\ 0 & 0
\end{pmatrix} \graph(T^*) = \big(0: \dom(T^*)\to H\big) \subseteq \big(0: K\to H\big) = (0 T)^*, \]
where the last equality is given by \ref{adjointBoundedEverywhereDefined}.
\begin{proof}
For the inclusion $\subseteq$, take $f$ to be an adjoint of $T$. It is enough to show that $\overline{\lambda}f$ is an adjoint of $\lambda T$. This follows from
\[ \inner{\overline{\lambda}f(w), v} = \lambda\inner{f(w), v} = \lambda\inner{w,Tv} = \inner{w,\lambda Tv} \qquad \forall w\in \dom(f), v\in \dom(T). \]

For the other inclusion, let $f$ be an adjoint of $\lambda T$. It is enough to show that $\overline{\lambda^{-1}}f$ is an adjoint of $T$, because then $f = \overline{\lambda}\cdot\overline{\lambda^{-1}}f \subseteq \begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*)$. Indeed
\[ \inner{\overline{\lambda^{-1}}f(w), v} = \lambda^{-1}\inner{f(w),v} = \inner{w,\lambda^{-1}\lambda Tv} = \inner{w,Tv} \quad \forall w\in \dom(f), v\in \dom(T). \]
\end{proof}

\begin{proposition} \label{adjointGraph}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\begin{align*}
\graph(T^*) &= \left( \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T) \right)^\perp 
=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp.
\end{align*}
If $T$ is densely defined, then $T^*$ is a closed operator.
\end{proposition}
\begin{proof}
We have
\[ \graph(T^*) = \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}}. \]
Take an adjoint $S$ and $(w, Sw)$ in $\graph(S)$. Then for all $v\in\dom(T)$:
\[ 0 = \inner{w, Tv}_K - \inner{Sw, v}_H = \inner{w, Tv}_K + \inner{Sw, -v}_H = \inner{(w, Sw), (Tv,-v)}_{K\oplus H}. \]
So $(Tv,-v) = \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix} (v,Tv) \in \graph(S)^\perp $.

The final equality follows from \ref{perpUnderIsometry}, using the fact that $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ is a surjective isometry.

If $T$ is densely defined, then $T^*$ is a function by \ref{maximalAdjointIsOperator}. It is closed by \ref{orthogonalComplementClosed}.
\end{proof}
\begin{corollary} \label{adjointDenselyDefinedClosable}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces.
Then
\begin{enumerate}
\item $\graph(T^{**}) = \overline{\graph(T)}$;
\item $T^*$ is densely defined \textup{if and only if} $T$ is closable;
\item If $T$ is closable, then $\overline{T} = T^{**}$.
\end{enumerate}
\end{corollary}
\begin{proof}
From the proposition we have
\begin{align*}
\graph(T^{**}) &=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T^*)^\perp 
=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\left(\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp\right)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}^2\graph(T)^{\perp\perp} = -\graph(T)^{\perp\perp}
= \overline{\graph(T)}.
\end{align*}
The right-hand side is the graph of an operator iff $T$ is closable and the left-hand side is the graph of an operator iff $T^*$ is densely defined, by \ref{maximalAdjointIsOperator}.

For a closable operator, the closure is defined by $\overline{\graph(T)} = \graph(\overline{T})$.
\end{proof}

\begin{proposition} \label{adjointBoundedEverywhereDefined}
Let $T: H\to K$ be a densely defined operator between Hilbert spaces. Then $\dom(T^*) = K$ \textup{if and only if} $T$ is bounded.
\end{proposition}
\begin{proof}
The direction $\Leftarrow$ is given by \ref{adjointDomain}.

For the other direction, note that $T^*$ is closed by \ref{adjointGraph}. Then $T^*$ is bounded by the closed graph theorem \ref{BanachClosedGraphTheorem}. We use the direction $\Leftarrow$ to see that $\dom(T^{**}) = H$. Similarly, $T^{**}$ is closed by \ref{adjointGraph} and bounded by the closed graph theorem \ref{BanachClosedGraphTheorem}. Thus $T\subseteq \overline{T} = T^{**}$ is bounded.
\end{proof}

An important application of this proposition is the Hellinger-Toeplitz theorem \ref{HellingerToeplitz}.

\begin{proposition} \label{adjointAlgebraicProperties}
Let $T,S$ be compatible operators between Hilbert spaces. Then
\begin{enumerate}
\item $S^* + T^* \subseteq (S+T)^*$;
\item $S^*T^* \subseteq (TS)^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Let $f$ be an adjoint of $S$ and $g$ an adjoint of $T$. It is enough to see that $f+g$ is an adjoint of $S+T$. Indeed $\forall w\in \dom(f + g), v\in \dom(S+T)$
\[ \inner{(f + g)(w), v} = \inner{f(w),v} + \inner{g(w), Tv} = \inner{w,Sv} + \inner{w,Tv} = \inner{w,(S+T)v}. \]

(2) Let $f$ be an adjoint of $T$ and $g$ an adjoint of $S$. It is enough to see that $gf$ is an adjoint of $TS$. Indeed
\[ \inner{g\circ f(w), v} = \inner{f(w), Sv} = \inner{w,TSv} \qquad \forall w\in \dom(g\circ f), v\in \dom(TS). \]
\end{proof}

\begin{example}
The inclusions in \ref{adjointAlgebraicProperties} are, in general, not equalities.
\begin{itemize}
\item If $S,T$ are densely defined, but $\dom(S+T) = \dom(S)\cap \dom(T)$ is not dense, then there can clearly not be an equality.
\item Let $T: H\to K$ be a densely defined unbounded operator. Then $\dom(T^*) \neq K$ by \ref{adjointBoundedEverywhereDefined}. Now we have
\[ T^* - T^* = \big(0: \dom(T^*) \to H\big) \subsetneq \big(0: K\to H\big) = \big(0: \dom(T) \to K\big)^* = (T-T)^*. \]
The penultimate equality follows from \ref{adjointBoundedEverywhereDefined}. In this case the domain of the sum is dense, but still there is no equality.
\end{itemize}
\end{example}

There exist various conditions that make the inclusions in \ref{adjointAlgebraicProperties} equalities.
\begin{proposition} \label{equalityAlgebraicPropertiesAdjoint}
Let $T,S$ be compatible operators between Hilbert spaces.
\begin{enumerate}
\item if $T$ is densely defined, $\dom(S) \subseteq \dom(T)$ and $\dom\big((S+T)^*\big) \subseteq \dom(T^*)$, then $S^* + T^* = (S+T)^*$;
\item if $T$ is densely defined, $\im(S)\subseteq \dom(T)$ and $\dom\big((TS)^*)\subseteq \dom(T^*)$, then $S^*T^* = (TS)^*$;
\item if $S$ is densely defined and $\im(S)$ has finite codimension, then $S^*T^* = (TS)^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) By \ref{adjointAlgebraicProperties}, we have
\[ (S+T)^* - T^* \subseteq (S+T-T)^* = S^*, \]
where the last equality is due to $\dom(S) \subseteq \dom(T)$. Now take $x,y$ such that $x\in \dom\big((S+T)^*\big)$. Then $T^*(x)$ exists and we have the implications
\begin{align*}
x(S+T)^*y \iff& x\big((S+T)^* - T^* + T^*\big)y \\
\iff& \exists z: \; x\big((S+T)^* - T^*\big)z \land (z+T^*(x) = y) \\
\implies& \exists z: \; x(S^*)z \land (z+T^*(x) = y) \\
\iff& x(S^* + T^*)y.
\end{align*}
Thus $(S+T)^* \subseteq S^* + T^*$.

(2) We need to prove $(TS)^* \subseteq S^*T^*$. Assume $(x,y)\in (TS)^*$. By \ref{adjointRelationLemma}, we have
\[ \forall z\in \dom(TS):\; \inner{x, TS(z)} = \inner{y, z}. \]
Because $\im(S)\subseteq \dom(T)$, we have $\dom(TS) = \dom(S)$. Also, by assumption, $x\in \dom(T^*)$. So we have
\[ \forall z\in \dom(S):\; \inner{x, TS(z)} = \inner{T^*(x), S(z)} = \inner{y, z}, \]
which means that $\big(T^*(x), y\big)\in S^*$, so $(x,y)\in S^*T^*$.

(3)
\end{proof}
\begin{corollary}
If $T$ is bounded and everywhere defined, then
\[ S^* + T^* = (S+T)^* \qquad\text{and}\qquad S^*T^* = (TS)^*. \]
\end{corollary}

\url{https://arxiv.org/pdf/1507.08418.pdf}
\url{https://link.springer.com/article/10.1007/s43036-020-00068-4}

\begin{lemma} \label{HilbertAdjointLemma}
Let $S,T\in\Bounded(H,K)$ and $\lambda \in \mathbb{F}$.
\begin{enumerate}
\item $(T^*)^* = T$;
\item $(S+T)^* = S^* + T^*$;
\item $(\lambda T)^* = \bar{\lambda}T^*$;
\item $\id_V^* = \id_V$.
\end{enumerate}
Let $T\in\Bounded(H_1,H_2), S\in\Bounded(H_2,H_3)$
\begin{enumerate}
\setcounter{enumi}{4}
\item $(ST)^* = T^*S^*$.
\end{enumerate}
\end{lemma}

\begin{note}
Useful exercise: The identities of \ref{HilbertAdjointLemma} can also be proven by elementary manipulations. For example, to prove (1), we take arbitrary $v\in H$ and $w\in K$, Then
\[ \inner{w,Tv} = \inner{T^*w,v} = \overline{\inner{v,T^*w}} = \overline{\inner{(T^*)^*v,w}} = \inner{w, (T^*)^*v}. \]
By lemma \ref{elementaryOrthogonality} we have $Tv = (T^*)^*v$ for all $v\in V$. 
\end{note}

\subsubsection{Adjoints of densely defined operators}
The adjoint of an operator is a function if and only the operator is densely defined.

\begin{proposition} \label{adjointRangeCriterion}
Let $S: K\not\to H$ and $T: H\not\to K$ be linear operators between Hilbert spaces. If
\[ \im(S\cap T^*) = H \qquad\text{and}\qquad \im(T\cap S^*) = K, \]
then $S$ and $T$ are densely defined with $S^* = T$ and $T^* = S$.
\end{proposition}
\begin{proof}
Notice that $S\cap T^*$ and $T\cap S^*$ are linear operators that are adjoints of each other.

We claim that they are densely defined: take $x\in \dom(S\cap T^*)^\perp$. Then there exists some $y\in H$ such that $x = (T\cap S^*)y$ because of surjectivity. Now for all $z\in \dom(S\cap T^*)$
\[ 0 = \inner{z,x} = \inner{z, (T\cap S^*)y} = \inner{(S\cap T^*)z, y}, \]
so $\inner{z',y} = 0$ for all $z'\in H$, by surjectivity. This means, by \ref{elementaryOrthogonality}, that $y=0$ and thus also $x = (T\cap S^*)y = 0$. We conclude that $\dom(S\cap T^*)^\perp = \{0\}$, meaning $(S\cap T^*)$ is densely defined. The argument for $(T\cap S^*)$ is similar.

It follows that $S$ and $T$ must be densely defined. We have, by \ref{kernelImageAdjoint},
\[ \ker(S) = \im(S^*)^\perp \subseteq \im(T\cap S^*)^\perp = \{0\}. \]
Similarly $\ker(T) = \ker(S^*) = \ker(T^*) = \{0\}$.

So we have $\ker(S) = \ker(T^*)$, $\im(S)\subseteq \im(S\cap T^*)$ and $\im(T^*)\subseteq \im(S\cap T^*)$. The equality $S = T^*$ follows from \ref{partialFunctionSubset}. The equality $T = S^*$ is similar.
\end{proof}


\begin{proposition} \label{kernelImageAdjoint}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \forall v\in K: \; (v,0)\in T^* \iff v\in \im(T)^\perp. \]
If $T^*$ is densely defined, this reduces to
\begin{enumerate}
\item $\ker(T^*) = \im(T)^\perp$;
\item $\ker(T) \subseteq \im(T^*)^\perp$;
\item if $T$ is closed, then $\ker(T) = \im(T^*)^\perp$
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Because $\dom(T)$ is dense in $H$, we have $\dom(T)^\perp = \{0\}$ by \ref{orthogonalComplementDenseSpace}. Take $v\in K$. We have the equivalences
\begin{align*}
v\in \im(T)^\perp &\iff \forall x \in\dom(T): \inner{v, T(x)} = 0 \\
&\iff \forall x \in\dom(T): \inner{v, T(x)} = \inner{v, 0} \\
&\iff (v,0)\in T^*,
\end{align*}
using \ref{adjointRelationLemma}.

Point (1) is a direct translation in the case that $T^*$ is a function.

For point (2) note that $T\subseteq T^{**}$ (by \ref{adjointDenselyDefinedClosable}) implies that $(v,0)\in T \implies (v,0)\in T^{**}$.

For point (3): in this case $\ker(T) = \ker(T^{**}) = \im(T^*)^\perp$.
\end{proof}
\begin{corollary}[Closed range theorem for Hilbert spaces]
Let $T$ be a closed, densely defined operator between Hilbert spaces. Then the following are equivalent:
\begin{enumerate}
\item $\im(T)$ is closed;
\item $\im(T^*)$ is closed;
\item $\im(T) = \ker(T^*)^\perp$;
\item $\im(T^*) = \ker(T)^\perp$.
\end{enumerate}
\end{corollary}
\begin{proof}
By the proposition and \ref{orthogonalComplementClosed}, we have $\overline{\im(T)} = \ker(T^*)^\perp$. This shows $(1) \Leftrightarrow (3)$ and $(2) \Leftrightarrow (4)$.

TODO equivalence $(1)\Leftrightarrow (2)$.
\end{proof}
TODO ref closed range theorem for Banach spaces. This is, e.g., the case when $T$ is bounded below, see \ref{boundedBelowClosedRange}.

\begin{proposition}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item $\im(T)$ is dense in $K$ \textup{if and only if} $T^*$ is injective;
\item if $T$ and $T^*$ are injective, then $(T^*)^{-1} = (T^{-1})^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) This is immediate from \ref{kernelImageAdjoint} and \ref{injectivityKernelTriviality}:
\[ \text{$\im(T)$ is dense} \quad\iff\quad \{0\} = \im(T)^\perp = \ker(T^*). \]

(2) We have $\graph(T^{-1}) = \begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)$. Also note that $\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}$ and $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ commute. Then we compute using \ref{adjointGraph}:
\begin{align*}
\graph((T^*)^{-1}) &= \begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\left(\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)\right)^\perp = \graph((T^{-1})^*).
\end{align*}
The penultimate equality follows from \ref{perpUnderIsometry}, using the fact that $\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}$ is a surjective isometry.
\end{proof}

\subsubsection{Adjoints of bounded operators}
\begin{proposition}
Let $T: H\to K$ be a densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item if $T\in\Bounded(H,K)$, then $T^*\in\Bounded(K,H)$;
\item if $T^*\in\Bounded(K,H)$, then $T$ is bounded. If $T$ is closed, then $T$ is defined everywhere.
\end{enumerate}
Assume $T\in\Bounded(H,K)$. Then
\begin{enumerate} \setcounter{enumi}{2}
\item $\norm{T} = \norm{T^*}$;
\item $T^* = C_H^{-1}T^tC_K$, where $C_K$ is the Riesz isometry from \ref{RieszIsometry}.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Assume $T\in\Bounded(H,K)$. Then $u\mapsto \inner{x,Tu}$ is a bounded functional for all $x\in K$, so $\dom(T^*) = K$ by \ref{adjointDomain}. Also $T^*$ is closed by \ref{adjointGraph}, so it is bounded by the closed graph theorem \ref{BanachClosedGraphTheorem}.

(2) Assume $T^*\in\Bounded(K,H)$. By the previous argument $T \subseteq \overline{T} = T^{**}\in\Bounded(H,K)$.

(3) The function $(x,u)\mapsto \inner{x,Tu}$ is a bounded sesquilinear form. By proposition \ref{sesquilinearRepresentation}, $T^*$ must be the unique $S$ from the proposition, which has norm $\norm{T}$.

(4) Finally we note that $C_H^{-1}T^tC_K$ is an adjoint with domain $K$ and conclude by \ref{everywhereDefinedAdjointLemma}.
\end{proof}

\begin{lemma}
The adjoint defines a map $*:\Bounded(H,K)\to \Bounded(K,H)$ that is anti-linear and continuous in the weak and uniform operator topologies. It is continuous in the strong operator topology \textup{if and only if} finite dimensional.
\end{lemma}
\begin{proof}
By the proposition the adjoint map is anti-linear. It is also bounded with norm $1$. Then by corollary \ref{boundedAntiLinearMaps} it must be bounded.

TODO
\end{proof}

\begin{proposition}
Let $H,K$ be Hilbert spaces and $T:H\to K$ a bijective bounded linear operator with bounded inverse. Then $(T^*)^{-1}$ exists and
\[ (T^*)^{-1} = (T^{-1})^*. \]
\end{proposition}
\begin{proof}
We prove $(T^{-1})^*$ is both a left- and a right-inverse of $T^*$: $\forall x\in H, y\in K$
\begin{align*}
\inner{T^*(T^{-1})^*x,y} &= \inner{x,T^{-1}Ty} = \inner{x,y} \\
\inner{x,(T^{-1})^*T^*y} &= \inner{TT^{-1}x,y} = \inner{x,y}
\end{align*}
So, by lemma \ref{elementaryOrthogonality}, $T^*(T^{-1})^* = \id_H$ and $(T^{-1})^*T^* = \id_K$.
\end{proof}

\begin{proposition} \label{normOfSquare}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\[ \norm{T^*T}= \norm{T}^2 = \norm{TT^*}. \]
\end{proposition}
\begin{proof}
For $\norm{T^*T}= \norm{T}^2$ first observe that
\[ \norm{T^*T} \leq \norm{T^*}\cdot\norm{T} = \norm{T}^2. \]
Conversely, $\forall x\in H$:
\[ \norm{T(x)}^2 = \inner{Tx,Tx} = \inner{T^*Tx,x} \leq \norm{T^*Tx}\cdot \norm{x} \leq \norm{T^*T}\cdot\norm{x}^2. \]
The other equality follows by applying the first to $T^*$ and using $\norm{T^*}=\norm{T}$.
\end{proof}

\subsection{Symmetric and self-adjoint operators}
\begin{definition}
Let $A$ be an operator on a Hilbert space.
\begin{itemize}
\item If $A^* = A$, we say $A$ is \udef{self-adjoint}.
\item If $A^* = -A$, we say $A$ is \udef{skew-adjoint}.
\end{itemize}
We denote the set of self-adjoint operators on a Hilbert space $H$ by $\SelfAdjoints(H)$.
\end{definition}

\begin{lemma} \label{selfAdjointLemma}
Let $A$ be a self-adjoint or skew-adjoint operator on a Hilbert space. Then
\begin{enumerate}
\item $A$ is densely defined;
\item $A$ is normal;
\item $A$ is closed;
\item $\lambda A$ is self-adjoint (resp.\ skew-adjoint) for all $\lambda\in \R$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) From $A = A^*$ or $-A = A^*$, we have that $A^*$ is a function. This implies that $A$ is densely defined by \ref{maximalAdjointIsOperator}.

(2) Sicne $A$ is densely defined, we can apply \ref{normalCriterion}.

(3) For any self-adjoint operator $A$, we have $A = A^* = A^{**} = \overline{A}$. Alternatively, note that all normal operators are closed (by definition).

(4) Since $A$ is densely defined, we have $(\lambda A)^* = \overline{\lambda}A^* = \lambda A^* = \pm\lambda A^*$, by \ref{adjointScalarMultiple}.
\end{proof}

\begin{lemma}
Let $A$ be an operator on a Hilbert space. Then $A$ is self-adjoint \textup{if and only if} $iA$ is skew-adjoint.
\end{lemma}
\begin{proof}
First suppose $A$ is self-adjoint, then $A$ is densely defined by \ref{selfAdjointLemma}, so $(iA)^* = \overline{i}A^* = -iA^*$ by \ref{adjointScalarMultiple}.

Now suppose $iA$ is skew-adjoint. Then $iA$ is densely defined by \ref{selfAdjointLemma}, so $A^* = \big(-i(iA)\big)^* = \overline{-i}(iA)^* = i(-iA) = A$, by \ref{adjointScalarMultiple}.
\end{proof}

\subsubsection{Domain related matters}
\begin{lemma} \label{symmetricOperatorAdjointInclusion}
Let $A$ be a densely defined operator on a Hilbert space. Then
\begin{enumerate}
\item $A$ is symmetric \textup{if and only if} $A\subseteq A^*$;
\item if $A$ is symmetric, then is $A$ closable and $\overline{A} = A^{**}$ is symmetric.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) $A$ is symmetric iff it is an adjoint of itself, iff $A\subseteq A^*$.

(2) From (1) we see that $A^*$ is densely defined, because the superset of a dense set is dense. Then $A$ is closable by \ref{adjointDenselyDefinedClosable}.

To show symmetry of $\overline{A}$, we have (using the properties implied by \ref{HilbertAdjointGaloisConnection}) $A^{**}\subseteq A^*$ from $A\subseteq A^*$ and thus
\[ \overline{A} = A^{**} \subseteq A^* = A^{***} = \overline{A}^*. \]
\end{proof}

A symmetric operator $A$ is self-adjoint if and only if $\dom(A) = \dom(A^*)$.

\begin{corollary}
A closed and densely defined symmetric operator $A$ is self-adjoint \textup{if and only if} $A^*$ is also symmetric.
\end{corollary}
\begin{proof}
If $A$ is self-adjoint, then $A^*$ is self-adjoint and thus symmetric,

If $A^*$ is symmetric, then $A\subseteq A^* \subseteq A^{**} = A$.
\end{proof}

\begin{example}
TODO example of closed symmetric operator that is not self-adjoint (see differential operator below)
\end{example}

\begin{theorem}[Hellinger-Toeplitz] \label{HellingerToeplitz}
Everywhere-defined symmetric operators are bounded.
\end{theorem}
\begin{proof}
Assume $A: H\to H$ an everywhere-defined symmetric operator. Then $\dom(A) = H$. Also $A\subseteq A^*$ by \ref{symmetricOperatorAdjointInclusion}. Thus $H = \dom(A) \subseteq \dom(A^*) \subseteq H$. So $\dom(A^*) = H$. By \ref{adjointBoundedEverywhereDefined}, $A$ is bounded. 
\end{proof}

\begin{proposition} \label{selfAdjointMaximal}
A self-adjoint operator cannot have a proper symmetric extension.
\end{proposition}
\begin{proof}
Assume $A$ self-adjoint and $A\subseteq B$ for some symmetric operator $B$. Then
\[ A \subseteq B \subseteq B^* \subseteq A^* = A, \]
so $A = B$. We have used \ref{symmetricOperatorAdjointInclusion} and \ref{HilbertAdjointAntitone}.
\end{proof}
\begin{corollary}
Let $A$ be a densely defined symmetric operator. If $\overline{A}$ is self-adjoint, then it is the unique self-adjoint extension of $A$.
\end{corollary}
Note that $\overline{A}$ is always an operator by \ref{symmetricOperatorAdjointInclusion}.
\begin{proof}
Let $B$ be a self-adjoint extension of $A$. Then $\overline{A} = A^{**}\subseteq B^{**} = B$, by \ref{HilbertAdjointAntitone}. This means that $B$ is symmetric extension of the self-adjoint operator $\overline{A}$, which, by the proposition, implies $B = \overline{A}$.
\end{proof}
In general it is possible for an unbounded,
symmetric operator to not have a self-adjoint extension or have multiple self-adjoint extensions, even if it is densely defined. (TODO example)

\begin{definition}
Let $A$ be a densely defined symmetric operator whose closure is self-adjoint. We call $A$
\begin{itemize}
\item \udef{essentially self-adjoint};
\item a \udef{core} for $A$.
\end{itemize}
\end{definition}

\begin{example}
Consider the operator
\[ A: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(A) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a) = 0 = f(b)}. \]
Then
\begin{align*}
\inner{g, Af} &= \int_{a}^b \overline{g(x)}i\od{f(x)}{x}\diff{x} \\
&= \overline{g(b)}f(b) - \overline{g(a)}f(a) - \int_{a}^b \Big(i \od{}{x}\overline{g(x)}\Big)f(x)\diff{x} \\
&= \int_a^b \overline{i \od{g(x)}{x}} f(x) \diff{x} = \inner{Ag, f}.
\end{align*}
So $A$ is symmetric and $\dom(A^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)}$. We cannot extend $\dom(A)$ while keeping $\dom(A^*)$ the same, because $A$ would no longer be symmetric due to boundary terms.

There are, however, multiple ways we can extend $A$ to a self-adjoint operator (in each case $\dom(A^*)$ must shrink).

Let $A_\alpha$, for $\alpha\in \R$, be the operator $A$ with domain
\[ \dom(A_\alpha) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(b) = e^{i\alpha}f(b)}. \]
We must have $\forall f\in \dom(A_\alpha)$ and $g\in\dom(A^*_\alpha)$ that
\[ \overline{g(b)}f(b) - \overline{g(a)}f(a) = f(a)\Big(e^{i\alpha}\overline{g(b)} - \overline{g(a)}\Big) = 0, \]
so we have $e^{-i\alpha}g(b) = g(a)$ and thus $g(b) = e^{i\alpha}g(a)$, which means $\dom(A_\alpha^*) = \dom(A_\alpha)$. So $A_\alpha$ is a self-adjoint extension of $A$ for all $\alpha\in \R$.

TODO: compare Aharonov-Bohm TODO show closure \url{https://math.stackexchange.com/questions/214218/uniform-convergence-of-derivatives-tao-14-2-7}.
\end{example}
Notice that the operator
\[ T: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(T) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)} \]
is not symmetric. In this case
\[  \dom(T^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a)=0=f(b)}, \]
so $\dom(T^*) \subseteq \dom(T)$.

\subsubsection{Spectrum and related criteria}
TODO: $iA$ dissipative!
\begin{lemma}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. If $\exists z \in \C\setminus\R: \; \im(A+z\id) = H$, then $A$ is densely defined.
\end{lemma}
\begin{proof}
Let $A+z\id$ be surjective and suppose, towards a contradiction that there exists an $y\perp \dom(A)$. Then $y = (A+z\id)x$ for some $x\in\dom(A)$ by surjectivity. Then
\[ 0 = \Im\inner{x,y} = \Im\inner{x, (A+z\id)x} = \cancel{\Im\inner{x,Ax}} + \Im \inner{x,zx} = \Im(z)\norm{x}^2. \]
By assumption, $\Im(z) \neq 0$, so $x=0$, meaning $y = (A+z\id)x = 0$ and thus $\dom(A)^\perp = \{0\}$.
\end{proof}

\begin{proposition} \label{symmetricPlusiBoundedBelow}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then $A + z\id_H$ is bounded below by $|\Im z|$ for all $z \in \C\setminus\R$.
\end{proposition}
\begin{proof}
We first calculate, $\forall x\in H$:
\[ \Im\inner{x, (A+ z\id_H)x} = \cancel{\Im\inner{x,Ax}} + \Im z\norm{x}^2. \]
Thus
\[ |\Im z|\;\norm{x}^2 = |\Im\inner{x, (A + z\id_H)x}| \leq |\inner{x, (A + z\id_H)x}| \leq \norm{x}\;\norm{(A + z\id_H)x}, \]
which means that $\norm{(A + z\id_H)x} \geq |\Im z|\;\norm{x}$, so $A + z\id_H$ is bounded below by $|\Im z|$.
\end{proof}
\begin{corollary} \label{approximateSpectrumSymmetricOperator}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then $\apspec(A) \subseteq \R$.
\end{corollary}
\begin{corollary}
The eigenvalues of a symmetric operator are real.
\end{corollary}
\begin{proof}
This is immediate using $\pspec(A)\subseteq \apspec(A)$. We can also give a direct calculation:

Assume there exists an $x\in \ker(\lambda\id_H - A)\setminus\{0\}$. Then $Ax = \lambda x$ and thus
\[ \lambda\norm{x}^2 = \lambda\inner{x,x} = \inner{x, \lambda x} = \inner{x,Ax} = \inner{Ax,x} = \inner{\lambda x, x} = \overline{\lambda}\inner{x,x} = \overline{\lambda}\norm{x}^2. \]
Because $\norm{x}^2 \neq 0$, we have $\lambda = \overline{\lambda}$, meaning $\lambda$ is real.
\end{proof}
\begin{corollary} \label{symmetricResolvent}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then for all $\lambda\in\C\setminus\R$, the resolvent $R_A(\lambda)$ well-defined and bounded by $\norm{R_A(\lambda)}\leq 1/|\Im \lambda|$.
\end{corollary}
Note this does not mean $\C\setminus\R\subseteq \res(A)$, as $\dom(R_A(\lambda))$ may not be all of $H$.
\begin{proof}
This is an application of \ref{boundedBelow}.
\end{proof}

\begin{proposition} \label{rangeSelfAdjointCriterion}
Let $A$ be a symmetric operator on a Hilbert space $H$. The following are equivalent:
\begin{enumerate}
\item $\forall z \in \C\setminus\R: \; \im(A+z\id) = H = \im(A+\overline{z}\id)$;
\item $\exists z \in \C: \; \im(A+z\id) = H = \im(A+\overline{z}\id)$;
\item $A$ is self-adjoint;
\item $\rspec(A) = \emptyset$;
\item $A$ is closed and $\forall z \in \C\setminus \R: \; \ker(A^*+z\id) = \{0\} = \ker(A^*+\overline{z}\id)$;
\item $A$ is closed and $\exists z \in \C\setminus \R: \; \ker(A^*+z\id) = \{0\} = \ker(A^*+\overline{z}\id)$.
\end{enumerate}
In this case $\spec(A) = \apspec(A)$.
\end{proposition}
Notice that in (2) we include $\R$ and in (6) we exclude $\R$.
\begin{proof}
$(1) \Rightarrow (2)$ Trivial.

$(2) \Rightarrow (3)$ From \ref{symmetricOperatorAdjointInclusion}, we have $A\subseteq A^*$ and thus $A+z\id = (A^* + z\id)\cap(A+z\id)$. Similarly, $A+\overline{z}\id = (A^* + \overline{z}\id)\cap(A+\overline{z}\id)$.

From point (1) of \ref{equalityAlgebraicPropertiesAdjoint}, we have $(A+z\id)^* = A^* + \overline{z}\id$ and $(A+\overline{z}\id)^* = A^* + z\id$.

We use \ref{adjointRangeCriterion} with $S = A+z\id$ and $T = A+\overline{z}\id$, which is applicable since
\begin{align*}
\im(S\cap T^*) &= \im\big((A+z\id)\cap (A+\overline{z}\id)^*\big) = \im\big((A+z\id)\cap (A^*+z\id)\big) = \im(A+z\id) = H \\
\im(T\cap S^*) &= \im\big((A+\overline{z}\id)\cap (A+z\id)^*\big) = \im\big((A+\overline{z}\id)\cap (A^*+\overline{z}\id)\big) = \im(A+\overline{z}\id) = H.
\end{align*}
Thus we have $A^* + \overline{z}\id = (A+z\id)^* = S^* = T = A+\overline{z}\id$. Subtracting $\overline{z}\id$ from each side yields the result.

$(3) \Rightarrow (4)$ Because self-adjoint operators are normal, we can use \ref{equalityKernelAdjointNormal}.

$(4) \Rightarrow (1)$ We have $\spec(A) = \apspec(A)$. Because $\apspec\subseteq \R$, by \ref{approximateSpectrumSymmetricOperator}, we have that $A+z\id$ is surjective for all $\C\setminus\spec(A) = \C\setminus\apspec(A) \supseteq \C\setminus\R$.

$(1,3) \Rightarrow (5)$ The closedness of $A$ follows from its self-adjointness.

Pick arbitrary $z \in \C\setminus\R$. Using \ref{kernelImageAdjoint}, we have
\[ \ker(A^* + z\id) = \ker(A+\overline{z}\id)^* =\im(A+\overline{z}\id)^\perp = H^\perp = \{0\}, \]
and something similar for $\ker(A^* + \overline{z}\id)$.

$(5) \Rightarrow (6)$ Trivial.

$(6) \Rightarrow (2)$ Pick some $z\in\C\setminus \R$ for which the statement holds. We have
\[ \overline{\im(A+z\id)} = \im(A+z\id)^{\perp\perp} = \ker\big((A^*+\overline{z})\big)^\perp = \{0\}^\perp = H. \]
We now just need to show that $\im(A+z\id)$ is closed. This follows because $A+z\id$ is bounded below by \ref{symmetricPlusiBoundedBelow} and thus we can apply \ref{boundedBelowClosedRange}.
\end{proof}
\begin{corollary}
Let $A$ be a symmetric operator on a Hilbert space $H$. The following are equivalent:
\begin{enumerate}
\item $A$ is essentially self-adjoint;
\item $\exists z \in \C\setminus\R: \; \overline{\im(A+z\id)} = H = \overline{\im(A+\overline{z}\id)}$;
\item $\exists z \in \C\setminus\R: \; \ker(A^*+z\id) = \{0\} = \ker(A^*+\overline{z}\id)$.
\end{enumerate}
\end{corollary}
\begin{corollary}
Every surjective symmetric operator is self-adjoint.
\end{corollary}
\begin{proof}
Take $z=0$ in point (1).
\end{proof}

\begin{proposition}
Let $A$ be a closed symmetric operator. Then one of the following cases holds:
\begin{itemize}
\item $A$ is self-adjoint, in which case $\spec(A) \subseteq \R$;
\item $\spec(A) = \overline{\C^{\uparrow}}$;
\item $\spec(A) = \overline{\C^{\downarrow}}$;
\item $\spec(A) = \C$.
\end{itemize}
If $A$ is not densely-defined, then the last case holds.
\end{proposition}
We have denoted the closed upper half plane $\overline{\C^{\uparrow}}$ and the closed lower half plane $\overline{\C^{\downarrow}}$.
\begin{proof}
First assume $A$ self-adjoint, then $\spec(A)\subseteq \R$ by a combination of \ref{approximateSpectrumSymmetricOperator} and \ref{rangeSelfAdjointCriterion}.

Now note that if there exists a real $\lambda\in\R$ such that $\lambda \in \res(A)$, then in particular $\lambda\id -A$ is surjective, so $A$ is self-adjoint by \ref{rangeSelfAdjointCriterion}.

Now assume $A$ not self-adjoint and pick a $\lambda\in \C^{\uparrow}$. From \ref{rangeSelfAdjointCriterion} we must have either $\lambda\in\spec(A)$ or $\overline{\lambda}\in\spec(A)$ (or both).

If $\lambda\in \res(A)$, then $\C^\uparrow \subseteq \res(A)$ and if $\overline{\lambda}\in\res(A)$, then $\C^\downarrow \subseteq \res(A)$.

Indeed take some $\mu\in\C$.
By \ref{symmetricResolvent} we only need to check surjectivity of $\mu\id - A$. We calculate
\begin{align*}
(\mu\id - A)R_A(\lambda) &= (\mu\id -\lambda\id+\lambda\id - A)R_A(\lambda) \\
&= (\mu-\lambda)R_A(\lambda) + (\lambda\id-A)R_A(\lambda) \\
&= (\mu-\lambda)R_A(\lambda) + \id \\
&= \id - (\lambda-\mu)R_A(\lambda).
\end{align*}
Now, using \ref{symmetricResolvent}, we have
\[ \norm{(\lambda-\mu)R_A(\lambda)} \leq |\mu-\lambda| \, |\Im(\lambda)|^{-1}. \]
If $|\mu-\lambda| < |\Im(\lambda)|$, then $(\lambda-\mu)R_A(\lambda)$ is a contraction and we can apply the Neumann series formula \ref{NeumannSeries} to see that $(\mu\id - A)R_A(\lambda)$ is bijective. In particular $\mu\id - A$ is surjective.

We can iterate this construction to cover the whole of $\C^\uparrow$. The argument for $\overline{\lambda}$ is similar.
\end{proof}

\begin{example}
Spectrum half plane TODO \url{https://math.stackexchange.com/questions/893899/spectrum-of-symmetric-non-selfadjoint-operator-on-hilbert-space}

\url{https://math.stackexchange.com/questions/925097/spectrum-of-self-adjoint-operator-on-hilbert-space-real}
\end{example}

\begin{proposition} \label{symmetryAdjointConstructions}
Let $T$ be a densely defined operator on a Hilbert space $H$. Then
\begin{enumerate}
\item $T+T^*$ is symmetric;
\item $T^*T$ and $TT^*$ are symmetric.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We use \ref{adjointAlgebraicProperties} to get
\[ T+T^* \subseteq T^{**} + T^* \subseteq (T+T^*)^*. \]
We conclude by \ref{symmetricOperatorAdjointInclusion}.

(2) We use \ref{adjointAlgebraicProperties} to get
\[ T^*T \subseteq T^*T^{**} \subseteq (T^*T)^* \qquad\text{and}\qquad TT^* \subseteq T^{**}T^* \subseteq (TT^*)^*, \]
which means that $T^*T$ and $TT^*$ are symmetric by \ref{symmetricOperatorAdjointInclusion}.
\end{proof}

\begin{theorem}[von Neumann] \label{vonNeumannTheoremSquareSelfAdjoint}
Let $T$ be a densely defined and closed operator on a Hilbert space $H$. Then
\begin{enumerate}
\item both $T^*T$ and $TT^*$ are self-adjoint;
\item both $\dom(T^*T)$ and $\dom(TT^*)$ are essential domains of $T$.
\end{enumerate}
\end{theorem}
\begin{proof}
(1) Because $T^*$ is closed, $\graph(T^*)$ is closed in $H\oplus H$. Thus
\begin{align*}
H\oplus H &= \graph(T^*) \oplus \graph(T^*)^\perp \\
&= \graph(T^*) \oplus \left(\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph{T}\right)^{\perp\perp} \\
&= \graph(T^*) \oplus \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph{T}.
\end{align*}
The last equality holds because $\graph(T)$ is closed (and $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ is a homeomorphism).

Then for all $v\in H$, we can write
\[ \begin{pmatrix}
0 \\ v
\end{pmatrix} = \begin{pmatrix}
y \\ T^*y
\end{pmatrix} + \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
x \\ Tx
\end{pmatrix} = \begin{pmatrix}
y - Tx \\ T^*y + x
\end{pmatrix}. \]
So $y = Tx$ and $v = T^*y + x = T^*Tx + x = (T^*T +\id)x$, which means that $T^*T +\id$ is surjective. Since $T^*T$ is symmetric, by \ref{symmetryAdjointConstructions}, it is self-adjoint by \ref{rangeSelfAdjointCriterion}.

We can show $TT^* + \id$ is surjective by writing
\[ \begin{pmatrix}
v \\ 0
\end{pmatrix} = \begin{pmatrix}
y \\ T^*y
\end{pmatrix} + \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
x \\ Tx
\end{pmatrix} = \begin{pmatrix}
y - Tx \\ T^*y + x
\end{pmatrix}, \]
so $x = -T^*y$ and $v = y - Tx = y+TT^*y = (TT^* + \id)y$.

(2) We need to show that $\dom(T^*T)$ is dense in $\dom(T)$ w.r.t. the graph norm. Take $h\in \dom(T^*T)^{\perp_{\graph(T)}}$. Then, for all $x\in \dom(T^*T)$, we have
\[ 0 = \inner{x,h}_{\graph(T)} = \inner{x,h} + \inner{Tx,Th} = \inner{x,h} + \inner{T^*Tx,h} = \inner{(\id + T^*T)x,h}. \]
Thus $h\in \im(\id+T^*T)^\perp = H^\perp = \{0\}$ and thus $h=0$ by the calculations in (1). So the orthogonal complement of $\dom(T^*T)$ w.r.t.\ the graph inner product is $\{0\}$, which shows density.

The argument for $\dom(TT^*)$ is similar.
\end{proof}

\begin{example}
Let $T$ be a densely defined operator. Then $T+T^*$ and $T^*T$ are in general not self-adjoint. Closedness of $T$ is enough to make $T^*T$ self-adjoint. This is not the case for $T+T^*$.
\begin{itemize}
\item If $T$ is not closed, then $T+T^* \subsetneq T^{**} + T^* \subseteq (T+T^*)^*$.
\item It is even not necessarily self-adjoint if $T$ is closed. Let $T$ be a closed, symmetric, but not self-adjoint operator, for example.
\end{itemize}
\end{example}

\begin{proposition}
Let $A$ be a self-adjoint operator on a Hilbert space. Then $A$ is positive \textup{if and only if} $\spec(A)\subseteq \interval[co]{0,\infty}$.
\end{proposition}
\begin{proof}
\ref{closureNumericRangeConvexHullSpectrum}
\end{proof}

\begin{proposition}
Let $A$ be a self-adjoint operator. Then
\begin{enumerate}
\item $\inf \sigma(A) = \inf\NumRange(A)$;
\item $\sup \sigma(A) = \sup\NumRange(A)$.
\end{enumerate}
\end{proposition}
\begin{proof}
\ref{closureNumericRangeConvexHullSpectrum}
\end{proof}

\begin{proposition}
Let $T$ be a densely defined self-adjoint operator. Then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item let $\lambda_1,\lambda_2 \in \pspec(T)$ and $\lambda_1\neq \lambda_2$, then 
\[ \Null(\lambda_1\id - T) \perp \Null(\lambda_2 \id - T). \]
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}


\begin{proposition}
Let $T$ be a symmetric operator on a Hilbert space $H$. Then
\begin{enumerate}
\item the eigenvalues of $T$ are real;
\item the eigenvectors corresponding to distinct eigenvalues are orthogonal.
\end{enumerate}
\end{proposition}
\begin{proof}
This is an application of \ref{eigenspaceOrthogonalAdjoint} and \ref{adjointSpectrumNoResidual}.
\end{proof}

\subsubsection{Compact self-adjoint operators}
\begin{proposition}
Every compact self-adjoint operator $L$ on a nontrivial Hilbert space has an eigenvalue $\lambda$ with $|\lambda| = \norm{L}$.
\end{proposition}

\begin{proposition}
Let $A$ be a compact self-adjoint operator. Then the only possible accumulation point of $\spec(A)$ is $0$.
\end{proposition}
TODO self-adjoint not necessary? See \ref{spectrumCompactOperator}?
\begin{proof}
Assume $\spec(A)$ is infinite. Then take $\seq{\lambda_n}\subset \spec(A)$. Any associated sequence $\seq{x_n}$ of eigenvectors is orthogonal. We can take it to be orthonormal. By \ref{limitCompactImageOrthonormalSequence} we have
\[ 0 = \lim_{n\to\infty} \norm{Ax_n}^2 = \lim_{n\to\infty}\inner{Ax_n,Ax_n} = \lim_{n\to\infty}\lambda_n^2\inner{x_n,x_n} = \lim_{n\to\infty}\lambda_n^2, \]
so $\seq{\lambda_n}$ converges to $0$.
\end{proof}

\begin{theorem}
Every spectral value $\lambda\neq 0$ of a compact self-adjoint linear
operator $A : H \to H$ is an eigenvalue of finite multiplicity that can only
accumulate at $\lambda = 0$. Conversely, a self-adjoint operator having these
properties is compact.
\end{theorem}
\begin{proof}
TODO See \ref{spectrumCompactOperator}
\end{proof}

TODO we can attain eigenvectors due to compactness and extreme value theorem. Cfr. Courant-Fisher-Weyl.

\subsubsection{Self-adjoint extensions of symmetric operators}
\paragraph{Cayley transform}
Consider the MÃ¶bius transform
\[ \C\setminus\{\overline{\lambda}\} \to \C: x\mapsto \frac{x - \lambda}{x-\overline{\lambda}} \qquad \text{for some $\lambda\in\C\setminus\R$.} \]
This transform maps
\begin{itemize}
\item the real line to $\T\setminus\{1\}$;
\item the half-plane above / below the real line containing $\lambda$ to the interior of the unit disk;
\item the half plane containing $\overline{\lambda}$ to the exterior of the unit disk;
\item in particular $\lambda \mapsto 0$ and $\overline{\lambda} \mapsto \infty$.
\end{itemize}
Conventional choice: $\lambda = i$.

\paragraph{Defect indices}
Or deficiency(?)
\url{https://link-springer-com.ezproxy.ulb.ac.be/content/pdf/10.1007/978-94-007-4753-1.pdf}

Cfr. dilation theory through Cayley transform.

See also Conway.

\subsubsection{Positive operators}
All positive operators are symmetric, \ref{positiveOperatorSymmetric}. They are not all self-adjoint (as in the bounded case), but can always be extended to a self-adjoint operator (which is not true in general for symmetric operators).

\begin{lemma} \label{negativeOperatorsDissipative}
Let $A$ be a positive operator on a Hilbert space $H$. Then $-A$ is dissipative.
\end{lemma}
\begin{proof}
For all $x\in H$, we have $\Re\inner{x,-Ax} = -\inner{x,Ax} \leq 0$. Since $\bra{x} \in \dualitySet(x)$, by \ref{dualitySetHilbertSpace}, we have that $A$ is dissipative by \ref{dissipativeNegativeRealPart}.
\end{proof}

\begin{definition}
Let $A$ be a positive operator on a Hilbert space $H$. We call
\[ H_A \defeq \setbuilder{x\in H}{\exists \seq{x_n}\subseteq \dom(A): \; \text{$\seq{x_n}$ is $\inner{}_{A+\id}$-Cauchy and $\seq{x_n}\overset{\inner{}}{\longrightarrow} x$}} \]
the \udef{form domain} of $A$.
\end{definition}

\begin{lemma}
Let $A$ be a positive operator on a Hilbert space $H$ with form domain $H_A$. Then
\begin{enumerate}
\item $H_A$ is the completion of $\sSet{\dom(A), \inner{\cdot, \cdot}_{A+\id}}$;
\item $\inner{\cdot, \cdot}_A$ can be extended to an inner product on $H_A$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) By \ref{energyNormTopology}, every $\inner{}_{A+\id}$-Cauchy sequence is $\inner{}$-Cauchy

TODO: what is convergence on $H_A$??

----

We use \ref{completenessCriterion} to show completeness. Take some Cauchy sequence $\seq{x_n}$ in $\sSet{\dom(A), \inner{\cdot, \cdot}_{A+\id}}$. Then $\seq{x_n}$ is a Cauchy sequence in $\sSet{H,\inner{\cdot, \cdot}}$ by \ref{energyNormTopology} and \ref{uniformContinuityGroupHomomorphism}. 

(2) We extend $\inner{\cdot, \cdot}_A$ using the fact that we can extend $\inner{\cdot, \cdot}_{A+\id}$ to $H_A$ and the equation
\[ \inner{x, y}_A = \inner{x, y}_{A+\id} - \inner{x,y}. \]
\end{proof}




\begin{lemma}
Let $A$ be a positive operator on a Hilbert space $H$. Consider the operator $B$ defined by
\begin{align*}
\dom(B) &\defeq \setbuilder{x\in \Closure_{\inner{}_{A+\id_H}}\big(\dom(A)\big)}{\exists x'\in H: \forall y\in \dom(A): \; \inner{x, y}_{A+\id} = \inner{x', y}} \\
Bx &= x' - x 
\end{align*}
Then
\begin{enumerate}
\item $A\subseteq B$;
\item $B$ is positive;
\item $\im(B+\id_H) = H$.
\end{enumerate}
\end{lemma}
\begin{proof}

\end{proof}

\begin{theorem}[Friedrich's extension]
Let $A$ be a positive symmetric operator on a Hilbert space $H$. Then $A$ has a unique positive self-adjoint extension $\widetilde{A}$ with domain $\dom(\widetilde{A}) \subseteq \Closure_{\inner{}_{A+\id_H}}(\dom(A))$.
\end{theorem}
By \ref{energyNormTopology}, we have
\[ \dom(\widetilde{A}) \subseteq \Closure_{\inner{}_{A+\id_H}}(\dom(A)) \subseteq \Closure_{\norm{\cdot}}(\dom(A)). \]
\begin{proof}
Set $H_A \defeq \Closure_{\inner{}_{A+\id_H}}(\dom(A))$.

For \undline{existence}, we can construct the operator $\widetilde{A}$ as follows:
\begin{align*}
\dom(\widetilde{A}) &\defeq \setbuilder{x\in H_A}{\exists x'\in H:\forall y\in H_A:\; \inner{y,x}_{A+\id} = \inner{y,x'}} \\
\widetilde{A}x &\defeq x' - x.
\end{align*}
Now $\widetilde{A}$ is an extension of $A$, because for all $x\in \dom(A)$, we can take $x' = Ax + x$. So $\widetilde{A}x = Ax$.

But $\dom(\widetilde{A})$ may be larger than $\dom(A)$, because we can extended $\inner{y,x}_{A+\id}$ to be defined on all of $H_A$ by continuity.

By construction $\dom(\widetilde{A}) \subseteq \Closure_{\norm{\cdot}_{E(A+\id)}}(\dom(A))$.

Now we claim $\im(\widetilde{A} + \id) = H$. Indeed for any $x'\in H$, the functional $H_A \to H_A: y\mapsto \inner{y,x'}$ is bounded. By Riesz representiation \ref{rieszRepresentation}, we can find an $x\in H_A$ such that $\inner{y,x}_{A+\id} = \inner{y,x'}$. Thus $(\widetilde{A} + \id)x = x'$.

By \ref{rangeSelfAdjointCriterion} we conclude that $\widetilde{A}$ is self-adjoint. 

For \undline{uniqueness}, assume there exists a second such extension $\widehat{A}$. For all $y\in \dom(A)$ and $x\in \dom(\widehat{A})$, we have
\[ \inner{y, (\widehat{A}+\id)x} = \inner{(\widehat{A}+\id)y, x} = \inner{(A+\id)y, x} = \overline{\inner{x, (A+\id)y}} = \overline{\inner{x, y}_{A+\id}} = \inner{y, x}_{A+\id}. \]
By continuity this holds for all $y\in H_A$. And thus by definition $\widetilde{A}x = \widehat{A}x$ for all $x\in\dom(\widetilde{A})$. Thus $\widetilde{A} \subseteq \widehat{A}$, but self-adjoint operators are maximal by \ref{selfAdjointMaximal}, so $\widetilde{A} = \widehat{A}$.
\end{proof}

\subsubsection{Bounded self-adjoint operators}
\begin{lemma}
Let $A, B\in\Bounded(H)$. Then
\begin{enumerate}
\item $A^*A, AA^*$ and $A+A^*$ are self-adjoint;
\item if $A,B$ are self-adjoint, then $AB$ is self-adjoint \textup{if and only if} $A,B$ commute.
\end{enumerate}
\end{lemma}
\begin{corollary}
Let $A\in\Bounded(H)$. Then there exist unique self-adjoint operators $S,T$ such that
\[ A = S+iT \qquad A^* = S-iT. \]
\end{corollary}
\begin{proof}
Indeed $S = (A+A^*)/2$ and $T = (A-A^*)/2i$ are self-adjoint.
\end{proof}
\begin{corollary}
The operator $A$ is normal \textup{if and only if} $S,T$ commute.
\end{corollary}
\begin{proof}
We calculate the commutator
\[ [S,T] = \left[\frac{A+A^*}{2}, \frac{A-A^*}{2i}\right] = \frac{A^*A - AA^*}{2i} = \frac{1}{2i}[A^*, A]. \]
\end{proof}

\begin{proposition}
The set of bounded self-adjoint operators forms an anti-lattice.
\end{proposition}
\begin{proof}
TODO + generalised to self-adjoint operators??
\end{proof}

\subsection{Normal operators}
\begin{definition}
A densely defined linear operator $T$ on a Hilbert space $H$ is \udef{normal} if it is closed and $TT^* = T^*T$.
\end{definition}
Self-adjoint and unitary operators are normal.

\begin{proposition} \label{normalCriterion}
Let $T: H\not\to H$ be a densely defined operator. Then $T$ is normal \textup{if and only if} $\dom(T) = \dom(T^*)$ and $\forall x\in \dom(T): \norm{Tx} = \norm{T^*x}$.
\end{proposition}
\begin{proof}
First, assume $T$ normal. Then, for all $x\in \dom(T^*T) = \dom(TT^*)$, we have $x\in \dom(T)$ and $x\in \dom(T^*)$ and
\[ \norm{Tx}^2 = |\inner{Tx,Tx}| = |\inner{T^*Tx,x}| = |\inner{TT^*x,x}| = |\inner{T^*x,T^*x}| = \norm{T^*x}^2. \]
By \ref{vonNeumannTheoremSquareSelfAdjoint}, $\dom(T^*T)$ is $\graph(T)$-dense in $\dom(T)$. Thus, for all $x\in \dom(T)$, there exists a sequence $\seq{x_n}\in \dom(T^*T)^\N$ such that $x_n \overset{\graph(T)}{\longrightarrow} x$.

In particular, $Tx_n \to Tx$, which means $\seq{Tx_n}$ is a Cauchy sequence. Since $x_n,x_m\in \dom(T^*T)$, we have shown that $\norm{Tx_n - Tx_m} = \norm{T^*x_n - T^*x_m}$ and thus $\seq{T^*x_n}$ is Cauchy by \ref{CauchyCriterion}. It converges to some $y\in H$ by completeness and so $x\in \dom(T^*)$ and $T^*x = y$ by \ref{closedGraphEquivalence}, since $T^*$ is closed \ref{adjointGraph}. 

This shows that $\dom(T) \subseteq \dom(T^*)$. The same reasoning with $T^*$ gives the opposite inclusion.

Finally, we calculate
\[ \norm{Tx} = \lim_n\norm{Tx_n} = \lim_n \norm{T^*x_n} = \norm{T^*x}. \]

For the converse, we first prove that $T$ is closed, using \ref{closedGraphEquivalence}. Suppose $\seq{x_n}\in \dom(T)^\N$ converges to $x$ and $\seq{Tx_n}$ also converges. Then $\seq{Tx_n}$ is Cauchy and, since $\norm{Tx_n - Tx_m} = \norm{T^*x_n - T^*x_m}$, the sequence $\seq{T^*x_n}$ is also Cauchy and thus convergent. Since $T^*$ is closed \ref{adjointGraph}, we have $x\in \dom(T^*) = \dom(T)$ and $T^*x_n \to T^*x$ by \ref{closedGraphEquivalence}. Thus
\[ \norm{Tx_n - Tx} = \norm{T^*x_n - T^*x} \to 0, \]
so $Tx_n \to Tx$.

For all $x,y\in\dom(T) = \dom(T^*)$, we have
\[ \inner{Tx,Ty} = \frac{1}{4}\sum_{k=0}^3\norm{i^kTx+ i^kTy}^2 = \frac{1}{4}\sum_{k=0}^3\norm{i^kT^*x+ i^kT^*y}^2 = \inner{T^*x, T^*y} \]
by the polarisation identity \ref{polarisationIdentities}.
Using \ref{adjointDomain}, we have
\begin{align*}
x\in \dom(T^*T) &\iff Tx \in \dom(T^*) \\
&\iff \text{$y\mapsto \inner{Tx, Ty}$ is a bounded functional} \\
&\iff \text{$y\mapsto \inner{T^*x, T^*y}$ is a bounded functional} \\ 
&\iff T^*x \in \dom(T^{**}) = \dom(T) \\
&\iff x\in \dom(TT^*), 
\end{align*}
where we have used $T^{**} = T$ by \ref{adjointDenselyDefinedClosable}.

Finally, take $x\in \dom(T^*T) = \dom(TT^*)$ and $y\in \dom(T) = \dom(T^*)$. Then
\[ \inner{Tx,Ty} = \inner{T^*x, T^*y} \implies \inner{T^*Tx,y} = \inner{TT^*x, y} \implies \inner{(T^*Tx - TT^*)x, y} = 0, \]
so $(T^*Tx - TT^*)x \in \dom(T)^\perp = \{0\}$, so $T^*Tx = TT^*x$.
\end{proof}
\begin{corollary} \label{equalityKernelAdjointNormal}
If $T$ is a normal operator, then $\ker T = \ker T^*$.
\end{corollary}
\begin{proof}
We have $x\in\ker(T) \iff \norm{Tx} = 0 \iff \norm{T^*x} = 0 \iff x\in\ker(T^*)$. 
\end{proof}
\begin{corollary}
If $T$ is a normal operator then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item $\spec(T) = \apspec(T)$.
\end{enumerate} 
\end{corollary}
\begin{proof}
If $T$ is normal, then so is $\lambda\id-T$. Now $\lambda\in\rspec(T)$ iff $\ker(\lambda\id - T) = \{0\}$ and $\im(\lambda\id-T)^\perp \neq \{0\}$, but $\im(\lambda\id-T)^\perp = \ker(\lambda\id-T)^* = \ker(\lambda\id-T)$. By \ref{kernelImageAdjoint} and the previous corollary. This is a contradiction.

(2) then follows straight from (1).
\end{proof}

\begin{theorem} \label{closureNumericRangeConvexHullSpectrum}
The closure of the numerical range of a normal operator is the
convex hull of its spectrum.
\end{theorem}
\begin{proof}
Normal operators $T$ are by definition closed, so $\spec(T)\subseteq \overline{\NumRange(T)}$ by \ref{spectralInclusionNumericalRange}. TODO
\end{proof}

\begin{lemma} \label{normalSpectralRadiusEqualsNorm}
For normal elements the spectral radius equals the norm.
\end{lemma}

\begin{lemma}
A normal operator on a Hilbert space is invertible \textup{if and only if} it is bounded below.
\end{lemma}

\begin{theorem}[Fuglede's theorem]
Let $H$ be a Hilbert space $N$ a normal operator on $H$ and $A\in\Bounded(H)$. Then $AN\subseteq NA$ implies $AN^* \subseteq N^*A$.
\end{theorem}
\begin{proof}
TODO
\end{proof}
\begin{corollary}[Putnam-Fuglede theorem]
Let $H, K$ be a Hilbert spaces, $A\in\Bounded(K, H)$ and both $N: H\not\to K$ and $M: K\not\to H$ normal operators. Then $AN\subseteq MA$ implies $AN^* \subseteq M^*A$.
\end{corollary}
\begin{proof}
Consider
\[ L \defeq \begin{pmatrix}
N & 0 \\ 0 & M
\end{pmatrix} \qquad\text{and}\qquad A' \defeq \begin{pmatrix}
0 & A \\ 0 & 0
\end{pmatrix}. \]
Then $L$ is normal (TODO) and $A'$ is bounded. Then we can apply Fuglede's theorem (TODO details).
\end{proof}

\subsubsection{Spectral measures}
\begin{definition}
Let $H$ be a Hilbert space and $\sSet{\Omega,\mathcal{A}}$ a measurable space.
\begin{itemize}
\item A measure $E$ from $\sSet{\Omega,\mathcal{A}}$ to the lattice of projectors on $H$ is called a \udef{projector-valued measure}.
\item A projector-valued measure $E$ such that $E(\Omega) = \id_H$ is called a \udef{spectral measure}.
\end{itemize}
Let $E$ be a projector-valued measure on $\sSet{\Omega,\mathcal{A}}$. For any measurable function $f: X\to \C$ we define
\[ \int_\Omega f\diff{E}: \dom\Big(\int_\Omega f\diff{E}\Big) \to H: x\mapsto \int_\Omega f\diff{E_x}, \]
where the integral is a Bochner integral,
\[ E_x: \mathcal{A}\to H: A\mapsto E(A)(x) \]
and
\[ \dom\Big(\int_\Omega f\diff{E}\Big) = \setbuilder{x\in H}{\text{$f$ if $E_x$-integrable}}. \]
\end{definition}
TODO: can we make definition of Bochner integral general enough such that this integral can be  considered as a Bochner integral??

\begin{lemma}
Let $H$ be a Hilbert space, $E$ a projector-valued measure on the measurable space $\sSet{\Omega,\mathcal{A}}$ and $x\in H$. Then $E_x$ is a vector-valued measure.
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{lemma} \label{projectorIntegrableSolid}
Let $H$ be a Hilbert space and $E$ a projector-valued measure on the measurable space $\sSet{\Omega,\mathcal{A}}$. Let $f,g: \Omega\to \C$ be measurable functions such that $|f|\leq |g|$. Then $\dom\Big(\int_\Omega f \diff{E}\Big) \supseteq \dom\Big(\int_\Omega g \diff{E}\Big)$.
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{proposition} \label{integralProjectorValuedMeasure}
Let $H$ be a Hilbert space and $E$ a projector-valued measure on the measurable space $\sSet{\Omega,\mathcal{A}}$. Let $f,g: \Omega\to \C$ be measurable functions. Then
\begin{enumerate}
\item $\int_\Omega f\diff{E}$ is a normal operator;
\item $\Big(\int_\Omega f\diff{E}\Big)^* = \int_\Omega \overline{f}\diff{E}$;
\item $\int_\Omega f\diff{E}\circ \int_\Omega g\diff{E} \subseteq \int_\Omega f\cdot g\diff{E}$ and $\dom\Big(\int_\Omega f\diff{E}\circ\int_\Omega g\diff{E}\Big) = \dom\Big(\int_\Omega f\cdot g\diff{E}\Big)\cap \dom\Big(\int_\Omega g\diff{E}\Big)$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Let $H$ be a Hilbert space and $E$ a projector-valued measure on the measurable space $\sSet{\Omega,\mathcal{A}}$. Let $f,g: \Omega\to \C$ be measurable functions. Then
\begin{enumerate}
\item $\Big(\int_\Omega f\diff{E}\Big)^*\circ \int_\Omega f\diff{E} = \int_\Omega |f|^2\diff{E}$;
\item if $g$ is bounded, then $\int_\Omega f\diff{E}\circ \int_\Omega g\diff{E} = \int_\Omega f\cdot g\diff{E} = \int_\Omega g\diff{E}\circ \int_\Omega f\diff{E}$.
\end{enumerate}
\end{corollary}

\subsubsection{Spectral theorem}
\begin{theorem}[Spectral theorem]
Let $H$ be a Hilbert space and $N$ a normal operator on $H$. Then there exists a unique spectral measure $E$ on $\C$ such that
\begin{enumerate}
\item $N = \int z\diff{E(z)}$;
\item $E(A) = 0$ for all Borel sets $A$ such that $A\perp \spec(N)$;
\item if $U\subseteq \C$ is an open set such that $U\mesh \spec(N)$, then $E(U) \neq 0$;
\item if $T\in \Bounded(H)$ is such that $TN\subseteq NT$, then $A\big(\int_\C f\diff{E}\big) \subseteq \big(\int_\C f\diff{E}\big)A$ for all measurable $f:\C\to \C$.
\end{enumerate}
\end{theorem}
Clearly $\int z\diff{E(z)}$ is shorthand for $\int_\C\id_\C\diff{E}$.
\begin{proof}
TODO
\end{proof}
\begin{corollary} \label{realSpectrumSelfAdjoint}
Let $H$ be a Hilbert space and $N$ a normal operator on $H$. If $\spec(N) \subseteq \R$, then $N = N^*$.
\end{corollary}
\begin{proof}
We have $\C\setminus\R \perp \spec(N)$, so $N = \int_\C z\diff{E(z)} = \int_\R z\diff{E(z)}$. By \ref{integralProjectorValuedMeasure}, we have
\[ N = \int_\R z\diff{E(z)} = \int_\R \overline{z}\diff{E(z)} = \Big(\int_\R z\diff{E(z)}\Big)^* = N^*. \]
\end{proof}

\begin{definition}
Let $H$ be a Hilbert space, $N$ a normal operator on $H$ and $f: \C\to \C$ a measurable function. Then we define
\[ f(N) \defeq \int_\C f\diff{E}, \]
where $E$ is the spectral measure associated to $N$.
\end{definition}

\begin{theorem} \label{spectralTheoremFunctionalCalculus}
Let $H$ be a Hilbert space, $N$ a normal operator on $H$ and $f,g: \C\to \C$ measurable functions. Then
\begin{enumerate}
\item $(f\circ g)(N) = f\big(g(N)\big)$;
\item $\spec(f(N)) = f^\imf\big(\spec(N)\big)$.
\end{enumerate}
\end{theorem}
\begin{proof}
TODO!
\end{proof}

\begin{proposition}
Let $T$ be a normal operator on a Hilbert space. If $\lambda$ is an isolated point of the spectrum, then $\lambda$ is an eigenvalue.
\end{proposition}
\begin{proof}
Because $\lambda$ is isolated, the function
\[ f: \spec(T)\to \C: x\mapsto \begin{cases}
1 & (x=\lambda) \\
0 & (x\neq \lambda)
\end{cases} \]
is continuous.

Set $P = f(T)$ by continuous functional calculus (TODo ref!!). This is a projector by (TODO ref).

For all $t\in \spec(T)$, we have $tf(t) = \lambda f(t)$. By functional calculus, this gives $TP = \lambda P$.
\end{proof}


\subsection{Orthogonal projections}
\url{https://planetmath.org/latticeofprojections}

\url{https://zfn.mpdl.mpg.de/data/Reihe_A/35/ZNA-1980-35a-0437.pdf}

We denote the set op projections on a Hilberts space $\mathcal{H}$ by $\Projections(\mathcal{H})$.

TODO: $\im(P) = \ker{P^*}^\perp$ shows that we need $P= P^*$ for orthogonality.

\begin{proposition}
Let $P$ be a bounded operator $P$ on a Hilbert space $\mathcal{H}$. Then the following are equivalent:
\begin{enumerate}
\item $P$ is an orthogonal projection onto a closed subspace of $\mathcal{H}$;
\item $P^2 = P$ and $P=P^*$;
\item $P^2 = P$ and $\norm{P}\in \{0,1\}$;
\item $P^2 = P$ and $\norm{P}\leq 1$;
\end{enumerate}
\end{proposition}
\begin{proof}
$\boxed{(1)\Rightarrow (2)}$  Suppose first that $P$ is the orthogonal projection operator onto a closed subspace $K$. Clearly $P^2 = P$. Let $x,y\in\mathcal{H}$ and write $x= x_1+x_2, y = y_1+y_2$ where $x_1,y_1\in K$ and $x_2,y_2\in K^\perp$. Then
\[ \inner{Px, y} = \inner{x_1, y_1+y_2} = \inner{x_1, y_1} + \inner{x_1,y_2} = \inner{x_1,y_1} = \inner{x_1+x_2, y_2} = \inner{x,Py}. \]
So $P = P^*$.

$\boxed{(2)\Rightarrow (3)}$ We calculate $\norm{P} = \norm{P^2} = \norm{P^*P} = \norm{P}^2$ using \ref{normOfSquare}. The solutions to this equation are $\{0,1\}$.

$\boxed{(3)\Rightarrow (4)}$ This is clear.

$\boxed{(4)\Rightarrow (1)}$ Define $K=\im P$, then $K$ is closed because $x\in K$ iff $Px=x$ and thus for any converging sequence $(x_n)_n\subset K$: $\lim x_n = \lim Px_n = P\left(\lim x_n\right)$, so the limit is in $K$.

We just need to show orthogonality: $Px \perp x- Px$. For this we use \ref{orthogonality}: for all $a\in\F$
\[ \norm{Px} = \norm{Px + aPx - aPx} = \norm{P(Px + a(x-Px))} \leq \norm{P}\cdot \norm{Px + a(x-Px)} \leq \norm{Px + a(x-Px)}. \]
We conclude $Px \perp x- Px$.
\end{proof}

\begin{proposition} \label{projectorOrthogonalComplement}
Let $\mathcal{H}$ be a Hilbert space and let $P$ be an orthogonal projector on a closed subspace $K$. Then $\id-P$ is the orthogonal projector on $K^\perp$.
\end{proposition}
\begin{proof}
Any $x\in \mathcal{H}$ can be uniquely decomposed as $x_1 + x_2\in K\oplus K^\perp$. If $Px = x_1$, then $(\id - P)x = x_1 +x_2 - x_1 = x_2$.
\end{proof}
\begin{corollary} \label{projectorsIn01}
The set of projectors $\Projections(\mathcal{H})$ is a subset of $[0,\id]$.
\end{corollary}
\begin{proof}
Let $P\in\Projections(\mathcal{H})$. Then $P\geq 0$ follows from $P = P^2 = P^*P$.
\end{proof}

\begin{proposition} \label{commutingProjectors}
Let $\mathcal{H}$ be a Hilbert space and $P,Q$ be projections. The following are equivalent:
\begin{enumerate}
\item $PQ = QP$;
\item $PQ$ is a projection;
\item $QP$ is a projection;
\item $P+Q-PQ$ is a projection;
\item $\im(PQP) = \im(P) \cap \im(Q)$;
\item $PQP = QP$;
\item $\mathcal{H} = \big(\im(P)\cap\im(Q)\big)\oplus \big(\im(P)\cap\im(Q)^\perp\big) \oplus \big(\im(P)^\perp\cap\im(Q)\big) \oplus \big(\im(P)^\perp\cap\im(Q)^\perp\big)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Points (1), (2), (3) are equivalent by the equation $(PQ)^* = Q^*P^* = QP$, and the fact that (1) implies $(PQ)^2 = PQPQ = PPQQ = PQ$.

(4) If $P,Q$ commute, then
\begin{align*}
(P+Q-PQ)^* &= P+Q-(PQ)^* = P+Q-Q^*P^* =P+Q-QP = P+Q-PQ \\
(P+Q-PQ)^2 &= P^2 + PQ -P^2Q + QP+Q^2 - QPQ - PQP -PQP +PQPQ \\
&= P + Q + 3PQ - 4PQ= P+Q-PQ.
\end{align*}
Assume (4), then $(P+Q-PQ)^* = P+Q-QP = P+Q-PQ$. This implies $PQ=QP$.

$\boxed{(1)\Rightarrow (5)}$ Clearly $\im(PQP) \subseteq \im(P) \cap \im(Q)$.
For the inverse inequality, take $x\in im(P)\cap\im(Q)$. Then $PQP(x) = PQ(x) = P(x) = x$, so $x\in\im(PQP)$.

$\boxed{(5)\Rightarrow (6)}$ We decompose $\mathcal{H} = \im(PQP) \oplus \ker(PQP)$ and show that the operators are the same on both parts. For all $x\in \mathcal{H}$ we have
\[ x\in \ker(PQP) \iff \inner{x,PQPx} = 0 \iff \inner{QPx,QPx} = 0 \iff \norm{QPx} = 0 \iff x\in\ker{QP}.  \]
Now let $x\in\im(PQP) = \im(P)\cap\im(Q)$. Then $QPx = Qx = x = PQPx$.

$\boxed{(6)\Rightarrow (3)}$ $PQP$ is always a projection.

$\boxed{(6)\Rightarrow (7)}$ Take some $x\in \mathcal{H}$. Then we can uniquely decompose $x = P(x) + (x-P(x)) = x_P + x_{P^\perp} \in \im(P)\oplus \im(P)^\perp$. We can then further decompose $x_P = x_{P,Q} + x_{P,Q^\perp}$ and $x_{P^\perp} = x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. In order to have the decomposition of the proposition, we need to show that $x_{P,Q},x_{P,Q^\perp}\in \im(P)$ and $x_{P^\perp, Q},x_{P^\perp, Q^\perp}\in\im(P)^\perp$.

First take $x_{P,Q} = QPx$. From (6) we have $P(QPx) = PQPx = QPx$, so $x_{P,Q}\in \im(P)$. For the others we have similar calculations (also using the identity $PQP = PQ$):
\begin{align*}
P(x_{P,Q^\perp}) &= P\big((\id-Q)P\big)x = Px - PQPx = Px - QPx = (\id-Q)Px = x_{P,Q^\perp} \\
(\id-P)(x_{P^\perp,Q}) &= (\id-P)\big(Q(\id-P)\big)x = (Q-QP-PQ+PQP)x = (Q-QP)x = Q(\id-P)x = x_{P^\perp,Q} \\
(\id-P)(x_{P^\perp,Q^\perp}) &= (\id-P)\big((\id-Q)(\id-P)\big)x = (\id-P-Q+QP-P+P+PQ-PQP)x \\
&= (\id-Q-P+QP)x = (\id-Q)(\id-P)x = x_{P^\perp,Q^\perp}.
\end{align*}
$\boxed{(7)\Rightarrow (1)}$ Take $x\in \mathcal{H}$ and decompose it as $x_{P,Q} + x_{P,Q^\perp} + x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. Then $PQx = P(x_{P,Q} + x_{P^\perp, Q}) = x_{P,Q}$ and $QP = Q(x_{P,Q} + x_{P, Q^\perp}) = x_{P,Q}$, so $PQ = QP$. 
\end{proof}

\begin{proposition} \label{perpendicularProjections} \label{subspaceProjections}
Let $P,Q$ be orthogonal projections onto subspaces $\im(P)$ and $\im(Q)$ of $\mathcal{H}$.
\begin{enumerate}
\item The following are equivalent to $\im(P) \perp \im(Q)$:
\begin{enumerate}
\item $QP = 0$;
\item $PQ = 0$;
\item $Q+P$ is an orthogonal projection.
\end{enumerate}
\item The following are equivalent to $\im(P) \subseteq \im(Q)$:
\begin{enumerate}
\item $QP = P$;
\item $PQ = P$;
\item $Q-P$ is an orthogonal projection;
\item $P\leq Q$;
\item $\norm{Px} \leq \norm{Qx}$ for all $x \in \mathcal{H}$.
\end{enumerate}
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \perp \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a, b)\Leftrightarrow (c)}$ We know $(P+Q)^* = P^*+Q^* =P+Q$ and we can write
\[ (P+Q)^2 = P^2 + Q^2 + PQ + QP = P+Q+ PQ+QP,  \]
So clearly (a) or (b) imply (c). Conversely, assume $PQ + QP = 0$, implying $PQ=-QP$. By left- and right-multiplication by $P$ this implies both
\[ PPQ = PQ = -PQP \qquad \text{and} \qquad PQP = -QPP = -QP. \]
So $PQ = -PQP = QP$, meaning $PQ = 1/2(PQ+QP) = 0$.

(2) We prove the following:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \subseteq \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a,b)\Rightarrow (c)}$ Obviously $(Q-P)^*= Q-P$. Also
\[ (Q-P)^2 = Q+P-PQ-QP= Q+P-2P = Q-P. \]

$\boxed{(c)\Rightarrow (a,b)}$ Now from
\[ Q-P = (Q-P)^2 = Q+P-PQ-QP \]
we obtain $2P = PQ+QP$. The result then follows if we can show that $PQ=QP$. This follows by multiplying the equality on the left and on the right by $P$ to obtain $QP = 2P-PQP$ and $PQ = 2P-PQP$, respectively. 

$\boxed{(c)\Rightarrow (d)}$ This follows because all projections are positive.

$\boxed{(d)\Rightarrow (a, b)}$ Assume, towards a contradiction, that $\im(P)\nsubseteq \im(Q)$. Then we can take $v\in\im(P)\setminus \im(Q)$. Then
\[ \inner{v,(Q-P)v} = \inner{v,Qv} - \inner{v,v} = \inner{Qv,Qv} - \inner{Qv,Qv} - \inner{v-Qv, v-Qv} = -\norm{v-Qv}^2. \]
Because $v\notin \im(Q)$, $\norm{v-Qv}$ is not zero and thus $Q-P$ is not positive.

$\boxed{(d)\Leftrightarrow (e)}$ By the equivalence
\[ \norm{Px} \leq \norm{Qx} \iff \inner{Px,Px} \leq \inner{Qx,Qx} \iff \inner{Px,x}\leq \inner{Qx,x} \iff \inner{(Q-P)x,x}\geq 0. \]
\end{proof}

We can generalise part 2(d) of the previous proposition to a slightly larger class of operators.
\begin{lemma} \label{comparisonSelfAdjointProjection}
Let $P\in \Projections(\mathcal{H})$ and $T \in [0,\id]$, then the following are equivalent:
\begin{enumerate}
\item $\im(T) \subseteq \im(P)$;
\item $T\leq P$.
\end{enumerate}
\end{lemma}
\begin{proof}
As $T$ is self-adjoint, we have $\norm{T} = \nr(T) \leq 1$ by \ref{normNumRadius}.

Assume (1) so that for all $x\in \mathcal{H}$ we get
\[ \inner{x,Tx} = \inner{x, PTx} = \inner{Px,PTx} \leq \norm{Px}^2\nr(T) \leq \norm{Px}^2 = \inner{Px,Px} = \inner{x,Px}. \]
So $\inner{x, (P-T)x}\geq 0$ and thus $T\leq P$.

Assume (2). The energy form associated with $T$ is a pre-inner product by \ref{positiveOperatorPositiveEnergyForm}. The Cauchy-Schwarz inequality \ref{CauchySchwarz} gives
\[ |\inner{v,Tw}|^2 \leq \inner{v,Tv}\inner{w,Tw} \leq \inner{v,Pv}\inner{w,Pw}. \]
So if $v\in\im(P)^\perp$, then $\inner{v,Tw} = 0$ for all $w\in \mathcal{H}$. So $\im(T)\perp \im(P)^\perp$, implying $\im(T)\subseteq \im(P)^{\perp\perp} = \im(P)$.
\end{proof}

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space. Let $\{P_i\}_{i\in I}$ be an arbitrary subset of $\Projections(\mathcal{H})$ and let $K_i = \im(P_i)$ for all $i\in I$. Then, as a subset of $[0,\id]$,
\begin{enumerate}
\item $\inf \{P_i\}_{i\in I} = P_M$ where $M = \bigcap_{i\in I}K_i$;
\item $\sup \{P_i\}_{i\in I} = P_N$ where $N = \bigcap\setbuilder{K \subseteq \mathcal{H}}{\text{$K$ is closed} \land \forall i\in I: K_i \subseteq K}$.
\end{enumerate}
The set of projections on $\mathcal{H}$ is thus a complete lattice as a subset of $[0,\id]$.

If $I$ is finite, then $N = \Span(\bigcup_{i\in I}K_i)$. TODO: always closure of this $N$????
\end{proposition}
In particular this means $\Projections(\mathcal{H})$ is a complete lattice as itself, with the same suprema and infima. It is not a lattice as a subset of $\SelfAdjoints(\mathcal{H})$ (TODO + example ??).
\begin{proof}
(1) By \ref{subspaceProjections} $P_M$ is a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. Let $T$ be a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. By \ref{comparisonSelfAdjointProjection} $\im(T)\subseteq K_i$ for all $i\in I$, so $\im(T)\subseteq M$ and thus $T\leq P$ again by \ref{comparisonSelfAdjointProjection}. This means $P$ is the greatest lower bound.

(2) The mapping $T\mapsto \id-T$ keeps $[0,\id]$ invariant and inverts the order. Then $\inf \{\id - P_i\}_{i\in I}$ is a projection due to the previous point and so $\sup \{P_i\}_{i\in I}$ is also a projection. The expression for $N$ is clear from \ref{subspaceProjections}.
\end{proof}

\begin{proposition}
Let $P,Q$ be idempotents such that $\norm{P-Q}<1$. Then $\im(P) \cong \im(Q)$.
\end{proposition}
\begin{proof}
Kato p.34 TODO
\end{proof}

\subsubsection{Sets of pairwise disjoint projections}
TODO!

\subsubsection{Derivatives of orthogonal projections}



\begin{proposition}
Let $\{P_i\}_{i\in I}$ be a set of pairwise disjoint orthogonal projectors which have derivatives and take $i\neq j$ in $I$. Then
\begin{enumerate}
\item $P_i'P_j = - P_iP_j'$;
\item if $\id \in \upset \{P_i\}_{i\in I}$, then
\[ P_iP_i' = \sum_{j\neq i}P'_iP_j \qquad\text{and}\qquad P_i'P_i = \sum_{j\neq i}P_jP_i'. \]
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have $P_iP_j = 0$, so $0 = P_i'P_j + P_iP_j'$.

(2) We calculate, using $\id = \sum_{j\in I}P_j$ and \ref{derivativeIdempotent}:
\[ P_iP_i' = P_iP_i'\left(\sum_{j\in I}P_j\right) = P_iP_i'P_i + \sum_{j\neq i}P_iP_i'P_j = 0 - \sum_{j\neq i}P_iP_iP_j' = -\sum_{j\neq i}P_iP_j' = \sum_{j\neq i}P_i'P_j. \]
\end{proof}
\begin{corollary}
Let $P_1, P_2$ be orthogonal projections such that $P_1 + P_2 = \id$. Then
\[ P_1P_1'= P_1'P_2 \qquad \text{and}\qquad P_1'P_1 = P_2P_1'. \]
\end{corollary}


\subsection{Isometries}
We recall that isometries are injective and continuous. On Hilbert spaces they are also closed. See \ref{isometryLemma}, \ref{isometryLemma} and \ref{isometryLemma}.

\begin{proposition} \label{isometryCharacterisation}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\begin{enumerate}
\item $T$ is an isometry \textup{if and only if} $T^*T = \id_H$;
\item $T$ is unitary \textup{if and only if} $T^*T = \id_H$ and $TT^* = \id_K$, i.e.\ $T^{-1} = T^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) For all $v,w\in H$ we have
\[ \inner{Tv,Tw} = \inner{T^*Tv,w}. \]
The left-hand side is equal to $\inner{v,w}$ iff $T$ is an isometry. The right-hand side is equal to $\inner{v,w}$ iff $T^*T = \id_H$, by \ref{equalityOfMapsInnerProductSpaces}.

(2) If $T$ is invertible, it must have a left and right inverse. By lemma \ref{leftRightInverse} they must be the same.
\end{proof}
\begin{corollary}
An isometry $T\in\Bounded(H)$ is unitary \textup{if and only if} it is normal.
\end{corollary}

\begin{lemma} \label{isometryRangeProjection}
Let $T$ be an isometry between Hilbert spaces $H$ and $K$. Then $TT^*$ is an orthogonal projection.
\end{lemma}
\begin{proof}
Clearly $(TT^*)^* = TT^*$. Also $(TT^*)^2 = T(T^*T)T^* = T\id_HT^* = TT^*$.
\end{proof}


\subsubsection{Wandering spaces and unilateral shifts}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear map. Then $\mathcal{V}$ is called a \udef{wandering space} for $T$ if $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$.
\end{definition}

\begin{lemma} \label{WoldLemma1}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear isometry.
\begin{enumerate}
\item $\mathcal{V}$ is a wandering space for $T$ \textup{if and only if} $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$;
\item $T[\mathcal{H}]^\perp$ is a wandering subspace for $T$;
\item if $\mathcal{V}$ is a wandering space for $T$, then $T^n[\mathcal{V}] \cong \mathcal{V}$ for all $n\in N$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) The direction $\Rightarrow$ is clear. For the converse, assume $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$. We need to show that $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$. WLOG we may assume $p\leq q$. Take arbitrary $x\in T^p[\mathcal{V}]$ and $y\in T^q[\mathcal{V}]$. Then
\[ \inner{x,y} = \inner{T^p(u), T^q(v)} = \inner{u, T^{q-p}(v)} = 0 \]
because $\mathcal{V} \perp T^{q-p}[\mathcal{V}]$.

(2) For all $n\geq 1$ we have
\[ T^{n}\big[T[\mathcal{H}]^\perp\big] \subset T^{n}[\mathcal{H}] = T\big[T^{n-1}[\mathcal{H}]\big] \subset T[\mathcal{H}] \perp T[\mathcal{H}]^\perp. \]

(3) For all $n\in \N$ the operator $T^n$ is an isometry. It is injective by \ref{isometryLemma}, and thus maps its domain bijectively to its image.
\end{proof}

\begin{definition}
An isometry $T$ on a Hilbert space $\mathcal{H}$ is called a \udef{unilateral shift} if there is a closed subspace $\mathcal{V}\subseteq \mathcal{H}$ that is wandering for $T$ such that
\[ \mathcal{H} = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
We call the subspace $\mathcal{V}$ \udef{generating} for $T$ and $\dim(\mathcal{V})$ the \udef{multiplicity} of $T$.
\end{definition}

By \ref{WoldLemma1}, we see that any isometry $T:\mathcal{H}\to\mathcal{H}$ is a unilateral shift when restricted to $\bigoplus_{n=0}^\infty T^n\big[T[\mathcal{H}]^\perp\big]$.



\begin{lemma} \label{WoldLemma2}
Let $T$ be an isometry on $\mathcal{H}$. If $T$ is a unilateral shift, then it is generated by $T[\mathcal{H}]^\perp$.
\end{lemma}
\begin{proof}
Let $\mathcal{V}$ be the generating subspace of the unilateral shift $T$. We calculate
\[ T[\mathcal{H}] = T\left[\bigoplus_{n=0}^\infty T^n[\mathcal{V}]\right] = \bigoplus_{n=1}^\infty T^n[\mathcal{V}] = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \ominus \mathcal{V} = \mathcal{H}\ominus \mathcal{V} = \mathcal{V}^\perp, \]
so $\mathcal{V} = T[\mathcal{H}]^\perp$.
\end{proof}

A unilateral shift is determined up to unitary equivalence by its multiplicity:
\begin{lemma}
Let $T: \mathcal{H}\to\mathcal{H}$ and $T':\mathcal{H}'\to\mathcal{H}'$ be unilateral shifts generated by $\mathcal{V}$ and $\mathcal{V}'$ such that $\dim(\mathcal{V}) = \dim(\mathcal{V}')$. Then there exists an unitary $U:\mathcal{H}'\to\mathcal{H}$ such that
\[ T' = U^*TU \]
\end{lemma}
\begin{proof}
Choose an isometric isomorphism $u:\mathcal{V}'\to\mathcal{V}$. Then any $x\in\mathcal{H}'$ can be written as $x = \sum_{n=0}^\infty T^n(x_n)$. Then define
\[ Ux = \sum_{n=0}^\infty T^n(ux_n). \]
\end{proof}

\begin{theorem}[Wold decomposition]
Let $\mathcal{H}$ be a Hilbert space and $T\in\Bounded(\mathcal{H})$ an isometry. Then $\mathcal{H}$ decomposes into an orthogonal sum $\mathcal{H} = \mathcal{H}_0\oplus \mathcal{H}_1$such that $\mathcal{H}_0, \mathcal{H}_1$ reduce $T$ and
\[ T|_{\mathcal{H}_0}\;\text{is unitary} \quad\text{and}\quad T|_{\mathcal{H}_1}\;\text{is a unilateral shift}. \]
This decomposition is uniquely determined and given by
\[ \mathcal{H}_0 = \bigcap_{n=0}^\infty T^n[\mathcal{H}] \qquad\text{and}\qquad \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \qquad\text{where}\qquad \mathcal{V} = T[\mathcal{H}]^\perp. \]
\end{theorem}
\begin{proof}
The subspace $\mathcal{V} = T[\mathcal{H}]^\perp$ is wandering by \ref{WoldLemma1}. Then $T$ is a unilateral shift in the subspace
\[ \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
Now $v\in\mathcal{H}_0 = \mathcal{H}_1^\perp$ if and only if it is perpendicular to $\bigoplus_{i=0}^n T^i[\mathcal{V}]$ for all $n$ and we have
\begin{align*}
\bigoplus_{i=0}^n T^i[\mathcal{V}] &= \bigoplus_{i=0}^n T^i[\mathcal{H}\ominus T[\mathcal{H}]] = \bigoplus_{i=0}^n T^i[\mathcal{H}]\ominus T^{i+1}[\mathcal{H}] \\
&= (\mathcal{H}\ominus T[\mathcal{H}])\oplus(T[\mathcal{H}]\ominus T^2[\mathcal{H}])\oplus \ldots \oplus (T^n[\mathcal{H}]\ominus T^{n+1}[\mathcal{H}])  = \mathcal{H} \ominus T^{n+1}[\mathcal{H}] 
\end{align*}
using \ref{perpUnderIsometry} and \ref{cancellationOminus}, which is applicable because $T^i[\mathcal{V}]$ is closed by \ref{isometryLemma}. So $\mathcal{H}_0\subseteq T^n[\mathcal{H}]$ for all $n$.

Finally $T|_{\mathcal{H}_0}$ is unitary because it is an isometry and surjective on $\mathcal{H}_0$.
\end{proof}

\subsubsection{Left and right shifts on $\ell^2$}
\begin{definition}
Consider the space $\ell^2(\N)$ with o.n. basis $\seq{e_i}$. Then
\begin{itemize}
\item the \udef{right shift operator} $S_r$ is the operator that maps $e_i \mapsto e_{i+1}$;
\item the \udef{left shift operator} $S_l$ is the operator that maps $e_i \mapsto \begin{cases}
e_{i-1} & i \geq 1 \\ 0 & i = 0
\end{cases}$.
\end{itemize}
\end{definition}

\begin{lemma}
$S_r$ is a unilateral shift
\end{lemma}

\begin{proposition}
$S_r = S^*_l$ (also converse?)
\end{proposition}

\subsubsection{Partial isometries}
\begin{definition}
An operator $T\in \Lin(H, H')$ is called a \udef{partial isometry} if there is a closed subspace $K\subseteq H$ such that
\begin{itemize}
\item $T|_K$ is an isometry;
\item $T|_{K^\perp} = 0$.
\end{itemize}
\end{definition}

Clearly every partial isometry is bounded.

\begin{lemma}
An operator $T\in \Lin(H, H')$ is a partial isometry \textup{if and only if} $T|_{\ker(T)^\perp}$ is an isometry.
\end{lemma}

\begin{proposition} \label{partialIsometryEquivalences}
Let $T\in \Bounded(H,H')$. The following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry;
\item $T^*TT^* = T^*$;
\item $TT^*T = T$;
\item $TT^*: H' \to H'$ is a projection;
\item $T^*T: H \to H$ is a projection;
\item $T^*$ is a partial isometry.
\end{enumerate}
Moreover,
\begin{enumerate}
\item $T^*T$ is the projection onto $\ker(T)^\perp$;
\item $\im(T)$ is closed and $TT^*$ is the projection onto $\im(T)$.
\end{enumerate}
\end{proposition}
\begin{proof}

$\boxed{(1)\Rightarrow (2)}$ By \ref{elementaryOrthogonality} it is enough to show that $\inner{T^*TT^*x,y} = \inner{T^*x,y}$ for all $x\in H', y\in H$. Take such $x,y$. We decompose $y = y_1\oplus y_2 \ker(T)\oplus \ker(T)^\perp$. Then
\[ \inner{T^*TT^*x, y_1} = \inner{TT^*x, Ty} = 0 = \inner{x,Ty_1} = \inner{T^*x, y_1} \]
and
\[ \inner{T^*TT^*x, y_2} = \inner{TT^*x, Ty_2} = \inner{T^*x,y_2}, \]
where we have used the fact that both $y_2$ and $T^*x$ are elements of $\ker(T)^\perp = \overline{\im(T^*)}$, and $T$ is an isometry on this space. In conclusion, we have
\[ \inner{T^*TT^*x,y} = \inner{T^*TT^*x,y_1} + \inner{T^*TT^*x,y_2} = \inner{T^*x,y_1} + \inner{T^*x,y_2} = \inner{T^*x,y} \]
for all $x\in H', y\in H$, so $T^*TT^* = T^*$.

$\boxed{(2) \Leftrightarrow (3)}$ By taking adjoints: $(TT^*T)^* = T^*TT^*$.

$\boxed{(2) \Rightarrow (4,5)}$ Clearly $T^*T$ and $TT^*$ are self-adjoint. We just need to show idempotency:
\[ (T^*T)^2 = (T^*T)(T^*T) = (T^*TT^*)T = T^*T \qquad (TT^*)^2 = (TT^*)(TT^*) = T(T^*TT^*) = TT^*. \]

$\boxed{(4) \Rightarrow (1)}$ Assume $TT^*$ a projection. Let $v\in \ker(T)^\perp = \overline{\im(T^*)}$. Then there exists a sequence $\seq{v_n}\in H^{\prime\N}$ such that $\lim_{n\to\infty}T^*v_n = v$. Then
\begin{align*}
\norm{Tv}^2 &= \lim_{n\to\infty}\norm{TT^*v_n}^2 = \lim_{n\to\infty}\inner{TT^*v_n,TT^*v_n} \\
&= \lim_{n\to\infty}\inner{(TT^*)^2v_n,v_n} = \lim_{n\to\infty}\inner{TT^*v_n,v_n} \\
&= \lim_{n\to\infty}\inner{T^*v_n,T^*v_n} = \lim_{n\to\infty}\norm{T^*v_n}^2 = \norm{v}^2,
\end{align*}
so $T$ is a partial isometry.

$\boxed{(5,6)}$ Applying the proposition to $T^*$ instead of $T$ yields the equivalences with $T=TT^*T$, and thus with the rest of the statements.

TODO + $\im(T^*) = \ker(T)^\perp$ means support and range are exchanged between $T$ and $T^*$.
\end{proof}

\begin{definition}
Let $T$ be a partial isometry. We call
\begin{itemize}
\item $T^*T$ the \udef{support projection} or \udef{initial projection} of $T$;
\item $TT^*$ the \udef{range projection} or \udef{final projection} of $T$.
\end{itemize}
\end{definition}

\begin{proposition}
Let $H,H'$ be Hilbert spaces with $K\subseteq H$ and $L\subseteq H'$ closed subspaces. Then the following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry with support $K$ and range $L$;
\item $(T,T^*)$ is a Galois connection between $\sSet{H, \perp_K}$ and $\sSet{H', \perp_L}$.
\end{enumerate}
Here $\perp_K$ is defined by
\[ x \perp_K y \quad\defequiv\quad P_K(x)\perp P_{K}(y). \]
\end{proposition}
\begin{proof}
The direction $(2) \Rightarrow (1)$ is immediate from \ref{partialIsometryEquivalences}, because $T,T^*$ are generalised inverses.

For the other direction, we first prove $T$ preserves the relational structure. Take arbitrary $x= x_1+x_2$ and $y=y_1+y_2$ in $K\oplus K^\perp$ such that $x\perp_K y$. Then
\[ \inner{T(x), T(y)} = \inner{T(x_1), T(y_1)} = \inner{x_1, y_1} = 0. \]
So $T(x)\perp T(y)$ and, because $T(x), T(y) \in L$, we have $T(x)\perp_L T(y)$. The argument for $T^*$ is similar.

For the Galois condition, we need to show that $T^*T(x)\perp_K y \implies x\perp_K y$. Indeed
\begin{align*}
T^*T(x)\perp_K y &\iff T^*T(x_1)\perp y_1 \\
&\iff 0= \inner{T^*T(x_1), y_1} = \inner{T(x_1), T(y_1)} = \inner{x_1,y_1} \\
&\iff P_K(x)\perp P_K(y).
\end{align*}
\end{proof}
\begin{corollary}
Let $T: H\to H'$ be a partial isometry with support $K$ and range $L$. Then
\[ T(x) \perp P_L(y) \iff P_K(x) \perp T^*(y) \]
for all $x\in H, y\in H'$.
\end{corollary}
\begin{proof}
This is the Galois identity \ref{GaloisIdentity}, although the direct proof is also very simple.
\end{proof}

\subsubsection{Unitaries}
\paragraph{Bilateral shifts}


\section{Dirac notation}
\url{https://core.ac.uk/download/pdf/25263496.pdf}
\url{https://michael-herbst.com/talks/2014.07.22_Mathematical_Concept_Dirac_Notation.pdf}
\url{http://galaxy.cs.lamar.edu/~rafaelm/webdis.pdf}
\url{https://plato.stanford.edu/entries/qt-nvd/}
\url{file:///C:/Users/user/Downloads/Abdus%20Salam,%20E.P.%20Wigner%20(Ed.)%20-%20Aspects%20of%20Quantum%20Theory%20-%20Dedicated%20to%20Dirac%E2%80%99s%2070th%20Birthday-Cambridge%20University%20Press%20(1972).pdf}
\url{https://aip.scitation.org/doi/pdf/10.1063/1.1705001}

\begin{lemma}
\begin{enumerate}
\item $T\ketbra{\varphi}{\psi} = \ketbra{T\varphi}{\psi} = \ketbra{\varphi}{\psi}T = \ketbra{\varphi}{T^*\psi}$;
\item $\ketbra{\varphi}{\psi}\ketbra{\xi}{\eta} = \inner{\psi, \xi}\ketbra{\varphi}{\eta}$;
\item $(\ketbra{\varphi}{\psi})^* = \ketbra{\psi}{\varphi}$.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $H$ be a Hilbert space and $\seq{e_i}_{i\in I}$ a basis for $H$. Then
\[ \id_H = \sum_{i\in I}\ketbra{e_i}{e_i} \qquad\text{in the strong limit.} \]
\end{lemma}
\begin{proof}
TODO!!
\end{proof}
\begin{lemma} \label{operatorBraketExpansion}
Let $H$ be a Hilbert space, $\seq{e_i}_{i\in I}$ a basis for $H$ and $T$ an operator on $H$. Then
\[ T = \sum_{i,j\in I}\braket[T]{e_i}{e_j}\; \ketbra{e_i}{e_j}. \]
in the strong limit.
\end{lemma}
\begin{proof}
TODO!! Tannery.
\end{proof}

\section{Ideals of operators on a Hilbert space}

\subsection{Finite-rank operators}
Remember that finite-rank operators are bounded by definition (this is not automatic, cfr. \ref{continuousMapCriterion}).

\begin{proposition}[Finite rank singular value decomposition] \label{finiteRankSingularValues}
Let $V$ be an inner product space and $T\in\Hom(V)$. Then $T$ is a finite-rank operator \textup{if and only if} $T$ can be written in the form
\[ T = \sum_{i=1}^N \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^N$ and $(w_i)_{i=1}^N$ are finite sets of vectors and $(\lambda_i)_{i=1}^N$ are positive (non-zero) numbers.

The numbers $(\lambda_i)_{i=1}^N$ in this decomposition are uniquely determined by the operator.
\end{proposition}
The numbers $(\lambda_i)_{i=1}^N$ are called the \udef{singular values} of the operator.
\begin{proof}
Because $\im(T)$ is finite-dimensional, we can find an orthonormal basis $(v_i)_{i=1}^N$ for it. Then we can write
\begin{align*}
Tx &= \sum_{i=1}^N \ket{v_i}\braket{v_i}{Tx} = \sum_{i=1}^N \ket{v_i}\braket{T^*v_i}{Tx} = \sum_{i=1}^N \ket{v_i}\braket{\lambda_i w_i}{Tx}  = \sum_{i=1}^N \lambda_i\ket{v_i}\braket{w_i}{Tx}
\end{align*}
where $\lambda_i = \norm{T^*v_i}$ and $w_i = \frac{T^*v_i}{\lambda_i}$.

We just need to show that the $\lambda_i$ are independent of the chosen basis $(v_i)_{i=1}^N$. TODO!!!!
\end{proof}
\begin{corollary}
Every finite-rank operator on a Hilbert space is a finite sum of rank-1 operators.
\end{corollary}

\begin{lemma}
Let $H$ be Hilbert space. The set of finite rank operators on $H$ is a $*$-ideal in $H$.
\end{lemma}

\subsection{Compact operators}

\url{https://math.stackexchange.com/questions/4198074/space-of-compact-operators-is-the-only-proper-closed-two-sided-ideal-of-the-spac}

\begin{lemma}
Let $K\in\Compact(H)$ be a compact operator. For all $\lambda\in\spec(K)\setminus\{0\}$, the eigenspace $E_\lambda$ is finite-dimensional.
\end{lemma}
Compare with \ref{spectrumCompactOperator}. Note that each such $\lambda$ is indeed an eigenvalue, by \ref{pointSpectrumCompactOperatorBanachSpace}.
\begin{proof}
Suppose, towards a contradiction, $E_\lambda$ is infinite dimensional, then we can find a sequence $\seq{x_n}$ of orthonormal vectors in $E_\lambda$. Since $\norm{x_n-x_m} = \sqrt{\norm{x_n}^2+\norm{x_m}^2} = \sqrt{2}$ for all $n,m\in\N$, by the Pythagorean theorem \ref{Pythagoras}, $\seq{x_n}$ has not Cauchy subsequence and thus also no convergent subsequence. This implies that $\seq{Kx_n} = \lambda\seq{x_n}$ also has no convergent subsequence, which implies that $K$ is not compact by \ref{compactOperatorEquivalents}.
\end{proof}

\begin{proposition}
Let $H_1,H_2$ be Hilbert spaces and $T\in \Bounded(H_1,H_2)$. If $T$ is bounded, then $T^*$ is bounded.
\end{proposition}
\begin{proof}
Let $\seq{x_n}$ be a bounded sequence in $H_2$ with $\norm{x_n}\leq M\in \R$ for all $n\in\N$. Since $T^*$ is bounded, we have that $\seq{T^*x_n}$ is also bounded and thus $\seq{TT^*x_n}$ has a convergent subsequence $\seq{TT^*x_{n_k}}_{k\in \N}$ by \ref{compactOperatorEquivalents}. Then, using the Cauchy-Schwarz inequality \ref{CauchySchwarz}, we calculate
\begin{align*}
\norm{T^*x_{n_k} - T^*x_{n_l}}^2 &= \inner{TT^*(x_{n_k} - x_{n_l}), x_{n_k} - x_{n_l}} \\
&\leq \norm{TT^*(x_{n_k} - x_{n_l})}\;\norm{x_{n_k} - x_{n_l}} \\
&\leq \norm{TT^*(x_{n_k} - x_{n_l})}\;\big(\norm{x_{n_k}} +\norm{x_{n_l}}\big) \\
&\leq 2M\norm{TT^*(x_{n_k} - x_{n_l})} \to 0.
\end{align*}
Thus $\seq{T^*x_{n_k}}_{k\in\N}$ is a Cauchy sequence in $H_1$, which means it converges. This means that $\seq{T^*x_n}_{n\in\N}$ has a convergent subsequence and thus that $T^*$ is compact by \ref{compactOperatorEquivalents}.
\end{proof}


\begin{proposition}
Let $T\in\Bounded(H)$. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item there exists a sequence $(T_n)_{n\in\N}$ of finite rank operators such that $\norm{T-T_n}\to 0$.
\end{enumerate}
\end{proposition}
This is false in Banach spaces. (TODO Enflo, approximation property, goose problem)
\begin{proof}
\ref{SchaudersTheorem}
\end{proof}
\begin{corollary}[Canonical expansion]
Any compact operator $T$ on a Hilbert space $\mathcal{H}$ can be written in the form
\[ T = \sum_{i=1}^\infty \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^\infty$ and $(w_i)_{i=1}^\infty$ are orthonormal sets and $(\lambda_i)_{i=1}^\infty$ is a monotonically decreasing sequence of positive numbers with $\lim_{i\to\infty}\lambda_i = 0$.
\end{corollary}
As in \ref{finiteRankSingularValues} for finite-rank operators we call $(\lambda_i)_{i=1}^\infty$ the \udef{singular values} of $T$. They are uniquely determined by the operator.
\begin{proof}
TODO (one way is with polar decomposition and spectral theorem. Are there others?)
\end{proof}
Compare with \ref{operatorBraketExpansion}.

\begin{lemma}
Let $H$ be a Hilbert space. Then the set of compact operators on $H$, $\Compact(H)$ is a two-sided $*$-ideal of $H$. 
\end{lemma}

\begin{proposition}
Let $H$ be a Hilbert space with orthonormal basis $(e_i)_{i\in I}$. If $T\in\Bounded(H)$ and
\[ \sum_{i\in I}\norm{Te_i}^2  < \infty, \]
then $T$ is a compact operator. + Converse??
\end{proposition}
\begin{proof}
TODO + weaken $T\in\Bounded(H)$?
\end{proof}
\begin{corollary}
An integral operator defined by a square integrable kernel $K\in L^2(A\times A, \mu)$ is compact.
\end{corollary}

\begin{proposition}
Let $T$ be an operator on a Hilbert space. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item for all sequences $\seq{x_n}$, weak convergence $x_n \overset{w}{\to} x$ implies the strong convergence $Ax_n \to Ax$;
\item for any two weakly convergent sequences $x_n\overset{w}{\to} x$ and $y_n\overset{w}{\to} y$ the energy form is continuous in both arguments:
\[ \lim_{n\to\infty}\inner{x_n,y_n}_T = \lim_{n\to\infty}\inner{x_n,Ty_n} = \inner{x,Ty} = \inner{x,y}_T. \]
\end{enumerate} 
\end{proposition}

\begin{lemma}
Let $H$ be a Hilbert space and $P\in\Projections(H)$. If $P$ is compact, then $P$ has finite rank.
\end{lemma}

\subsection{Positive operators}

\subsubsection{Polar decomposition}
\begin{proposition}
Let $H$ be a Hilbert space and $T$ a closed operator on $H$. There exists a unique pair of operators $V,|T|$ on $H$ such that
\begin{enumerate}
\item $T = V|T|$
\item $|T|$ is positive self-adjoint and $V$ is a partial isometry;
\item $\dom(T) = \dom(|T|)$, $\ker(T) = \ker(|T|)$;
\item the support projection of $V$ is $\ker(T)^\perp$ and range is $\overline{\im(T)}$.
\end{enumerate}
\end{proposition}
The unique $|T|$ is given by $\sqrt{T^*T}$.
\begin{proof}
By \ref{vonNeumannTheoremSquareSelfAdjoint}, we have that $T^*T$ is self-adjoint. By \ref{spectralTheoremFunctionalCalculus} we have $\spec(\sqrt{T^*T}) \subseteq \interval[co]{0,+\infty}$ and $\sqrt{T^*T}^2 = T^*T$. By \ref{realSpectrumSelfAdjoint} we have that $\sqrt{T^*T}$ is self-adjoint.

Now, for all $x\in $

\ref{projectorIntegrableSolid}
\end{proof}

\subsubsection{Square root}
\begin{proposition} \label{squareRootUnboundedOperator}
Let $H$ be a Hilbert space and $A$ a positive operator on $H$. Then there exists a unique self-adjoint operator $\sqrt{A}$ on $H$ with $\dom(\sqrt{A}) = \dom(A)$ and $\sqrt{A}^2 = A$.
\end{proposition}


\subsubsection{Trace-class operators}
TODO: move away from ideal heading.

\begin{definition}
Let $H$ be a Hilbert space with orthonormal basis $\seq{e_i}_{i\in I}$. Let $T\in\Bounded(H)$ be a bounded positive operator. We define the \udef{trace} of $T$ as
\[ \Tr(T) \defeq \sum_{i\in I}\inner{e_i, Te_i}. \]
If $\Tr(T) < \infty$, we say $T$ is \udef{trace-class}.
\end{definition}
Since $T$ is positive, the trace is well-defined.

\begin{lemma}
The trace of an operator does not depend on the choice of orthonormal basis.
\end{lemma}
\begin{proof}
Let $H$ be a Hilbert space and $\seq{e_i}_{i\in I}, \seq{f_i}_{i\in I}$ two orthonormal bases for $H$. Let $T$ be a bounded positive operator on $H$. Then $T$ has a square root $\sqrt{T}$ by (TODO ref) and we can calculate, using the Parseval identity \ref{totalONBParsevalEquivalence},
\begin{align*}
\sum_{i\in I}\inner{e_i, Te_i} &= \sum_{i\in I}\inner{\sqrt{T}e_i, \sqrt{T}e_i} \\
&= \sum_{i\in I}\norm{\sqrt{T}e_i}^2 \\
&= \sum_{i\in I}\sum_{j\in I}\left|\inner{f_j, \sqrt{T}e_i}\right|^2 \\
&= \sup_{F\subseteq I \text{finite}}\sup_{F'\subseteq I \text{finite}}\sum_{i\in F}\sum_{j\in F'}\left|\inner{f_j, \sqrt{T}e_i}\right|^2 \\
&= \sup_{F'\subseteq I \text{finite}}\sup_{F\subseteq I \text{finite}}\sum_{j\in F'}\sum_{i\in F}\left|\inner{f_j, \sqrt{T}e_i}\right|^2 \\
&= \sum_{j\in I}\sum_{i\in I}\left|\inner{\sqrt{T}f_j, e_i}\right|^2 \\
&= \sum_{j\in I}\norm{\sqrt{T}f_i}^2 \\
&= \sum_{i\in I}\inner{\sqrt{T}f_i, \sqrt{T}f_i} \\
&= \sum_{i\in I}\inner{e_i, Te_i}.
\end{align*} 
\end{proof}


\begin{proposition} \label{traceCommutatorCompactSA}
Let $H$ be a Hilbert space and $A,B\in\Lin(X)$ such that $B$ is compact self-adjoint, then $\Tr[A,B] = 0$.
\end{proposition}
\begin{proof}
Let $\seq{e_n}$ be an orthonormal basis of eigenvectors of $B$, with corresponding real eigenvalues $\lambda_n$. This exists by the spectral theorem (TODO ref). Then
\begin{align*}
\Tr[A,B] &= \sum_n\inner{e_n, [A,B]e_n} \\
&= \sum_n\inner{e_n, ABe_n} - \inner{e_n, BAe_n} \\
&= \sum_n\inner{e_n, ABe_n} - \inner{Be_n, Ae_n} \\
&= \sum_n\lambda_n\inner{e_n, Ae_n} - \lambda_n\inner{e_n, Ae_n} \\
&= 0.
\end{align*}
\end{proof}
\begin{corollary}
Let $H$ be a Hilbert space and $A,B\in\Lin(X)$ such that $A$ is self-adjoint and $B$ compact. If $[A,B]$ is trace-class, then $\Tr[A,B] = 0$.
\end{corollary}
\begin{proof}
If $[A,B]$ is trace-class, then $-[A,B]^* = [A,B^*]$ is also 
\end{proof}

TODO: continuity: \url{https://math.stackexchange.com/questions/3511787/is-the-trace-map-continuous-on-the-trace-class}

\section{Dilation theory}
TODO: move below continuous functional calculus
\subsection{Dilations, $N$-dilations and power dilations}
\begin{definition}
Let $\mathcal{H} \subseteq \mathcal{H}'$ be Hilbert spaces and let $P_\mathcal{H}$ be the projection on $\mathcal{H}$. If a pair of linear maps $S: \mathcal{H}'\to\mathcal{H}'$ and $T: \mathcal{H}\to \mathcal{H}$ satisfy the relation
\[ T = P_\mathcal{H} S |_\mathcal{H} \]
then $T$ is called a \udef{compression} of $S$ and $S$ a \udef{dilation} of $T$. This is abbreviated $T\prec U$.

\begin{itemize}
\item Let $N\in\N$. If $T^k = P_\mathcal{H} S^k |_\mathcal{H}$ for all $k\leq N$, then $S$ is called an \udef{$N$-dilation}.
\item If this holds for all $k\in\N$, then $S$ is called a \udef{power dilation}.
\item If $T^* = P_\mathcal{H} S^* |_\mathcal{H}$, we call TODO??
\end{itemize}
We call $\mathcal{H}'$ \udef{minimal} if the only reducing subspace for $S$ that contains $\mathcal{H}$ is $\mathcal{H}'$.
\end{definition}

If $S$ is a dilation of $T$, then we clearly have $T = P_\mathcal{H} S P_\mathcal{H}|_\mathcal{H}$.

\begin{lemma}
Let $S:\mathcal{H}'\to\mathcal{H}'$ be an $N$-dilation of $T: \mathcal{H}\to \mathcal{H}$ and $p$ a polynomial of degree at most $N$. Then
\[ p(T) = P_\mathcal{H}p(S)|_\mathcal{H}. \]
\end{lemma}

Let $\mathcal{H}$ be a Hilbert space. We call $T\in\Bounded(\mathcal{H})$ a \udef{contraction} if $\norm{T}\leq 1$.
\begin{proposition} \label{dilationOfContraction}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
From $\norm{T}\leq 1$ (and the fact that $T^*T$ is normal), we have that $\vec{1}-T^*T\geq 0$ by spectral mapping. We can define $D_T = \sqrt{\vec{1}-T^*T}$. Then
\[ U = \begin{pmatrix}
T & D_{T^*} \\ D_T & -T^*
\end{pmatrix} \]
is a dilation of $T$ and it is unitary:
\begin{align*}
UU^* &= \begin{pmatrix}
TT^* + D_{T^*}^2 & TD_T^* - D_{T^*}T \\
D_TT^* - T^*D_{T^*}^* & D^2_{T} + T^*T
\end{pmatrix} = \begin{pmatrix}
\vec{1} & TD_T - D_{T^*}T \\
D_TT^* - T^*D_{T^*} & \vec{1}
\end{pmatrix} \\
U^*U &= \begin{pmatrix}
T^*T + D_{T}^2 & T^*D_{T^*} - D_{T}^*T^* \\
D_{T^*}^*T - TD_{T} & D^2_{T^*} + TT^*
\end{pmatrix} = \begin{pmatrix}
\vec{1} & T^*D_{T^*} - D_{T}T^* \\
D_{T^*}T - TD_{T} & \vec{1}.
\end{pmatrix}
\end{align*}
We have used that $D_T$ is self-adjoint for all contractions $T$. We just need to show that $TD_T = D_{T^*}T$. Clearly we have
\[ T(D_T)^2 = T(\vec{1} - T^*T) = T - TT^*T = (\vec{1} - TT^*)T = (D_{T^*})^2T. \]
By iterating this calculation, we have $T(D_T)^{2n} = (D_{T^*})^{2n}T$ for all $n\in \N$. By linearity, we have $Tp(D_T^2) = p(D_{T^*}^2)T$ for all every polynomical $p$.

Choose a sequence $\seq{p_n}$ of real polynomials that converges to the square root function $\sqrt{-}$ uniformly on $\interval{0,1}$ (e.g.\ Bernstein polynomials \ref{uniformApproximationByBernsteinPolynomials}; the existence of such a sequence is guaranteed by the Weierstrass approximation theorem \ref{WeierstrassApproximationTheorem}).

Let $\Phi_{D_T^2}$ and $\Phi_{D_{T^*}^2}$ be the continuous functional calculi of $D_T^2$ and $D_{T^*}^2$, respectively. Using the fact that $D_T$ and $D_{T^*}$ are positive operators (by spectral mapping), we calculate
\begin{align*}
TD_{T} &= T\sqrt{D_T^2} \\
&= T\Phi_{D_T^2}(\sqrt{-}) \\
&= T\Phi_{D_T^2}(\lim_{n\to \infty}p_n) \\
&= \lim_{n\to \infty}T\Phi_{D_T^2}(p_n) \\
&= \lim_{n\to \infty}Tp_n\big(D_T^2\big) \\
&= \lim_{n\to \infty}p_n\big(D_{T^*}^2\big)T \\
&= \lim_{n\to \infty}\Phi_{D_{T^*}^2}(p_n)T \\
&= \Phi_{D_{T^*}^2}(\lim_{n\to \infty} p_n)T \\
&= \Phi_{D_{T^*}^2}(\sqrt{-})T \\
&= \sqrt{D_{T^*}^2}T = D_{T^*}T.
\end{align*}
\end{proof}
The operator $D_T$ in the previous proof is sometimes called the \udef{defect operator} of $T$. It measures in some sense how far $T$ is from being a unitary operator. If $T$ is unitary, then $D_T = 0 = D_{T^*}$. If $T$ is an isometry, then $D_T = 0$ (by \ref{isometryRangeProjection}) and $D_{T^*}$ is a projector ($TT^*$ is a projector by \ref{isometryCharacterisation}, so $\vec{1} - TT^*$ is too by \ref{projectorOrthogonalComplement} and $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$).

There are other possible unitary dilations:
\begin{lemma}
Let $\mathcal{H}$ be a Hilbert space, $T$ an operator on $\mathcal{H}$ with unitary dilation $U$ on $\mathcal{H}^2$ and $W$ a unitary on $\mathcal{H}$. Then
\[ \begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix}U \qquad\text{and}\qquad U\begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix} \]
are also unitary dilations of $T$.
\end{lemma}
\begin{proof}
To see that these operators are unitary, it is enough to observe that $\begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix}$ is unitary and that any product of unitaries is unitary (TODO ref). Indeed,
\begin{align*}
\begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix}\begin{pmatrix}
\vec{1} & 0 \\
0 & W^*
\end{pmatrix}
&= \begin{pmatrix}
\vec{1} & 0 \\
0 & WW^*
\end{pmatrix} = \begin{pmatrix}
\vec{1} & 0 \\
0 & \vec{1}
\end{pmatrix} \\
\begin{pmatrix}
\vec{1} & 0 \\
0 & W^*
\end{pmatrix}\begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix}
&= \begin{pmatrix}
\vec{1} & 0 \\
0 & W^*W
\end{pmatrix} = \begin{pmatrix}
\vec{1} & 0 \\
0 & \vec{1}
\end{pmatrix}.
\end{align*} 

To see that these operators are dilations, we calculate
\begin{align*}
\begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix}U &= \begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix}\begin{pmatrix}
T & U_{0,1} \\
U_{1,0} & U_{1,1} \\
\end{pmatrix} = \begin{pmatrix}
T & U_{0,1} \\
WU_{1,0} & WU_{1,1}
\end{pmatrix} \\
U\begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix} &= \begin{pmatrix}
T & U_{0,1} \\
U_{1,0} & U_{1,1} \\
\end{pmatrix}\begin{pmatrix}
\vec{1} & 0 \\
0 & W
\end{pmatrix} = \begin{pmatrix}
T & U_{0,1}W \\
U_{1,0} & U_{1,1}W
\end{pmatrix}
\end{align*}
\end{proof}
In particular, for any contraction $T$, the operators
\[ \begin{pmatrix}
T & -D_{T^*} \\ D_T & T^*
\end{pmatrix}, \qquad \begin{pmatrix}
T & D_{T^*} \\ -D_T & T^*
\end{pmatrix} \qquad\text{and}\qquad \begin{pmatrix}
T & iD_{T^*} \\ iD_T & T^*
\end{pmatrix} \]
are also unitary dilations (which we obtain by setting $W = -\vec{1}$ and $W = i\vec{1}$).


\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every isometry $T$ on $\mathcal{H}$ has a unitary power dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
Consider the unitary dilation of \ref{dilationOfContraction}. When $T$ is an isometry this reduces to
\[ U = \begin{pmatrix}
T & D_{T^*} \\ 0 & -T^*
\end{pmatrix} = \begin{pmatrix}
T & \vec{1}-TT^* \\ 0 & -T^*
\end{pmatrix}, \]
where we have used that $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$ is a projector.

Now for all $n\in\N$ we have $U^n = \begin{pmatrix}
T^n & * \\ 0 & (-T^*)^n
\end{pmatrix}$, so in particular $P_\mathcal{H}U^n|_\mathcal{H} = T^n$, meaning $U$ is a power dilation of $T$. 
\end{proof}

\begin{lemma}
Let $T$ a contraction on a Hilbert space $\mathcal{H}$. Then $V_T: \mathcal{H} \to \mathcal{H}\oplus\mathcal{H}: x\mapsto (Tx, D_Tx)$ is an isometry.
\end{lemma}
\begin{proof}
For all $x\in \mathcal{H}$ we have
\[ \norm{V_Tx} = \sqrt{\norm{Tx}^2 + \norm{D_Tx}^2} = \sqrt{\inner{Tx,Tx} + \inner{D_Tx,D_Tx}} = \sqrt{\inner{T^*Tx,x} + \inner{D_T^2x,x}} = \sqrt{\inner{x,x}} = \norm{x}. \]
\end{proof}

\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\}^N \subseteq \mathcal{H}^{N+1}$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary $N$-dilation $U$ on $\mathcal{H}^{N+1}$.
\end{proposition}
\begin{proof}
Let $U'$ be a unitary dilation of $T$ on $\mathcal{H}^2$. Let $C_1 = U'_{-,1}$ and $C_2 = U'_{-,2}$ denote the columns. Then
\[ U = \begin{pmatrix}
C_1 & \mathbb{0}^{2\times N-1} & C_2 \\
\mathbb{0}^{N-1\times 1} & \mathbb{1}^{N-1\times N-1} & \mathbb{0}^{N-1\times 1}
\end{pmatrix} \]
is unitary by
\[ \begin{pmatrix}
C_1^* & \mathbb{0} \\
\mathbb{0} & \mathbb{1} \\
C_2^* & \mathbb{0}
\end{pmatrix}\begin{pmatrix}
C_1 & \mathbb{0} & C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0}
\end{pmatrix} = \begin{pmatrix}
C_1^*C_1 & \mathbb{0} & C_1^*C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0} \\
C_2^*C_1 & \mathbb{0} & C_2^*C_2
\end{pmatrix} = \mathbb{1}^{N+1\times N+1}. \]
We just need to show that the (1,1)-component of $U^k$ is $T^k$ for all $k\in 1:N$. In order to perform the multiplication, we rewrite $U$ such that the row and column partitions are the same, i.e.\ $(2|(N-3)|2)\times (2|(N-3)|2)$:
\[ U = \begin{pmatrix}
\begin{bmatrix}
T & 0 \\ D_T & 0
\end{bmatrix} & \mathbb{0} & \begin{bmatrix}
0 & D_{T^*} \\ 0 & -T^*
\end{bmatrix} \\
\begin{bmatrix}
0 & 1 \\ \mathbb{0} & \mathbb{0}
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 0 \\ \mathbb{1} & \mathbb{0}
\end{bmatrix} & \mathbb{0} \\
\begin{bmatrix}
0 & 0 \\ 0 & 0
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 1 \\ \mathbb{0} & 0
\end{bmatrix} & \begin{bmatrix}
0 & 0 \\ 1 & 0
\end{bmatrix}
\end{pmatrix} \]
TODO
\end{proof}

\begin{proposition}[von Neumann's inequality]
Let $T$ be a contraction on some Hilbert space $\mathcal{H}$. Then, for every polynomial $p\in\C[z]$,
\[ \norm{p(T)}\leq \sup_{|z|=1}|p(z)|. \]
\end{proposition}
\begin{proof}
Suppose the degree of $p$ is $N$. Let $U$ be a unitary $N$-dilation of $T$. Then
\[ \norm{p(T)} = \norm{P_\mathcal{H}p(U)|_\mathcal{H}}\leq \norm{p(U)} = \sup_{z\in\sigma(U)}|p(z)| \leq \sup_{|z|=1}|p(z)| \]
since the spectrum of $U$ is contained in the unit circle.
\end{proof}

\begin{theorem}[Sz.-Nagy's dilation theorem]
Let $\mathcal{H} \subseteq \ell^2(\N)\otimes\mathcal{H}$ be Hilbert spaces. Every contraction on $\mathcal{H}$ has a unitary power dilation on $\ell^2(\N)\otimes\mathcal{H}$.
\end{theorem}




\section{Constructions}
\subsection{Direct sum}

Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. By considering them as Banach spaces we can take the $\ell^2$-direct sum. (TODO: other sequence spaces?)
\begin{proposition}
Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. The $\ell^2$-direct sum is a Hilbert space.
\end{proposition}
This gives the conventional interpretation of the \udef{Hilbert space direct sum}: it is the $\ell^2$-direct sum of the summands as Banach spaces.


\subsection{Tensor product}
\url{https://web.ma.utexas.edu/mp_arc/c/14/14-2.pdf}

TODO: tensor product of operators defined on algebraic tensor product of their domains!

\begin{proposition}
If $S,T$ are closable operators, then $S\otimes_a T$ is closable.
\end{proposition}
\begin{proof}
TODO
\end{proof}

For closable operators, we define $S\otimes T \defeq \overline{S\otimes_a T}$.

\begin{proposition}
For bounded operators $S,T$, we have $\norm{S\otimes T} = \norm{S}\,\norm{T}$.
\end{proposition}

\section{Spectral properties of operators on Hilbert spaces}
\begin{lemma}
Let $T \in \Bounded(H)$ for some Hilbert space $H$. Then
$\rho(T) = \overline{\rho(T^*)}$, where the bar denotes complex conjugation.
\end{lemma}
\begin{proof}
TODO?? Take $\lambda\in\rho(T)$. Then
\[ ((\lambda\id - A)^{-1})^* = (\overline{\lambda}\id - A^*)^{-1} \]
so $\overline{\lambda}\in\rho(T^*)$ iff $((\lambda\id - A)^{-1})^*$ is bounded iff $(\lambda\id - A)^{-1}$ is bounded iff $\lambda\in \rho(T)$.
\end{proof}

\begin{lemma} \label{eigenspaceOrthogonalAdjoint}
Let $L$ be a densely defined operator on a Hilbert space $H$. Take $\lambda\in \pspec(L)$ and $\mu\in\pspec(L^*)$. If $\lambda \neq \overline{\mu}$, then
\[ \ker(\lambda\id - L)\perp \ker(\mu\id - L^*). \]
\end{lemma}
\begin{proof}
Take non-zero eigenvectors $x,y$ such that $Ax = \lambda x$ and $A^*y = \mu y$. Then
\[ \lambda \inner{y,x} = \inner{y,\lambda x} = \inner{y, Ax} = \inner{A^*y,x} = \inner{\mu y,x} = \overline{\mu}\inner{y,x}. \]
So we have $(\lambda - \overline{\mu})\inner{y,x} = 0$.
\end{proof}

\begin{proposition} \label{adjointSpectrumNoResidual}
Let $L$ be a densely defined operator on a Hilbert space $H$. Then the following are equivalent:
\begin{enumerate}
\item the residual spectrum of $L$ is empty;
\item $\overline{\pspec(L^*)} \subseteq \pspec(L)$;
\end{enumerate}
as are the following:
\begin{enumerate}
\item the residual spectrum of $L^*$ is empty;
\item $\pspec(L) \subseteq \overline{\pspec(L^*)}$.
\end{enumerate}
In particular all these statements hold if $L$ is normal.
\end{proposition}
\begin{proof}
Consider, for all $x\in \dom(L), y\in\dom(L^*)$, the equality
\[ \inner{(\lambda\id-L)x,y} = \inner{x,(\overline{\lambda}\id-L^*)y}. \]
We can make the following inferences:
\begin{itemize}
\item If $\lambda\in \overline{\pspec(L^*)}$, then the equality holds in particular for all eigenvectors $y$. This implies $\inner{(\lambda\id-L)x,y} = 0$. By \ref{perpToDenseSet} $\im(\lambda\id-L)$ may then not be dense, so it cannot be injective because the residual spectrum of $L$ is empty.
\item Assume $\lambda\id-L$ injective and take  $y\perp \im(\lambda\id-L)$. Then by the equality $\inner{x, (\overline{\lambda}\id - L^*)y} = 0$ for all $x\in\dom(L)$, which is dense. So $(\overline{\lambda}\id - L^*)y = 0$ by \ref{perpToDenseSet}. Now $\lambda\notin \pspec(L)$, so $\overline{\lambda}\notin \pspec(L^*)$. Thus $y = 0$ and $\im(\lambda\id-L)^\perp = \{0\}$, meaning $\im(\lambda\id-L)$ is dense.
\end{itemize}
The arguments for the second set of statements are similar.

If $L$ is normal, then $\ker(\lambda \id - L) = \ker{\overline{\lambda}\id -L^*}$ by \ref{equalityKernelAdjointNormal}, so $\pspec(L) = \overline{\pspec(L^*)}$.
\end{proof}

\begin{proposition}
Let $T$ be a closed, densly defined operator on a Hilbert space.
\begin{enumerate}
\item If $\lambda\in\rho(T)$, then $\overline{\lambda}\in\rho(T^*)$.
\item If $\lambda\in\rspec(T)$, then $\overline{\lambda}\in\pspec(T^*)$.
\item If $\lambda\in\pspec(T)$, then $\overline{\lambda}\in\rspec(T^*)\cup\pspec(T^*)$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO Compare with \ref{adjointSpectrumNoResidual}. CLosure necessary?
\end{proof}


\begin{proposition}
Let $T$ be a unitary operator. Then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item $\spec(T) \subset \setbuilder{\lambda\in\C}{|\lambda| = 1}$.
\end{enumerate}
\end{proposition}
TODO: move to more general place??

\begin{lemma}
The eigenvalues of a bounded dissipative linear operator
lie in the half-plane $\Im\lambda \geq 0$.
\end{lemma}


\subsubsection{Residual spectrum}
\begin{proposition}
Let $L$ be a densely defined linear operator on a Hilbert space. If $\lambda$ is in the residual spectrum of $L$ with deficiency $m$, then $\overline{\lambda}$ is in the point spectrum of $L^*$ with multiplicity $m$.
\end{proposition}
\begin{proof}
By \ref{kernelImageAdjoint} we have
\[ \im(\lambda \id - L)^\perp = \ker(\lambda\id - L)^* = \ker(\overline{\lambda}\id - L^*). \]
\end{proof}

\subsection{Rayleigh quotient}
\begin{lemma}
Let $L$ be an operator on a Hilbert space. If $x$ is an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \lambda. \]
\end{lemma}
\begin{proof}
Let $x$ be an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \frac{\inner{x,Lx}}{\inner{x,x}} = \lambda \frac{\inner{x,x}}{\inner{x,x}} = \lambda. \]
\end{proof}

\begin{proposition}
If $U$ is unitary, then $\spec(U)\subset \mathbb{T}$.
\end{proposition}

\section{Perturbation and approximation of operators}
\subsection{Rank-one perturbation}
TODO \url{https://www.researchgate.net/profile/Barry-Simon/publication/2433404_Spectral_Analysis_Of_Rank_One_Perturbations_And_Applications/links/004635214ec1c68781000000/Spectral-Analysis-Of-Rank-One-Perturbations-And-Applications.pdf}


\begin{proposition}[Aronszajn-Krein formula]
Let $A$ be an operator on a Hilbert space $\mathcal{H}$, $v\in \mathcal{H}$ and $\alpha\in \R$. Set $A_\alpha \defeq A + \alpha \ketbra{v}{v}$. Then
\[ \inner{v, R_{A_\alpha}(\lambda)v} = \frac{\inner{v, R_A(\lambda)v}}{1+\alpha \inner{v,R_A(\lambda)v}}, \]
for all $\lambda \in \res(A)\cap \res(A_\alpha)$.
\end{proposition}
\begin{proof}
For all $\lambda \in \res(A)\cap \res(A_\alpha)$ we have the second resolvent identity \ref{resolventIdentitiesOperators}, so
\begin{align*}
R_{A_\alpha}(\lambda) - R_{A}(\lambda) &= R_{A_\alpha}(\lambda)\big(A_\alpha - A\big)R_{A}(\lambda) \\
&= \alpha R_{A_\alpha}(\lambda)\ketbra{v}{v}R_{A}(\lambda).
\end{align*}
Taking the matrix element $\inner{v,(\cdot) v}$ yields
\[ \inner{v,R_{A_\alpha}(\lambda)v} - \inner{v,R_{A}(\lambda)v} = \alpha \inner{v,R_{A_\alpha}(\lambda)v}\inner{v,R_{A}(\lambda)v}. \]
Rearranging gives the result.
\end{proof}

\section{Examples of Hilbert spaces}

\subsection{The $L^2$ spaces}

\subsubsection{Multiplication operators}
\begin{lemma} \label{multiplicationOperatorDenselyDefined}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and $f\in \meas(\Omega, \C)$. Then $M_f: L^2(\Omega,\diff\mu)\to L^2(\Omega,\diff\mu)$ is densely defined.
\end{lemma}
\begin{proof}
Suppose $g\in \dom(M_f)^\perp$. Now $\underline{0}\leq (|f|+1)^2 = |f|^2 + 2|f| + 1$, so $|f|\leq \frac{1}{2}(|f|^2 + 1)$ and thus $\frac{2|f|}{|f|^2 + 1} \leq \underline{1}$. Now $\frac{2f}{|f|^2 + 1}g\in L^2(\Omega,\diff\mu)$ by \ref{propertiesIntegralPositiveFunctions}, using
\[ \Big|\frac{2f}{|f|^2 + 1}g\Big| = \frac{2|f|}{|f|^2 + 1}|g| \leq \underline{1}|g| = |g|. \]
Thus $\frac{2g}{|f|^2+1} \in \dom(M_f)$. Then
\[ 0 = \inner{g, \frac{2g}{|f|^2+1}} = \int_\Omega |g|^2 \frac{2}{|f|^2+1}\diff{\mu}, \]
which, by \ref{functionPropertiesFromIntegral}, implies $|g|^2 \frac{2}{|f|^2+1} = 0$ a.e. and thus $g = 0$ a.e.

We have shown that $\dom(M_f)^\perp = \{0\}$, which means that $\dom(M_f)$ is dense in $L^2(\Omega,\diff\mu)$.
\end{proof}

\begin{proposition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and $f\in \meas(\Omega, \C)$. Consider the multiplication operator $M_f: L^2(\Omega,\diff\mu)\to L^2(\Omega,\diff\mu)$. Then $M_f^* = M_{\overline{f}}$ and $\dom(M_f*) = \dom(M_f)$.
\end{proposition}
\begin{proof}
For any $g\in \dom(M_{\overline{f}})$ and $h\in \dom(M_f)$, we have
\[ \inner{g, M_fh} = \int_\Omega \overline{g}fh \diff\mu = \int_\Omega \overline{\overline{f}g}h \diff\mu = \inner{M_{\overline{f}}g, h}, \]
so $M_{\overline{f}}$ is an adjoint of $M_f$ and thus $M_{\overline{f}}\subseteq M_f^*$.

Now take $g\in \dom(M_f^*)$. Then, for all $h\in \dom(M_f)$, we have
\[ \int_\Omega \overline{M_f^*g}h \diff\mu = \inner{M_f^*g,h} = \inner{g,M_f h} = \int_\Omega \overline{g}fh\diff\mu, \]
so $\int_\Omega \overline{(M_f^*g - g\overline{f})}h\diff\mu = 0$. Set $A_n \defeq \{|f| \leq n\}$. Then $\charFunc{A_n}h'\in \dom(M_f)$ for all $h\in L^2(\Omega,\diff\mu)$. Thus $\int_\Omega \overline{(M_f^*g - g\overline{f})}\charFunc{A_n}h\diff\mu = 0$, which implies $\overline{(M_f^*g - g\overline{f})}\charFunc{A_n} = 0$ a.e. Since this holds for all $n\in \N$, we have $\overline{M_f^*g - g\overline{f}} = 0$. Thus $g\in \dom(M_{\overline{f}})$.

It is clear that $\dom(M_f) = \dom(M_{\overline{f}}) = \dom(M_f^*)$.
\end{proof}
\begin{corollary}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and $f\in \meas(\Omega, \C)$. Then $M_f$ is a closed operator.
\end{corollary}
\begin{proof}
Since $M_{f} = M_{\overline{f}}^*$, the closedness of $M_f$ follows from \ref{adjointGraph}.
\end{proof}

\subsection{The $\ell^2$ spaces}
Sequence spaces $\ell^p$ Hilbert iff $p=2$. (TODO: other sequence spaces?)

\chapter{Types of operators}

\section{Integral operators and transforms}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. Then an \udef{integral operator} or \udef{integral transform} is a map of the form
\[ T: U\subset (\Omega\to\C) \to (\Omega\to\C): f \mapsto \int_\Omega K(x,y)f(y) \diff{\mu(y)} \]
where $K\in (\Omega\times \Omega \to \C)$ is the \udef{kernel} or \udef{nucleus} of $T$.

The kernel is called
\begin{itemize}
\item \udef{symmetric} if $K(x,y) = \overline{K(y,x)}$;
\item \udef{Volterra} if $\Omega = \R$ and $K(x,y) = 0$ for $y>x$;
\item \udef{convolutional} if $\Omega$ is a group and $K(x,y) = F(x-y)$ for some function $F$;
\item \udef{Hilbert-Schmidt} if $K\in L^2(\Omega\times \Omega)$, i.e.\
\[ \int_{\Omega\times \Omega}|K(x,y)|^2\diff{x}\diff{y} < \infty; \]
\item \udef{singular} if $K(x,y)$ is unbounded on $\Omega\times \Omega$.
\end{itemize}
\end{definition}

\begin{lemma}
Hilbert-Schmidt integral operators are compact operators on $L^2(\Omega\times \Omega)$.
\end{lemma}
\begin{proof}
A Hilbert-Schmidt integral operator $T$ maps $L^2(\Omega)$ to $L^2(\Omega)$ functions:
\begin{align*}
\norm{Tu}^2_{L^2} &= \int_\Omega \left|\int_{\Omega} K(x,y)u(y)\diff{\mu(y)}\right|^2\diff{\mu(x)} \\
&\leq \int_\Omega \left(\int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg)\diff{\mu(x)} \\
&= \left(\int_\Omega \int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\diff{\mu(x)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg) < \infty
\end{align*}
where we have used the Cauchy-Schwarz inequality \ref{CauchySchwarz}. This also immediately shows Hilbert-Schmidt integral operators are bounded.

TODO Compact
\end{proof}

\begin{proposition}
Let $T$ be an integral operator with kernel $K(x,y)$, then $T^*$ is the integral operator with kernel $\overline{K(y,x)}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A$ be a Borel set and $K:A\times A\to \C$ a measurable function such that the integral operator with kernel $K$ is bounded. Then the adjoint of the integral operator is again an integral operator with kernel $K^*(x,y) = \overline{K(y,x)}$.
\end{proposition}

\begin{proposition}
Let $T$ be a Volterra integral operator. Then $\spec(T) = \cspec(T) = \{0\}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsection{Integral equations}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. An \udef{integral equation} is an equation containing an unknown function on $\Omega$ and an integral over $\Omega$.

An integral equation is 
\begin{itemize}
\item \udef{of the first kind} if it is of the form
\[ \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function and $u$ is the unknown function;
\item \udef{of the second kind} if it is of the form
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function, $\lambda$ is a scalar and $u$ is the unknown function.
\end{itemize}
\end{definition}

\begin{proposition}
Let
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x)\]
be an integral equation of the second kind. This integral equation has a unique solution $u$ if
\[ |\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}. \]
\end{proposition}
\begin{proof}
Let the map $T$ be defined by
\[ T(u) = x\mapsto \frac{1}{\lambda}\left(\int_\Omega K(x,y)u(y)\diff{\mu(y)} + f(x)\right) \]
so that solutions of the integral equation are exactly the fixed points of $T$. Then
\[ \norm{Tu-Tv}_\infty = \sup_{x\in\Omega} \frac{1}{|\lambda|} \left|\int_\Omega K(x,y)(u(y)- v(y))\diff{\mu(y)}\right| \leq \frac{1}{|\lambda|} \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)} \cdot \norm{u-v}_\infty. \]
So $T$ is a contraction if $|\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}$. The result follows from \ref{contractionFixedPoint}.
\end{proof}

\subsection{Positive kernels}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. A function $K: \Z\times \Z\to \Bounded(\mathcal{H})$ is called a \udef{positive definite kernel} if $[A(i,j)]_{i,j\in [-n:n]} \in \Bounded(\bigotimes_{i = -n}^n \mathcal{H})$ is positive for all $n\in \N$.
\end{definition}

\begin{proposition}[Kolmogorov decomposition]
Let $\mathcal{H}$ be a Hilbert space and $K: \Z\times \Z\to \Bounded(\mathcal{H})$ a positive kernel. Then there exists a function $V: \Z\to A$ such that $A(i,j) = V^*(i)V(j)$ for all $i,j\in \Z$.
\end{proposition}
\begin{proof}

\end{proof}

\section{Convolution operators}






\chapter{Fourier transforms}

\section{Types of Fourier transform}

\subsection{Discrete Fourier transform}
\begin{definition}
Then $N$-dimensional \udef{discrete Fourier transform} (DFT) is the linear transformation $\C^N \to \C^N$ defined by the matrix $DFT_N$ with components
\[ [DFT_N]_{j,k} = \frac{1}{\sqrt{N}}\omega_N^{(j-1)(k-1)}, \]
where $\omega_N$ is the $N^\text{th}$ root of unity.
\end{definition}

\begin{lemma} \mbox{}
\begin{enumerate}
\item The $DFT_N$ matrix is the Vandermonde matrix of the roots of unity, up to the normalisation factor $1/\sqrt{N}$.
\item The $DFT_N$ matrix is unitary.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Just an observation.

(2) We calculate
\[ [DFT_N\cdot DFT_N]_{j,l} = \frac{1}{N}\sum_{k=1}^N\omega_N^{jk}\overline{\omega_N}^{kl} = \frac{1}{N}\sum_{k=1}^N\omega_N^{k(j-l)} = \delta_{j,l}. \]
\end{proof}


\chapter{$C^*$-algebras}
\section{$*$-algebras}
TODO: move higher: $*$-algebras much higher and Banach stuff with Banach stuff.
\begin{definition}
A \udef{$*$-algebra} is a $*$-r(i)ng $(A,+,\cdot, *)$, with involution $*$, that is an associative algebra over a commutative $*$-ring $(R,+,\cdot, ')$, with involution $'$,
such that
\[ \forall r\in R, x\in A: \quad (rx)^* = r'x^*. \]

A \udef{complex $*$-algebra} is a $*$-algebra where the $*$-ring $R$ is $\C$ with complex conjugation as the involution $'$.

A \udef{real $*$-algebra} is a $*$-algebra where the $*$-ring $R$ is $\R$ with the identity map as the involution $'$.

If a $*$-algebra is also a Banach algebra and for all elements $\norm{x^*} =\norm{x}$, then it is called a \udef{Banach-$*$-algebra}.
\end{definition}

\begin{lemma} \label{starIsometry}
Let $A$ be a Banach-$*$-algebra. Then the map $x\mapsto x^*$ is an isometry and thus continuous.
\end{lemma}

\begin{lemma}
Let $A$ be a $*$-algebra. The unitisation $A^\dagger = A\oplus \F$ can also be seen as a $*$-algebra with the involution defined by
\[ (a, \lambda)^* = (a^*, \overline{\lambda}) \qquad \forall a\in A, \lambda\in \F.\]
\end{lemma}
\begin{lemma} \label{elementaryStarLemma}
Let $A$ be a unital $*$-algebra and $x\in A$. Then
\begin{enumerate}
\item $\vec{1}^* = \vec{1}$;
\item $x$ is invertible \textup{if and only if} $x^*$ is invertible; in this case with $(x^*)^{-1} = (x^{-1})^*$;
\item $\sigma(x^*) = \setbuilder{\overline{\lambda}}{\lambda \in \sigma(x)}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $\vec{1}^* x = (x^*\cdot \vec{1})^* = x^{**} = x$.
Similarly $x\vec{1}^* = x$, so $\vec{1}^*$ is an identity, which is unique by \ref{identityAbsorbingElementUnique}.

(2) First assume $x$ is invertible. We have
\[ x^*\cdot (x^{-1})^* = (x^{-1}x)^* = \vec{1}^* = \vec{1} \qquad\text{and}\qquad (x^{-1})^*\cdot x^* = (x\cdot x^{-1})^* = \vec{1}^* = \vec{1}. \]
This means that $x^*$ is invertible with $(x^*)^{-1} = (x^{-1})^*$.

Now assume $x^*$ is invertible. By the previous calculation, we have that $x = x^{**}$ is invertible.
\end{proof}

\begin{proposition} \label{smallestBanach*Algebra}
Let $A$ be a Banach-$*$-algebra and $S\subset A$ a subset. Then
\[ \mathcal{B}^*(S) \defeq \mathcal{B}(S\cup S^*) \]
is the smallest Banach-$*$-subalgebra in $A$ that contains $S$, where $\mathcal{B}$ is defined as in \ref{smallestBanachAlgebra} and $S^* = \setbuilder{s^*\in A}{s\in S}$.
\end{proposition}

\begin{definition}
Let $A$ be a $*$-algebra and $x\in A$. We say that $x$ is
\begin{enumerate}
\item \udef{normal}, if $x^*x = xx^*$;
\item \udef{self-adjoint}, if $x=x^*$;
\item a \udef{projection}, if $x=x^*=x^2$;
\item a \udef{partial isometry} if $x^*x$ and $xx^*$ are both projections. Then $x^*x$ is called the \udef{support projection} and $xx^*$ is called the \udef{range projection}.
\end{enumerate}
If $A$ is unital, we call $x$
\begin{itemize}
\item an \udef{isometry}, if $x^*x = \vec{1}$;
\item a \udef{co-isometry}, if $xx^* = \vec{1}$;
\item \udef{unitary}, if $x^*x = xx^* = \vec{1}$.
\end{itemize}
The set of all
\begin{enumerate}
\item normal elements in $A$ is denoted $\Normals(A)$;
\item self-adjoint elements in $A$ is denoted $\SelfAdjoints(A)$;
\item unitaries in $A$ is denoted $\Unitaries(A)$;
\item projections in $A$ is denoted $\Projections(A)$.
\end{enumerate}
\end{definition}
\begin{lemma}
We have the following implications:
\[ \text{projection} \Rightarrow \text{self-adjoint} \Rightarrow \text{normal} \Leftarrow \text{unitary}, \]
where the right-most arrow is only valid in unital $*$-algebras.
\end{lemma}

\begin{lemma}
Let $A$ be a unital $*$-algebra and $x\in A$. If $x$ is either an isometry or a co-isometry, then $x$ is a partial isometry.
\end{lemma}
\begin{proof}
First assume $x$ is an isometry. Then $x^*x = \vec{1}$ is clearly a projection. We calculate $(xx^*)^* = x^{**}x^* = xx^*$ and $(xx^*)^2 = x(x^*x)x^* = x\vec{1}x^* = xx^*$, which means that $xx^*$ is a projection.
\end{proof}

\begin{lemma} \label{orthogonalProjection}
Let $A$ be a unital $*$-algebra and $p\in\Projections(A)$. Then $\vec{1}-p$ is a projection.
\end{lemma}
\begin{proof}
We simply calculate
\[ (\vec{1}-p)^2 = (\vec{1}-p)(\vec{1}-p) = \vec{1} - p -p + p = \vec{1}-p = (\vec{1}-p)^*. \]
\end{proof}

\begin{proposition} \label{closedSubsetsBanachStarAlgebra}
Let $A$ be a Banach-$*$-algebra. Then
\begin{enumerate}
\item $\Normals(A)$ is a closed subset of $A$;
\item $\SelfAdjoints(A)$ is a closed subset of $A$;
\item $\Projections(A)$ is a closed subset of $A$;
\item if $A$ is unital, then $\Unitaries(A)$ is a closed subset of $A$.
\end{enumerate}
\end{proposition}
\begin{proof}
Note that the map $x\mapsto x^*$ is continuous, \ref{starIsometry}.

(1) Consider the continuous function $g: A\to A: x\mapsto x^*x - xx^*$. Then $\Normals(A) = g^\preimf(\{0\})$ and so is closed by \ref{preimageOpenClosed}.

(2) Consider the continuous function $g: A\to A: x\mapsto x - x^*$. Then $\SelfAdjoints(A) = g^\preimf(\{0\})$ and so is closed by \ref{preimageOpenClosed}.

(3) Consider the continuous functions $g_1: A\to A: x\mapsto x - x^*$ and $g_2: A\to A: x\mapsto x-x^2$. Then $\SelfAdjoints(A) = g_1^\preimf(\{0\}) \cap g_2^\preimf(\{0\})$ and so is closed by \ref{preimageOpenClosed} and \ref{propertiesTopology}.

(4) Consider the continuous functions $g_1: A\to A: x\mapsto xx^*$ and $g_2: A\to A: x\mapsto x^*x$. Then $\SelfAdjoints(A) = g_1^\preimf(\{\vec{1}\}) \cap g_2^\preimf(\{\vec{1}\})$ and so is closed by \ref{preimageOpenClosed} and \ref{propertiesTopology}.
\end{proof}

\begin{lemma} \label{realImaginaryParts}
Let $A$ be a $*$-algebra and $x\in A$. Then there are unique self-adjoint elements $x_1,x_2\in A$ such that $x = x_1+i\cdot x_2$. They are given by
\[ x_1 = \frac{x+x^*}{2} \qquad \text{and} \qquad x_2 = \frac{x-x^*}{2i}. \]
\end{lemma}
We call $x_1$ and $x_2$ the \udef{real part} and \udef{imaginary part} of $x$, respectively.

\subsection{Ideals}
\begin{definition}
Let $A$ be a $*$-algebra. A $*$-ideal is an (algebra) ideal that is closed under the $*$ operation.
\end{definition}

\begin{lemma} \label{starIdealCongruence}
Let $A$ be a $*$-algebra. A translation invariant binary relation $\mathfrak{q}$ on $A$ is a
$\{+, Â·, (z\cdot -), *\}_{z\in \C}$-congruence if and only if $\widetilde{\mathfrak{q}}$ is a $*$-ideal.
\end{lemma}
\begin{proof}
Compare \ref{congruenceRingIdeals} and \ref{congruenceSubspace}.

For all $(a,b)\in \mathfrak{q}$, we have
\[ (a,b)^* = (a^*, b^*) \in \mathfrak{q} \iff a^* - b^* = (a-b)^*\in \widetilde{\mathfrak{q}}. \]
\end{proof}

\subsection{$*$-homomorphisms}
\begin{definition}
Let $A,B$ be $*$-algebras. A \udef{$*$-homomorphism} is a linear, multiplicative, $*$-preserving map $\Psi: A \to B$.

If $A,B$ are unital and $\Psi(\vec{1}_A) = \vec{1}_B$, then we say $\Psi$ is \udef{unital}.
\end{definition}
\begin{lemma}
Let $A$ be a $*$-algebra, then $*$-homomorphisms map
\begin{enumerate}
\item normal elements to normal elements;
\item self-adjoints to self-adjoints;
\item projections to projections;
\item unitaries to unitaries, if the $*$-homomorphism is unital.
\end{enumerate}
\end{lemma}

\subsection{$*$-matrix algebras}
TODO define matrix algebra.
\begin{definition}
Let $A$ be a $*$-algebra. Then the matrix algebra $A^{n\times n}$ is considered a $*$-algebra with the star operation given defined by
\[ [a^*]_{i,j} \defeq [a]_{j,i}^*. \]
for all components of $a\in A^{n\times n}$.
\end{definition}
Notice that the $*$-operation acts as the element-wise $*$-operation composed with the transpose.
\section{$C^*$-algebras}
\begin{definition}
A (complex) \udef{$C^*$-algebra} is a complex Banach-$*$-algebra $A$ such that
\[\forall x\in A: \quad \norm{x^*x} = \norm{x}^2.\]
This identity is known as the \udef{$B^*$-identity}.
\end{definition}
TODO: originally 

\begin{definition}
A \udef{real} $C^*$-algebra is a real Banach-$*$-algebra such that 
\end{definition}

\begin{example}
TODO: Concrete $C^*$-algebras.

$\mathcal{C}(X)$ for some compact $X$ (need Hausdorff?). TODO: norm well defined (i.e.\ bounded) and for $g\in \mathcal{C}(X)$, $\sigma(g) = g[X]$.
\end{example}

\begin{proposition}
The $B^*$-identity is equivalent to the $C^*$-identity
\[\forall x\in A: \quad \norm{x^*x} = \norm{x^*}\cdot\norm{x}.\]
\end{proposition}
\begin{proof}
TODO. Highly non-trivial. TODO: move later.
\end{proof}


\begin{lemma} \label{C*identityEquivalent}
The $C^*$-identity is equivalent to
\[\forall x\in A: \quad \norm{x^*x} \geq \norm{x}^2.\]
\end{lemma}
\begin{proof}
Clearly the $C^*$-identity implies this inequality.

For the converse, let $x\in A$, then $\norm{x}^2 \leq \norm{x^*x} \leq \norm{x}\cdot\norm{x^*}$ and so $\norm{x}\leq \norm{x^*}$. By replacing $x$ with $x^*$ and using $x^{**}=x$ we also get $\norm{x}\geq \norm{x^*}$. Then $\norm{x^*x}\leq \norm{x^*}\cdot\norm{x} = \norm{x}^2$. Together with the original inequality this implies the $C^*$-identity.
\end{proof}

\begin{definition}
Let $A$ be a $C^*$-algebra and $D$ a subset of $A$. The $C^*$-algebra \udef{generated} by $D$, $C^*(D)$, is the smallest $C^*$-subalgebra of $A$ containing $D$.
TODO refine def.
\end{definition}
\begin{lemma}
$C^*(\vec{1},a)$ is commutative.
\end{lemma}

\begin{lemma} \label{consequencesC*}
Let $A$ be a $C^*$-algebra. The $B^*$-identity implies
\begin{enumerate}
\item  $\norm{\vec{1}} = 1$.
\item the involution $*$ is isometric: $\norm{x^*} = \norm{x}$;
\item the involution $*$ is continuous.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $A$ be a $C^*$-algebra. Then the sets $\Normals(A), \SelfAdjoints(A), \Unitaries(A)$ and $\Projections(A)$ are closed in $A$. 
\end{lemma}
\begin{proof}
This follows from the continuity of the multiplication and the involution $*$.
\end{proof}

\begin{proposition} \label{normNormalElement}
Let $A$ be a $C^*$-algebra and $x\in A$ a normal element. Then $\spr(x) = \norm{x}$.
\end{proposition}
\begin{proof}
We compute
\[ \norm{x}^4 = \norm{x^*x}^2 = \norm{x^*xx^*x} = \norm{(x^*)^2x^2} = \norm{x^2}^2 \]
where we have repeatedly applied the $C^*$-identity and used normality once. We conclude that $\norm{x^2} = \norm{x}^2$. Inductively we obtain $\norm{x^{(2^n)}} = \norm{x}^{2^n}$. By the spectral radius formula, \ref{spectralRadiusFormula}, we get
\[ \spr(x) = \lim_{n\to\infty}\norm{x^{(2^n)}}^{1/2^n} = \lim_{n\to\infty}\norm{x} = \norm{x}. \]
\end{proof}
\begin{corollary}
Let $A$ be a $C^*$-algebra and $x\in A$. If $x$ is normal and nilpotent, then $x=0$.
\end{corollary}
\begin{proof}
Every nilpotent is quasinilpotent by the spectral radius formula. By the proposition, we have $\norm{x} = 0$, so $x=0$.
\end{proof}
\begin{corollary} \label{atMostOneNorm}
Let $A$ be a $*$-algebra. There exists at most one norm on $A$ turning it into a $C^*$-algebra. If there is such a norm, it is given by $\norm{x} = \sqrt{\spr(x^*x)}$.
\end{corollary}
\begin{proof}
By the $C^*$-identity $\norm{x} = \sqrt{\norm{x^*x}}$ and $x^*x$ is normal, so we can apply the proposition.
\end{proof}
It is important to note that the spectral radius is a purely algebraic property and is independent of the norm.

\begin{lemma} \label{C*normSupNorm}
Let $A$ be a $C^*$-algebra and $a\in A$. Then $\norm{a} = \sup_{\norm{x}\leq 1}\norm{ax}$.
\end{lemma}
\begin{proof}
It is clear that the map $a\mapsto \sup_{\norm{x}\leq 1}\norm{ax}$ is a norm. By \ref{atMostOneNorm}, it is enough to verify that it satisfies the $C^*$-identity.

We calculate, using $\norm{x^*} = \norm{x}$ (\ref{consequencesC*}),
\begin{align*}
\sup_{\norm{x}\leq 1}\norm{a^*ax} &\geq \sup_{\norm{x}\leq 1}\norm{x^*}\cdot\norm{a^*ax} \\
&\geq \sup_{\norm{x}\leq 1}\norm{x^*a^*ax} = \sup_{\norm{x}\leq 1}\norm{(ax)^*ax} = \sup_{\norm{x}\leq 1}\norm{ax}^2 = \big(\sup_{\norm{x}\leq 1}\norm{ax}\big)^2.
\end{align*}
The $C^*$-identity follows from \ref{C*identityEquivalent}.
\end{proof}

\subsection{Properties of elements}
\subsubsection{Self-adjoint elements}
\begin{lemma} \label{normSelfAdjoint}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a self-adjoint element. Then
\[ \forall t\in\R: \quad \norm{x+it}^2 = \norm{x^2+t^2} \leq \norm{x}^2 + t^2. \]
\end{lemma}
\begin{proof}
Since $(x+it\cdot\vec{1})^* = x^* - it\cdot\vec{1}^* = x - it\cdot\vec{1}$ (using \ref{elementaryStarLemma}), we have
\begin{align*}
\norm{x+it}^2 &= \norm{(x+it)(x+it)^*} \\
&= \norm{(x+it)(x-it)} \\
&= \norm{x^2+t^2} \\
&\leq \norm{x^2}+t^2\norm{\vec{1}} = \norm{x^2}+t^2 \leq \norm{x}^2+t^2.
\end{align*}
\end{proof}

\begin{proposition}
Let $A$ be a unital $C^*$-algebra and $\varphi: A\to \C$ a linear functional satisfying $\norm{\varphi} = \varphi(\vec{1})$. If $a\in A$ is self-adjoint, then $\varphi(a) \in \R$.
\end{proposition}
\begin{proof}
We may assume $\varphi \neq 0$ and $\varphi(\vec{1}) = 1$. Using \ref{normSelfAdjoint}, we calculate
\[ |\varphi(x)+it|^2 = |\varphi(x+it)|^2 \leq \norm{x+it}^2 \leq \norm{x}^2 + t^2. \]
By \ref{boundedThenReal} this means $\varphi(x)\in\R$.
\end{proof}
TODO: alternate proof in Fillmore using exponential map.
\begin{corollary} \label{selfAdjointSpectrumReal}
Let $x\in A$ be self-adjoint. Then $\sigma(x) \subseteq \R$.
\end{corollary}
\begin{proof}
By \ref{commutativeSameSpectrum}, and the fact that $x$ is self-adjoint (TODO ref) we may assume $A$ commutative. Let $\lambda \in \sigma(x)$. By \ref{spectrumFromSpectrum} there is a $\varphi\in\hat{A}$ such that $\lambda = \varphi(x)$. By \ref{charactersUnital}, $\norm{\varphi} = \varphi(\vec{1}) = 1$. Then by the proposition $\lambda = \varphi(x) \in \R$.
\end{proof}
\begin{corollary}
Let $\varphi:A\to \C$ be a \emph{linear} functional satisfying $\norm{\varphi} = \varphi(\vec{1})$. Then $\varphi$ is $*$-preserving, i.e.\ for all $x\in A$
\[\varphi(x^*) = \overline{\varphi(x)}. \] 
\end{corollary}
\begin{proof}
By \ref{realImaginaryParts} we can write $x= x_1+ix_2$. Then $\varphi(x^*) = \varphi(x_1-ix_2) = \varphi(x_1) - i \varphi(x_2)$. By the proposition $\varphi(x_1), \varphi(x_2)\in \R$.
\end{proof}
\begin{corollary} \label{characters*Preserving}
Every character on $A$ is $*$-preserving.
\end{corollary}


\subsubsection{Partial isometries}
Partial isometry is the same as saying the adjoint is the Moore-Penrose pseudoinverse (i.e. $x^* = x^+$).

\begin{lemma} \label{partialIsometries}
Let $A$ be a $C^*$-algebra and $v\in A$.
If either $v^*v$ or $vv^*$ is a projection, then
\begin{enumerate}
\item $v=vv^*v$ and $v^* = v^*vv^*$;
\item the other is also a projection.
\end{enumerate}
Thus either condition is sufficient for $v$ to be a partial isometry.
\end{lemma}
\begin{proof}
First assume $v^*v$ is a projection. Put $z = (\vec{1}- vv^*)v$. Then
\[ z^*z = v^*(\vec{1}-vv^*)(\vec{1}-vv^*)v = v^*v - v^*vv*v -v^*vv^*v + v^*vv^*vv^*vv^*v = v^*v - v^*v - v^*v + v^*v = 0. \]
Now by the $C^*$-identity $\norm{z} = \sqrt{\norm{z^*z}} = 0$, so $0=z= v - vv^*v$. Multiplying this on the right by $v^*$ gives $vv^* = (vv^*)^2$. Because $vv^*$ is self-adjoint, this shows us it is a projection.

If we assume $vv^*$ is a projection, we put $z = (\vec{1}- v^*v)v^*$ instead and again $z^*z = 0$.
\end{proof}

\begin{lemma}
Let $v\in\Bounded(\mathcal{H})$ be a partial isometry. Then
\[\ker v = (I - v^*v)\mathcal{H} \qquad \ker v^* = (I - vv^*)\mathcal{H}\]
and
\[ v\mathcal{H} = vv^*\mathcal{H} \qquad v^*\mathcal{H} = v^*v\mathcal{H}. \]
\end{lemma}
\begin{proof}
First assume $x\in\ker v$. Then $(I-vv^*)x = Ix = x$, so $x\in (I - vv^*)\mathcal{H}$. Conversely, $v(I-v^*v)\mathcal{H} = (v-vv^*v)\mathcal{H} = 0\mathcal{H} = \{0\}$ by \ref{partialIsometries}.

Also by \ref{partialIsometries}, we have
\[ v\mathcal{H} \subseteq vv^*\mathcal{H} \subseteq vv^*v\mathcal{H} = v\mathcal{H}. \]
\end{proof}

\subsection{Algebraic aspects}
\subsubsection{Ideals}

\begin{proposition} \label{closedIdealIsStarIdeal}
Let A be a $C^*$-algebra and $I\subseteq A$ a closed two-sided ideal. Then $I$ is closed under the $*$ operation.
\end{proposition}
\begin{proof}
page 41 of \url{https://www.math.uvic.ca/faculty/putnam/ln/C*-algebras.pdf}
\end{proof}

\subsubsection{The lattice of $C^*$-algebras}
\begin{lemma}
The $C^*$-algebraic closure is given by the norm-closure of the algebraic closure.
\end{lemma}

\subsection{$C^*$-homomorphisms}

\begin{proposition} \label{starHomomorphismCstarProperties}
Let $A,B$ be $C^*$-algebras and $\Psi: A\to B$ a $*$-homomorphism. Then
\begin{enumerate}
\item $\Psi$ is bounded (and thus continuous) with $\norm{\Psi} \leq 1$;
\item if $\Psi$ is injective, then it is isometric;
\item $\ker(\Psi)$ is a closed $*$-ideal;
\item $\im(\Psi)$ is closed and thus a $C^*$ algebra;
\item if $A,B$ and $\Psi$ are unital, then $\norm{\Psi} = 1$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) WLOG we may assume $\Psi$ is unital, since we may replace it by $\Psi^\dagger$ and this operation preserves relevant properties, by \ref{DaggerMorphismProperties}. Because $x^*x$ and $\Psi(x^*x)$ are normal, we calculate using \ref{normNormalElement}
\[ \norm{x}^2 = \norm{x^*x} = \spr(x^*x) \geq \spr(\Psi(x^*x)) = \norm{\Psi(x^*x)} = \norm{\Psi(x)^*\Psi(x)} = \norm{\Psi(x)}^2, \]
where the inequality follows from an application of lemma \ref{spectrumOfImage} to $x^*x$ (using unitality of $\Psi$). Hence $\norm{\Psi}\leq 1$.

(2) By the calculation in (1), it is enough to prove $\spr(x^*x) \leq \spr(\Psi(x^*x))$. We can define a $*$-homomorphism $\Psi^{-1}: \im(\Psi)\to A$. Then
\[ \spr(\Psi(x^*x)) = \spr_{\im(\Psi)}(\Psi(x^*x)) \geq \spr\big((\Psi^{-1}\circ\Psi)(x^*x)\big) = \spr(x^*x) \]
by \ref{spectrumIndependentOfSurroundingAlgebra} and \ref{spectrumOfImage}.

(3) By \ref{kernelIsIdeal} $\ker(\Psi)$ is an ideal. By \ref{closedIdealIsStarIdeal} it is enough to observe that $\ker(\Psi)$ is closed, which it is by \ref{preimageOpenClosed}.

(4) By the factor theorem \ref{factorTheorem} and \ref{starIdealCongruence}, we obtain an injective $*$-homomorphism $\Psi': A/\ker(\Psi) \to B$ with $\im(\Psi') = \im(\Psi)$. Now $\Psi'$ is isometric by point (2), so $\im(\Psi) = \im(\Psi')$ is closed by \ref{isometryLemma}.

(5) In this case we have $\norm{\Psi(\vec{1})} = \norm{\vec{1}}$, so $\norm{\Psi} \geq 1$.
\end{proof}


\begin{lemma}
A surjective $*$-homomorphism from a unital $C^*$-algebra is unital.
\end{lemma}
\begin{proof}
Let $\Psi: A \to B$ be a surjective $*$-homomorphism with $A$ unital. For all $a\in A$:
\[ \Psi(a)\Psi(\vec{1}) = \Psi(a \vec{1}) = \Psi(a) \qquad \text{and} \qquad \Psi(\vec{1})\Psi(a) = \Psi(\vec{1} a) = \Psi(a). \]
As all elements of $B$ are of the form $\Psi(a)$, $\Psi(\vec{1})$ is a multiplicative identity for $B$.
\end{proof}

\subsubsection{Lifts}
TODO: move to $*$-algebra homomorphisms?
\begin{proposition}
Let $\Psi: A \to B$ be a surjective $*$-homomorphism between $C^*$-algebras. Then
\begin{enumerate}
\item every self-adjoint element $b\in B$ has a self-adjoint lift $a\in A$, such that $\norm{a} = \norm{b}$;
\item every positive element $b\in B$ has a positive lift $a\in A$, such that $\norm{a} = \norm{b}$;
\item every element $b\in B$ has a lift $a\in A$ such that $\norm{a} = \norm{b}$.
\end{enumerate}
\end{proposition}
In general normal elements, unitaries and projections do not lift to normal elements, unitaries and projections, unless $\Psi$ is injective.
\begin{lemma} \label{injectiveLifts}
Let $\Psi: A \to B$ be an injective $*$-homomorphism between $C^*$-algebras.
\begin{enumerate}
\item if $\Psi(a)$ is a normal element, then $a$ is a normal element;
\item if $\Psi(p)$ is a projection, then $p$ is a projection;
\item if $\Psi(u)$ is a unitary element, then $u$ is a unitary element.
\end{enumerate}
\end{lemma}
\begin{proof}
If $\Psi(a)\Psi(a)^* = \Psi(a)^*\Psi(a)$, then $\Psi(a^*a) = \Psi(aa^*)$ and $a^*a = aa^*$ by injectivity.

The other conditions are verified similarly.
\end{proof}

\subsection{Direct sums of $C^*$-algebras}
Note that multiplication in direct sum is pointwise: $(a,b)(c,d) = (ac, bd)$.

\begin{proposition}
Let $A,B$ be $C^*$-algebras. Then there exists a norm on $A\oplus B$ that makes it a $C^*$-algebra. This norm is given by
\[ \norm{(a,b)}_{A\oplus B} \defeq \max\{\norm{a}_A, \norm{b}_B\}. \]
\end{proposition}
\begin{proof}
TODO it is a norm.

We check that is satisfies the $C^*$-identity. Take $(a,b)\in A\oplus B$. Then
\begin{align*}
\norm{(a,b)^*(a,b)}_{A\oplus B} &= \norm{(a^*a,b^*b)}_{A\oplus B} \\
&= \max\{\norm{a^*a}_A, \norm{b^*b}_B\} \\
&= \max\{\norm{a}_A^2, \norm{b}_B^2\} \\
&= \big(\max\{\norm{a}_A, \norm{b}_B\}\big)^2 \\
&= \norm{(a,b)}_{A\oplus B}^2.
\end{align*}
\end{proof}

\subsubsection{Unitisation of $C^*$-algebras}
For Banach-$*$-algebras we may have a choice of norms to put on the unitisation. For $C^*$-algebras there is exactly one.

Note that the norm of $A^\dagger$ is not the norm of $A\oplus \C$. This is because the multiplication in $A^\dagger$ is not the multiplication in $A\oplus \C$.
\begin{proposition}
Let $A$ be a $C^*$-algebra. Then there exists a unique norm on $A^\dagger$ that turns it into a $C^*$-algebra: the operator norm
\[ \norm{(a,\lambda)} \defeq \sup\setbuilder{\norm{ax + \lambda x}}{x\in A \land \norm{x}\leq 1} .\]
\end{proposition}
\begin{proof}
There is at most one such norm, by \ref{atMostOneNorm}. Because the operator norm is a suitable norm for $A^\dagger$ by \ref{normsOfUnitisation}, we just need to verify the $C^*$-identity:
\begin{align*}
\norm{(a,\lambda)^*(a,\lambda)} &= \sup_{\norm{x}\leq 1} \norm{(a,\lambda)^*(a,\lambda)x} \\
&\geq \sup_{\norm{x}\leq 1} \norm{x^*}\cdot\norm{(a,\lambda)^*(a,\lambda)x} \geq \sup_{\norm{x}\leq 1} \norm{x^*(a,\lambda)^*(a,\lambda)x} \\
&= \sup_{\norm{x}\leq 1} \norm{((a,\lambda)x)^*(a,\lambda)x} = \sup_{\norm{x}\leq 1} \norm{(a,\lambda)x}^2 = \norm{(a,\lambda)}^2,
\end{align*}
where we have used the $C^*$-identity in $A$ because $(a,\lambda)x = ax + \lambda x \in A$.
\end{proof}

\section{Continuous functional calculus}

\begin{theorem}[Stone-Weierstrass] \label{StoneWeierstrass}
Let $X$ be a compact Hausdorff space. Let $A\subseteq \mathcal{C}(X)$ be a unital $*$-subalgebra. Suppose that $A$ separates points, i.e.\ for all $x\neq y$ in $X$ there exists $f\in A$ with $f(x) \neq f(y)$. Then $A$ is dense in $\mathcal{X}$ with respect to $\norm{\cdot}_\infty$.
\end{theorem}
TODO: can we relate this to canonical mapping to second dual, and general duality theory?

\subsection{Spectral stability of $C^*$-algebras}
\begin{proposition}
Let $A$ be a unital $C^*$-algebra, $B\subseteq A$ a unital $C^*$-subalgebra and $x\in B$. Then $x$ is invertible in $B$ \textup{if and only if} $x$ is invertible in $A$.
\end{proposition}
\begin{proof}
If an inverse exists in $B$, said inverse will also be in $A$.

Conversely, suppose $x$ not invertible in $B$. Then either $x^*x$ or $xx^*$ is not invertible in $B$ by \ref{elementaryStarLemma} and \ref{productInvertibility}. Let $y$ be one of the two that is not invertible. Because $y$ is self-adjoint, $\sigma_B(y)\subset \R$, by \ref{selfAdjointSpectrumReal}. Thus $(y_n) = (y+\frac{i}{n})$ is a sequence of invertibles converging to $y$. By \ref{openSetInvertibles}, $\norm{y_n - y}\geq \norm{y_n^{-1}}^{-1}$ must hold for all $n$, otherwise $y$ would be invertible. Thus $\norm{y_n^{-1}}^{-1}$ must converge to zero and $\norm{y_n^{-1}}$ must diverge (TODO ref). Since the inversion map is continuous on $\GL(A)$, by \ref{inverseMapContinuous}, it follows that $y$ cannot be invertible in $A$. Since $x$ is invertible if and only if $x^*$ is invertible, by \ref{elementaryStarLemma}, $y$ being non-invertible implies $x$ is not invertible, by \ref{productInvertibility}.
\end{proof}
\begin{corollary} \label{spectrumIndependentOfSurroundingAlgebra}
Let $A$ be a $C^*$-algebra, $B\subseteq A$ a $C^*$-subalgebra and $x\in B$.

The spectrum of $x$ is independent of the surrounding algebra:
\[ \sigma_B(x) = \sigma_A(x). \]
\end{corollary}
\begin{proof}
Apply the proposition to $\tilde{A}$ and $\tilde{B}$. Note that if $A,B$ are non-unital, the unit of $B^\dagger$ is the same as that of $A^\dagger$.
\end{proof}
This does not hold in general for Banach-$*$-algebras!

TODO example


\subsection{The Gelfand-Naimark theorem}
\begin{theorem}[Unital Gelfand-Naimark] \label{GelfandNaimarkCommutative}
Let $A$ be a unital commutative $C^*$-algebra. Then the Gelfand transform
\[ \evalMap_{-}|_{\hat{A}}: A\to\cont(\hat{A}): x\mapsto \evalMap_{x} \]
is an isometric $*$-isomorphism.
\end{theorem}
Here we view $\cont(\hat{A})$ as a $C^*$-algebra with involution $f^*(\varphi) = \overline{f(\varphi)}$ for all $\varphi\in\hat{A}$.
\begin{proof}
We already know the Gelfand transform is a homomorphism by \ref{GelfandTransformHomomorphism}.

We first prove it is a $*$-homomorphism: $\forall x\in A: (\evalMap_{x})^* = \evalMap_{x^*}$. To that end, take some $\varphi\in\hat{A}$. Then
\[ (\evalMap_{x})^*(\varphi) = \overline{\evalMap_{x}(\varphi)} = \overline{\varphi(x)} = \varphi(x^*) = \evalMap_{x^*}(\varphi), \]
where the third equality is due to \ref{characters*Preserving}.

For isometry, notice that every element is normal due to commutativity. By \ref{normNormalElement} and \ref{GelfandTransformHomomorphism}, we have
\[ \norm{x} = \spr(x) = \norm{\evalMap_{x}}. \]

Finally, to show that the Gelfand transform is an isomorphism, we just need to show that it is bijective by \ref{inverseHomomorphism}. Injectivity follows from isometry, \ref{isometryLemma}.

For surjectivity, note that $\im(\evalMap_{-})$ separates $\hat{A}$ (indeed, for $\varphi \neq \psi$ in $\hat{A}$, there exists $x\in A$ such that $\varphi(x) \neq \psi(x)$and thus $\evalMap_x(\varphi) \neq \evalMap_x(\psi)$).
By the Stone-Weierstrass theorem \ref{StoneWeierstrass}, $\im(\evalMap_{-})$ is dense in $\cont(\hat{A})$. By \ref{isometryLemma} the image of an isometry from a complete space is closed. Thus the image of $A$ is all of $\cont(\hat{A})$.
\end{proof}
\begin{corollary}
Every commutative unital $C^*$-algebra is isomorphic to $\cont(X)$ for some compact Hausdorff space $X$.
This space $X$ is unique up to homeomorphism.
\end{corollary}
\begin{proof}
The first part follows from the Gelfand-Naimark theorem and the fact that $\hat{A}$ is compact, \ref{characterSpaceLocallyCompact}.


For the second part, we know that $\widehat{C(X)}\cong X$ for all compact topological spaces by \ref{charactersFunctionAlgebraCompactSpace}. Suppose that $X,Y$ are suitable spaces, then $X \cong\cont(X) \cong \cont(Y)\cong Y$.
\end{proof}

\begin{theorem}[Non-unital Gelfand-Naimark] \label{nonUnitalGelfandNaimarkCommutative}
Let $A$ be a commutative $C^*$-algebra. Then the Gelfand transform
\[ \evalMap_{-}|_{\hat{A}}: A\to\cont_0(\hat{A}): x\mapsto \evalMap_{x} \]
is an isometric $*$-isomorphism.
\end{theorem}
\begin{proof}
By \ref{GelfandNaimarkCommutative}, the function $\evalMap_{-}|_{\hat{A^\dagger}}: A^\dagger\to\cont(\hat{A^\dagger}): x\mapsto \evalMap_{x}$ is an isometric $*$-isomorphism. Let $h: \hat{A}^\dagger \to \widehat{A^\dagger}$ be the homeomorphism from \ref{compactificationOfCharacterSpaceIsCharacterSpaceOfUnitisation}, so $h^\star$ is an algebra $*$-isomorphism. Let $\Phi: \cont_0(\hat{A}^\dagger) \to \cont(\hat{A}^\dagger)$ be the $*$-isomorphism from \ref{unitisationOnePointCompactificationIsomorphism}. Then
\[ \Phi^{-1}\circ h^\star\circ \big(\evalMap|_{\widehat{A^\dagger}}\big): A^\dagger \to \cont(\widehat{A^\dagger}) \to \cont(\hat{A}^\dagger) \to \cont_0(\hat{A})^\dagger \]
is a $*$-isomorphism by construction.

Now we claim that $\Phi^{-1}\circ h^\star\circ \big(\evalMap|_{\widehat{A^\dagger}}\big) = \big(\evalMap_{-}|_{\hat{A}}\big)^\dagger$. Indeed, for all $(a, \lambda)\in A^\dagger$, we have
\begin{align*}
\Big(\Phi^{-1}\circ h^\star\circ \big(\evalMap|_{\widehat{A^\dagger}}\big)\Big)(a, \lambda) &= \Phi^{-1}\big(\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}\circ h\big) \\
&= \Big(\big(\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}\circ h\big)|_{\hat{A}} - \constant{\big(\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}\circ h\big)(\infty)}, \big(\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}\circ h\big)(\infty)\Big) \\
&= \Big(\big(\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}\circ h\big)|_{\hat{A}} - \constant{\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}(\proj_2)}, \evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}(\proj_2)\Big) \\
&= \Big(\big(\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}\circ h\big)|_{\hat{A}} - \constant{\lambda}, \lambda\Big) \\
&= \Big(\evalMap_{(a, \lambda)}|_{\widehat{A^\dagger}}\circ \widetilde{(-)} - \constant{\lambda}, \lambda\Big) \\
&= \Big(\evalMap_{a}|_{\hat{A}} + \constant{\lambda} - \constant{\lambda}, \lambda\Big) \\
&= \big(\evalMap_{a}|_{\hat{A}}, \lambda\big) \\
&= \big(\evalMap_{-}|_{\hat{A}}\big)^\dagger(a,\lambda).
\end{align*}
Thus the restriction $\evalMap_{-}|_{\hat{A}}$ is a $*$-isomorphism and it is isometric by \ref{starHomomorphismCstarProperties}.
\end{proof}



\begin{proposition}
If $A$ (commutative) generated by one element $a$, then $A$ is isomorphic to the $C^*$-algebra of continuous functions on the spectrum of $a$ which vanish at $0$.
\end{proposition}

TODO non-unital Gelfand-Naimark!

\subsection{Continuous functional calculus}

\begin{lemma} \label{generatedAlgebraSpectrumHomeomorphism}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a normal element. Then the map
\[ \evalMap_x|_{\widehat{C^*(x,\vec{1})}}: \widehat{C^*(x,\vec{1})}\to \spec(x): \varphi \mapsto \varphi(x) \]
is a homeomorphism.
\end{lemma}
\begin{proof}
By \ref{spectrumIndependentOfSurroundingAlgebra} the $\spec_A(x) = \spec_{C^*(x,\vec{1})}$. As this is the only part of the function definition that could have depended on the rest of $A$, not just $C^*(x,\vec{1})$, we may take $A = C^*(x,\vec{1})$ WLOG. Because $x$ is normal, $C^*(x,\vec{1})$ is commutative (TODO ref: closure of commutative subset commutative?). By \ref{spectrumFromSpectrum} the map is surjective and well-defined, in that it maps into the codomain $\sigma(x)$. It is also continuous by definition of the weak-$*$ topology on $\widehat{C^*(x,\vec{1})}$.

To show injectivity, suppose $\varphi(x) = \psi(x)$ for some $\varphi,\psi\in\widehat{C^*(x,\vec{1})}$. Because (TODO ref)
\[ C^*(x,\vec{1}) = \overline{\Span}\setbuilder{x^n(x^*)^m}{n,m\geq 0} \]
and characters are continuous homomorphisms, \ref{charactersUnital}, we see that $\varphi = \psi$.

Finally we need to show the map is open. Because $\sigma(x)$ is compact, this follows from \ref{compactToHausdorffHomeomorphism}.
\end{proof}


TODO: relocate.
\begin{lemma} \label{WeierstrassApproximation}
Let $D\subseteq \C$. Then
\[ \cont(D) = C^*(\id_D, \constant{1}_D). \]
\end{lemma}
\begin{proof}
TODO ref to Stone-Weierstrass.
\end{proof}

\begin{theorem}[Continuous functional calculus for unital $C^*$-algebras] \label{continuousFunctionalCalculus}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a normal element. There exists a unique $*$-homomorphism
\[ \Phi_x: \cont(\spec(x))\to A \qquad \text{such that} \qquad \text{$\Phi_x(\id_{\spec(x)}) = x$ and $\Phi_x(\constant{1}_{\spec(x)}) = \vec{1}$.} \]
This $*$-homomorphism is isometric.
\end{theorem}
We will usually write $f(x)$ to mean $\Phi_x(f)$. Note that $\Phi_x$ is continuous by \ref{isometryLemma}.

The result can be reformulated as saying that there exists a unique \emph{unital} $*$-homomorphism $\Phi_x: \cont(\spec(x))\to A$ such that $\Phi_x(\id_{\spec(x)}) = x$.
\begin{proof}
For uniqueness, note that $\cont(\spec(x)) = C^*(\id_{\spec(x)}, \constant{1}_{\spec(x)})$ by the Weierstrass approximation theorem \ref{WeierstrassApproximation}. Thus any $*$-homomorphism defined on $\cont(\spec(x))$ is completely determined by its value at $\id_{\spec(x)}$ and $\constant{1}_{\spec(x)}$.

For existence, let $B \defeq C^*(x,\vec{1})$, which is commutative because $x$ is normal. Then $\evalMap_x: \hat{B}\to \sigma(x): \varphi\mapsto \varphi(x)$ is a homeomorphism by \ref{generatedAlgebraSpectrumHomeomorphism} and so the pre-composition $\evalMap_x^\star: \mathcal{C}(\sigma(x))\to \mathcal{C}(\hat{B})$ is an isometric $*$-isomorphism. The we have the $*$-homomorphism
\[ \Phi_x \defeq \iota_B\circ\big(\evalMap_{-}|_{\hat{B}}\big)^{-1}\circ \evalMap_x^\star: \begin{tikzcd} \mathcal{C}(\sigma(x)) \ar[r, "{\evalMap_x^\star}"] & \mathcal{C}(\hat{B}) \ar[r, "\big(\evalMap_{-}|_{\hat{B}}\big)^{-1}"] & B \ar[r , hook, "\iota_B"] & A \end{tikzcd} \]
where the inverse Gelfand transform $\big(\evalMap_{-}|_{\hat{B}}\big)^{-1}$ exists and is an isometric isomorphism by the Gelfand-Naimark theorem \ref{GelfandNaimarkCommutative}, since $B$ is commutative.

We verify
\begin{align*}
\Phi_x(\id_{\sigma(x)}) &= \Big(\iota_B\circ\big(\evalMap_{-}|_{\hat{B}}\big)^{-1}\circ\evalMap_x^\star\Big)(\id_{\sigma(x)}) \\
&= (\iota_B\circ\big(\evalMap_{-}|_{\hat{B}}\big)^{-1})(\id_{\sigma(x)}\circ\evalMap_x) \\
&= (\iota_B\circ\big(\evalMap_{-}|_{\hat{B}}\big)^{-1})(\evalMap_x) \\
&= \iota_B(x) = x
\end{align*}
using the definition of the transpose, cancellation of $I_{\sigma}(x)$ and inverse of Gelfand transform. We also verify
\begin{align*}
\Phi_x(\constant{1}_{\sigma(x)}) &= (\iota_B\circ\big(\evalMap_{-}|_{\hat{B}}\big)^{-1}\circ\evalMap_x^\star)(\constant{1}_{\sigma(x)}) \\
&= (\iota_B\circ\big(\evalMap_{-}|_{\hat{B}}\big)^{-1})(\constant{1}_{\sigma(x)}\circ\evalMap_x) \\
&= (\iota_B\circ\big(\evalMap_{-}|_{\hat{B}}\big)^{-1})(\constant{1}_{\hat{B}}) \\
&= \iota_B(\vec{1}) = \vec{1}
\end{align*}
using the fact that $\evalMap_{\vec{1}}|_{\hat{B}}(\varphi) = \varphi(\vec{1}) = 1$ for all $\varphi\in \hat{B}$ by \ref{charactersUnital}, so $\evalMap_{\vec{1}}|_{\hat{B}} = \constant{1}_{\hat{B}}$.
\end{proof}
For polynomials in $z,\overline{z}$, this functional calculus works as expected, because it is a $*$-homomorphism.

In fact we can apply functional calculus to any continuous defined on a superset of the spectrum: restricting to the spectrum still yields a continuous function by \ref{continuousConstructions}.

\begin{proposition} \label{compositionPropertiesCFC}
Let $A$ be a unital $C^*$-algebra and $x\in A$ be a normal element with functional calculus $\Phi_x$. Then
\begin{enumerate}
\item if $B$ is a unital $C^*$-algebra and $\Psi: A\to B$ a unital $*$-homomorphism, then $\Psi\circ\Phi_x = \Phi_{\Psi(x)}|_{\cont(\spec(x))}$;
\item if $f\in\cont(\spec(x))$ and $g\in\cont\big(f^{\imf}(\spec(x))\big)$, then $\Phi_x(g\circ f) = \Phi_{\Phi_x(f)}(g)$.
\end{enumerate}
\end{proposition}
The first point means that $\Psi(f(x)) = f(\Psi(x))$ for all $f\in \cont(\spec(x))$. The second that $(g\circ f)(x) = g\big(f(x)\big)$.
\begin{proof}
(1) The claim is well-defined because $\spec(\Psi(x))\subseteq \spec(x)$, by \ref{spectrumOfImage}. It is easy to check, by \ref{continuousFunctionalCalculus}, that both sides are unital $*$-homomorphisms from $\cont(\spec(x))$ to $B$, sending the identity function to $\Psi(x)$, meaning they assign the same value to $\id_{\spec(x)}$ and $\constant{1}_{\spec(x)}$. Since $\cont(\spec(x)) = C^*(\id_{\spec(x)}, \constant{1}_{\spec(x)})$ by the Weierstrass approximation theorem \ref{WeierstrassApproximation}, this uniquely defines the $*$-homomorphism.

(2) We need to prove $\Phi_x\circ f^\star = \Phi_{\Phi_x(f)}$. We calculate
\[ (\Phi_x\circ f^\star)(\id_{f^\imf(\spec(x))}) = \Phi_x(\id_{f^\imf(\spec(x))}\circ f) = \Phi_x(f) \]
and
\[ (\Phi_x\circ f^\star)(\constant{1}_{f^\imf(\spec(x))}) = \Phi_x(\constant{1}_{f^\imf(\spec(x))}\circ f) = \Phi_x(\constant{1}_{\spec(x)}) = \vec{1}, \]
so the equality holds by uniqueness of the continuous functional calculus \ref{continuousFunctionalCalculus}.
\end{proof}

\begin{lemma} \label{unitisationNonUnitalFunctionalC*algebra}
Let $X$ be a compact Hausdorff space and $x_0\in X$. Then
\[ \Psi: \cont(X) \to \setbuilder{f \in \cont(X)}{f(x_0) = 0}^\dagger: f \mapsto \big(f - \constant{f(x_0)}, f(x_0)\big). \]
is a unital $*$-isomorphism. If $X$ is topological and has no isolated points, then $\Psi$ is isometric.
\end{lemma}
TODO: relax condition on $X$ for isometricity?
\begin{proof}
Linearity is easily checked, as is bijectivity.
For multiplicativity, take $f,g \in \cont(X)$. Then
\begin{align*}
\Psi(f\cdot g) &= \big((f\cdot g) - \constant{(f\cdot g)(x_0)}, (f\cdot g)(x_0)\big) \\
&= \big(f\cdot g - \constant{f(x_0)}\cdot\constant{g(x_0)} - f\cdot\constant{g(x_0)} - g\cdot\constant{f(x_0)} + f\cdot\constant{g(x_0)} + g\cdot\constant{f(x_0)}, (f\cdot g)(x_0)\big) \\
&= \big((f - \constant{f(x_0)})(g - \constant{g(x_0)})  + g(x_0)f + f(x_0)g, f(x_0)\cdot g(x_0)\big) \\
&= \big(f - \constant{f(x_0)}, f(x_0)\big)\big(g - \constant{g(x_0)}, g(x_0)\big) \\
&= \Psi(f)\cdot\Psi(g).
\end{align*}


Finaly we prove that $\Psi$ is isometric. We have that $X$ is normal by \ref{compactHausdorffSpacesNormal}, so disjoint closed sets are functionally separated by \ref{UrysohnLemma}.

For all $f\in \cont(X)$, we have
\begin{align*}
\norm{\Psi(f)} = \norm{\big(f - \constant{f(x_0)}, f(x_0)\big)} &= \sup_{\substack{g \in \cont(X) \\ g(x_0) = 0 \\ \norm{g} \leq 1}}\norm{f\cdot g - \constant{f(x_0)}g + f(x_0)g} \\
&= \sup_{\substack{g \in \cont(X) \\ g(x_0) = 0 \\ \norm{g} \leq 1}}\norm{f\cdot g} \leq \sup_{\substack{g \in \cont(X) \\ g(x_0) = 0 \\ \norm{g} \leq 1}}\norm{f}\,\norm{g} = \norm{f}.
\end{align*}
We need to prove the other inequality.
We have that $|f(x)|$ attains a maximum at $x_1 \in X$ by the extreme value theorem, \ref{extremeValueTheorem}. If $x_1 \neq x_0$, then $\{x_0\}$ and $\{x_1\}$ are disjoint closed sets and we can find a continuous $g: X\to \interval{0,1}$ such that $g(x_0) = 0$ and $g(x_1) = 1$ by \ref{UrysohnLemma}. This implies
\[ \sup_{\substack{g \in \cont(X) \\ g(x_0) = 0 \\ \norm{g} \leq 1}}\norm{f\cdot g} \geq |f(x_1)g(x_1)| = |f(x_1)| = \norm{f}. \]

Now assume $x_0 = x_1$. Take arbitrary $\epsilon >0$. Since $|f|$ is continuous and $x_0$ is not an isolated point, there must be $x_2\in X\setminus\{x_0\}$ such that $|f(x_2)| \geq \norm{f} - \epsilon$. Now $\{x_0\}$ and $\{x_2\}$ are disjoint closed sets and we can find a continuous $g: X\to \interval{0,1}$ such that $g(x_2) = 1$ and $g(x_0) = 0$. This implies
\[ \sup_{\substack{g \in \cont(X) \\ g(x_0) = 0 \\ \norm{g} \leq 1}}\norm{f\cdot g} \geq |f(x_2)g(x_2)| = |f(x_2)| \geq \norm{f}-\epsilon. \]
Since this holds for all $\epsilon > 0$, we have $\norm{\Psi(f)} \geq \norm{f}$.
\end{proof}

\begin{theorem}[Continuous functional calculus for non-unital $C^*$-algebras] \label{nonUnitalContinuousFunctionalCalculus}
Let $A$ be a non-unital $C^*$-algebra and $x\in A$ a normal element. There exists a unique $*$-homomorphism
\[ \Phi'_x: \setbuilder{f\in \cont\big(\spec(x)\big)}{f(0) = 0} \to A \qquad \text{such that} \qquad \Phi'_x(\id_{\spec(x)}) = x. \]
This $\Phi'_x$ is given by
\[ \Phi'_x \defeq \proj_1\circ \Phi_{\iota(x)}|_{\setbuilder{f\in \cont(\spec(x))}{f(0) = 0}}, \]
where $\iota: A\to A^\dagger$ is the canonical embedding and $\Phi_{\iota(x)}$ is the continuous functional calculus of $\iota(x)\in A^\dagger$. In particular
\begin{enumerate}
\item $\Phi'_x$ is isometric;
\item $\iota\big(\Phi'_x(f)\big) = \Phi_{\iota(x)}(f)$ for all $f\in \cont\big(\spec(x)\big)$ such that $f(0) = 0$.
\end{enumerate}
\end{theorem}
Note that the requirement $f(0) = 0$ is well-defined, since $0\in \spec(x)$ by \ref{zeroSpectrumNonunitalAlgebra}.

Point (2) means that $\iota(f(x)) = f(\iota(x))$.
\begin{proof}
For existence, we just need to verify that the proposed solution works. By \ref{continuousFunctionalCalculus}, $\Phi_{\iota(x)}: \cont\big(\spec(x)\big)\to A^\dagger$ is an isometric $*$-homomorphism that maps $\id_{\spec(x)}$ to $\iota(x)$. By \ref{unitalProjectionsAlgebraHomomorphisms} $\Phi'_x$ is an isometric algebra homomorphism if $\proj_2\big(\Phi_x(f)\big) = 0$ for all $f\in \cont\big(\spec(x)\big)$ such that $f(0) = 0$.

To prove this, take arbitrary $f\in \cont\big(\spec(x)\big)$ such that $f(0) = 0$. By \ref{compositionPropertiesCFC} (and the fact that $\proj_2$ is unital \ref{unitalProjectionsAlgebraHomomorphisms}), we have
\[ \proj_2\big(\Phi_{\iota(x)}(f)\big) = \Phi_{\proj_2(\iota(x))}(f) = \Phi_{0}(f) = f(0) = 0. \]
For the second point, we use this result to write
\[ \Phi_{\iota(x)}(f) = \big((\proj_1\circ \Phi_{\iota(x)})(f), (\proj_2\circ \Phi_{\iota(x)})(f)\big) = \big((\proj_1\circ \Phi_{\iota(x)})(f), 0\big) = \iota\big((\proj_1\circ \Phi_{\iota(x)})(f)\big). \]

For uniqueness, suppose $\Phi''_x$ is some other such $*$-homomorphism. Let
\[ \Psi: \cont\big(\spec(x)\big) \to \setbuilder{f\in \cont\big(\spec(x)\big)}{f(0) = 0}^\dagger \]
be the unital $*$-homomorphism of \ref{unitisationNonUnitalFunctionalC*algebra}. Consider the unitisation $(\Phi''_x)^\dagger$. Then $(\Phi''_x)^\dagger\circ \Psi: \cont\big(\spec(x)\big) \to A^\dagger$ is a unital $*$-homomorphism that maps $\id_{\spec(x)}$ to $x$, by \ref{unitalExtensionLinearFunction}. 
It is equal to $\Phi_{\iota(x)}$ by the uniqueness result in \ref{continuousFunctionalCalculus}. This implies, for all $f\in \setbuilder{f\in \cont\big(\spec(x)\big)}{f(0) = 0}$,
\begin{align*}
\Phi'_x(f) &= \Big(\proj_1\circ \Phi_{\iota(x)}|_{\setbuilder{f\in \cont(\spec(x))}{f(0) = 0}}\Big)(f) \\
&= \Big(\proj_1\circ \Phi_{\iota(x)}\Big)(f) \\
&= \Big(\proj_1\circ (\Phi''_x)^\dagger\circ \Psi\Big)(f) \\
&= \Big(\proj_1\circ (\Phi''_x)^\dagger\Big)\big((f, 0)\big) \\
&= \proj_1\Big(\big(\Phi''_x(f), 0\big)\Big) \\
&= \Phi''_x(f).
\end{align*}
So $\Phi'_x = \Phi''_x$.
\end{proof}

\begin{proposition}[Spectral mapping] \label{spectralMappingCFC}
Let $A$ be a $C^*$-algebra and $x\in A$ be a normal element and $f\in\mathcal{C}(\spec(x))$. If $A$ is non-unital, we additionally assume $f(0) = 0$. Then
\[ \spec(f(x)) = f^\imf(\spec(x)). \]
\end{proposition}
\begin{proof}
Let $\iota: A\to \widetilde{A}$ be the canonical embedding (i.e.\ it is identity if $A$ is unital).

Set $B = C^*(\iota(x),\vec{1})$, which is commutative because $\iota(x)$ is normal. We calculate
\begin{align*}
\spec\big(f(x)\big) &= \spec\big(\iota(f(x))\big) \\
&= \spec\big(f(\iota(x))\big) \\
&= \setbuilder{\varphi(f(\iota(x)))}{\varphi\in\hat{B}} \\
&= \setbuilder{f(\varphi(\iota(x)))}{\varphi\in\hat{B}} \\
&= f^\imf\big(\setbuilder{\varphi(\iota(x))}{\varphi\in\hat{B}}\big) \\
&= f^\imf\big(\spec(\iota(x))\big).\\
&= f^\imf\big(\spec(x)\big),
\end{align*}
using \ref{spectrumFromSpectrum}, \ref{compositionPropertiesCFC} and, if $A$ is not unital, \ref{nonUnitalContinuousFunctionalCalculus}.
\end{proof}





\begin{proposition} \label{commutativityFunctionalCalculus}
Let $A$ be a unital $C^*$-algebra and $x\in A$ be a normal element. Let $f$ be a continuous function on the spectrum of $x$ and let $y\in A$ commute with $x$. Then $f(x)$ commutes with $y$.
\end{proposition}
TODO proof + restructure + define joint functional calculus? (but we still need case where $y$ is not necessarily normal)


\begin{lemma}
Let $X$ be a compact space and view $\mathcal{C}(X)$ as a unital commutative $C^*$-algebra. Fix $g\in\mathcal{C}(X)$. The functional calculus is then given simply by composition:
\[ \mathcal{C}\big(g^\imf(X)\big) \to \mathcal{C}(X): f\mapsto f(g) = f\circ g. \]
\end{lemma}
\begin{proof}
Composition is a unital $*$-homomorphism with the right properties. The claim follows from uniqueness of the functional calculus.
\end{proof}


\begin{proposition} \label{continuityContinuousFunctionalCalculus}
Let $K\subset \R$ be non-empty and compact; $f:K\to \C$ a continuous function; $A$ a unital $C^*$-algebra and $\Omega_K$ the set of self-adjoint elements in $A$ with spectrum contained in $K$. The function
\[ f: \Omega_K\subset A \to A: a\mapsto f(a) \]
is continuous.
\end{proposition}
\begin{proof}
The map $A\to A$ given by $a\mapsto a^n$ is continuous for every $n\geq 0$, because multiplication is continuous. Then every polynomial $f$ induces a continuous map $A\to A$.
Then $\epsilon/3$ by Stone-Weierstrass.
\end{proof}
TODO: also for non-compact $K$ and $K\subseteq \C$. Then $\Omega_K$ set of normal elements.

\begin{proposition}
Let $A$ be a unital $C^*$-algebra, $x\in A$ a normal element and $f\in \cont(\sigma(x))$. Then
\[ \norm{f(x)} = \sup_{\lambda\in \sigma(x)}|f(\lambda)|. \]
\end{proposition}
\begin{proof}
We calculate
\[ \norm{f(x)} = \spr(f(x)) = \sup |\sigma(f(x))| = \sup |f(\sigma(x))| = \sup_{\lambda\in \sigma(x)}|f(\lambda)|. \]
\end{proof}
\begin{corollary}
Let $x$ be an normal element in a unital $C^*$-algebra and $\lambda_0 \in \rho(x)$. Then $d(\lambda_0, \sigma(x)) = \norm{R_x(\lambda_0)}^{-1}$.
\end{corollary}
\begin{proof}
We calculate
\[ \norm{R_x(\lambda_0)} = \norm{\frac{1}{x-\lambda_0\cdot \vec{1}}} = \sup_{\lambda\in \sigma(x)}\left|\frac{1}{\lambda - \lambda_0}\right| = \frac{1}{\inf_{\lambda\in \sigma(x)}|\lambda- \lambda_0|} = \frac{1}{d(\lambda_0, \sigma(x))}. \]
\end{proof}
TODO: for general closed operators $d(\lambda_0, \sigma(x)) \geq \norm{R_x(\lambda_0)}^{-1}$??

\begin{lemma}
If two polynomials in $z,\overline{z}$ agree on the spectrum of a normal element, they give an equation the element obeys.
\end{lemma}
The proof is the unicity of the functional calculus.
\begin{corollary} \label{propertiesFromSpectrum}
Let $A$ be a unital $C^*$-algebra and $x\in A$ a normal element. Then
\begin{enumerate}
\item $x$ is self-adjoint \textup{if and only if} $\sigma(x)\subseteq \R$;
\item $x$ is unitary \textup{if and only if} $\sigma(x)\subseteq \mathbb{T}$;
\item $x$ is a projection \textup{if and only if} $\sigma(x)\subseteq \{0,1\}$.
\end{enumerate}
\end{corollary}
\begin{proof}
\hspace{1em} TODO spectral mapping
\begin{enumerate}
\item By \ref{selfAdjointSpectrumReal} we have that self-adjoint implies real spectrum. The converse follows from the lemma applied to $z = \overline{z}$.
\item Assume $x$ unitary. By the $C^*$-identity $\norm{x} = \sqrt{\norm{x^*x}} = 
\sqrt{\norm{\vec{1}}} = 1$, by \ref{consequencesC*}. By \ref{normNormalElement}, $\spr(x) = \norm{x} = 1$. Also $\norm{x^{-1}}^{-1} = 1$. So $\sigma(x)\subseteq \mathbb{T}$ by \ref{openSetInvertibles}. The converse follows from the lemma applied to $1 = z\overline{z} = \overline{z}z$.
\item The direction $\Rightarrow$ follows from \ref{spectrumIdempotent}.
The converse follows from the lemma applied to $z = \overline{z} = z^2$.
\end{enumerate}
\end{proof}



\begin{proposition}
Let $A$ be a unital $C^*$-algebra. Then every element in $A$ can be written as a linear combination of at most four unitaries.
\end{proposition}
\begin{proof}
Let $x\in A$. By \ref{realImaginaryParts} we can write $x=x_1+ix_2$ for some self-adjoint $x_1,x_2$.
TODO
\end{proof}



\section{Positivity}
\url{https://link.springer.com/content/pdf/10.1023/A:1009717500980.pdf}
\subsection{Positive elements}
\begin{definition}
Let $A$ be a $C^*$-algebra. An element $a\in A$ is \udef{positive} if it is normal and $\spec(a) \subset \interval[co]{0,+\infty}$.

The set of all positive elements of $A$ is the \udef{positive cone} of $A$
\[ A^+ \defeq \setbuilder{a\in \Normals(A)}{ \spec(a) \subset \interval[co]{0,+\infty} }. \]
\end{definition}
By \ref{propertiesFromSpectrum} every projection is positive and every positive element is in fact self-adjoint.

By \ref{normalSpectralRadiusEqualsNorm} we have for all positive $a\in A$:
\[ \norm{a} = \sup\sigma(a) = \max\sigma(a). \]

By definition, $A^+ = \widetilde{A}^+ \cap A$.

\begin{lemma} \label{positiveMultipleOfUnity}
Let $A$ be a unital $C^*$-algebra and $\lambda\in \C$. Then $\lambda\vec{1} \in A^+$ \textup{if and only if} $\lambda \in \R^+$.
\end{lemma}
\begin{proof}
We have $\spec(\lambda \vec{1}) = \{\lambda\}$.
\end{proof}

\begin{lemma} \label{positivityDistanceToNorm}
Let $A$ be a unital $C^*$-algebra and $a\in A$ self-adjoint. Then the following are equivalent:
\begin{enumerate}
\item $a$ is positive;
\item $\norm{\frac{1}{2}\vec{1} - a / \norm{a}} \leq \frac{1}{2}$;
\item $\norm{r\vec{1} - a / \norm{a}} \leq r$ for all $r\geq 1/2$.
\item $\norm{r\vec{1} - a / \norm{a}} \leq r$ for some $r\geq 1/2$.
\end{enumerate}
\end{lemma}
\begin{proof}
The proof is cyclic:

$(1.\Rightarrow 2.)$ Assume $a$ positive. Then $\sigma(a) \subseteq [0,\norm{a}]$. By spectral mapping, \ref{spectralMappingCFC}, we have $\spec(\frac{1}{2}\vec{1} - a / \norm{a}) \subseteq [-1/2, 1/2]$ and thus $\norm{\frac{1}{2}\vec{1} - a / \norm{a}} \leq \frac{1}{2}$, by \ref{normNormalElement}.

$(2.\Rightarrow 3.)$ Write $r = 1/2 + r'$, so $r'\geq 0$. Then
\[ \norm{r\vec{1} - \frac{a}{\norm{a}}} = \norm{\frac{1}{2}\vec{1} + r'\vec{1} - \frac{a}{\norm{a}}} \leq \norm{r'\vec{1}}+ \norm{\frac{1}{2}\vec{1} - \frac{a}{\norm{a}}} \leq r' + \frac{1}{2} = r. \]

$(3.\Rightarrow 4.)$ Clear.

$(4.\Rightarrow 1.)$ By \ref{normNormalElement}, $\sigma(r\vec{1} - a / \norm{a}) \subseteq [-r, r]$. By spectral mapping, this means $\sigma(a) \subseteq [0, 2r\norm{a}]$ and thus $a$ is positive.
\end{proof}


\begin{proposition} \label{CstarPositiveConeProperties}
Let $A$ be a $C^*$-algebra. The set $A^+$ is
\begin{enumerate}
\item a salient pointed convex cone;
\item closed.
\end{enumerate}
\end{proposition}
In particular $A^+$ is closed under addition.
\begin{proof}
(1) That $A^+$ is a cone follows from spectral mapping (\ref{spectralMappingCFC}), as does the salience of $A^+$.

For convexity, we verify additive closure (see \ref{convexityAdditiveClosure}) Take $a,b\in A$ and set $p= \frac{\norm{a}+\norm{b}}{\norm{a+b}} \geq 1 \geq 1/2$. Then
\[ \norm{p\vec{1} - \frac{a+b}{\norm{a+b}}} \leq \frac{1}{\norm{a+b}}\Big(\norm{\big.\norm{a}\vec{1}-a} + \norm{\big.\norm{b}\vec{1}-b}\Big) \leq \frac{\norm{a}+\norm{b}}{\norm{a+b}} = p \]
using the triangle inequality and point 3. of \ref{positivityDistanceToNorm} with $r=1$.

(2) It is enough to prove this for unital $C^*$-algebras since $A^+ = \widetilde{A}^+ \cap A$.

Set $g: A\setminus\{0\}\to \R^+: a\mapsto \norm{\vec{1} - a/\norm{a}\big.}$, which is continuous by construction. By the proposition, a non-zero self-adjoint vector $a$ is positive iff $g(a) \in \interval{0,1}$. So the set of positive non-zero vectors is $X \defeq g^\preimf\big(\interval{0,1}\big) \cap \SelfAdjoints(A)$. This is a closed subset of $A\setminus\{0\}$ by \ref{preimageOpenClosed}, \ref{closedSubsetsBanachStarAlgebra} and \ref{propertiesTopology}.

By \ref{subspaceAdherence}, we have, by \ref{propertiesTopology},
\[ A^+\setminus\{0\} = \closure_{A\setminus\{0\}}\big(A^+\setminus\{0\}\big) = \closure_{A}\big(A^+\setminus\{0\}\big)\cap \big(A\setminus\{0\}\big) = \closure_{A}\big(A^+\setminus\{0\}\big)\setminus \{0\}. \]
Since $0\in A^+$ (because $\spec(0) = \{0\}$), we have
\begin{align*}
A^+ &= \big(A^+ \setminus \{0\}\big) \cup \{0\} = \Big(\closure_{A}\big(A^+\setminus\{0\}\big)\setminus \{0\}\Big) \cup \{0\} \\
&= \closure_{A}\big(A^+\setminus\{0\}\big) \cup \closure_A\big(\{0\}\big) = \closure_{A}\big(A^+\setminus\{0\} \cup \{0\}\big) = \closure_A(A^+).
\end{align*}
Thus $A^+$ is closed.
\end{proof}

\begin{lemma} \label{positiveElementEqualsItsAbsValue}
Let $A$ be a $C^*$-algebra and $a\in A^+$. Then $a = \sqrt{a^2}$.
\end{lemma}
If $a,b\in A^+$ are such that $a^2 = b^2$, then $a = \sqrt{a^2} = \sqrt{b^2} = b$.
\begin{proof}
Since $\sqrt{(-)^2}$ is equal to the identity on $\spec(a)$, the result follows from continuous functional calculus \ref{continuousFunctionalCalculus} or \ref{nonUnitalContinuousFunctionalCalculus}.
\end{proof}

\begin{proposition}[Jordan decomposition] \label{positiveNegativeParts}
Let $A$ be a $C^*$-algebra, and $a\in A$ a self-adjoint element. Then there exists a unique decomposition $a=a^+ - a^-$ where $a^+,a^-\in A^+$ and $a^+a^- = 0 = a^-a^+$.
\end{proposition}
\begin{proof}
For existence we can use functional calculus \ref{continuousFunctionalCalculus} or \ref{nonUnitalContinuousFunctionalCalculus}, by setting $a^+ = f^+(a)$ and $a^- = f^-(a)$ where
\[ f^+: x\mapsto \begin{cases}
x & (x\geq 0)\\ 0 & (x < 0)
\end{cases} \qquad\text{and}\qquad f^-: x\mapsto \begin{cases}
0 & (x\geq 0)\\ -x & (x < 0)
\end{cases}. \]
Then $a^+, a^-$ are positive by spectral mapping \ref{spectralMappingCFC} and
\[ a^+a^- = \Phi_{a}(f^+)\Phi_{a}(f^-) = \Phi_{a}(f^+\cdot f^-) = \Phi_{a}(\constant{0}) = 0. \]
Now for uniqueness, suppose $a^+_1, a^-_1$ is another such decomposition. Then
\begin{align*}
a^2 &= (a^+ - a^-)^2 \\
&= (a^{+})^2 + (a^{-})^2 - a^+a^- - a^-a^+ \\
&= (a^{+})^2 + (a^{-})^2 \\
&= (a^{+})^2 + (a^{-})^2 + a^+a^- + a^-a^+ \\
&= (a^+ + a^-)^2.
\end{align*}
Similarly $a^2 = (a^+_1 + a^-_1)^2$. Since both $a^+ + a^-$ and $a^+_1 + a^-_1$ are positive, by \ref{CstarPositiveConeProperties}, we have
\[ a^+ + a^- = \sqrt{(a^+ + a^-)^2} = \sqrt{a^2} = \sqrt{(a^+_1 + a^-_1)^2} = a^+_1 + a^-_1 \]
by \ref{positiveElementEqualsItsAbsValue}. Thus
\[ 2a_1^+ = (a_1^+ + a_1^-) + (a_1^+ - a_1^-) = (a^+ + a^-) + (a^+ - a^-) = 2a^+, \]
which implies $a_1^+ = a^+$. Similarly $a_1^- = a^-$.
\end{proof}
\begin{corollary}[Cartesian decomposition] \label{CartesianDecomposition}
Let $A$ be a $C^*$-algebra and $a\in A$. Then we can decompose $a$ as
\[ (p_1-p_2) + i(p_3-p_4), \]
where $p_1,p_2,p_3,p_4$ are positive.
\end{corollary}
\begin{proof}
By \ref{realImaginaryParts}.
\end{proof}
\begin{corollary}
Let $A$ be a $C^*$-algebra. Then $\Span_\C(A^+) = A$.
\end{corollary}


\begin{proposition}\label{existenceSquareRoot}
Let $A$ be a $C^*$-algebra and $a\in A$. Then the following are equivalent:
\begin{enumerate}
\item $a$ is positive;
\item $a = b^2$ for some $b\in A^+$;
\item $a = c^*c$ for some $c\in A$.
\end{enumerate}
The $b$ in point (2) is unique.
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ If $a$ is positive, then $a$ is normal and the square root is a continuous function on $\spec(a)$, with $\sqrt{0} = 0$. Then define $b = \sqrt{a}$ by spectral calculus \ref{continuousFunctionalCalculus} or \ref{nonUnitalContinuousFunctionalCalculus}. We have
\[ b^2 = (\Phi_a(\sqrt{\mbox{\;\;}}))^2 = \Phi_a(\sqrt{\mbox{\;\;}})\cdot \Phi_a(\sqrt{\mbox{\;\;}}) = \Phi_a(\sqrt{\mbox{\;\;}}^2) = \Phi_a(\id_{\spec(a)}) = a. \]
By spectral mapping, \ref{spectralMappingCFC}, we have that $b$ is positive and thus self-adjoint. Thus $a = b^2$.

$(2) \Rightarrow (3)$ We can set $c\defeq b$. We have $c^* = b^* = b$.

$(3) \Rightarrow (1)$ Suppose $a = c^*c$. Clearly $a$ is self-adjoint, so we can write $a = a^+ - a^-$ as in \ref{positiveNegativeParts}. Set $d\defeq ca^-$. Then
\[ d^*d = a^-c^*ca- = a^-aa^- = a^-(a^+ - a^-)a^- = - (a^-)^3. \]
Thus $\spec(d^*d) = -\spec\big((a^-)^3\big) \subseteq -\R^+$, so $-d^*d$ is positive.

Now, by \ref{realImaginaryParts}, $d = d_0 + id_1$, where $d_0,d_1$ are self-adjoint. Then
\[ d^*d + dd^* = d_0^2 + id_0d_1 - id_1d_0 + d_1^2 + d_0^2 - id_0d_1 + id_1d_0 + d_1^2 = 2(d_0^2+ d_1^2). \]
Since $d_0^2, d_1^2$ are positive by \ref{spectrumAlgebraicOperationsOnCommutingElements} (or spectral mapping), and a sum of positive elements is positive by \ref{CstarPositiveConeProperties}, so $dd^* = 2(d_0^2+ d_1^2) + (-d^*d) \in A^+$.

We have $\spec(d^*d) \cup \{0\} = \spec(dd^*)\cup \{0\}$ by \ref{spectrumSwappedElements}. The left-hand side is a subset of $\R^-$, the right-hand side a subset of $\R^+$. Thus $\spec(d^*d) \subseteq \{0\}$. Since $\spec(d^*d)$ is non-empty by \ref{spectrumNonEmpty}, we have $\spec(d^*d) = 0$, so $d^*d$ is quasinilpotent. Since $d^*d$ is normal, we have $d^*d = 0$ by \ref{normNormalElement}. Thus $(a^-)^3 = 0$, so $a^- = \sqrt[3]{(a^-)^3} = \sqrt[3]{0} = 0$. This implies that $a = a^+$, which is positive.

(Uniqueness) Suppose $a = b^2 = b_0^2$ for $b,b_0\in A^+$. Then
\[ b_0 = \sqrt{b_0^2} = \sqrt{b^2} = b, \]
by \ref{positiveElementEqualsItsAbsValue}.
\end{proof}

\begin{proposition}
Let $A$ be a concrete $C^*$-algebra of bounded operators on some Hilbert space $\mathcal{H}$ and $a\in A$. Then $a$ is positive as an element of the $C^*$ algebra \textup{if and only if} $a$ is positive as an operator on $\mathcal{H}$, i.e.\ for all $x\in \mathcal{H}: \inner{x,ax}\geq 0$.
\end{proposition}
\begin{proof}
If $a$ is positive, then $a = b^*b$ by \ref{existenceSquareRoot} and thus
\[ \forall x\in \mathcal{H}: \inner{x,ax} = \inner{x, b^*bx} = \inner{bx,bx} = \norm{bx}^2 \geq 0, \]
meaning $a$ is a positive operator.

Conversely, the spectrum is contained in the closure of the numerical range (TODO ref), which is a subset of $\interval[co]{0,\infty}$.
\end{proof}
Consequently if $\inner{x,ax}\geq 0$ for all $x\in\mathcal{H}$, then $a$ is self-adjoint.

\begin{lemma} \label{selfAdjointUnitSphereFromOrder}
Let $A$ be a unital $C^*$-algebra and $a\in \SelfAdjoints(A)$. Then $\norm{a} \leq 1$ \textup{if and only if} $\{\vec{1}+a, \vec{1}- a\} \subseteq A^+$. 
\end{lemma}
\begin{proof}
By \ref{normNormalElement}, we have $\norm{a} = \spr(a)$, so, by spectral mapping \ref{spectralMappingCFC},
\begin{align*}
\norm{a}\leq 1 \iff \spec(a) \subseteq \interval{-1,1} \quad&\iff\quad \big(\spec(a) \subseteq \interval{-1,+\infty}\big) \land \big(\spec(a) \subseteq \interval{-\infty,1}\big) \\
&\iff \quad \big(\spec(\vec{1} + a) \subseteq \interval{0,+\infty}\big) \land \big(\spec(a - \vec{1}) \subseteq \interval{-\infty,0}\big) \\
&\iff \quad \big(\spec(\vec{1} + a) \subseteq \interval{0,\infty}\big) \land \big(\spec(\vec{1}-a) \subseteq \interval{0, \infty}\big) \\
&\iff \quad \big(\vec{1} + a\in A^+\big) \land \big(\vec{1}-a\in A^+\big).
\end{align*}
\end{proof}


\subsection{Partial order on self-adjoint elements}

\begin{definition}
Let $A$ be a $C^*$-algebra and $x,y \in A$. We define the order relation $\leq$ on $A$ by $x\leq y \iff y-x\in A^+$.
\end{definition}


\begin{lemma} \label{C*PositivityVectorOrder}
Let $A$ be a $C^*$-algebra. The order relation on $A$ is a vector partial order on $A$.
\end{lemma}
\begin{proof}
This follows from \ref{positiveCone} and \ref{CstarPositiveConeProperties}.
\end{proof}

In particular $x\leq y$ implies $x+z \leq y+z$ and $\lambda \geq 0\in \R$ implies $\lambda x \leq \lambda y$.

TODO in general $\sSet{A, \leq}$ is not a Riesz space. (See e.g.\ the absolute value)

\begin{lemma} \label{CstarOrderLemma}
Let $A$ be a $C^*$-algebra, $x\leq y$ and $a\in A$. Then $a^*xa \leq a^*ya$.
\end{lemma}
\begin{proof}
By \ref{existenceSquareRoot}, there exists $c\in A$ such that $y-x = c^*c$. Then
\[ 0 \leq (ac)^*ac = a^*c^*ca = a^*(y-x)a = a^*ya - a^*xa. \]
This implies $a^*xa \leq a^*ya$.
\end{proof}

\begin{lemma} \label{normInequalityFromSAInequality}
Let $A$ be a unital $C^*$-algebra, $a\in \SelfAdjoints(A)$ and $x\in A$. Then
\begin{enumerate}
\item $a\leq \norm{a}\vec{1}$;
\item $x^*x \leq \norm{x}^2 \vec{1}$. 
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have that $\frac{a}{\norm{a}}$ is a self-adjoint element with norm less than one. Thus, by \ref{selfAdjointUnitSphereFromOrder}, we have $0\leq \vec{1} - \frac{a}{\norm{a}}$. Since $\leq$ is a vector order, this can be rewritten as $a\leq \norm{a}\vec{1}$.

(2) We have that $x^*x$ is self-adjoint and $\norm{x^*x} = \norm{x}^2$. The result then follows from (1).
\end{proof}

\begin{lemma} \label{orderSubUnitVectorLemma}
Let $A$ be a $C^*$-algebra and $a, b\in A^+$ with $\norm{a}\leq 1$ and $\norm{b}\leq 1$. Then
\begin{enumerate}
\item $a^2 \leq a$;
\item $\norm{a-b} \leq 1$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Since the function $x\mapsto x-x^2$ is positive on $\interval{0,1}$ and $\spec(a) \subseteq \interval{0,1}$, we have that $a - a^2$ is positive by spectral mapping \ref{spectralMappingCFC}.

(1, Alternative) From \ref{normInequalityFromSAInequality} and \ref{positiveMultipleOfUnity}, we have $a\leq \norm{a}\vec{1} \leq \vec{1}$. Then \ref{CstarOrderLemma} implies $a^2 = \sqrt{a}a\sqrt{a} \leq \sqrt{a}\sqrt{a} = a$.

(2) Since $\norm{a}_{\widetilde{A}} = \norm{a} \leq 1$ and $\norm{b}_{\widetilde{A}} = \norm{b} \leq 1$, we have $\vec{1}-a\in \widetilde{A}^+$ and $\vec{1}-b\in \widetilde{A}^+$ from \ref{selfAdjointUnitSphereFromOrder}. Thus $0\leq \vec{1}-a+b = \vec{1} - (a-b)$ and $0\leq \vec{1} -b +a = \vec{1}+ (a-b)$. Then $\norm{a-b} = \norm{a-b}_{\widetilde{A}} \leq 1$ by \ref{selfAdjointUnitSphereFromOrder}.
\end{proof}

\begin{lemma} \label{inequalityPositiveElementsLemma}
Let $0\leq a \leq b$ in a unital $C^*$-algebra. Then
\begin{enumerate}
\item $\norm{a} \leq \norm{b}$;
\item if $a$ invertible, then $b$ is invertible and $0\leq b^{-1} \leq a^{-1}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $a \leq b \leq \norm{b}\vec{1}$ by \ref{normInequalityFromSAInequality}, so $0\leq \norm{b}\vec{1} - a$. Thus $\spec\big(\norm{b}\vec{1} - a\big) \subseteq \interval[co]{0,+\infty}$, which implies $\spec(a) \subseteq \interval[oc]{-\infty, \norm{b}}$. Then $\norm{a} = \spr(a) \leq \norm{b}$ by \ref{normNormalElement}.

(2) We have
\[ \vec{1} = \frac{1}{\sqrt{a}}a\frac{1}{\sqrt{a}} \leq \frac{1}{\sqrt{a}}b\frac{1}{\sqrt{a}} \]
by \ref{CstarOrderLemma}. Thus $\spec\big(\frac{1}{\sqrt{a}}b\frac{1}{\sqrt{a}} - \vec{1}\big) \subseteq \R^+$, so $\spec\big(\frac{1}{\sqrt{a}}b\frac{1}{\sqrt{a}}\big) \subseteq \interval[co]{1,+\infty}$. This implies that $\frac{1}{\sqrt{a}}b\frac{1}{\sqrt{a}}$ is invertible, and thus so is $b$.

By spectral mapping \ref{spectralMappingCFC}, we have $\vec{1} \leq \big(\frac{1}{\sqrt{a}}b\frac{1}{\sqrt{a}}\big)^{-1} = \sqrt{a}b^{-1}\sqrt{a}$. So
\[ a^{-1} = \frac{1}{\sqrt{a}}\vec{1}\frac{1}{\sqrt{a}} \leq \frac{1}{\sqrt{a}}\sqrt{a}b^{-1}\sqrt{a}\frac{1}{\sqrt{a}} = b^{-1}. \]
\end{proof}

\begin{lemma}
Let $A$ be a unital $C^*$-algebra, $x\in \Normals(A)$ a normal element and $f,g\in \cont\big(\spec(x)\big)$. Then
\begin{enumerate}
\item if $\im(f)\subseteq \R^+$, then $0\leq f(x)$;
\item if $\im(f)\cup\im(g) \subseteq \R$ and $f\leq g$ pointwise, then $f(x) \leq g(x)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Immediate, by spectral mapping \ref{spectralMappingCFC}.

(2) Let $\Phi_x$ be the functional calculus at $x$. Then we have $\im(g-f) \subseteq \R^+$, so
\[ 0 \leq \Phi_x(g-f) = \Phi_x(g)- \Phi_x(f) = g(x) - f(x), \]
and thus $f(x)\leq g(x)$.
\end{proof}

\begin{lemma}
Let $A$ be a unital $C^*$-algebra, $x,y\in A$ and $\delta > 0$. Suppose $x\in\Normals(A)$ and $\cball_\C(0,\delta) \subseteq \res(x)$. Then $\delta\norm{y} \leq \norm{xy}$.
\end{lemma}
\begin{proof}
By spectral mapping, we have $\spec(x^*x) \subseteq \interval[co]{\delta^2,+\infty}$. So $\spec(x^*x - \delta^2\vec{1}) \subseteq \interval[co]{0, +\infty}$ and thus $\delta^2\vec{1} \leq x^*x$. This means that $\delta^2y^*y = y^*(\delta^2\vec{1})y \leq y^*x^*xy = (xy)^*xy$ by \ref{CstarOrderLemma}. By \ref{inequalityPositiveElementsLemma}, and the $C^*$ identity, we calculate
\[ \delta^2\norm{y}^2 = \norm{\delta^2y^*y} \leq \norm{(xy)^*xy} = \norm{xy}^2. \]
Taking square roots gives $\delta\norm{y} \leq \norm{xy}$.
\end{proof}

\subsubsection{Lattice properties of self-adjoint operators}
\url{https://www.ams.org/journals/proc/1951-002-03/S0002-9939-1951-0042064-2/S0002-9939-1951-0042064-2.pdf}

\begin{proposition}
Let $A$ be a $C^*$-algebra. The real vector space of self-adjoint operators $\SelfAdjoints(A)$ is a Riesz space \textup{if and only if} $A$ is commutative.
\end{proposition}
\begin{proof}
TODO
\end{proof}
Let $A$ be a $C^*$-algebra and $a\in \SelfAdjoints(A)$. Since the $C^*$-subalgebra generated by $\{a\}$ is commutative (because $a$ commutes with $a^* = a$), general Riesz space theory (in particular \ref{PositiveNegativeElements}) gives an alternative proof of the existence of the positive and negative parts of $a$, see \ref{positiveNegativeParts}.


\begin{proposition}
The set of all bounded self-adjoint operators on a Hilbert space is an anti-lattice.
\end{proposition}

\subsubsection{Operator monotonicity}
Delicate!
\begin{proposition}
Assume $0\leq a\leq b$. Then
\begin{enumerate}
\item $\sqrt{a}\leq \sqrt{b}$;
\item $0\leq ab$ if $a,b$ commute.
\end{enumerate}
\end{proposition}
It is not true that $a^2\leq b^2$ or that $ab$ is positive in general!

\subsubsection{Increasing approximate units}
\begin{proposition} \label{approximateUnitInIdeal}
Let $A$ be a $C^*$-algebra and $J \subseteq A$ a two-sided $*$-ideal. Then there exists a net $(e_\lambda)_{\lambda\in\Lambda}$ in $J$ that that is an increasing approximate unit of $\overline{J}$.
\end{proposition}
\begin{proof}
See Dana Williams
\end{proof}

\subsection{Absolute value}
\begin{definition}
Let $A$ be a $C^*$-algebra. For each $a\in A$, the \udef{absolute value} of $a$ is $|a| = (a^*a)^{1/2}$.
\end{definition}
The square root is well defined using functional calculus on the self-adjoint element $a^*a$.

\begin{lemma} \label{propertiesAbsoluteValue}
Let $A$ be a unital $C^*$-algebra.
\begin{enumerate}
\item Let $u\in\Unitaries$, then $|u| = \vec{1}$.
\item Let $a\in A$, then $|a|$ is positive and thus self-adjoint.
\item The map $a\mapsto |a|$ is continuous.
\item If $b\in \GL(A)$, then $|b|\in \GL(A)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $|u| = (u^*u)^{1/2} = \vec{1}^{1/2} = \vec{1}$.

(2) This follows from spectral mapping \ref{spectralMappingCFC} using the fact that $z\mapsto \overline{z}z$ has positive image in $\C$.

(3) We have that $a\mapsto a^*a$ is continuous by \ref{multiplicationContinuous} and \ref{consequencesC*}. Then $a\mapsto |a|$ is continuous by \ref{continuityContinuousFunctionalCalculus}.

(4) Suppose $b\in \GL(A)$. Then $b^*\in \GL(A)$ by \ref{elementaryStarLemma}, so $b^*b$ is invertible. Thus $0\notin \spec(b^*b)$, which implies $0\notin \spec(\sqrt{b^*b}) = \spec(|b|)$ by spectral mapping \ref{spectralMappingCFC}. Thus $|b|$ is invertible.
\end{proof}

\begin{lemma}
Let $T$ be a bounded operator on a Hilbert space $\mathcal{H}$. Then $|T|$ is the only positive operator $A$ in $\Bounded(\mathcal{H})$ such that $\norm{Ax} = \norm{Tx}$ for all $x\in\mathcal{H}$.
\end{lemma}
\begin{proof}
We have for all $x\in\mathcal{H}$,
\[ \inner{Ax,Ax} = \inner{Tx,Tx} \implies \inner{(A^*A-T^*T)x,x}=0, \]
which implies $T^*T = A^*A$. Now $A$ is positive, so $A^*A = A^2$ and taking the squared root give $A = \sqrt{T^*T} = |T|$.
\end{proof}

We do \emph{not}, in general, have a triangle inequality $|a+b| \leq |a| + |b|$.
\begin{example}
Consider the $C^*$ algebra $\C^{2\times 2}$ with
\[ A = \begin{pmatrix}
1 & 1 \\ 1 & 1
\end{pmatrix} = \frac{1}{2}\begin{pmatrix}
1 & 1 \\ -1 & 1
\end{pmatrix}\begin{pmatrix}
0 & 0 \\ 0 & 2
\end{pmatrix}\begin{pmatrix}
1 & -1 \\ 1 & 1
\end{pmatrix} = |A| \qquad\text{and}\qquad B = \begin{pmatrix}
0 & 0 \\ 0 & -2
\end{pmatrix} \]
Then
\[ A + B = \begin{pmatrix}
1 & 1 \\ 1 & -1
\end{pmatrix} = \begin{pmatrix}
1-\sqrt{2} & 1=\sqrt{2} \\ 1 & 1
\end{pmatrix}\begin{pmatrix}
-\sqrt{2} & 0 \\ 0 & \sqrt{2}
\end{pmatrix}\begin{pmatrix}
1-\sqrt{2} & 1=\sqrt{2} \\ 1 & 1
\end{pmatrix}^{-1} \]
So
\[ |A+B| = \begin{pmatrix}
\sqrt{2} & 0 \\ 0 & \sqrt{2}
\end{pmatrix} \qquad\text{and}\qquad |A| + |B| = \begin{pmatrix}
1 & 1 \\ 1 & 3
\end{pmatrix}. \]
Finally $|A| + |B| - |A+B| = \begin{pmatrix}
1 - \sqrt{2} & 1 \\ 1 & 3-\sqrt{2}
\end{pmatrix}$ is not positive:
\[ \begin{pmatrix}
1 & 0
\end{pmatrix}\begin{pmatrix}
1 - \sqrt{2} & 1 \\ 1 & 3-\sqrt{2}
\end{pmatrix}\begin{pmatrix}
1 \\ 0
\end{pmatrix} = 1-\sqrt{2} < 0. \]
\end{example}

\begin{lemma} \label{firstFactorInNormToAbsValue}
Let $A$ be a $C^*$-algebra and $x,a\in A$. Then $\norm{xa} = \norm{|x|a}$.
\end{lemma}
\begin{proof}
We calculate
\[ \norm{xa}^2 = \norm{a^*x^*xa} = \norm{a^*\sqrt{x^*x}\sqrt{x^*x}a} = \norm{(\sqrt{x^*x}a)^*\sqrt{x^*x}a} = \norm{\sqrt{x^*x}a}^2 = \norm{|x|a}^2. \]
Taking the square root gives the result.
\end{proof}



\subsection{Positive maps}
\begin{definition}
Let $A,B$ be $C^*$-algebras. Then $f:A\to B$ is a \udef{positive map} if
\[ \forall x\in A: \quad x\geq 0 \implies f(x)\geq 0. \]
\end{definition}
By \ref{existenceSquareRoot}, this is equivalent to the condition that $f(x^*x)\geq 0$ for all $x\in A$.

TODO: \url{https://www-m5.ma.tum.de/foswiki/pub/M5/CQC/Masterarbeit.pdf}
\url{https://iopscience.iop.org/article/10.1088/0305-4470/34/29/308}

\begin{lemma} \label{starHomomorphismPositive}
Let $A,B$ be $C^*$-algebras and $f:A\to B$ a $*$-homomorphism. Then $f$ is positive.
\end{lemma}
\begin{proof}
Take $a\in A^+$. Then there exists $b\in A$ such that $a = b^*b$ and so $f(a) = f(b^*b) = f(b)^*f(b)$, which is positive.
\end{proof}

\subsubsection{Positive functionals}
\begin{definition}
Let $A$ be a $C^*$-algebra. A linear functional $\rho$ on $A$ is positive, written $\rho\geq 0$, if
\[ \forall x\in A: \quad x\geq 0 \implies \rho(x)\geq 0. \]
\end{definition}
Not necessarily multiplicative!

\begin{lemma} \label{positiveLinearFunctionalPreinnerProduct}
Let $\omega$ be a positive linear functional on a $C^*$-algebra $A$. Then the map
\[ \inner{\cdot, \cdot}_\omega: A\times A \to \C: (x,y)\mapsto \omega(x^*y) \]
is a pre-inner product on $A$.
\end{lemma}
\begin{proof}
It is clear that $\omega$ is $\inner{\cdot, \cdot}_\omega$ a sesquilinear form. By \ref{HermitianRealQuadratic}, it is now enough to prove positivity. Since $x^*x$ is self-adjoint for all $x\in A$, the positivity of $\inner{\cdot, \cdot}_\omega$ follows from the positivity of $omega$.
\end{proof}

\begin{proposition} \label{positiveLinearFunctionalBounded}
Let $A$ be a $C^*$-algebra and $f: A\to \C$ a positive linear functional on $A$. Then $f$ is bounded.
\end{proposition}
\begin{proof}
We first show that $f$ is bounded on $A^+$. Suppose, towards a contradiction, that $f$ is not bounded on $A^+$. Then for all $n\in \N$, we can find a unit vector $a_n\in A^+$ such that $f(a_n) \geq n$. Now $\sum_{n=1}^\infty \frac{a_n}{n^2}$ is absolutely convergent by \ref{pseriesConvergence} and thus is convergent by \ref{absoluteConvergenceImpliesConvergence}. We call the limit $a$. Let $b_N \defeq \sum_{n=1}^N \frac{a_n}{n^2}$ be the partial sum for all $N\geq 1$.

For all such $N$ we have $0 \leq b_N \leq a$. Indeed, $a-b_N$ is the limit of $\seq{b_M - b_N}_{M\geq N}$ which is a sequence of positive elements. Since $A^+$ is closed by \ref{CstarPositiveConeProperties} in $A$, $a-b_N$ is also an element of $A^+$. Thus
\[ f(a) \geq f(b_N) = \sum_{n=1}^N \frac{f(a_n)}{n^2} \geq \sum_{n=1}^N \frac{1}{n}. \]
Since, by \ref{pseriesConvergence}, $\sum_{n=1}^N \frac{1}{n}$ grows arbitrarily large, this is a contradiction. We have established that $m \defeq\sup_{\substack{a \geq 0  \\ \norm{a} = 1}} f(a)$ is finite.

Now take an arbitrary unit vector $c\in A$ and consider the Cartesian decomposition $c = (p_1 - p_2) + i(p_3 - p_4)$ (\ref{CartesianDecomposition}). Then
\[ |f(c)| = \big|(p_1 - p_2) + i(p_3 - p_4)\big| \leq |p_1| + |p_2| + |p_3| + |p_4| \leq 4m. \]
This implies that $f$ is bounded.
\end{proof}

\begin{proposition} \label{positiveFunctionalNormValueAtUnit}
Let $A$ be a $C^*$-algebra, $f: A\to \C$ a bounded linear functional on $A$ and $\seq{e_\lambda}_{\lambda\in \Lambda}$ an increasing approximate unit. Then the following are equivalent
\begin{enumerate}
\item $f$ is positive;
\item $\norm{f} = \lim_\lambda f(e_\lambda)$.
\end{enumerate}
In particular, if $A$ is unital, then $\norm{f} = f(\vec{1})$.
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$  For all $\lambda \leq \mu$ we have $e_\lambda \leq e_\mu$, so $f(e_\lambda) \leq f(e_\mu) \leq \norm{f}$. Since $\seq{f(e_\lambda)}$ is a bounded monotone sequence, it converges (TODO ref) to a limit less than $\norm{f}$, i.e. $\lim_\lambda f(e_\lambda)$ exists and $\lim_\lambda f(e_\lambda)\leq \norm{f}$.

We now prove the other inequality. Let $a\in A$ be a unit vector. Since $f$ determines a pre-inner product, \ref{positiveLinearFunctionalPreinnerProduct}, the Cauchy-Schwarz inequality \ref{CauchySchwarz} holds. Then
\begin{align*}
|f(a)|^2 &= \big|f\big(\lim_\lambda e_\lambda a\big)\big| \\
&= \lim_\lambda |f(e_\lambda a)|^2 = \lim_\lambda |f(e_\lambda^* a)|^2 \\
&\leq \limsup_\lambda |f(e_\lambda^* a)|^2 \\
&\leq \limsup_\lambda |f(e_\lambda^*e_\lambda)|\, |f(a^*a)| \\
&= \limsup_\lambda f(e_\lambda^*e_\lambda)f(a^*a).
\end{align*}
By positivity of $f$ we have $|f(e_\lambda^*e_\lambda)| = f(e_\lambda^2) \leq f(e_\lambda)$, using \ref{orderSubUnitVectorLemma}. Since $\lim_\lambda f(e_\lambda)$ exists, as was previously established, we have $\limsup_\lambda f(e_\lambda) = \lim_\lambda f(e_\lambda)$. Thus
\[ |f(a)|^2 \leq \limsup_\lambda f(e_\lambda^*e_\lambda)f(a^*a) \leq \limsup_\lambda f(e_\lambda) f(a^*a) = \lim_\lambda f(e_\lambda) f(a^*a). \]
Since $\norm{a^*a} \leq \norm{a^*}\,\norm{a} = 1$, we have $|f(a^*a)|\leq \norm{f}$ and thus $|f(a)|^2 \leq \lim_\lambda f(e_\lambda) \norm{f}$. Taking the supremum over $a$ yields $\norm{f}^2 \leq \norm{f}\lim_\lambda f(e_\lambda)$ or $\norm{f} \leq \lim_\lambda f(e_\lambda)$.

$(2) \Rightarrow (1)$ We first prove that $f(b)\in \R$ for all $b\in \SelfAdjoints(A)$. Indeed, set $f(b) = \alpha + i\beta$. WLOG we may take $\norm{b} \leq 1$ (by potentially rescaling) and $\beta \geq 0$ (by potentially replacing $b\to -b$).

Since $e_\lambda b - be_\lambda \to 0$, we can, for all $n\in \N$ find $\lambda_n\in \Lambda$ such that for all $\lambda \geq \lambda_n$ we have $\norm{e_\lambda b - be_\lambda} \leq n^{-1}$. Then, for $\lambda \geq \lambda_n$, we have
\begin{align*}
\big|f(ne_\lambda - ib)\big|^2 &\leq \norm{f}^2\norm{ne_\lambda - ib}^2 \\
&= \norm{f}^2\norm{(ne_\lambda - ib)(ne_\lambda - ib)^*} \\
&= \norm{f}^2\norm{(ne_\lambda - ib)(ne_\lambda + ib)} \\
&= \norm{f}^2\norm{n^2e_\lambda^2 + b^2 - in(e_\lambda b - be_\lambda)} \\
&\leq \norm{f}^2(n^2 + 2).
\end{align*}
For arbitrary $n\in \N$, we calculate
\begin{align*}
\lim_\lambda \big|f(ne_\lambda - ib)\big|^2 &= \lim_\lambda \big|nf(e_\lambda) - i\alpha + \beta\big|^2 \\
&= \big|n\norm{f} - i\alpha + \beta\big|^2 \\
&= \big(n\norm{f}+\beta\big)^2 + \alpha^2 \\
&= n^2\norm{f}^2 + 2n\beta\norm{f}+\beta^2 + \alpha^2.
\end{align*}
Putting everything together gives
\[ n^2\norm{f}^2 + 2n\beta\norm{f}+\beta^2 + \alpha^2 = \lim_\lambda \big|f(ne_\lambda - ib)\big|^2 \leq \norm{f}^2(n^2 + 2). \]
Cancelling $n^2\norm{f}^2$ from both sides gives $2n\beta\norm{f}+\beta^2 + \alpha^2 \leq 2\norm{f}^2$. This can only hold for all $n\in \N$ if $\beta = 0$. Thus $f(b)\in \R$.

Now take arbitrary $a\in A^+$. WLOG we may take $\norm{a}\leq 1$. Since we have already seen that $f(e_\lambda - a) \in \R$, we have
\[ f(e_\lambda) - f(a) = f(e_\lambda - a) \leq |f(e_\lambda - a)| \leq \norm{f}\,\norm{e_\lambda - a} \leq \norm{f}, \]
where we have used that $\norm{e_\lambda - a} \leq 1$ by \ref{orderSubUnitVectorLemma}.
Thus $f(a) \geq f(e_\lambda) - \norm{f} \to 0$.
\end{proof}


\begin{lemma}
Let $A$ be a $C^*$-algebra, $f: A\to \C$ a bounded linear functional on $A$ and $a,x\in A$. Then
\begin{enumerate}
\item $f(a) = \overline{f(a^*)}$;
\item $|f(a)|^2 \leq \norm{f}f(a^*a) = \norm{f}\inner{a,a}_f$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Since $f$ determines a pre-inner product, as in \ref{positiveLinearFunctionalPreinnerProduct}, we have that $\inner{\cdot,\cdot}_f$ has conjugate symmetry. 

Let $\seq{e_\lambda}_{\lambda\in \Lambda}$ be an increasing approximate unit. Since $f$ is bounded \ref{positiveLinearFunctionalBounded}, we have
\[ f(a) = \lim_{\lambda}f(e_\lambda a) = \lim_{\lambda}\inner{e_\lambda, a}_f = \lim_{\lambda}\overline{\inner{a, e_\lambda}_f} = \lim_{\lambda}\overline{f(a^*e_\lambda)} = \overline{f(a^*)}. \]

(2) The inequality $|f(a)|^2 \leq \lim_\lambda f(e_\lambda) f(a^*a)$ appears in the proof of \ref{positiveFunctionalNormValueAtUnit}. Using the result of this proposition, we have $|f(a)|^2 \leq \norm{f} f(a^*a)$.
\end{proof}

\subsubsection{States}
\begin{definition}
A \udef{state} on a $C^*$-algebra $A$ is a positive linear functional of norm $1$. The set $\states(A)$ of all states on $A$ is called the \udef{state space} of $A$.
\end{definition}


\begin{example}
Let $A$ be a concrete $C^*$-algebra of operators acting non-degenerately on $\mathcal{H}$ and $\xi \in \mathcal{H}$. Define
\[ \rho_\xi: A \to \C: x\mapsto \inner{\xi, x\xi}, \]
then $\rho_\xi$ is a positive linear functional on $A$ of norm $\norm{\xi}^2$, so  $\rho_\xi$ is a state if $\norm{\xi} =1$. Such a state is called a \udef{vector state} of $A$.
\end{example}

\begin{lemma}
Let $B\subseteq A$ be unital $C^*$-algebras. State on $B$ extends to a state on $A$.
\end{lemma}
\begin{proof}
Let $\omega\in\states(B)$ be a state on $B$. It can be extended to a bounded functional $\omega'$ of the same norm on $A$ by \ref{existenceBoundedFunctionalOfSameNorm}. Since $\vec{1} \in B\subseteq A$, we have, by \ref{positiveFunctionalNormValueAtUnit},
\[\omega'(\vec{1}) = \omega(\vec{1}) = \norm{\omega} = \norm{\omega'}. \]
This implies that $\omega'$ is positive by \ref{positiveFunctionalNormValueAtUnit}. 
\end{proof}

\begin{lemma} \label{normalElementExtendsToState}
Let $A$ be a $C^*$-algebra and $a\in \Normals(A)\setminus\{0\}$. Then there exists a state $\omega\in \states(A)$ such that $|f(a)| = \norm{a}$.
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $\omega$ be a positive linear functional over a $C^*$-algebra $A$ and $a,b\in A$, then
\begin{enumerate}
\item $\omega(a^*) = \overline{\omega(a)}$;
\item $|\omega(a)|^2 \leq \omega(a^*a)\norm{\omega}$;
\item $|\omega(a^*ba)| \leq \omega(a^*a)\norm{b}$;
\item $\norm{\omega} = \sup\setbuilder{\omega(a^*a)}{\norm{a} = 1}$
\end{enumerate}
\end{proposition}
\begin{proof}
By \ref{realImaginaryParts} and \ref{positiveNegativeParts} we can write $a\in A$ as
\[ a = x_{1,+} - x_{1,-} + i(x_{2,+} - x_{2,-}).\]
Where $x_{1,+}, x_{1,-}, x_{2,+}, x_{2,-}$ are self-adjoint. Then
\[ \rho(a^*) = \rho(x_{1,+} - x_{1,-} - i(x_{2,+} - x_{2,-})) = \rho(x_{1,+}) - \rho(x_{1,-}) - i(\rho(x_{2,+}) - \rho(x_{2,-})) = \overline{\rho(a)}. \]
Where the last equality follows because $\rho$ takes real values on self-adjoint elements. (TODO!)
\end{proof}
\begin{corollary}
Let $\omega_1$ and $\omega_2$ be positive linear functionals over a $C^*$-algebra $A$. Then $\omega_1+\omega_2$ is a positive linear functional and
\[ \norm{\omega_1+\omega_2} = \norm{\omega_1} + \norm{\omega_2}. \]
Thus the state space is a convex subset of the dual of $A$.
\end{corollary}
\begin{corollary}
Let $X$ be a compact Hausdorff space. Let $\omega$ be a positive linear functional on $\cont(X)$. Then $\omega$ is continuous and $\norm{\omega} = 1$.
\end{corollary}
TODO: move up for Riesz-Markov?

\begin{proposition}
If $A$ is commutative, the pure states are exactly the characters.
\end{proposition}

\subsection{Comparison of projectors}
Lattice
\subsection{General comparison theory}

\section{Constructions}
\subsection{Moore-Penrose pseudoinverse}
\begin{definition}
Let $A$ be a $C^*$ algebra and $x\in A$. Let $x^+$ be an element of $A$ such that
\begin{itemize}
\item $xx^+x = x$;
\item $x^+xx^+ = x^+$;
\item $xx^+$ is Hermitian;
\item $x^+x$ is Hermitian.
\end{itemize}
Then $x^+$ is called a \udef{Moore-Penrose pseudoinverse} of $x$.
\end{definition}
We will show that an element has at most one Moore-Penrose pseudoinverse, so we are justified in speaking of \emph{the} Moore-Penrose pseudoinverse of $x$, which we denote $x^+$.

If $x$ has a generalised inverse $x^+$, then $xx^+$ and $x^+x$ are automatically idempotent, so the requirement that they be Hermitian is equivalent to requiring they be orthogonal projectors.

\begin{lemma} \label{MoorePenrosePseudoinverseSwapLemma}
Let $A$ be a $C^*$ algebra, $x\in A$ and $x^+$ be a Moore-Penrose pseudoinverse of $x$. Then
\begin{enumerate}
\item $xx^+ = (x^+)^*x^*$;
\item $x^+x = x^*(x^+)^*$;
\item $x = xx^*x^{+*} = x^{+*}x^*x$;
\item $x^+ = x^+x^{+*}x^* = x^*x^{+*}x^+$;
\item $x^* = x^*xx^+ = x^+xx^*$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1,2) Immediate from $xx^+ = (xx^+)^* = (x^+)^*x^*$ and similarly for (2).

(3, 4) We have $x = xx^+x$, which is equal to $xx^*x^{+*}$ and $x^{+*}x^*x$ by (1) and (2). For (3) we start with $x^+ = x^+xx^+$.

(5) Follows by taking the adjoint of (3).
\end{proof}

\begin{proposition} \label{MPPseudoinverseUnique}
Let $A$ be a $C^*$ algebra and $x\in A$. Then $x$ has at most one Moore-Penrose pseudoinverse.
\end{proposition}
\begin{proof}
Suppose $a,b$ are two Moore-Penrose pseudoinverses of $x$. Then we first show $xa = xb$. Indeed, using \ref{MoorePenrosePseudoinverseSwapLemma}, we calculate
\[ xa = a^*x^* = a^*(xbx)^* = a^*x^*(xb)^* = xaxb = xb. \]
Similarly $ax = bx$. Thus
\[ a = axa = (ax)a = (bx)a = b(xa) = b(xb) = bxb = b. \]
\end{proof}
\begin{corollary}
Let $A$ be a $C^*$ algebra and $x\in A$ such that $x$ has a Moore-Penrose pseudoinverse. Then $(x^+)^+ = x$.
\end{corollary}
\begin{proof}
Since the definition is symmetric w.r.t.\ $x\leftrightarrow x^+$, we have that $x^+$ is a Moore-Penrose pseudoinverse of $x$. By the proposition it is the only one.
\end{proof}

\begin{lemma} \label{MPPseudoinverseAdjoint}
Let $A$ be a $C^*$ algebra and $x\in A$. Then $x^*$ has a Moore-Penrose pseudoinverse \textup{if and only if} $x$ has one. In this case $(x^*)^+ = (x^+)^*$.
\end{lemma}
\begin{proof}
Assume $x$ has a Moore-Penrose pseudoinverse $x^+$. We show that $(x^+)^*$ satisfies the requirements to be a Moore-Penrose pseudoinverse of $x^*$. We have $(x^*)^+ = (x^+)^*$ by uniqueness of the Moore-Penrose pseudoinverse \ref{MPPseudoinverseUnique}.

Taking the adjoint of $x = xx^+x$ gives $x^* = x^*(x^+)^*x^*$ and the adjoint of $x^+ = x^+xx^+$ gives $(x^+)^* = (x^+)^*x^*(x^+)^*$.

Next, we have
\begin{align*}
\big(x^*(x^+)^*\big)^* &= x^+x = (x^+x)^* = x^*(x^+)^* \\
\big((x^+)^*x^*\big)^* &= xx^+ = (xx^+)^* = (x^+)^*x^*.
\end{align*}

Finally, note that if $x^*$ has a Moore-Penrose pseudoinverse, then we have shown that $x^{**} = x$ has one.
\end{proof}
\begin{corollary} \label{MPPseudoinverseHermitianElement}
The Moore-Penrose pseudoinverse of a Hermitian element is Hermitian, if it exists.
\end{corollary}
\begin{corollary}
The Moore-Penrose pseudoinverse of a Hermitian element commutes with the element, if it exists.
\end{corollary}
Thus Hermitian elements with Moore-Penrose pseudoinverse are simply polar.
\begin{proof}
Follows from \ref{MoorePenrosePseudoinverseSwapLemma} and \ref{MPPseudoinverseHermitianElement}.
\end{proof}

\begin{proposition}
Let $A$ be a $C^*$ algebra and $x\in A$. Then the following are equivalent:
\begin{enumerate}
\item $x$ has a Moore-Penrose pseudoinverse;
\item $x$ is regular;
\item $x\widetilde{A}$ is closed;
\item $|x|\widetilde{A}$ is closed;
\item $|x|$ is simply polar;
\item $x^*x$ is simply polar;
\item $0$ is not an accumulation point of $\spec(x^*x)$;
\item $x^*x$ has a Moore-Penrose pseudoinverse.
\end{enumerate}
\end{proposition}
In this case we also have that $xA$ is closed, since $xA = x\widetilde{A}$ for regular elements.

Note that we do not assume $A$ unital.
\begin{proof}
$(1) \Rightarrow (2)$ Immediate.

$(2) \Rightarrow (3)$ Let $x'$ be the generalised inverse of $x$. Suppose $y \in x\widetilde{A}$. Then there exists $b\in \widetilde{A}$ such that $y = xb = xx'xb = xx'y$, so $0 = y- xx'y$. Conversely, if $0 = y- xx'y$, then $y = x(x'b) \in x\widetilde{A}$. Thus $x\widetilde{A}$ is exactly the kernel of the function $A\to A: y\mapsto y - xx'y$, which is continuous. Thus $x\widetilde{A}$ is closed.

$(3) \Rightarrow (4)$ Let $\seq{|x|y_n}_{n\in \N}$ be a Cauchy sequence in $|x|\widetilde{A}$. For all $m,n\in \N$ we have $\norm{|x|(y_m - y_n)} = \norm{x(y_m - y_n)}$ by \ref{firstFactorInNormToAbsValue}. This means that $\seq{xy_n}$ is a Cauchy sequence in $x\widetilde{A}$. Since $x\widetilde{A}$ is closed, its limit lies in $x\widetilde{A}$ and thus can be written $xy$ for some $y\in \widetilde{A}$. Thus $\norm{x(y_n - y)} \to 0$, so also $\norm{|x|(y_n - y)} \to 0$, which means that $|x|y_n \to |x|y \in |x|\widetilde{A}$.

$(4) \Rightarrow (5)$ Choose a sequence $\seq{p_n}$ of real polynomials that converges to the square root function $\sqrt{-}$ uniformly on $\interval{0,\norm{x}}$ such that no $p_n$ has a constant term (e.g.\ rescale the Bernstein polynomials \ref{uniformApproximationByBernsteinPolynomials}).

For all $p_n$, we have $p_n(|x|) \in |x|\widetilde{A}$. Since $|x|\widetilde{A}$ is closed, $\sqrt{|x|} = \lim_{n\to \infty} p_n(|x|) \in |x|\widetilde{A}$. This means that $\sqrt{|x|} = |x|c$ for some $c\in \widetilde A$.

Now we have
\[ \sqrt{|x|}\widetilde{A} \supseteq \sqrt{|x|}\sqrt{|x|}\widetilde{A} = |x|\widetilde{A} \supseteq |x|c\widetilde{A} = \sqrt{|x|}\widetilde{A}, \]
so $\sqrt{|x|}\widetilde{A} = |x|\widetilde{A}$. This also implies $\widetilde{A}\sqrt{|x|} = \big(\sqrt{|x|}\widetilde{A}\big)^* = \big(|x|\widetilde{A}\big)^* = \widetilde{A}|x|$.

So we can apply \ref{simplyPolarLemma} to obtain that $\sqrt{|x|}$ is simply polar in $\widetilde{A}$. This means $\sqrt{|x|}$ has a commuting generalised inverse $d$ in $\widetilde{A}$. We do not know whether $d$ lies in $A$, but we can use it to construct a commuting generalised inverse of $|x|$ in $A$. Indeed, consider $|x|'\defeq \sqrt{|x|}d^3$. Clearly $|x|' \in \sqrt{|x|}\widetilde{A} \subseteq A$. Also it commutes with $\sqrt{|x|}$ and thus also with $|x|$. Finally, we calculate
\begin{align*}
|x||x|'|x| &= \sqrt{|x|}^5d^3 = \sqrt{|x|}^2 = |x| \\
|x|'|x||x|' &= \sqrt{|x|}^4d^6 = d^3\sqrt{|x|} = |x|'.
\end{align*}

$(5) \Rightarrow (6)$ By \ref{squareSimplyPolar} and the fact that $x^*x = |x|^2$.

$(6) \Rightarrow (7)$ Suppose $x^*x$ is simply polar with commuting generalised inverse $(x^*x)'$. Then we have, in $\widetilde{A}$, that
\[ x^*x = \big(x^*x + (\vec{1} - (x^*x)'(x^*x))\big)(x^*x)'(x^*x) \]
Now $(x^*x)'(x^*x)$ is idempotent, so $\spec\big((x^*x)'(x^*x)\big) \subseteq \{0,1\}$ by \ref{spectrumIdempotent}, and $\big(x^*x + (\vec{1} - (x^*x)'(x^*x))\big)$ is invertible. Indeed it has an inverse given by $\big((x^*x)' + (\vec{1} - (x^*x)'(x^*x))\big)$:
\begin{align*}
\big(x^*x + (\vec{1} - (x^*x)'(x^*x))\big)\big((x^*x)' + (\vec{1} - (x^*x)'(x^*x))\big) &= \begin{aligned}[t]x^*&x(x^*x)' + (x^*x)' - (x^*x)'x^*x(x^*x)' + x^*x \\
&+ \vec{1} - (x^*x)'x^*x - x^*x(x^*x)'x^*x - (x^*x)'x^*x \\
&+ (x^*x)'x^*x(x^*x)'x^*x\end{aligned} \\
&= \vec{1} + x^*x(x^*x)' - (x^*x)'x^*x \\
&= \vec{1}.
\end{align*}
This inverse is two-sided since everything commutes. Thus $0\notin \spec\big(x^*x + (\vec{1} - (x^*x)'(x^*x))\big)$. Since the spectrum is compact (\ref{spectrumCompact}), there exists $\epsilon >0$ such that $\interval{-\epsilon, \epsilon} \subseteq \res\big(x^*x + (\vec{1} - (x^*x)'(x^*x))\big)$. 

Finally, by \ref{spectrumAlgebraicOperationsOnCommutingElements} (using commutativity), we have
\begin{align*}
\spec(x^*x) &= \spec\Big(\big(x^*x + (\vec{1} - (x^*x)'(x^*x))\big)(x^*x)'(x^*x)\Big) \\
&\subseteq \spec\big(x^*x + (\vec{1} - (x^*x)'(x^*x))\big)\spec\big((x^*x)'(x^*x)\big) \\
&\subseteq \big(\interval[oc]{-\infty, -\epsilon} \cup \interval[co]{\epsilon, \infty}\big)\cdot \{0,1\} \\
&= \interval[oc]{-\infty, -\epsilon} \cup \interval[co]{\epsilon, \infty} \cup \{0\},
\end{align*}
so $0$ is not an accumulation point of $\spec(x^*x)$.

$(7) \Rightarrow (8)$ In this case the function $f: \interval[co]{0,\infty} \to \interval[co]{0,\infty}: y \mapsto \begin{cases}
y^{-1} & (y \neq 0) \\ 0 & (y = 0).
\end{cases}$ is continuous when restricted to $\spec(x^*x)$. It also maps $0$ to $0$, so we can define $(x^*x)^+ \defeq f(x^*x)$ using \ref{nonUnitalContinuousFunctionalCalculus}.

By functional calculus, we see that $f(x^*x)$ is a Moore-Penrose pseudoinverse: both $f\cdot \id_{\sigma(x^*x)}\cdot f - f$ and $\id_{\sigma(x^*x)}\cdot f\cdot \id_{\sigma(x^*x)} - \id_{\sigma(x^*x)}$ are identically zero; $f\cdot \id_{\sigma(x^*x)} = \id_{\sigma(x^*x)}\cdot f$ is real-valued.

$(8) \Rightarrow (1)$ Suppose $x^*x$ has a Moore-Penrose pseudoinverse $(x^*x)^+$. We set $x^+ \defeq (x^*x)^+x^*$ and claim this is a Moore-Penrose pseudoinverse. Indeed, to show $x = xx^+x$, we calculate, using the fact that $x^*x$ is Hermitian and \ref{MPPseudoinverseAdjoint},
\begin{align*}
(x - xx^+x)^*(x - xx^+x) &= \big(x - x(x^*x)^+x^*x\big)^*\big(x - x(x^*x)^+x^*x\big) \\
&= x^*x - x^*x(x^*x)^+x^*x - x^*x(x^*x)^{+*}x^*x + x^*x(x^*x)^{+*}x^*x(x^*x)^+x^*x \\
&= x^*x - x^*x - (x^*x)^* + x^*x(x^*x)^{+*}x^*x \\
&= x^*x - x^*x - x^*x + (x^*x)^{*} = 0.
\end{align*}
Thus, by the $B^*$-identity,
\[ \norm{x - xx^+x} = \sqrt{\norm{(x - xx^+x)^*(x - xx^+x)}} = 0, \]
so $x = xx^+x$.

We also calculate
\[ x^+xx^+ = (x^*x)^+x^*x(x^*x)^+x^* = (x^*x)^+x^* = x^+. \]

Finally, we note that since $(x^*x)^+x^*x$ is Hermitian, we have that $x^+x = \big((x^*x)^+x^*\big)x = (x^*x)^+x^*x$ is also Hermitian. Since $(x^*x)^{+}$ is Hermitian, by \ref{MPPseudoinverseHermitianElement}, we have
\[ xx^+ = x(x^*x)^+x^* = \big(x(x^*x)^+x^*\big)^*, \]
so $xx^+$ is Hermitian.
\end{proof}

\subsubsection{The product formula}

\subsubsection{Tychonoff regularisation}

\subsection{Polar decomposition}
\begin{proposition}[Polar decomposition for invertible elements]
Let $A$ be a unital $C^*$-algebra and $a\in \GL(A)$ an invertible element. Then there exists a unique decomposition
\[ a = u(a) |a| \]
such that $u(a)$ is unitary. The map $u: \GL(A) \to \Unitaries(A)$ is continuous.
\end{proposition}
\begin{proof}
If $a$ is invertible, then $|a|$ is invertible by \ref{propertiesAbsoluteValue}. Put $u(a) = a|a|^{-1}$. Clearly $a = u(a)|a|$ and $u(a)$ is unitary because it is invertible and
\[ u(a)^*u(a) = |a|^{-1}a^*a|a|^{-1} = |a|^{-1}|a|^2|a|^{-1} = \vec{1}. \]

The continuity of u: $a\mapsto a|a|^{-1}$ follows from the continuity of multiplication, \ref{multiplicationContinuous}, the continuity of the absolute value, \ref{propertiesAbsoluteValue} and the continuity of the inverse, \ref{inverseMapContinuous}.
\end{proof}

In some cases we can find a polar decomposition for non-invertible elements.
\begin{definition}
A $C^*$ algebra $A$ is said to have the \udef{polar decomposition property} if for each $a\in A$ there exists a \emph{unique} pair $(v,p)$ of elements in $A$ such that
\begin{enumerate}
\item $a = vb$;
\item $v$ is a partial isometry and $b$ is positive;
\item $a^*a = b^2$;
\item $ap = 0$ implies $vp = 0$ for all projections $p\in\Projections(A)$.
\end{enumerate}
\end{definition}
Von Neumann algebras have the polar decomposition property. (TODO ref).

TODO: Rickart $C^*$ algebras have the polar decomposition property.

\begin{proposition}[Polar decomposition for elements with pseudoinverse]
Let $A$ be a $C^*$-algebra and $a\in A$ an element that has a Moore-Penrose pseudoinverse. Then there exists a unique decomposition
\[ a = v(a)|a| \]
such that $v(a)$ is a partial isometry.
\end{proposition}
TODO: The map $v$ is continuous??
\begin{proof}
TODO: clean up and complete! TODO: Show uniqueness of decomposition!?!

Set $v(a) \defeq a|a|^+$. Then (can we give a less convoluted argument?? In particular avoiding $\vec{1}$??)
\begin{align*}
\norm{a - v(a)|a|} &= \norm{a - a|a|^+|a|} \\
&= \norm{a(\vec{1} - |a|^+|a|)} \\
&= \norm{|a|(\vec{1} - |a|^+|a|)} \\
&= \norm{|a| - |a||a|^+|a|} = 0.
\end{align*}
To show that $v(a)$ is a partial isometry, we use \ref{partialIsometries}.
\end{proof}


\subsection{Singular value decomposition}
Suppose $a$ has a pseudoinverse $a^+$, can we construct an SVD if $aa^+$ and $a^+a$ are Murray-von Neumann equivalent??

TODO: Link to $K$-theory?? Does this have algorithmic implications??

\section{Hilbert $C^*$-modules}
\subsection{$C^*$-sesquilinear forms}
\begin{definition}
Let $A$ be a $C^*$-algebra and $V$ a complex vector space. An \udef{$A$-sesquilinear form} on $V$ is a function $S: V\times V\to A$ such that
\begin{itemize}
\item $S(x+\lambda y, z) = S(x,z) + \overline{\lambda}S(y,z)$ for all $x,y,z\in V$ and $\lambda \in \C$;
\item $S(x, y+\lambda z) = S(x,y) + \lambda S(x,z)$ for all $x,y,z\in V$ and $\lambda \in \C$.
\end{itemize}
\end{definition}

\begin{theorem}[Polarisation identity for $C^*$-sesquilinear forms] \label{C*PolarisationIdentity}
Let $A$ be a $C^*$-algebra, $V$ a complex vector space and $S: V\times V \to A$ an $A$-sesquilinear form. Then
\[ S(x,y) = \frac{1}{4}\sum_{k=0}^3 i^k S(i^k x+y, i^k x+y). \]
\end{theorem}
The proof is essentially the same as normal sesquilinear forms, \ref{polarisationIdentities}. TODO: merge?
\begin{proof}
We calculate
\begin{align*}
\sum_{k=0}^3 i^k S(i^k x+y, i^k x+y) &= \sum_{k=0}^3 i^ki^k(-i)^k S(x, x) + i^k(-i)^kS(x,y) + i^ki^kS(y,x) + i^kS(y,y) \\
&= \sum_{k=0}^3 i^k S(x, x) + S(x,y) + (-1)^kS(y,x) + i^kS(y,y) \\
&= \sum_{k=0}^3 S(x,y) \\
&= 4 S(x,y),
\end{align*}
where we have used that $\sum_{k=0}^3 i^k = 0$ and $\sum_{k=0}^3 (-1)^k = 0$.
\end{proof}
\begin{corollary} \label{zeroSesquilinearForm}
Let $A$ be a $C^*$-algebra, $V$ a complex vector space and $S: V\times V \to A$ an $A$-sesquilinear form. Then $S = \constant{0}$ \textup{if and only if} $S(v,v) = 0$ for all $v\in V$.
\end{corollary}
\begin{proof}
The direction $\Rightarrow$ is immediate. For the converse we use the polarisation identity:
\[ S(u,v) = \frac{1}{4}\sum_{k=0}^3 i^k S(i^k v+u, i^k u+v) = 0. \]
\end{proof}

\subsection{Inner-product $C^*$-modules}
\begin{definition}
Let $A$ be a $C^*$-algebra. A vector space $V$ is a \udef{pre-inner-product $A$-module} if it is a \emph{right} $A$-module and is equipped with a function $(\cdot, \cdot): V\times V\to A$ such that
\begin{itemize}
\item $(\cdot, \cdot): V\times V\to A$ is linear in the second component;
\item $(v,wa) = (v,w)a$ for all $v,w\in V$ and $a\in A$;
\item $(v,w) = (w,v)^*$ for all $v,w\in V$;
\item $(v,v) \in A^+$   for all $v\in V$.
\end{itemize}
We call $V$ a \udef{inner-product $A$-module} if $(v,v) = 0$ implies $v=0$ for all $v\in V$.
\end{definition}

\begin{lemma} \label{preInnerProductAmodule}
Let $A$ be a $C^*$-algebra and $V$ a pre-inner-product $A$-module, $v,w\in V$ and $a\in A$. Then
\begin{enumerate}
\item $(xa, y) = a^*(x,y)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have
\[ (xa, y) = (y, xa)^* = \big((y, x)a\big)^* = a^*(y,x)^* = a^*(x,y). \]
\end{proof}

\subsubsection{The norm}
\begin{definition}
Let $A$ be a $C^*$-algebra and $V$ a pre-inner-product $A$-module. We equip $V$ with a seminorm defined by $\norm{v} \defeq \sqrt{\norm{(v,v)}}$ for all $v\in V$.

If $V$ is an inner-product $A$-module, then the seminorm is a norm.
\end{definition}

\subsubsection{Cauchy-Schwarz}

\begin{proposition}[Cauchy-Schwarz for pre-inner-product $C^*$-modules]
Let $V$ be a pre-inner-product $A$-module and $v,w\in V$. Then
\[ (y,x)(x,y) \leq \norm{(x,x)}(y,y). \]
\end{proposition}
\begin{proof}
For all $a\in A$, we can calculate,
\begin{align*}
0 &\leq (xa-y, xa-y) \\
&= a^*(x,x)a - (y,x)a - a^*(x,y) + (y,y) \\
&\leq \norm{(x,x)}a^*a - (y,x)a - a^*(x,y) + (y,y),
\end{align*}
where we have used \ref{preInnerProductAmodule} and $\norm{(x,x)}a^*a = a^*\norm{(x,x)}a \leq a^*(x,x)a$ follows from \ref{normInequalityFromSAInequality} and \ref{CstarOrderLemma}.

Now the inequality follows from setting $a = \norm{(x,x)}^{-1}(x,y)$.
\end{proof}
\begin{corollary} \label{CSnormInequality}
Let $V$ be a pre-inner-product $A$-module and $v,w\in V$. Then
\[ \norm{(x,y)} \leq \norm{x}\norm{y}. \]
\end{corollary}
\begin{proof}
We calculate, using \ref{inequalityPositiveElementsLemma},
\begin{align*}
\norm{(x,y)}^2 &= \norm{(x,y)^*(x,y)} \\
&= \norm{(y,x)(x,y)} \\
&\leq \norm{\big.\norm{(x,x)}(y,y)} \\
&= \norm{(x,x)}\norm{(y,y)} \\
&= \norm{x}^2\norm{y}^2.
\end{align*}
Taking the square root yields the result.
\end{proof}

\subsection{Hilbert $C^*$-modules}
\begin{definition}
Let $A$ be a $C^*$-algebra and $V$ an inner-product $A$-module. We call $V$ a \udef{Hilbert $C^*$-module} if it is complete as a normed space.
\end{definition}

\section{Matrix $C^*$-algebras}
TODO: more information in \cite{lance_hilbert_1995}.

\begin{proposition}
Let $A$ be a $C^*$-algebra and $n\in \N$. Then $A^n$ is a right-module with multiplication defined by
\[ \cdot: A^n \times A\to A^n: \big(\seq{x_k}, a\big) \mapsto \seq{x_ka}. \]
When equipped with
\[ (\cdot, \cdot): A^n\times A^n \to A: \big(\seq{x_k}, \seq{y_k}\big) \mapsto \sum_{k=0}^{n-1}x_k^*y_k, \]
it is a Hilbert $A$-module.
\end{proposition}
\begin{proof}
Mostly routine verification.

For definiteness, take $\seq{x_k} \in A^n$ such that $(\seq{x_k}, \seq{x_k}) = 0$. Now take arbitrary $m \in [0:n-1]$. We have $0\leq x^*_mx_m \leq \sum_{k=0}^{n-1}x_k^*x_k = (\seq{x_k}, \seq{x_k}) = 0$. By \ref{inequalityPositiveElementsLemma}, this implies $0\leq \norm{x^*_mx_m} \leq 0$, so $\norm{x_m}^2 = \norm{x^*_mx_m} = 0$. By definiteness of the norm of $A$, this implies $x_m=0$. Since $m$ was chosen arbitrarily, we have $\seq{x_k} = \constant{0}$.

Completeness. TODO.
\end{proof}

\begin{proposition}
Let $A$ be a $C^*$-algebra. There exists a norm that makes $A^{n\times n}$ a $C^*$-algebra, which is given by, for $a\in A^{n\times n}$,
\begin{align*}
\norm{a}_{A^{n\times n}} &\defeq \sup\setbuilder{\norm{\sum_{i,j=0}^{n-1}x_i^*[a]_{i,j}y_{j}}_A}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \sup\setbuilder{\norm{\Bigg(\seq{x_k}_k, \seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k\Bigg)}_{A}}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \sup\setbuilder{\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}_{A^n}}{\seq{y_j}\in A^n, \, 1 = \norm{\seq{y_j}}}.
\end{align*}
\end{proposition}
\begin{proof}
The first equality is immediate from the definitions.

For the second equality, the inequality $\leq$ follows from \ref{CSnormInequality}. For the other inequality, we calculate
\begin{align*}
\sup_{\seq{y_j}\in B_{A^n}(0,1)}\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k} &= \sup_{\seq{y_j}\in B_{A^n}(0,1)}\sqrt{\norm{\Bigg(\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k, \seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k\Bigg)}} \\
&= \sup_{\seq{y_j}\in B_{A^n}(0,1)}\sqrt{\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}}\sqrt{\norm{\Bigg(\frac{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}{\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}}, \seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k\Bigg)}} \\ 
&\leq \sup_{\seq{x_i}, \seq{y_j}\in B_{A^n}(0,1)}\sqrt{\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}}\sqrt{\norm{\Bigg(\seq{x_i}, \seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k\Bigg)}} \\ 
&\leq \sup_{\seq{y_j}\in B_{A^n}(0,1)}\sqrt{\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}}\sup_{\seq{x_i}, \seq{y_j}\in B_{A^n}(0,1)}\sqrt{\norm{\Bigg(\seq{x_i}, \seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k\Bigg)}}.
\end{align*}
Dividing both sides by $\sup_{\seq{y_j}\in B_{A^n}(0,1)}\sqrt{\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}}$ and taking the square yields the result.

It is clear that $\norm{\cdot}_{A^{n\times n}}$ is a norm.

We check submultiplicativity, using \ref{CSnormInequality}. Take $a,b\in A^{n\times n}$. Then
\begin{align*}
\norm{ab}_{A^{n\times n}} &= \sup\setbuilder{\norm{\sum_{i,j=0}^{n-1}x_i^*[ab]_{i,j}y_{j}}}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \sup\setbuilder{\norm{\sum_{i,j,k=0}^{n-1}x_i^*[a]_{i,k}[b]_{k,j}y_{j}}}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \sup\setbuilder{\norm{\Bigg(\seq{\sum_{i=0}^{n-1}[a]_{i,k}^*x_i}_k, \seq{\sum_{j=0}^{n-1}[b]_{k,j}y_{j}}_k\Bigg)}}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&\leq \sup\setbuilder{\norm{\seq{\sum_{i=0}^{n-1}[a^*]_{k,i}^*x_i}_k}\, \norm{\seq{\sum_{j=0}^{n-1}[b]_{k,j}y_{j}}_k}}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \norm{a}_{A^{n\times n}}\norm{b}_{A^{n\times n}}.
\end{align*}
We now just need to verify the $C^*$-identity, which we can do using \ref{C*identityEquivalent}.
Take $a\in A^{n\times n}$. Then we calculate, using \ref{CSnormInequality},
\begin{align*}
\norm{a}_{A^{n\times n}}^2 &= \sup\setbuilder{\norm{\sum_{i,j=0}^{n-1}x_i^*[a]_{i,j}y_{j}}^2}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \sup\setbuilder{\norm{\Bigg(\seq{x_k}, \seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k\Bigg)}^2}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&\leq \sup\setbuilder{\norm{\seq{x_k}}^2\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}^2}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \sup\setbuilder{\norm{\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k}^2}{\seq{y_j}\in A^n, \, \norm{\seq{y_i}} = 1} \\
&= \sup\setbuilder{\norm{\Bigg(\seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k, \seq{\sum_{j=0}^{n-1}[a]_{k,j}y_{j}}_k\Bigg)}}{\seq{y_j}\in A^n, \, \norm{\seq{y_i}} = 1} \\
&= \sup\setbuilder{\norm{\sum_{i,j,k=0}^{n-1}y^*_{i}[a]^*_{k,i}[a]_{k,j}y_{j}}}{\seq{y_j}\in A^n, \, \norm{\seq{y_i}} = 1} \\
&= \sup\setbuilder{\norm{\sum_{i,j,k=0}^{n-1}y^*_{i}[a^*]_{i,k}[a]_{k,j}y_{j}}}{\seq{y_j}\in A^n, \, \norm{\seq{y_i}} = 1} \\
&= \sup\setbuilder{\norm{\sum_{i,j=0}^{n-1}y^*_{i}[a^*a]_{i,j}y_{j}}}{\seq{y_j}\in A^n, \, \norm{\seq{y_i}} = 1} \\
&\leq \sup\setbuilder{\norm{\sum_{i,j=0}^{n-1}x^*_{i}[a^*a]_{i,j}y_{j}}}{\seq{x_i}, \seq{y_j}\in A^n, \, \norm{\seq{x_i}} = 1 = \norm{\seq{y_j}}} \\
&= \norm{a^*a}_{A^{n\times n}}.
\end{align*}
\end{proof}

\subsection{Matrix $C^*$-algebra multiplication}
\begin{definition}
Let $A$ be a $C^*$-algebra and $l,m,n \in \N$. Then for all $\mathfrak{a} \in A^{l\times m}$ and $\mathfrak{b} \in A^{m\times n}$ we define $\mathfrak{a}\cdot \mathfrak{b} \in A^{l\times n}$ by
\[ [\mathfrak{a}\cdot \mathfrak{b}]_{i,j} \defeq \sum_{k=0}^{m-1} [\mathfrak{a}]_{i,k}\cdot [\mathfrak{b}]_{k, j} \]
for all $i\in [0:l-1]$ and $j\in [0:n-1]$.
\end{definition}

\subsection{Elementary properties}
\subsubsection{Inverses}
\begin{lemma} \label{2by2TriangularMatrixInverse}
Let $A$ be a unital $C^*$-algebra and $a\in A$. Then $\begin{pmatrix}
\vec{1} & a \\ 0 & \vec{1}
\end{pmatrix}$ is invertible with inverse $\begin{pmatrix}
\vec{1} & -a \\ 0 & \vec{1}
\end{pmatrix}$.
\end{lemma}
\begin{proof}
We calculate
\[ \begin{pmatrix}
\vec{1} & a \\ 0 & \vec{1}
\end{pmatrix}\begin{pmatrix}
\vec{1} & -a \\ 0 & \vec{1}
\end{pmatrix} = \begin{pmatrix}
\vec{1} & -a + a \\ 0 & \vec{1}
\end{pmatrix} = \begin{pmatrix}
\vec{1} & 0 \\ 0 & \vec{1}
\end{pmatrix}. \]
\end{proof}

\subsubsection{Diagonal and block-diagonal elements}

\begin{lemma} \label{invertibilityBlockDiagonalMatrix}
Let $A$ be a unital $C^*$-algebra, $\mathfrak{a}\in A^{n\times n}$ and $\mathfrak{b}\in A^{m\times m}$. Then
\[ \begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \mathfrak{b}
\end{pmatrix} \]
is invertible \textup{if and only if} both $\mathfrak{a}$ and $\mathfrak{b}$ are invertible.
\end{lemma}
\begin{proof}
First suppose $\begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \mathfrak{b}
\end{pmatrix}$ is invertible, with inverse $Y$. Then we can partition $Y = \begin{pmatrix}
Y_{0,0} & Y_{0,1} \\ Y_{1,0} & Y_{1,1}
\end{pmatrix}$, where $Y_{0,0} \in A^{n\times n}, Y_{0,1} \in A^{n\times m}, Y_{1,0} \in A^{m\times n}$ and $Y_{1,1} \in A^{m\times m}$. Then
\begin{align*}
\begin{pmatrix}
\mathbb{1} & \mathbb{0} \\
\mathbb{0} & \mathbb{1}
\end{pmatrix} &= \begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \mathfrak{b}
\end{pmatrix} \begin{pmatrix}
Y_{0,0} & Y_{0,1} \\ Y_{1,0} & Y_{1,1}
\end{pmatrix} = \begin{pmatrix}
\mathfrak{a}Y_{0,0} & \mathfrak{a}Y_{0,1} \\
\mathfrak{b}Y_{1,0} & \mathfrak{b}Y_{1,1}
\end{pmatrix} \\
&= \begin{pmatrix}
Y_{0,0} & Y_{0,1} \\ Y_{1,0} & Y_{1,1}
\end{pmatrix}\begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \mathfrak{b}
\end{pmatrix} = \begin{pmatrix}
Y_{0,0}\mathfrak{a} & Y_{0,1}\mathfrak{b} \\
Y_{1,0}\mathfrak{a} & Y_{1,1}\mathfrak{b}
\end{pmatrix}.
\end{align*}
This implies $\mathfrak{a}Y_{0,0} = \mathbb{1} = Y_{0,0}\mathfrak{a}$, so $Y_{0,0}$ is the inverse of $\mathfrak{a}$. Similarly $Y_{1,1}$ is the inverse of $\mathfrak{b}$.

Now assume $\mathfrak{a}, \mathfrak{b}$ are invertible. It is clear that $\begin{pmatrix}
\mathfrak{a}^{-1} & \mathbb{0} \\ \mathbb{0} & \mathfrak{b}^{-1}
\end{pmatrix}$ is the inverse of $\begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\ \mathbb{0} & \mathfrak{b}
\end{pmatrix}$.
\end{proof}
\begin{corollary}
Let $A$ be a unital $C^*$-algebra and $\seq{x_k}_{k=0}^{n-1} \in A^n$. Then $\diag\big(\seq{x_k}_{k=0}^{n-1}\big)$ is invertible \textup{if and only if} each $x_k$ is invertible. In this case  $\diag\big(\seq{x_k}_{k=0}^{n-1}\big)^{-1} = \diag\big(\seq{x_k^{-1}}_{k=0}^{n-1}\big)$.
\end{corollary}
\begin{proof}
Straightforward consequence of the lemma, by induction on $n$.
\end{proof}

\begin{lemma} \label{spectrumBlockDiagonalMatrix}
Let $A$ be a unital $C^*$-algebra, $\mathfrak{a}\in A^{n\times n}$ and $\mathfrak{b}\in A^{m\times m}$. Then
\[ \spec\begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \mathfrak{b}
\end{pmatrix} = \spec(\mathfrak{a}) \cup \spec(\mathfrak{b}). \]
\end{lemma}
\begin{proof}
We have
\begin{align*}
\lambda \in \spec\begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \mathfrak{b}
\end{pmatrix} &\iff \text{$\lambda \mathbb{1} - \begin{pmatrix}
\mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \mathfrak{b}
\end{pmatrix}$ is not invertible} \\
&\iff \text{$\begin{pmatrix}
\lambda \mathbb{1} -  \mathfrak{a} & \mathbb{0} \\
\mathbb{0} & \lambda \mathbb{1} - \mathfrak{b}
\end{pmatrix}$ is not invertible} \\
&\iff \text{either $\lambda \mathbb{1} -  \mathfrak{a}$ is not invertible or $\lambda \mathbb{1} -  \mathfrak{b}$ is not invertible} \\
&\iff \lambda \in \spec(\mathfrak{a}) \cup \spec(\mathfrak{b}),
\end{align*}
where we have used \ref{invertibilityBlockDiagonalMatrix}.
\end{proof}
\begin{corollary} \label{spectrumDiagonalMatrix}
Let $A$ be a unital $C^*$-algebra and $\seq{x_k}_{k=0}^{n-1} \in A^n$. Then
\[ \spec\Big(\diag\big(\seq{x_k}_{k=0}^{n-1}\big)\Big) = \bigcup_{k=0}^{n-1}\spec(x_k). \]
\end{corollary}
\begin{proof}
Straightforward consequence of the lemma, by induction on $n$.
\end{proof}

\subsubsection{Schur complements}
TODO: merge with matrix section.

\begin{lemma} \label{schurComplementInAlgebra}
Let $A$ be a unital $C^*$-algebra and $a,b,c,d\in A$. If $d$ is invertible, then 
\[ M = \begin{bmatrix}
a & b \\ c & d
\end{bmatrix} = \begin{bmatrix}
\vec{1} & bd^{-1} \\ 0 & \vec{1}
\end{bmatrix}\begin{bmatrix}
a-bd^{-1}c & 0 \\ 0 & d
\end{bmatrix}\begin{bmatrix}
\vec{1} & 0 \\ d^{-1}c & \vec{1}
\end{bmatrix}  \]
and if $a$ is invertible,
\[ M = \begin{bmatrix}
a & a \\ c & d
\end{bmatrix} = \begin{bmatrix}
\vec{1} & 0 \\ ca^{-1} & \vec{1}
\end{bmatrix}\begin{bmatrix}
a & 0 \\ 0 & d - ca^{-1}b
\end{bmatrix}\begin{bmatrix}
\vec{1} & a^{-1}b \\ 0 & 1
\end{bmatrix}.  \]
\end{lemma}
\begin{definition}
The element $M/d \defeq a-bd^{-1}c$ is the \udef{Schur complement} of $d$ in $M$.

Similarly $M/a \defeq d-ca^{-1}b$ is the Schur complement of $a$ in $M$.
\end{definition}

TODO: Link with Cholesky decomposition.

\subsection{Positivity in matrix $C^*$-algebras}

\begin{proposition}
Let $A$ be a unital $C^*$-algebra and $\mathfrak{a} \in A^{n\times n}$. Then the following are equivalent:
\begin{enumerate}
\item $\mathfrak{a}$ is positive;
\item $\sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j \geq 0$ for all $\seq{x_k} \in A^n$.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ By \ref{existenceSquareRoot}, there exists $\mathfrak{b} \in A^{n\times n}$ such that $\mathfrak{a} = \mathfrak{b}^*\mathfrak{b}$. Take $\seq{x_k} \in A^n$. We calculate
\begin{align*}
\sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j &= \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{b}^*\mathfrak{b}]_{i,j}x_j \\
&= \sum_{i,j,k=0}^{n-1}x^*_i[\mathfrak{b}^*]_{i,k}[\mathfrak{b}]_{k,j}x_j \\
&= \sum_{i,j,k=0}^{n-1}\big([\mathfrak{b}]_{k,i}x_i\big)^*[\mathfrak{b}]_{k,j}x_j \\
&= \sum_{k=0}^{n-1}\Big(\sum_{i=0}^{n-1}[\mathfrak{b}]_{k,i}x_i\Big)^*\Big(\sum_{j=0}^{n-1}[\mathfrak{b}]_{k,j}x_j\Big) \\
&= \Bigg(\seq{\sum_{i=0}^{n-1}[\mathfrak{b}]_{k,i}x_i}_k, \seq{\sum_{j=0}^{n-1}[\mathfrak{b}]_{k,j}x_j}_k\Bigg) \geq 0.
\end{align*}


$(2) \Rightarrow (1)$ First we prove that (2) implies that $\mathfrak{a}$ is self-adjoint.

For arbitrary $\seq{x_k} \in A^n$, we have
\[ \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j = \Big(\sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j\Big)^* = \sum_{i,j=0}^{n-1}x^*_j[\mathfrak{a}]^*_{i,j}x_i = \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}]^*_{j,i}x_j = \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}^*]_{i,j}x_j, \]
so
\[ 0 = \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j - \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}^*]_{i,j}x_j = \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a} - \mathfrak{a}^*]_{i,j}x_j. \]
Then \ref{zeroSesquilinearForm} implies that $\sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a} - \mathfrak{a}^*]_{i,j}y_j$ for all $\seq{x_k}, \seq{y_k}\in A^n$. In particular, for all $k,l\in [0:n-1]$, we have
\[ 0 = \sum_{i,j=0}^{n-1}\delta_{k,i}^*[\mathfrak{a} - \mathfrak{a}^*]_{i,j}\delta_{l,j} = [\mathfrak{a} - \mathfrak{a}^*]_{k,l}, \]
so $[\mathfrak{a}]_{k,l} = [\mathfrak{a}^*]_{k,l}$. This implies that $\mathfrak{a}$ is self-adjoint.

We prove the positivity of $\mathfrak{a}$ by induction on $n$. The base case is straightforward: take $a\in A^1$ such that $x^*ax \geq 0$ for all $x\in A$. Then, in particular, taking $x = \vec{1}$ yields $a \geq 0$.

Now suppose $(2)\Rightarrow (1)$ holds for all $\mathfrak{b} \in A^{n-1\times n-1}$. Take arbitrary self-adjoint $\mathfrak{a} = \begin{pmatrix}
\mathfrak{b} & \arrow{c} \\
\arrow{c}^* & d
\end{pmatrix} \in A^{n\times n}$, where $\arrow{c} \in A^{n-1}$ and $d\in A$, such that the property in (2) holds.

We first show that $\mathfrak{b}$ is positive. To that end, take arbitrary $\seq{x_k}\in A^{n-1}$ and consider the sequence $\seq{x_k'} = \seq{x_k} \concat \seq{0}\in A^n$. Then
\[ \sum_{i,j = 0}^{n-2}x^*_{i}[\mathfrak{b}]_{i,j}x_j = \sum_{i,j = 0}^{n-1}x^{\prime*}_{i}[\mathfrak{a}]_{i,j}x'_j \geq 0. \]
By the induction hypothesis, this implies that $\mathfrak{b}$ is positive.

By spectral mapping, $\mathfrak{b}_n \defeq \mathfrak{b} + n^{-1}\mathbb{1}$ is invertible and positive for all $n\in \N$. Also define $\mathfrak{a}_n \defeq \mathfrak{a} + n^{-1}\mathbb{1}$.

Then
\[ \mathfrak{a}_n = \begin{pmatrix}
\mathfrak{b}_n & \arrow{c} \\
\arrow{c}^* & d + n^{-1}\vec{1}
\end{pmatrix} = \begin{pmatrix}
\mathbb{1} & \mathbb{0} \\
\arrow{c}^* \mathfrak{b}_n^{-1} & \vec{1}
\end{pmatrix}\begin{pmatrix}
\mathfrak{b}_n & \mathbb{0} \\
\mathbb{0} & d + n^{-1}\vec{1} - \arrow{c}^* \mathfrak{b}_n^{-1} \arrow{c}
\end{pmatrix}\begin{pmatrix}
\mathbb{1} & \mathfrak{b}_n^{-1}\arrow{c} \\
\mathbb{0} & \vec{1}
\end{pmatrix}. \]
By \ref{CstarOrderLemma}, the positivity of $\mathfrak{a}_n$ follows from the positivity of $\begin{pmatrix}
\mathfrak{b}_n & \mathbb{0} \\
\mathbb{0} & d + n^{-1}\vec{1} - \arrow{c}^* \mathfrak{b}_n^{-1} \arrow{c}
\end{pmatrix}$, which, by \ref{spectrumBlockDiagonalMatrix}, follows from the positivity of both $\mathfrak{b}_n$ and $d + n^{-1}\vec{1} - \arrow{c}^* \mathfrak{b}^{-1}_n \arrow{c}$. The positivity of $\mathfrak{b}_n$ has already been established. For the positivity of $d + n^{-1}\vec{1} - \arrow{c}^* \mathfrak{b}^{-1}_n \arrow{c}$, consider the sequence
\[ \seq{x_k} \defeq \seq{- \sum_{l=0}^{n-2}[\mathfrak{b}_n^{-1}]_{k,l}c_l}_k \concat\seq{\vec{1}} \in A^{n}. \]
Now
\begin{align*}
0 &\leq \sum_{i,j = 0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j \\
&\leq \sum_{i,j = 0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j + n^{-1}\sum_{i = 0}^{n-1}x^*_ix_j = \sum_{i,j = 0}^{n-1}x^*_i[\mathfrak{a}_n]_{i,j}x_j \\
&= d + n^{-1}\vec{1} - \sum_{i= 0}^{n-2}\Big(\sum_{l=0}^{n-2}[\mathfrak{b}_n^{-1}]_{i,l}c_l\Big)^*c_i - \sum_{j = 0}^{n-2}c^*_j\Big(\sum_{l=0}^{n-2}[\mathfrak{b}_n^{-1}]_{j,l}c_l\Big) + \sum_{i,j = 0}^{n-2}\Big(- \sum_{l=0}^{n-2}[\mathfrak{b}_n^{-1}]_{i,l}c_l\Big)^*[\mathfrak{b}_n]_{i,j}\Big(- \sum_{l=0}^{n-2}[\mathfrak{b}_n^{-1}]_{j,l}c_l\Big) \\
&= d + n^{-1}\vec{1} - \sum_{i,l = 0}^{n-2}c_l^*[\mathfrak{b}_n^{-1}]^*_{i,l}c_i - \sum_{j,l = 0}^{n-2}c^*_j[\mathfrak{b}_n^{-1}]_{j,l}c_l + \sum_{i,j,k,l = 0}^{n-2}c_k^*[\mathfrak{b}_n^{-1}]_{i,k}^*[\mathfrak{b}_n]_{i,j}[\mathfrak{b}_n^{-1}]_{j,l}c_l \\
&= d + n^{-1}\vec{1} - \sum_{i,l = 0}^{n-2}c_l^*[\mathfrak{b}_n^{-1}]_{l,i}c_i - \sum_{j,l = 0}^{n-2}c^*_j[\mathfrak{b}_n^{-1}]_{j,l}c_l + \sum_{i,j,k,l = 0}^{n-2}c_k^*[\mathfrak{b}_n^{-1}]_{k,i}[\mathfrak{b}_n]_{i,j}[\mathfrak{b}_n^{-1}]_{j,l}c_l \\
&= d + n^{-1}\vec{1} - \sum_{i,l = 0}^{n-2}c_l^*[\mathfrak{b}_n^{-1}]_{l,i}c_i - \sum_{j,l = 0}^{n-2}c^*_j[\mathfrak{b}_n^{-1}]_{j,l}c_l + \sum_{k,l = 0}^{n-2}c_k^*[\mathfrak{b}_n^{-1}\mathfrak{b}_n\mathfrak{b}_n^{-1}]_{k,l}c_l \\
&= d + n^{-1}\vec{1} - \sum_{i,l = 0}^{n-2}c_l^*[\mathfrak{b}_n^{-1}]_{l,i}c_i = d + n^{-1}\vec{1} - \arrow{c}^* \mathfrak{b}^{-1}_n \arrow{c}.
\end{align*}
This proves that $\mathfrak{a}_n$ is positive, for all $n\in \N$. Since the set of positive elements is closed, \ref{CstarPositiveConeProperties}, $\mathfrak{a} = \lim_{n\to \infty}\mathfrak{a}_n$ is also positive.
\end{proof}
\begin{corollary} \label{diagonalPositiveMatrixPositive}
Let $A$ be a unital $C^*$-algebra and $\mathfrak{a} \in A^{n\times n}$. If $\mathfrak{a}$ is positive, then $[\mathfrak{a}]_{k,k} \geq 0$ for all $k\in [0:n-1]$.
\end{corollary}
\begin{proof}
Take positive $\mathfrak{a} \in A^{n\times n}$ and $\seq{x_m}$, with $x_m \defeq \begin{cases}
0 & (m \neq k) \\ \vec{1} & (m = k)
\end{cases}$. Then
\[ [\mathfrak{a}]_{k,k} = \sum_{i,j=0}^{n-1}x^*_i[\mathfrak{a}]_{i,j}x_j \geq 0. \]
\end{proof}

\begin{proposition}
Let $A$ be a unital $C^*$-algebra and $a,b,c\in A$. Suppose $a,b\in A^+$ and $a,b$ are invertible. Then the following are equivalent:
\begin{enumerate}
\item $\begin{pmatrix}
a & c \\
c^* & b
\end{pmatrix}$ is positive;
\item there exists $x \in A$ such that $\norm{x} \leq 1$ and $c = \sqrt{a}x\sqrt{b}$.
\end{enumerate}
\end{proposition}
TODO: can we drop requirement that $a,b$ be invertible? By considering $\begin{pmatrix}
a & c \\
c^* & b
\end{pmatrix} + \epsilon \begin{pmatrix}
\vec{1} & 0 \\ 0 & \vec{1}
\end{pmatrix}$? See Schur parameters, Constantinescu.
\begin{proof}
$(1) \Rightarrow (2)$ We have, see \ref{schurComplementInAlgebra},
\[ \begin{pmatrix}
a & c \\
c^* & b
\end{pmatrix} = \begin{pmatrix}
\vec{1} & 0 \\ c^*a^{-1} & \vec{1}
\end{pmatrix}\begin{pmatrix}
a & 0 \\ 0 & b - c^*a^{-1}c
\end{pmatrix}\begin{pmatrix}
\vec{1} & a^{-1}c \\ 0 & \vec{1}
\end{pmatrix}. \]
By \ref{2by2TriangularMatrixInverse}, we can write
\[ \begin{pmatrix}
a & 0 \\ 0 & b - c^*a^{-1}c
\end{pmatrix} = \begin{pmatrix}
\vec{1} & 0 \\ -c^*a^{-1} & \vec{1}
\end{pmatrix}\begin{pmatrix}
a & c \\
c^* & b
\end{pmatrix}\begin{pmatrix}
\vec{1} & -a^{-1}c \\ 0 & \vec{1}
\end{pmatrix}, \]
so $\begin{pmatrix}
a & 0 \\ 0 & b - c^*a^{-1}c
\end{pmatrix}$ is positive by \ref{CstarOrderLemma} and $b - c^*a^{-1}c$ is positive by \ref{spectrumDiagonalMatrix}. Then $\sqrt{b}^{-1}(b - c^*a^{-1}c)\sqrt{b}^{-1} = \vec{1} - \sqrt{b}^{-1}c^*a^{-1}c\sqrt{b}^{-1}$ is positive, again by \ref{CstarOrderLemma}. This implies, by \ref{inequalityPositiveElementsLemma}, that 
\[ \norm{\sqrt{a}^{-1}c\sqrt{b}^{-1}}^2 = \norm{\sqrt{b}^{-1}c^*a^{-1}c\sqrt{b}^{-1}} \leq 1, \]
so $x\defeq \sqrt{a}^{-1}c\sqrt{b}^{-1}$ is a contraction. Also $c = \sqrt{a}x\sqrt{b}$.

$(2) \Rightarrow (1)$ We have that $x^*x$ is self-adjoint and $\norm{x^*x} = \norm{x}^2 \leq 1$, so $\vec{1} - x^*x$ is positive by \ref{selfAdjointUnitSphereFromOrder} and $\begin{pmatrix}
\vec{1} & 0 \\ 0 & \vec{1} - x^*x
\end{pmatrix}$ is positive by \ref{spectrumDiagonalMatrix}. Now
\[ \begin{pmatrix}
a & c \\
c^* & b
\end{pmatrix} = \begin{pmatrix}
a & \sqrt{a}x\sqrt{b} \\
\sqrt{b}x^*\sqrt{a} & b
\end{pmatrix} = \begin{pmatrix}
\sqrt{a} & 0 \\ \sqrt{b}x^* & \sqrt{b}
\end{pmatrix}\begin{pmatrix}
\vec{1} & 0 \\ 0 & \vec{1} - x^*x
\end{pmatrix}\begin{pmatrix}
\sqrt{a} & x\sqrt{b} \\ 0 & \sqrt{b}
\end{pmatrix}, \]
so $\begin{pmatrix}
a & c \\
c^* & b
\end{pmatrix}$ is positive by \ref{CstarOrderLemma}.
\end{proof}
\begin{proof}[Alternate proof of $(2) \Rightarrow (1)$]
We can also factorise
\begin{align*}
\begin{pmatrix}
a & \sqrt{a}x\sqrt{b} \\
\sqrt{b}x^*\sqrt{a} & b
\end{pmatrix} &= \begin{pmatrix}
\sqrt{a} & 0 \\
\sqrt{b}x^* & \sqrt{b}D_x
\end{pmatrix}\begin{pmatrix}
\sqrt{a} & x\sqrt{b} \\
0 & D_x\sqrt{b} \\
\end{pmatrix} = \begin{pmatrix}
\sqrt{a} & x\sqrt{b} \\
0 & D_x\sqrt{b} \\
\end{pmatrix}^*\begin{pmatrix}
\sqrt{a} & x\sqrt{b} \\
0 & D_x\sqrt{b} \\
\end{pmatrix} \\
&= \begin{pmatrix}
\sqrt{a}D_{x^*} & \sqrt{a}x \\
0 & \sqrt{b}
\end{pmatrix}\begin{pmatrix}
D_{x^*}\sqrt{a} & 0 \\
x^*\sqrt{a} & \sqrt{b} \\
\end{pmatrix} = \begin{pmatrix}
D_{x^*}\sqrt{a} & 0 \\
x^*\sqrt{a} & \sqrt{b} \\
\end{pmatrix}^*\begin{pmatrix}
D_{x^*}\sqrt{a} & 0 \\
x^*\sqrt{a} & \sqrt{b} \\
\end{pmatrix},
\end{align*}
where $D_x = \sqrt{\vec{1} - x^*x}$. This implies $\begin{pmatrix}
a & c \\
c^* & b
\end{pmatrix}$ is positive by \ref{existenceSquareRoot}.
\end{proof}
\begin{corollary}
Let $A$ be a unital $C^*$-algebra and $a, b\in A$. Then the following are equivalent:
\begin{enumerate}
\item $\begin{pmatrix}
\vec{1} & a \\ a^* & b
\end{pmatrix}$ is positive;
\item $a^*a \leq b$.
\end{enumerate}
\end{corollary}
Note that both conditions imply that $b$ is positive, so we do not need to assume it a priori.
\begin{proof}
We first observe that that both conditions imply that $b$ is positive. In the first case, TODO use \ref{diagonalPositiveMatrixPositive}.


note that
\[ \begin{pmatrix}
0 & 0 \\ 0 & 1
\end{pmatrix}\begin{pmatrix}
\vec{1} & a \\ a^* & b
\end{pmatrix}\begin{pmatrix}
0 & 0 \\ 0 & 1
\end{pmatrix} = \begin{pmatrix}
0 & 0 \\ 0 & b
\end{pmatrix} \]
is positive by \ref{CstarOrderLemma} and thus $b$ is positive by \ref{spectrumDiagonalMatrix}. The second case follows by positivity of $a^*a$ and transitivity of the order.

$(1) \Rightarrow (2)$ By the proposition, there exists a contraction $x\in A$ such that $a = x\sqrt{b}$. Since $x^*x \leq 1$, by \ref{selfAdjointUnitSphereFromOrder}, we have
\[ a^*a \; = \; \sqrt{b}x^*x\sqrt{b} \; \leq \; \sqrt{b}\vec{1}\sqrt{b} \; = \; b, \]
by \ref{CstarOrderLemma}.

$(2) \Rightarrow (1)$ TODO
\end{proof}
\begin{corollary}
Let $A$ be a unital $C^*$-algebra and $a\in A$. Then $\norm{a}\leq 1$ \textup{if and only if} $\begin{pmatrix}
\vec{1} & a \\ a^* & \vec{1}
\end{pmatrix}$ is positive in $A^{2\times 2}$.
\end{corollary}



\subsection{Completely positive maps}
\begin{definition}
Let $A,B$ be $C^*$-algebras and $f:A\to B$ a linear function. We call $f$ \udef{completely positive} if for all $n\in\N$ the pointwise extension of $f$ in $(A^{n\times n}\to B^{n\times n})$ is positive.
\end{definition}

\begin{lemma}
Let $A,B$ be $C^*$-algebras and $\phi:A\to B$ a $*$-homomorphism. Then
\begin{enumerate}
\item $\phi_n$ is a $*$-homomorphism;
\item $\phi$ is completely positive.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Take $X,Y\in A^{n\times n}$. Then we calculate
\begin{align*}
\big[\phi_n(XY)\big]_{i,j} &= \phi\big([XY]_{i,j}\big) \\
&= \phi\Big(\sum_{k=1}^n[X]_{i,k}[Y]_{k,j}\Big) \\
&= \sum_{k=1}^n\phi\big([X]_{i,k}\big)\phi\big([Y]_{k,j}\big) \\
&= \sum_{k=1}^n[\phi_n(X)]_{i,k}[\phi_n(Y)]_{k,j} \\
&= \big[\phi_n(X)\phi_n(Y)\big]_{i,j}.
\end{align*}
We also have
\[ \big[\phi_n(X^*)\big]_{i,j} = \phi\big([X^*]_{i,j}\big) = \phi\big([X]_{j,i}^*\big) = \phi\big([X]_{j,i}\big)^* = [\phi_n(X)]_{j,i}^* = [\phi_n(X)^*]_{i,j}. \]

(2) Immediate from \ref{starHomomorphismPositive} and point (1).
\end{proof}

\subsection{Completely bounded maps}
\begin{definition}
Let $A,B$ be $C^*$-algebras and $f:A\to B$ a linear function. We call $f$ \udef{completely bounded} if for all $n\in\N$ the pointwise extension of $f$ in $(A^{n\times n}\to B^{n\times n})$ is bounded.
\end{definition}

\chapter{Von Neumann Algebras}
TODO: definitions of SOT and WOT!
\begin{definition}
A concrete $C^*$-algebra $A\subseteq \Bounded(\mathcal{H})$ is a \udef{von Neumann algebra} if it is closed in the SOT.
\end{definition}

\section{von Neumann bicommutant theorem}
\begin{proposition} \label{commutantBanachAlgebra}
Let $S\subset \Bounded(\mathcal{H})$ be a set for some Hilbert space $\mathcal{H}$. Then
\begin{enumerate}
\item $\comm{S}$ is a Banach algebra;
\item $\comm{S}$ is a $C^*$-algebra if $S = S^*$;
\item $\comm{S}\subseteq \Bounded(\mathcal{H})$ is WOT-closed.
\end{enumerate}
\end{proposition}

\chapter{Representations and states}
\section{Representations}
\begin{definition}
A \udef{representation} of a $C^*$-algebra $A$ on a Hilbert space $\mathcal{H}$ is a $*$-homomorphism $\pi: A \to \Bounded(\mathcal{H})$.

A \udef{subrepresentation} of $\pi$ is the restriction of $\pi$ to a closed invariant subspace of $\mathcal{H}$.

We say a representation $\pi: A \to \Bounded(\mathcal{H})$ is
\begin{enumerate}
\item \udef{faithful} if it is injective;
\item \udef{non-degenerate} if $\overline{\pi(A)\mathcal{H}} = \mathcal{H}$;
\item \udef{cyclic} w.r.t. a unit vector $\xi\in\mathcal{H}$ if $\overline{\pi(A)\xi} = \mathcal{H}$.
\end{enumerate}
\end{definition}

\begin{lemma}
Let $A$ be a $C^*$-algebra and $\pi: A \to \Bounded(\mathcal{H})$ a representation of $A$. Then $\pi$ is a faithful representation of $A/\ker\pi$.
\end{lemma}

\begin{proposition}
Let $\pi:A\to\mathcal{H}$ be a representation of a $C^*$-algebra, then $\pi$ being faithful is equivalent to any of the following:
\begin{enumerate}
\item $\ker \pi = \{0\}$;
\item $\norm{\pi(a)} = \norm{a}$ for all $a\in A$;
\item $\pi(a) > 0$ for all $a>0$.
\end{enumerate}
\end{proposition}

\begin{lemma}
Let $\pi: A\to\mathcal{H}$ be a representation and $P_1$ be a projector with closed range $\mathcal{H}_1$. Then $\pi|_{\mathcal{H}_1}$ is a subrepresentation \textup{if and only if}
\[ \forall a\in A: \quad \pi(a)P_1 = P_1\pi(a).  \]
\end{lemma}
\begin{proof}
Assume $P_1\pi(a) = \pi(a)P_1$ for all $a\in A$. Then multiplying by $P_1$ gives
\[ P_1\pi(a)P_1 = \pi(a)P_1 \quad \forall a\in A \]
which expresses invariance. Conversely, assume this invariance condition. Then
\[ \pi(a)P_1 = P_1\pi(a)P_1 = (P_1\pi(a)P_1)^{**} = (P_1\pi(a^*)P_1)^{*} = (\pi(a^*)P_1)^* = P_1\pi(a). \]
\end{proof}

\begin{lemma} \label{nonDegeneracyAlgebraRepresentation}
Let $\pi:A\to \mathcal{H}$ be a representation of a $C^*$-algebra $A$. Define
\[ \mathcal{H}_0 \defeq \setbuilder{x\in\mathcal{H}}{\forall a\in A: \pi(a)x = 0}. \]
Then $\pi$ is non-degenerate \textup{if and only if} $\mathcal{H}_0 = \{0\}$.
\end{lemma}
\begin{proof}
It is enough to prove that
\[ (\pi(A)\mathcal{H})^\perp = \mathcal{H}_0. \]
This implies $\overline{\pi(A)\mathcal{H}} = \mathcal{H}_0^\perp$ by \ref{doubleComplementClosure}. The claim then follows from \ref{OrthogonalComplementProperties}.

The proof then rests on the equality
\[ \forall x,y\in \mathcal{H}, a\in A: \quad \inner{x,\pi(a)y} = \inner{\pi(a^*)x,y}. \]
An $x\in\mathcal{H}$ is an element of $(\pi(A)\mathcal{H})^\perp$ iff the left side is zero for all $y\in\mathcal{H},a\in A$. An $x\in\mathcal{H}$ is an element of $\mathcal{H}_0$ iff $\pi(a^*)x = 0$ for all $a^*\in A$. This is equivalent to saying the right side is zero for all $y\in\mathcal{H},a\in A$ by the non-degeneracy of the inner product \ref{nonDegeneracyInnerProduct}.
\end{proof}

\begin{proposition}
Let $\pi:A\to \mathcal{H}$ be a representation. Then $\pi$ is non-degenerate \textup{if and only if} $\pi$ is the direct sum of a family of cyclic representations.
\end{proposition}

\begin{definition}
Two representations $\pi,\rho$ of $A$ on Hilbert spaces $\mathcal{X}$ and $\mathcal{Y}$ respectively are \udef{(unitarily) equivalent} if there is a unitary operator $U\in\Bounded(\mathcal{X}, \mathcal{Y})$ such that
\[ \forall x\in A: \quad U\pi(x)U^* = \rho(x). \]
\end{definition}

\subsection{Irreducible representations}
TODO move: to do with subsets of operators, not really representations.

\begin{definition}
A set $D$ of bounded operators on a Hilbert space $\mathcal{H}$ is called \udef{algebraically irreducible} if the only subspaces of $\mathcal{H}$ invariant under the action of $D$ are the trivial subspaces $\{0\}$ and $\mathcal{H}$.

The set $D$ is called \udef{topologically irreducible} if the only closed invariant subspaces of $\mathcal{H}$ are the trivial subspaces.

A representation $\pi: A\to\mathcal{H}$ is called irreducible if $\pi[A]$ is irreducible.
\end{definition}


Let $D$ be a set of bounded operators on the Hilbert space $\mathcal{H}$. Then $\mathcal{H}$ is a left $D$-module and $\mathcal{H}$ is algebraically irreducible if and only if it is a simple module. The commutant $D^\commute$ can be seen as a set of module morphisms.

\begin{proposition} \label{equivalentsIrreducibleSetsOperatorsHilbertSpace}
Let $D$ be a set of bounded operators on the Hilbert space $\mathcal{H}$. The following are equivalent:
\begin{enumerate}
\item $D$ is topologically irreducible;
\item all non-zero elements of the commutant $D^\commute$ are invertible; 
\item the commutant $D^\commute$ consists of multiples of the identity operator;
\item the commutant $D^\commute$ contains no projectors other than the identity;
\item if $D \neq \{0\}$ is a subalgebra of $\Bounded(\mathcal{H})$, then every non-zero vector $x\in\mathcal{H}$ is cyclic for $D$ in $\mathcal{H}$.
\end{enumerate}
\end{proposition}
If $D = \{0: \C\to \C\}$, then $D$ is irreducible, but no element of $\C$ is cyclic.
\begin{proof}
$(1) \Rightarrow (2)$ Immediate by Schur's lemma \ref{SchursLemma}.

$(2) \Rightarrow (3)$ Immediate by the Gelfand-Mazur theorem \ref{GelfandMazur}. Note that we use the fact that the commutant is a Banach algebra, \ref{commutantBanachAlgebra}.

$(3) \Rightarrow (4)$ Immediate.

$(4) \Rightarrow (1)$ Suppose $D$ were topologically reducible. Then there would be an orthogonal projector $P$ whose image was a closed invariant subspace of $\mathcal{H}$ and $P\neq \id_\mathcal{H}$. Invariance means $P\in D^\commute$, which goes against the assumption.

$(1) \Leftrightarrow (5)$ We have that $V \defeq \closure\setbuilder{d(x)}{d\in D}$ is a closed invariant subspace of $\mathcal{H}$. Since there exists $d\in D$ that is non-zero, there exists a non-zero element in $V$, so $V\neq \{0\}$. The equivalence of (1) and (5) is then immediate.
\end{proof}
\begin{corollary}
For $C^*$-algebras the notions of algebraic and topological irreducibility are equivalent.
\end{corollary}
\begin{proof}
TODO: should distinction algebraic and topological irreducibility be removed?
\end{proof}

\begin{lemma} \label{idealRepresentationExtensionLemma}
Let $A$ be a $C^*$-algebra and $J$ an ideal in $A$. Let $\rho: J\to \Bounded(\mathcal{H})$ be a nondegenerate representation of $J$. For all $b_1,\ldots, b_n\in J$, $h_1,\ldots, h_n\in\mathcal{H}$ and $a\in A$, we have
\[ \norm{\sum_{k=1}^n\rho(ab_k)h_k} \leq \norm{a}\, \norm{\sum_{k=1}^n\rho(b_k)h_k}. \]
\end{lemma}
\begin{proof}
By \ref{approximateUnitInIdeal} we can find an approximate unit $\seq{e_\lambda} \subseteq J$. Then
\begin{align*}
\norm{\sum_{k=1}^n\rho(ab_k)h_k} &= \lim_\lambda\norm{\sum_{k=1}^n\rho(ae_\lambda b_k)h_k} \\
&= \lim_\lambda\norm{\rho(ae_\lambda)\sum_{k=1}^n\rho(b_k)h_k} \\
&\leq \limsup_\lambda\norm{\rho(ae_\lambda)}\norm{\sum_{k=1}^n\rho(b_k)h_k} \\
&\leq \limsup_\lambda\norm{ae_\lambda}\norm{\sum_{k=1}^n\rho(b_k)h_k} \\
&= \lim_\lambda\norm{a}\norm{\sum_{k=1}^n\rho(b_k)h_k},
\end{align*}
where we have used \ref{starHomomorphismCstarProperties}.
\end{proof}

\begin{proposition}
Let $A$ be a $C^*$-algebra and $J$ an ideal in $A$. Let $\rho: J\to \Bounded(\mathcal{H})$ be a nondegenerate representation of $J$. Then there is a unique representation $\overline{\rho}: A\to \Bounded(\mathcal{H})$ of $A$ that extends $\rho$. If $\rho$ and $\sigma$ are equivalent representations of $J$, then $\overline{\rho}$ and $\overline{\sigma}$ are equivalent representations of $A$.
\end{proposition}
\begin{proof}
Set $\mathcal{H}_0 = \rho^\imf(J)\mathcal{H}$. For all $x = \sum_{k=1}^n\rho(b_k)h_k\in \mathcal{H}_0$, we define $\overline{\rho}$ by
\[ \overline{\rho}(a)x \defeq \sum_{k=1}^n\rho(ab_k)h_k. \]
Then \ref{idealRepresentationExtensionLemma} implies that this definition is well-defined. It also implies that $\overline{\rho}$ is bounded and so can be extended to a bounded operator on $\mathcal{H} = \overline{\mathcal{H}_0}$ by (TODO ref).

To show $\overline{\rho}$ is a representation of $A$, take $a_1,a_2\in A$. Then we calculate
\begin{align*}
\overline{\rho}(a_1a_2)(x) &= \sum_{k=1}\rho(a_1a_2b_k)h_k \\
&= \overline{\rho}(a_1)\big(\sum_{k=1}\rho(a_2b_k)h_k\big) \\
&= \overline{\rho}(a_1)\Big(\overline{\rho}(a_2)\big(\sum_{k=1}\rho(b_k)h_k\big)\Big) \\
&= \overline{\rho}(a_1)\big(\overline{\rho}(a_2)(x)\big) = \big(\overline{\rho}(a_1)\overline{\rho}(a_2)\big)(x).
\end{align*}
To show $\overline{\rho}$ is a $*$-homomorphism, take $y = \sum_{l=1}^m \rho(c_l)g_l \in \mathcal{H}_0$. Then we calculate
\begin{align*}
\inner{\overline{\rho}(a)^*x, y} &= \inner{x, \overline{\rho}(a)y} \\
&= \sum_{l=1}^m\inner{x, \rho(ac_l)g_l} \\
&= \sum_{l=1}^m\inner{\rho(c_l^*a^*)x, g_l} \\
&= \sum_{k,l}\inner{\rho(c_l^*a^*)\rho(b_k)h_k, g_l} \\
&= \sum_{k,l}\inner{\rho(c_l^*)\rho(a^*b_k)h_k, g_l} \\
&= \sum_{k,l}\inner{\rho(a^*b_k)h_k, \rho(c_l)g_l} \\
&= \inner{\overline{\rho}(a^*)(x), y}.
\end{align*}

Finally let $\sigma(b) = U\rho(b)U^*$ we need to show that $\overline{\rho}$ and $\overline{\sigma}$ are equivalent representations. We have
\[ U^*x = U^*\sum_{k=1}^n\rho(b_k)h_k = \sum_{k=1}^nU^*\rho(b_k)h_k = \sum_{k=1}^n\rho(b_k)\big(U^*h_k\big), \]
so we can calculate
\begin{align*}
\big(U\overline{\rho}(a)U^*\big)(x) &= U\big(\overline{\rho}(a)(U^*x)\big) \\
&= \sum_{k=1}^nU\Big(\overline{\rho}(a)\big(\rho(b_k)(U^*h_k)\big)\Big) \\
&= \sum_{k=1}^nU\big(\rho(ab_k)(U^*h_k)\big) \\
&= \sum_{k=1}^n\big(U\rho(ab_k)U^*\big)h_k \\
&= \sum_{k=1}^n\sigma(ab_k)h_k = \overline{\sigma}(a)(x).
\end{align*}
\end{proof}

\begin{proposition}
Let $A$ be a $C^*$-algebra and $J$ an ideal in $A$. Let $\rho$ be an irreducible representation of $J$ and $\pi$ an irreducible representation of $A$.
Then
\begin{enumerate}
\item $\overline{\rho}$ is an irreducible representation of $A$;
\item if $\pi^\imf(J)\neq \{0\}$, then $\pi|_J$ is an irreducible representation of $J$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Any subspace that is invariant under $\im(\overline{\rho})$ is in particular invariant under $\im(\overline{\rho}|_J) = \im(\rho)$ and thus trivial by assumption.

(2) Suppose $\pi$ maps elements of $A$ to bounded operators on $\mathcal{H}$. By \ref{equivalentsIrreducibleSetsOperatorsHilbertSpace}, it is enough to show that $\overline{\pi^{\imf}(J)h} = \mathcal{H}$ for all $h\in \mathcal{H}\setminus\{0\}$.

Since $J$ is an ideal, $\overline{\pi^{\imf}(J)h}$ is invariant under $\pi^\imf(A)$, which means it is either $\{0\}$ or $\mathcal{H}$. We show that the first case leads to a contradiction, indeed this means that $\inner{\pi(b)x, h} = \inner{x, \pi(b^*)h} = 0$ for all $x\in \mathcal{H}$ and $b\in J$, so $h\in \big(\pi^\imf(J)\mathcal{H}\big)^\perp$. Since $\pi^\imf(J)\mathcal{H}$ is invariant under $\pi^\imf(A)$ and non-zero by assumption, we have $h\in \big(\pi^\imf(J)\mathcal{H}\big)^\perp = \mathcal{H}^\perp = \{0\}$. This is a contradiction.
\end{proof}

\subsection{The spectrum}
\begin{definition}
Let $A$ be a $C^*$-algebra. Then set of equivalence classes of irreducible
representations is called the \udef{spectrum} of A.
\end{definition}

The following allows us to identify the spectrum of a commutative $C^*$-algebra with the set of non-zero complex-valued algebra homomorphisms.

\begin{proposition}
Let $A$ be a commutative $C^*$-algebra and $\pi: A\to \Bounded{\mathcal{H}}$ a representation. Then $\pi$ is irreducible \textup{if and only if} $\mathcal{H}$ is one-dimensional.
\end{proposition}
\begin{proof}
If $\mathcal{H}$ is one-dimensional, then the representation is clearly irreducible.

Now assume $\pi$ is irreducible.
By \ref{starHomomorphismCstarProperties} $\im(\pi)$ is a $C^*$-algebra, which is clearly commutative. Then $\im(\pi) \subseteq \im(\pi)^\commute = \C\cdot \id_\mathcal{H}$ by \ref{equivalentsIrreducibleSetsOperatorsHilbertSpace}, so all subspaces of $\mathcal{H}$ are invariant. This implies that the trivial subspaces are the only subspaces and thus $\mathcal{H}$ is one-dimensional.
\end{proof}

\subsection{The GNS construction}

\begin{lemma} \label{GNSLeftIdeal}
Let $\omega$ be a positive linear functional on a $C^*$-algebra $A$. Then
\[ N_\omega \defeq \setbuilder{x\in A}{\inner{x,x}_\omega = 0} \]
is a closed left ideal in $A$.
\end{lemma}
\begin{proof}
For all $x,y\in N_\omega$, we have $\inner{x,y}_\omega = 0$ by \ref{preInnerProductCSBZero}.
Take $x,y\in N_\omega$ and $\lambda \in C$, then
\[ \inner{x+\lambda y,x+\lambda y}_\omega = \inner{x,x}_\omega + \overline{\lambda}\inner{y,x}_\omega + \lambda\inner{x,y}_\omega + |\lambda|^2\inner{y,y}_\omega = 0. \]
This shows that $N_\omega$ is a subspace by \ref{subspaceCriterion}.

Now, in order to see that $N_\omega$ is a left ideal, take $x\in N_\omega$ and $a\in A$. Then
\[ \inner{ax,ax}_{\omega} = |\inner{ax,ax}_{\omega}| = |\inner{x,a^*ax}_{\omega}| \leq \cancel{\sqrt{\inner{x,x}_\omega}}\, \sqrt{\inner{a^*ax,a^*ax}_\omega} = 0. \]
Thus $ax\in N_\omega$.

That $N_\omega$ is closed follows immediately from the fact that $N_\omega = (x\mapsto \inner{x,x}_\omega)^{\preimf}\big(\{0\}\big)$, \ref{preimageOpenClosed} and the fact that $(x\mapsto \inner{x,x}_\omega)$ is continuous.
\end{proof}
That $N_\omega$ is a left-ideal means that the representation $\pi_\omega$ of \ref{GNSconstruction} is well-defined.

\begin{lemma} \label{GNSlemma}
Let $\omega\in\states(A)$ be a state on a $C^*$-algebra $A$. Then
\begin{enumerate}
\item $A/N_\omega$ is an inner product space with inner product $\inner{[x], [y]}_\omega = \inner{x,y}_\omega$;
\item the quotient map $[\cdot]: A\to A/N_\omega$, when $A/N_\omega$ is equipped with the topology derived from the inner product;
\item $\seq{[e_\lambda]}$ is a Cauchy net for any increasing approximate unit $\seq{e_\lambda}_{\lambda\in\Lambda}$;
\item for any increasing approximate units $\seq{e_\lambda}$ and $\seq{e'_\mu}$, the Cauchy nets $\seq{[e_\lambda]}$ and $\seq{[e'_\mu]}$ are equivalent.
\end{enumerate}
\end{lemma}
We also define $H_\omega$ as the Hilbert space completion of $A/N_\omega$.
\begin{proof}
(1) We have that $A/N_\omega$ is a vector space.

We verify this inner product is well-defined: take $x,x'\in [x]$ and $y,y'\in[y]$. Then $(x'-x) \in N_\omega$ and $(y'-y) \in N_\omega$. Using \ref{preInnerProductCSBZero}, we see
\[ \omega(x^*y) = \omega(x^*y) + \omega(x^*(y'-y)) = \omega(x^*y') = \omega(x^*y') + \omega((x'-x)^*y') = \omega(x^{\prime *}y'). \]

(2) Suppose $x_n \to x$ in $A$, so $\norm{x_n - x}\to 0$. Then we calculate
\begin{align*}
\norm{[x_n] - [x]}_\omega &= \norm{[x_n - x]}_\omega \\
&= \sqrt{\omega\big((x_n - x)^*(x_n-x)\big)} \\
&\leq \sqrt{\norm{\omega}\norm{(x_n - x)^*(x_n-x)}} \\
&= \norm{x_n - x} \to 0.
\end{align*}

(3) Take $\epsilon >0$. Since $\seq{\omega(e_\lambda)}$ converges, by \ref{positiveFunctionalNormValueAtUnit}, it is a Cauchy net and there exists $\lambda\in\Lambda$ such that for all $\mu,\nu \geq \lambda$, $|f(e_\mu) - f(e_\nu)| \leq \epsilon/4$.

Now \ref{orderSubUnitVectorLemma} gives $\norm{e_\mu - e_\lambda} \leq 1$ and thus $(e_\mu - e_\lambda)^2\leq (e_\mu - e_\lambda)$, since $e_\mu \geq e_\lambda$. Similarly $(e_\nu - e_\lambda)^2\leq (e_\nu - e_\lambda)$.

We calculate
\begin{align*}
\norm{[e_\mu] - [e_\nu]}_\omega^2 &\leq \big(\norm{[e_\mu] - [e_\lambda]}_\omega + \norm{[e_\nu] - [e_\lambda]}_\omega\big)^2 \\
&= \big(\sqrt{\omega\big((e_\mu - e_\lambda)^2\big)} + \sqrt{\omega\big((e_\nu - e_\lambda)^2\big)}\big)^2 \\
&\leq \big(\sqrt{\omega(e_\mu - e_\lambda)} + \sqrt{\omega(e_\nu - e_\lambda)}\big)^2 \\
&\leq \Big(\sqrt{\frac{\epsilon}{4}} + \sqrt{\frac{\epsilon}{4}}\Big)^2 = \epsilon^2.
\end{align*}
Thus $\norm{[e_\mu] - [e_\nu]}_\omega \leq \epsilon$, which implies $\seq{[e_\lambda]}$ is a Cauchy net.

(4) TODO
\end{proof}

\begin{theorem}[Gelfand-Naimark-Segal construction] \label{GNSconstruction}
Let $\omega\in\states(A)$ be a state on a $C^*$-algebra $A$. The map $\pi_\omega: A\to \Bounded(A/N_\omega)$ defined by
\[ \pi_\omega(x) : A/N_\omega \to A/N_\omega: [y] \mapsto [xy] \]
is a well-defined $*$-representation of $A$ on $A/N_\omega$. This can be extended to a representation of $A$ on $H_\omega$. There exists $\xi_\omega\in H_\omega$ such that
\begin{enumerate}
\item $\pi_\omega(a)\xi_\omega = [a]$;
\item $\omega(a) = \inner{\xi_\omega, \pi_\omega(a)\xi_\omega}_\omega$ for all $a\in A$;
\item $\pi_\omega$ is cyclic w.r.t. $\xi_\omega$.
\end{enumerate}
\end{theorem}
\begin{proof}
Since $N_\omega$ is a left ideal, \ref{GNSLeftIdeal}, we have by \ref{congruenceRingIdeals} and \ref{quotientAlgebra}, that $A/N_\omega$ is a $\{\lambda_a, +, (\lambda \cdot -)\}_{a\in A, \lambda\in \C}$-algebra. For all $x\in A$, the operator $(\lambda_x)_{A/N_\omega}$ is exactly $\pi_\omega(x)$.

Next we show that $\pi_\omega(x)$ is bounded for all $x\in A$. This follows from \ref{CstarOrderLemma}:
\[ \norm{\pi_\omega(x)\big([y]\big)}_\omega = \sqrt{\inner{[xy], [xy]}_\omega} = \sqrt{\inner{xy,xy}_{\omega}} = \sqrt{\omega(y^*x^*xy)} \leq \norm{x}\sqrt{\omega(y^*y)} = \norm{x}\,\norm{[y]}_\omega. \]
Thus $\pi_\omega(x)$ can be extended to a bounded operator on $\Bounded(H_\omega)$, which we also denote $\pi_\omega(x)$.

Now we consider multiplicativity and $*$-preservation of $\pi_\omega$. Take $a,b\in A$ and $x,y\in H_\omega$. Then there exists $\seq{[x_n]}, \seq{[y_n]} \in (A/N_\omega)^\N$ such that $[x_n]\overset{\inner{}_\omega}{\longrightarrow} x$ and $[y_n]\overset{\inner{}_\omega}{\longrightarrow} y$. We calculate
\begin{align*}
\pi_\omega(ab)(x) &= \lim_n\pi_\omega(ab)\big([x_n]\big) \\
&= \lim_n[abx_n] \\
&= \lim_n\pi_\omega(a)\big([bx_n]\big) \\
&= \lim_n\big(\pi_\omega(a)\circ \pi_\omega(b)\big)\big([x_n]\big) \\
&= \big(\pi_\omega(a)\circ \pi_\omega(b)\big)(x).
\end{align*}
Thus $\pi_\omega(ab) = \pi_\omega(a)\circ \pi_\omega(b)$. We also calculate
\begin{align*}
\inner{x, \pi_\omega(y)}_\omega &= \lim_n \inner{[x_n], \pi_\omega(a)[y_n]}_\omega \\
&= \lim_n \inner{[x_n], [ay_n]}_\omega \\
&= \lim_n \omega(x_n^*ay_n) \\
&= \lim_n \omega\big((a^*x_n)^*y_n\big) \\
&= \lim_n \inner{[a^*x_n], [y_n]}_\omega \\
&= \lim_n \inner{\pi_\omega(a^*)[x_n], [y_n]}_\omega \\
&= \inner{\pi_\omega(a^*)x, y}_\omega.
\end{align*}
This implies $\pi_\omega(a)^* = \pi_\omega(a^*)$.

Let $\seq{e_\lambda}$ be an increasing approximate unit in $A$. Then $\seq{[e_\lambda]}$ is a Cauchy net in $A/N_\omega$ by \ref{GNSlemma}, which means it has a limit in $H_\omega.$ We call this limit $\xi_\omega$.

(1) Now, since the quotien map is continuous, \ref{GNSlemma}, we have
\[ \pi_\omega(a)\xi_\omega = \lim_{\lambda}\pi_\omega(a)[e_\lambda] = \lim_{\lambda}[ae_\lambda] = \big[\lim_{\lambda}ae_\lambda\big] = [a]. \]
(2) We have
\[ \inner{\xi_\omega, \pi_\omega(a)\xi_\omega}_\omega = \inner{\xi_\omega, [a]}_\omega = \lim_\lambda \inner{[e_\lambda], [a]}_\omega = \lim_\lambda \omega(e_\lambda a) = \omega(a). \]
(3) Immediate from (1). 
\end{proof}

\begin{theorem}[Gelfand-Naimark theorem]
Every $C^*$-algebra has a faithful non-degenerate representation.
\end{theorem}
\begin{proof}
By \ref{normalElementExtendsToState}, there exists a state $\omega_a$ such that $\omega_a(a^*a) = \norm{a^*a} = \norm{a}^2$ for all $a\in A$. Let $\pi_a$ be the corresponding GNS-representation \ref{GNSconstruction}. Then $\pi \defeq \bigoplus_{a\in A\setminus\{0\}}\pi_a$ is a faithful representation. It is also non-degenerate (TODO ref).
\end{proof}

\begin{proposition}
Let $\omega$ be a state over the $C^*$-algebra $A$ and $\tau: A\to A$ a $*$-automorphism that leaves $\omega$ invariant:
\[ \forall a\in A: \quad \omega(\tau(a)) = \omega(a). \]
Then there exists a unique unitary operator $U$ on $\mathcal{H}_\omega$ such that
\[ \forall a\in A: \quad U\pi_\omega(a)U^* = \pi_\omega(\tau(a)) \]
and $U\xi_\omega = \xi_\omega$.
\end{proposition}

\begin{definition}
Let $\omega: A\to \C$ be a state.
We call the cyclic representation $(\mathcal{H}_\omega,\pi_\omega,\xi_\omega)$ constructed in the GNS theorem is called the \udef{canonical cyclic representation} of $A$ associated with $\omega$.
\end{definition}

\begin{lemma}
Let $\omega$ be a state over a $C^*$-algebra $A$ and $(\mathcal{H}_\omega,\pi_\omega,\xi_\omega)$ the associated cyclic representation. There is a bijective correspondence 
\[ \omega_T(a) = \inner{T\xi_\omega, \pi_\omega(a)\xi_\omega} \]
between positive functionals $\omega_T$ over $A$ majorised by $\omega$ and positive operators $T$ in the commutant $\comm{\pi_\omega}$ with $\norm{T}\leq 1$.
\end{lemma}

\begin{proposition}
Let $\omega$ be a state over a $C^*$-algebra $A$ and $(\mathcal{H}_\omega,\pi_\omega,\xi_\omega)$ the associated cyclic representation. The following are equivalent:
\begin{enumerate}
\item $(\mathcal{H}_\omega,\pi_\omega)$;
\item $\omega$ is a pure state;
\item $\omega$ is an extremal point of the state space $\mathcal{S}(A)$.
\end{enumerate}
\end{proposition}

\begin{theorem}
Let $A$ be a $C^*$-algebra. Then $A$ is isomorphic to a norm-closed self-adjoint algebra of bounded operators on a Hilbert space.
\end{theorem}

\subsection{Stinespring's dilation theorem}
\begin{theorem}[Stinespring's dilation theorem]
Let $A$ be a unital $C^*$-algebra and $\phi: A\to \Bounded(H)$ a completely positive map. Then there exits a Hilbert space $H'$, a unital representation $\pi: A\to \Bounded(H')$ and a $V\in \Bounded(H,H')$ such that
\[ \forall a\in A: \quad \phi(a) = V^*\pi(a)V. \]
If $\phi$ is unital, then $V$ is an isometry. Then we can identify $H$ with a subspace of $H'$ and have
\[ \forall a\in A: \quad \phi(a) = P_H\pi(a)|_H. \]
\end{theorem}

\begin{proposition}
All Stinespring dilations such that $\big(\pi^\imf(A)V\big)^\imf(H)$ is dense in $H'$ are unitarily equivalent.
\end{proposition}

\section{Multiplier algebras}
\subsection{Essential ideals}
TODO move to section about ideals
\begin{definition}
Let $J$ be an ideal of a $C^*$-algebra $A$. Then $J$ is called \udef{essential} if for all $a\in A$ we have that $aJ = \{0\}$ implies $a=0$.
\end{definition}

\begin{lemma} \label{C*idealSquared}
Let $A$ be a $C^*$-algebra and $I\subset A$ an ideal. Then $I^2 = I$.
\end{lemma}
\begin{proof}
Let $a\in I^+$. Then $a = (a^{1/2})^2\in I^2$. As $I$ and $I^2$ are $C^*$-algebras, they are spanned by their positive elements. So $I^2 
\subset I \subset I^2$.
\end{proof}
\begin{lemma} \label{productC*ideals}
Let $A$ be a $C^*$-algebra and $I,J\subset A$ ideals. Then $IJ = I\cap J$.
\end{lemma}
\begin{proof}
We calculate $I\cap J = (I\cap J)^2 \subset IJ \subset I\cap J$ using \ref{C*idealSquared}.
\end{proof}

\begin{proposition}
Let $J$ be an ideal of a $C^*$-algebra $A$. Then the following are equivalent:
\begin{enumerate}
\item $J$ is essential;
\item $\forall a\in A:\;Ja = \{0\}$ implies $a=0$;
\item every other non-zero ideal in $A$ has a non-zero intersection with $J$.
\end{enumerate}
\end{proposition}
\begin{proof}
Assume (3) and let $a\in A$ such that $aJ=\{0\}$. Let $I= \overline{AaA}$ be the ideal generated by $a$.
\end{proof}

\subsection{Multiplier algebras}
\begin{proposition}
Let $A$ be a $C^*$-algebra and $\pi_1, \pi_2$ faithful, non-degenerate representations. Then the idealisers $I(\pi_1[A]), I(\pi_2[A])$ of $\pi_1[A]$ and $\pi_2[A]$ are isomorphic to each other.
\end{proposition}
\begin{proof}
We need to show
\[ I(\pi_1[A]) = \setbuilder{T\in \Bounded(\mathcal{H}_1)}{T\pi_1[A] \cup \pi_1[A]T \subseteq \pi_1[A]} \cong \setbuilder{T\in \Bounded(\mathcal{H}_2)}{T\pi_2[A] \cup \pi_2[A]T \subseteq \pi_2[A]} = I(\pi_2[A]). \]

We first show $I(\pi[A])$ contains $\pi[A]$ as an essential ideal. That it contains $A$ as an ideal is obvious. Non-degeneracy of the representation means that the only $x\in\mathcal{H}$ that is mapped to $0$ by all $\pi[A]$ is $0$, by \ref{nonDegeneracyAlgebraRepresentation}. 

The idealiser is the largest subalgebra of $\Bounded(\mathcal{H})$ that contains $A$ as an ideal. The ideal is necessarily 
\end{proof}

\begin{definition}
Let $A$ be a $C^*$-algebra. The \udef{multiplier algebra} $M(A)$ of $A$ is the largest $C^*$-algebra that contains $A$ as an essential ideal.
\end{definition}

The multiplier algebra is the non-commutative analogue of StoneâÄech compactification: if $A$ is commutative, then $A\cong C(X)$ and
\[ M(A) \cong C_b(X) \cong C(\beta(X)), \]
where $\beta(X)$ denotes the StoneâÄech compactification of $X$.

\begin{lemma}
If $A$ is a unital $C^*$-algebra, then $M(A) = A$.
\end{lemma}
If we view the $C^*$-algebra $A$ as a Hilbert $A$-module, then $M(A)$ is the set of adjointable operators on $A$.

\begin{proposition}
Let $A$ be a $C^*$-algebra and $\pi_1, \pi_2$ faithful, non-degenerate representations. Then the idealisers $I(\pi_1[A]), I(\pi_2[A])$ of $\pi_1[A]$ and $\pi_2[A]$ are isomorphic to each other and to the multiplier algebra $M(A)$.


$M(A)$ can be realised as the idealiser
\[ M(A) \cong I(\pi[A]) = \setbuilder{T\in \Bounded(\mathcal{H})}{T\pi[A] \cup \pi[A]T \subseteq \pi[A]} \]
of $A$ in $\Bounded(\mathcal{H})$.
\end{proposition}
\begin{proof}
We need to show
\[ I(\pi_1[A]) = \setbuilder{T\in \Bounded(\mathcal{H}_1)}{T\pi_1[A] \cup \pi_1[A]T \subseteq \pi_1[A]} \cong \setbuilder{T\in \Bounded(\mathcal{H}_2)}{T\pi_2[A] \cup \pi_2[A]T \subseteq \pi_2[A]} = I(\pi_2[A]). \]

We first show $I(\pi[A])$ contains $\pi[A]$ as an essential ideal. That it contains $A$ as an ideal is obvious. Non-degeneracy of the representation means that the only $x\in\mathcal{H}$ that is mapped to $0$ by all $\pi[A]$ is $0$, by \ref{nonDegeneracyAlgebraRepresentation}. 

The idealiser is the largest subalgebra of $\Bounded(\mathcal{H})$ that contains $A$ as an ideal. The ideal is necessarily 
\end{proof}

For example, let $\mathcal{H}$ be a Hilbert space. Then $M(\mathcal{K}(\mathcal{H})) = \Bounded(\mathcal{H})$, where $\mathcal{K}(\mathcal{H})$ is the algebra of compact operators on $\mathcal{H}$.

We write $\mathcal{U}M(A)$ to mean the unitary elements of the multiplier algebra.

Let $\pi: A\to M(B)$ be a $*$-homomorphism. If $\overline{\Span}(\pi(A)B) = B$, then $\pi$ can be uniquely extended to $\overline{\pi}: M(A) \to M(B)$. 

\section{Universal $C^*$-algebras}
\begin{definition}
Let $\mathcal{X}$ be a non-empty set. We formally write $\mathcal{X}^* = \setbuilder{x^*}{x\in \mathcal{X}}$ and view it as a set disjoint from $\mathcal{X}$. A noncommutative-$*$-polynomial with variables in $\mathcal{X}$ is a formal expression of the form
\[ \sum_{k=1}^m\lambda_k x_{k,1}x_{k,2}\ldots x_{k,n_k} \]
where $m, n_k\in \N$, $x_{k,n}\in \mathcal{X}\cup\mathcal{X}^*$ and $\lambda_k\in \C$.

a \udef{polynomal relation} $\mathcal{R}$ on $\mathcal{X}$ is a collection of formal statements of the form
\[ \norm{p_j(\mathcal{X})}\leq r_j \]
indexed by some index set $J$ where $r_j \in\R^{\geq 0}$ and $p_j$ is a noncommutative-$*$-polynomial with variables in $\mathcal{X}$.
\end{definition}

\begin{definition}
Let $\mathcal{X}$ be a non-empty set and $\mathcal{R}$ a set of polynomial relations on $\mathcal{X}$.

A \udef{representation} of $(\mathcal{X}\;|\;\mathcal{R})$ is a $C^*$-algebra $A$ together with a map $\pi:\mathcal{X}\to A$ such that $\mathcal{R}$ becomes true in the image of $\pi$.

A representation $\pi_u:\mathcal{X}\to B$ of $(\mathcal{X}\;|\;\mathcal{R})$ is called \udef{universal} if for any other representation $\pi:\mathcal{X}\to A$ of $(\mathcal{X}\;|\;\mathcal{R})$, there exists a unique $*$-homomorphism $\varphi: B\to A$ such that $\varphi\circ \pi_u = \pi$.

In this case we call $B$ the \udef{universal $C^*$-algebra} generated by $(\mathcal{X}\;|\;\mathcal{R})$ and write $B= C^*(\mathcal{X}\;|\;\mathcal{R})$.
\end{definition}

\begin{definition}
A polynomial relation $\mathcal{R}$ on $\mathcal{X}$ is said to be \udef{bounded}, if for every $x\in \mathcal{X}$, we have
\[ \sup\setbuilder{\norm{\pi(x)}}{\pi:\mathcal{X}\to A\;\text{is a representation of}\;(\mathcal{X}\;|\;\mathcal{R})} < \infty. \] 
\end{definition}

\begin{example}
\begin{itemize}
\item The relation $(\mathcal{X}\;|\;\mathcal{R}) = (\{a\}\;|\;\{\norm{a-a^*}\leq 0\})$ is not bounded.
\item The relation $(\mathcal{X}\;|\;\mathcal{R}) = (\{x,y\}\;|\;\{\norm{\vec{1}-x^*x-y^*y}\leq 0\})$ is bounded: writing $x$ for $\pi(x)$, the spectrum of $x^*x = 1-y^*y$ is positive and bounded by $1$ according to the spectral mapping theorem \ref{spectralMappingCFC}. Now $\norm{\pi(x)} = \sqrt{\spr(\pi(x)^*\pi(x))} \leq 1$ by \ref{normNormalElement} and so
\[ \sup\setbuilder{\norm{\pi(x)}}{\pi:\mathcal{X}\to A\;\text{is a representation of}\;(\mathcal{X}\;|\;\mathcal{R})} \leq 1 < \infty. \]
\item The relation $(\mathcal{X}\;|\;\mathcal{R}) = (\mathcal{X}\;|\;\bigcup_{x\in\mathcal{X}}\{\norm{x-x^*}\leq 0,\norm{x-x^2}\leq 0 \})$ is bounded. It gives rise to a universal $C^*$-algebra generated by projections.
\end{itemize}
\end{example}

\begin{proposition}
Let $\mathcal{X}$ be a non-empty set and $\mathcal{R}$ a polynomial relation on $\mathcal{X}$. Then $(\mathcal{X}\;|\; \mathcal{R})$ is bounded if and only if $C^*(\mathcal{X}\;|\;\mathcal{R})$ exists.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\section{Direct limits}
\subsection{AF}
\subsection{UHF}
\subsection{Stable algebras}
\url{http://web.math.ku.dk/~rordam/manus/encyc.pdf}

\section{Tensor products}
\subsection{Algebraic tensor product}
\subsection{Spatial tensor product}

\chapter{Hilbert-Schmidt space}
\begin{proposition}
\begin{enumerate}
\item $\phi$ unital iff $\phi^*$ trace-preserving
\item
\end{enumerate}
\end{proposition}


\chapter{Group algebras and harmonic analysis}
\section{Group $C^*$-algebras}
\subsection{Discrete groups}
\begin{definition}
Let $G$ be a finite group and $R$ a r(i)ng. The \udef{group ring} $RG$ is the set of functions $(G\to R)$ with pointwise addition and the convolution product
\[ (x\star y)(g) = \sum_h x(h)y(h^{-1}g) = \sum_{g=hk}x(h)y(k) \]
for all $x,y\in RG$ and $g\in G$. 
\end{definition}
The a group ring can be seen as a free module generated by $G$. (TODO: this as definition?)

The group algebra $\C G$ has an involution:
\[ x^*(g) = \overline{x(g^{-1})} \qquad \text{for all $x\in \C G$}. \]
And it admits a norm making it a $C^*$-algebra.

\subsection{Locally compact Hausdorff groups}
For topological groups we are not restricted to finite sums.

\begin{definition}
Let $G$ be a locally compact group and $f,g\in L^1(G)$. The \udef{convolution product} $f\star g$ of $f$ and $g$ is the partial function defined by
\[ (f\star g)(x) = \int_Gf(y)g(y^{-1}x)\diff{y}, \]
whenever the integral exists.
\end{definition}

\begin{proposition}
Let $G$ be a locally compact group and $f,g\in L^1(G)$. Then convolution makes $L^1(G)$ a Banach-$*$-algebra.
\end{proposition}
We need to show:
\begin{enumerate}
\item $f\star g$ exists a.e.;
\item $\norm{f\star g}_1 = \int_G |f\star g|\diff{\mu}< \infty$;
\item $\norm{f \star g}_1 \leq \norm{f}_1\norm{g}_1$;
\item the convolution is a bilinear and associative.
\end{enumerate}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
The algebra $L^1(G)$ is commutative \textup{if and only if} $G$ is a commutative group.
\end{proposition}

\begin{lemma}
Let $G$ be a locally compact Hausdorff group. Convolution is a bilinear operation that maps $C_c(G)\times C_c(G)\to C_c(G)$ defined by
\[ (f\star g)(t) \defeq \int_Gf(s)g(s^{-1}t)\diff\mu(s). \]
\end{lemma}
\begin{proof}
Continuity follows from the dominated convergence theorem. Also
\[ \operatorname{supp}(f\star g)\subseteq \operatorname{supp}(f)\cdot\operatorname{supp}(g) \]
where $\cdot$ is the group multiplication.
\end{proof}
\begin{lemma}
The algebra $C_c(G)$ has an involutive anti-linear anti-automorphism
\[ *: f \mapsto f^* = (s\mapsto \overline{f(s^{-1})}\Delta(s^{-1})) \]
where $\Delta$ is the modular function on $G$. This means $C_c(G)$ is a $*$-algebra with $*$ as involution.
\end{lemma}


\section{$C^*$-dynamical systems}
\begin{definition}
A \udef{$C^*$-dynamical system} is a triple $(G,\alpha, A)$ consisting of a locally  
compact group $G$, a $C^*$-algebra $A$ and a homomorphism $\alpha$ of $G$ into $\Aut(A)$, such that $g \mapsto a_g(a)$ is continuous for all $a \in A$. 
\end{definition}

\subsection{Covariant homomorphisms and representations}
\begin{definition}
Let $(G,\alpha, A)$ be a $C^*$-dynamical system. A \udef{covariant homomorphism} into the multiplier algebra $M(D)$ of some $C^*$-algebra $D$ is a pair $(\rho, U)$ where
\begin{itemize}
\item $\rho: A\to M(D)$ is a $*$-homomorphism and
\item $U: G\to \Unitaries M(D)$ is a strictly continuous homomorphism between groups
\end{itemize}
satisfying
\[ \rho(\alpha_g(a))= U_g\rho(a)U_{g}^* \qquad \text{for all $g\in G$.} \]
We say $(\rho, U)$ is non-degenerate if $\rho$ is.
\end{definition}

\subsubsection{Integrated forms}
\begin{definition}
Given a covariant homomorphism $(\rho, U)$ on a $C^*$-dynamical system $(G,\alpha, A)$ into $M(D)$ we can parcel these two functions into one function $C_c(G,A)\to M(D)$, called the \udef{integrated form}
\[ (\rho \rtimes U) (f)  \defeq \int_G\rho(f(r))U_r \diff\mu(r) \]
where $\mu$ is the left Haar measure.
\end{definition}
\begin{lemma}
Let $(\rho, U)$ be a covariant homomorphism. Then $\rho \rtimes U$ is a $*$-homomorphism.
\end{lemma}

\subsubsection{Induced covariant morphisms}
Given a $*$-homomorphism $\rho: A\to M(D)$ we can extend it naturally to a covariant homomorphism.
\begin{definition}
Let $(G, \alpha, A)$ be a $C^*$-dynamical system and $\rho: A\to M(D)$ a $*$-homomorphism. Then the \udef{covariant homomorphism induced from $\rho$} $\Ind \rho$ is the covariant homomorphism $(\widetilde{\rho}, 1\otimes \lambda)$ of $(G, \alpha, A)$ into $M(D\otimes\Compact(L^2(G))$ where
\begin{itemize}
\item $\lambda: G\to \Unitaries(L^2(G))$ is the left regular representation of $G$ given by $(\lambda_s\xi)(t) =\xi(s^{-1}t)$;
\item $\rho$ is the composition
\[ \begin{tikzcd}
A \rar{\widetilde{\alpha}} & C_b(G,A) \ar[r,hook] & M(A\otimes C_0(G)) \rar{\rho\otimes M} & M(D\otimes \Compact(L^2(G)))
\end{tikzcd} \]
where $\widetilde{\alpha}: A\to C_b(G,A)$ is defined by $\widetilde{\alpha}(a)(s)= \alpha_{s^{-1}}(a)$ and 
\[ M: C_0(G)\to \Bounded(L^2(G)) = M(\Compact(L^2(G))) \]
denotes the representation by multiplication operators.
\end{itemize}
\end{definition}
The \udef{regular representation} of $(G, \alpha, A)$ is $\Lambda^G_A \defeq \Ind(\id_A)$.

\begin{lemma}
Let $\rho: A\to M(D)$ be a $*$-homomorphism. Then
\[ \Ind\rho = () \]
\end{lemma}

\subsubsection{Covariant representations}
\begin{definition}
A \udef{(covariant) representation} of a $C^*$-dynamical system $(G,\alpha, A)$ on a Hilbert space $\mathcal{H}$ is a covariant homomorphism $(\pi, U)$ into $M(\Compact(\mathcal{H})) = \Bounded(\mathcal{H})$.
\end{definition}

\begin{definition}
Covariant representations $(\pi, U)$ on $\mathcal{H}$ and $(\pi', U')$ on $\mathcal{H}'$ are \udef{unitarily equivalent} if there is a unitary operator $W: \mathcal{H}\to \mathcal{H}'$ such that
\[ \pi'(a) = W\pi(a)W^* \qquad \text{and} \qquad U_g' =  WU_gW^* \]
for all $a\in A,g\in G$.
\end{definition}

Suppose $(\pi,U)$ and $(\rho, V)$ are covariant representations on $\mathcal{H}$ and $\mathcal{V}$ respectively. Their direct sum $(\pi, U) \oplus (\rho, V )$ is the covariant representation $(\pi \oplus \rho, U \oplus V )$ on $\mathcal{H} \oplus \mathcal{V}$ given by $(\pi \oplus \rho)(a) \defeq \pi(a)\oplus \rho(a)$ and $(U \oplus V)_s \defeq U_s \oplus V_s$.
\subsection{Crossed products}
The crossed product $A \rtimes_\alpha G$ will be defined as the completion of $C_c(G,A)$, viewed as a $*$-algebra in a certain way, with respect to a certain norm.

First the algebra: the set of functions $G\to A$ with compact support naturally comes equipped with scalar multiplication and vectorial addition. We define the multiplication as
\[ f\star g: G \to \C: x\mapsto \int_G f(s)\alpha_s(g(s^{-1}x))\diff\mu(s) \]
and the involution $*$ by
\[ f^*: x\mapsto \Delta(x^{-1})\alpha_x(f(x^{-1})^*). \]
Notice the appearance of $\alpha$ in the definitions.

Next we define a norm. This will be done using integrated forms.
Let $(\pi, U)$ be a covariant representation of a $C^*$-dynamical system $(A,G,\alpha)$ on $\mathcal{H}$. Then
\[ (\pi \rtimes U) (f)  \defeq \int_G\pi(f(r))U_r \diff\mu(r)\]
defines a $*$-representation of the $*$-algebra $C_c(G,A)$ on $\mathcal{H}$, called the \udef{integrated form}.

Then we can define a norm, called the \udef{universal norm}, on $C_c(G,A)$ by
\[ \norm{f} \defeq \sup\setbuilder{\norm{(\pi\rtimes U)(f)}}{(\pi, U)\;\text{is a covariant representation of} \; (A,G,\alpha)} \]
The supremum\footnote{One may worry we are taking the supremum over a class and not a set (the covariant representations do not form a set). Luckily the class is a subclass of the real numbers and thus a set.} is finite because $\norm{f} \leq \norm{f}_1$.

The completion of $C_c(G,A)$ with respect to the universal norm is called the \udef{crossed product} $A \rtimes_\alpha G$.

In fact, when evaluating the supremum for the universal norm, we do not need to consider all representations of $(A,G,\alpha)$: Let $(\pi, U)$ be a covariant representation of $(A,G,\alpha)$ on $\mathcal{H}$. Let 
\[ \mathcal{E} \defeq \overline{\Span}\setbuilder{\pi(a)h}{a\in A; h\in \mathcal{H}} \]
be the \udef{essential subspace} of $\pi$. We call the corresponding subrepresentation $\operatorname{ess} \pi$. Because
\[ U_s h =  \pi(\alpha_{s^{-1}}(a))U_s\pi(a) h \qquad \forall a\in A, h\in\mathcal{H}, \]
it is clear $\mathcal{E}$ is invariant under $U$ as well. Call $U'$ the restriction of $U$ to $\mathcal{E}$.

Then $\norm{(\operatorname{ess}\pi\rtimes U')(f)} = \norm{(\pi\rtimes U)(f)}$ and so
\[ \norm{f} =\sup\setbuilder{\norm{(\pi\rtimes U)(f)}}{(\pi, U)\;\text{is a non-degenerate covariant representation of} \; (A,G,\alpha)} \]

\begin{proposition}
If $(A,G,\alpha)$ is a dynamical system, then the map sending
a covariant pair $(\pi, U)$ to its integrated form $\pi\rtimes U$ is a one-to-one correspondence
between non-degenerate covariant representations of $(A, G, \alpha)$ and non-degenerate
representations of $A\rtimes_\alpha G$. This correspondence preserves direct sums, irreducibility
and equivalence.
\end{proposition}

\subsubsection{Universal property}

In general the crossed product $A\rtimes_\alpha G$ does not contain a copy of either $A$ or $G$. The multiplier algebra $M(A\rtimes_\alpha G)$ does however: There exist injective homomorphisms
\[ i_A: A\to M(A\rtimes_\alpha G) \qquad i_G: G\to \mathcal{U}M(A\rtimes_\alpha G) \]
satisfying
\begin{enumerate}
\item $i_A(\alpha_r(a)) = i_G(r)i_A(a)i_G(r)^*$ for all $a\in A, r\in G$;
\item $A \rtimes_\alpha G = \overline{\Span}\setbuilder{i_A(a)\int_Gf(s)i_G(s)\diff\mu(s)}{a\in A, f\in C_c(G)}$;
\item if $(\pi, U)$ is a covariant representation of $(G,A,\alpha)$, then
\[ \pi = (\pi\rtimes U)\circ i_A \qquad \text{and} \qquad U = (\pi \rtimes U)\circ i_G. \]
\end{enumerate}
This is a universal property. Suppose another $C^*$-algebra $B$ and maps
\[ j_A: A\to M(B) \qquad \text{and} \qquad j_G: G\to \mathcal{U}M(B) \]
satisfy these conditions, then there exists an isomorphism $\Psi: A\rtimes_\alpha G \to B$ such that
\[ \Psi \circ i_A = j_A \qquad \text{and} \qquad \Psi\circ i_G = j_G. \]

The existence of such homomorphisms $i_A,i_G$ is proved by explicitly giving them.
They are defined by
\begin{align*}
(i_A(a)f)(t) &= af(t) & (i_G(s)f)(t) &= \alpha_s(f(s^{-1}t) \\
(fi_A(a))(t) &= f(t)\alpha_t(a) & (fi_G(s))(t) &= f(t^{-1}s)\Delta(s^{-1})
\end{align*}
for all $f\in C_c(G,A), a\in A, t\in G$.

We can use the existence of these embeddings to prove the proposition. We need to show the existence of an inverse of the map from non-degenerate covariant representations to integrated forms.

Let $\omega$ be a non-degenerate covariant representation of $A\rtimes_\alpha G$. Because it is non-degenerate, we can extend it uniquely to a representation of $M(A\rtimes_\alpha G)$. Using $i_A, i_G$ we can restrict this representation to a representation of $A$ and $G$. Together they form a covariant representation and one can check it is exactly the original representation $\omega$.

\subsubsection{Induced representations}