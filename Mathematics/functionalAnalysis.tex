\chapter{Functionals on vector spaces}

\begin{theorem}[Riesz–Markov–Kakutani representation theorem]
Let $X$ be a locally compact Hausdorff space. For any positive linear functional $\psi$ on $C_c(X)$, there is a unique Radon measure $\mu$ on $X$ such that
\[ \forall f\in C_c(X): \quad \psi(f) = \int_X f(x)\diff{\mu(x)}. \]
\end{theorem}

\begin{definition}
Let $V$ be a vector space over a field $\mathbb{F}$.
\begin{enumerate}
\item A \udef{functional} on $V$ is a map $V\to \F$;
\item A \udef{linear functional} on $V$ is a linear map from $V$ to $\mathbb{F}$.
\end{enumerate}
\end{definition}

\section{Non-linear functionals}
\subsection{Sublinear functionals}
\begin{definition}
Let $V$ be a vector space. A functional $p:V\to \R$ is called \udef{sublinear} if it is
\begin{enumerate}
\item subadditive: $\forall x,y\in V: p(x+y) \leq p(x) + p(y)$;
\item positive homogenous: $\forall x\in V,\lambda\geq 0: p(\lambda x) = \lambda p(x)$.
\end{enumerate}
\end{definition}
Thus norms and seminorms are sublinear.

\subsection{Convex functionals}
\begin{definition}
Let $V$ be a vector space. A functional $p:V\to \R$ is called \udef{convex} if for all $x,y\in V$ and $\lambda\in [0,1]$
\[ p(\lambda x + (1-\lambda)y) \leq \lambda p(x) + (1-\lambda)p(y). \]
\end{definition}
Clearly every sublinear functional is convex.

\begin{lemma}
Let $p: V\to\R$ be convex functional. Then
\[ P: V\to\R: x\mapsto \inf_{t>0} t^{-1}p(tx) \]
is sublinear and $P(x)\leq p(x)$.

Also, if $f:V\to \R$ is a linear functional, then $f\leq p \iff f\leq P$.
\end{lemma}
\begin{proof}
For sublinearity: let $x,y\in V$, then for all $s,t>0$
\[ P(x+y) \leq \frac{s+t}{st}p\left(\frac{st}{s+t}(x+y)\right) = \frac{s+t}{st}p\left(\frac{s}{s+t}(tx)+\frac{t}{s+t}(sy)\right) \leq t^{-1}p(tx) + s^{-1}p(sy). \]
This implies that $P(x+y)\leq P(x)+P(y)$.

For positive homogeneity: let $x\in V,\lambda\geq 0$
\[ P(\lambda x) = \inf_{t>0} t^{-1}p(t\lambda x) = \inf_{t\lambda>0} \lambda (t\lambda)^{-1}p(t\lambda x) = \inf_{t>0} \lambda (t)^{-1}p(tx) = \lambda P(x). \]

Finally we prove that $f\leq p \implies f\leq P$ for linear functionals $f$. For all $t>0$ we have $f(tx) \leq p(tx)$, which implies $f(x) = t^{-1}f(tx) \leq t^{-1}p(tx)$. So $f\leq P$.
\end{proof}

\section{Hahn-Banach extension theorems}
\begin{theorem}[Hahn-Banach majorised by convex functionals] \label{convexHahnBanach}
Let $V$ be a real vector space, $U\subset V$ a subspace and $p$ a convex functional on $V$. Let $f:U\to\R$ be a linear functional that is bounded by $p$:
\[ \forall u\in U: \quad f(u) \leq p(u). \]
Then $f$ has an extension $\tilde{f}: V\to \R$ such that $\tilde{f}$ is a linear functional on $V$ bounded by $p$:
\[ \forall v\in V: \tilde{f}(v) \leq p(v) \qquad \text{and} \qquad \forall u\in U: \tilde{f}(u) = f(u). \]
\end{theorem}
\begin{proof}
As a first step, we want to extend $f$ to a functional $g$ on a space that is one dimension larger than $U$. This means $g$ is of the form
\[ g: U\oplus\Span\{v_1\}\to\R: v + \alpha v_1 \mapsto f(v) + \alpha c \]
for some $v_1\in V\setminus U$.

If we want $g$ to be majorised by $p$, then we need to find a $c$ such that
\[ \forall v\in U: \forall \alpha\in\R: \; g(\alpha v_1 + v) = \alpha c + f(v) \leq p(\alpha v_1 + v) \]
this means that we need
\[ \forall v\in U: \forall \alpha\in\R:\; \frac{-p(v - |\alpha|v_1) + f(v)}{|\alpha|} \leq c \leq \frac{p(v + |\alpha|v_1) - f(v)}{|\alpha|} \]
and we can find such a $c$ if and only if
\[ \forall v\in U: \forall \alpha\in\R:\; -p(v - |\alpha|v_1) + f(v) \leq p(v + |\alpha|v_1) - f(v), \]
which is equivalent to $2f(v) \leq p(v+|\alpha|v_1)+p(v-|\alpha|v_1)$. This follows from
\begin{align*}
f(v) \leq p(v) &= p(\tfrac{1}{2}(v+|\alpha|v_1) + \tfrac{1}{2}(v-|\alpha|v_1)) \\
&\leq \tfrac{1}{2}p(v+|\alpha|v_1) + \tfrac{1}{2}p(v-|\alpha|v_1).
\end{align*}
So we can extend the domain of $f$ by one dimension such that it is still majorised by $p$.

An extension by multiple dimensions is determined by a subset of $V\times \R$. Consider the family of all such subsets that determine a majorised extension of $f$. This is a family of finite character. We apply the Teichmüller-Tukey lemma, \ref{ZornEquivalents}, to obtain a maximal element.

This maximal element has domain $V$, because if it did not, it could be extended and was not a maximal element.
\end{proof}
Clearly if $V$ has a well-ordered Hamel basis, we do not need choice as we can just take successive $v$s in the basis and find $c$s constructively.
\begin{corollary}[Hahn-Banach majorised by sublinear functionals] \label{sublinearHahnBanach}
Any majorant $p$ that is sublinear is also convex and can be used in the Hahn-Banach theorem.
\end{corollary}
\begin{corollary}[Hahn-Banach majorised by seminorms] \label{seminormHahnBanach}
Let $(\mathbb{F},V,+)$ be a real or complex vector space, $U\subset V$ a subspace and $p$ a seminorm on $V$. Let $f:U\to\mathbb{F}$ be a linear functional that is bounded by $p$:
\[ \forall u\in U: \quad |f(u)| \leq p(u). \]
Then $f$ has an extension $\tilde{f}: V\to \R$ such that $\tilde{f}$ is a linear functional on $V$ bounded by $p$:
\[ \forall v\in V: |\tilde{f}(v)| \leq p(v) \qquad \text{and} \qquad \forall u\in U: \tilde{f}(u) = f(u). \]
\end{corollary}
\begin{proof}
For \emph{real} vector fields, we notice that every seminorm is a sublinear function, so we can use \ref{sublinearHahnBanach} to find an extension $\tilde{f}$. We then just need to check it satisfies $\forall v\in V: |\tilde{f}(v)| \leq p(v)$.
From \ref{sublinearHahnBanach} we know $\forall v\in V: \tilde{f}(v) \leq p(v)$.
To prove $-\tilde{f}(v) \leq p(v)$, we calculate
\[ -\tilde{f}(v) = \tilde{f}(-v) \leq p(-v) = |-1|p(v) = p(v). \]

For \emph{complex} vector fields, we can write $f= f_1 + if_2$ with $f_1,f_2$ real functionals on $U$, which can also be seen as a real vector space. First take $f_1$. Now $\forall u\in U f_1(u) \leq |f(x)| \leq p(x)$, so we can extend $f_1$ to $\tilde{f}_1$ by \ref{sublinearHahnBanach}.

Now by complex linearity, $if(u) = f(iu)$ so
\[ i[f_1(u) + if_2(u)] = -f_2(u) + if_1(u) = f_1(iu) + if_2(iu) \implies f_2(u) = -if_1(iu). \]
So we set $\tilde{f}(v) = \tilde{f}_1(v)-i\tilde{f}_1(iv)$. It is easy to show $\tilde{f}$ is $\C$-linear. For boundedness, write $\tilde{f}(v) = |\tilde{f}(v)|e^{i\theta}$ then
\[ |\tilde{f}(v)| = e^{-i\theta}\tilde{f}(v) = \tilde{f}(e^{-i\theta}v) = \tilde{f}_1(e^{-i\theta}v) \leq p(e^{-i\theta}v) = |e^{-i\theta}|p(v) = p(v). \]
\end{proof}

\begin{corollary}
Let $X$ be a normed space and $Z\subset X$ a subspace. Any bounded linear functional in $\tdual{Z}$ can be extended to a bounded linear functional in $\tdual{X}$ with the same norm.
\end{corollary}
\begin{proof}
Let $f:Z\to \mathbb{F}$ be such a functional. Extend $f$ by the previous theorem, \ref{seminormHahnBanach}, using $p(x) = \norm{f}_Z\norm{x}$.
\end{proof}
\begin{corollary} \label{existenceBoundedFunctionalOfSameNorm}
Let $X$ be a normed space and $x_0\neq 0$ an element of $X$. Then there exists a bounded linear functional $\omega_{x_0}$ such that
\[ \norm{\omega_{x_0}} = 1 \qquad \text{and} \qquad \omega_{x_0}(x_0)=\norm{x_0}. \]
\end{corollary}
\begin{proof}
Extend the functional $f: \Span\{x_0\}\to \mathbb{F}$ defined by
\[ f(x) = f(ax_0) = a\norm{x_0}. \]
\end{proof}
\begin{corollary}
Let $X$ be a normed space. Then $\forall x\in X:$
\[ \norm{x} = \sup_{\substack{f\in X' \\ f\neq 0}}\frac{|f(x)|}{\norm{f}}. \]
\end{corollary}
\begin{proof}
We calculate
\[ \norm{x} \geq \sup_{\substack{f\in X' \\ f\neq 0}}\frac{|f(x)|}{\norm{f}} \geq \frac{|\omega_{x}(x)|}{\norm{\omega_{x}}} = \frac{\norm{x}}{1} = \norm{x} \]
where the first inequality follows from $|f(x)|\leq \norm{f}\norm{x}$ for all $f\in X', x\in X$.
\end{proof}

TODO: locally convex Hausdorff spaces.

\subsection{Banach limits}
\begin{proposition}
There exists a linear map $L:l^\infty(\N) \to \C$ satisfying
\begin{enumerate}
\item $\displaystyle L(x) = \lim_{n\to \infty}x_n$ if the limit exists;
\item $L((x_{n+1})_{n\in\N}) = L((x_n)_{n\in\N})$;
\item if $\forall n\in\N:x_n\geq 0$, then $L(x) \geq 0$;
\item $\norm{L} = 1$.
\end{enumerate}
Such a linear map is called a \udef{Banach limit}.
\end{proposition}
\begin{proof}
TODO, after Cesàro means.
\end{proof}

\subsection{Hahn-Banach separation}



\section{Algebraic duality}
\subsection{Linear functionals}
\begin{definition}
Let $V$ be a vector space over a field $\mathbb{F}$.

The \udef{(algebraic) dual} of $V$, denoted $V^*$, is the vector space of all linear functionals on $V$.
\[ V^* = \Hom_{\mathbb{F}}(V,\mathbb{F}). \]
\end{definition}

\begin{proposition} \label{dualBasisDimension}
Let $V$ be a vector space. Then $\dim V^* \geq \dim V$ and
\[ \dim V^* = \dim V \iff \text{$V$ is finite-dimensional}. \]
If $V$ is finite-dimensional with a basis $v_1, \ldots, v_n$, then the \udef{dual basis} $\varphi_1, \ldots, \varphi_n$ is the set of linear functionals on $V$ such that
\[ \varphi_j(v_k) = \begin{cases}
1 & (k=j), \\ 0 & (k\neq j)
\end{cases}. \]
This dual basis is indeed a basis of $V^*$.
\end{proposition}
\begin{proof}
We first assume $V$ is finite-dimensional and prove the dual basis is a basis, which proves $\dim V^* = \dim V$. We then assume $V$ is infinite-dimensional and prove $\dim V^* \neq \dim V$.\footnote{Reference: \url{https://mathoverflow.net/questions/13322/slick-proof-a-vector-space-has-the-same-dimension-as-its-dual-if-and-only-if-i}}
\begin{enumerate}
\item Assume $V$ is finite-dimensional. To show the dual basis spans $V^*$, take a linear functional $\varphi$. Now define $a_i = \varphi(v_i)$. It is clear that $\varphi = \sum_{i=1}^n a_i\varphi_i$. To show linear independence, take a combination
\[ b_1\varphi_1 + \ldots +b_n\varphi_n =0. \]
Filling in all basis vectors $v_i$ in turn, gives $b_i=0$ for all $i$.
\item Assume $V$ is infinite-dimensional. At first let us assume $\dim_{\mathbb{F}}V \geq |\mathbb{F}|$. Then we can apply lemma \ref{vsCardinality} to obtain $\dim_{\mathbb{F}}V = |V|$. Let $\beta$ be a basis for $V$. The elements of $V^*$ correspond bijectively to functions from $\beta$ to $\mathbb{F}$. Thus
\[ |V^*| = |\mathbb{F}^\beta| = |\mathbb{F}|^{|\beta|} > |\beta| = |V|. \]
Now we relax the condition $\dim_{\mathbb{F}}V \geq |\mathbb{F}|$. We first note that every field contains a subfield that is at most denumerable. Take such a field $K\subset \mathbb{F}$. We introduce the new vector space $W = \Span_K(\beta)$. Every functional from $W$ to $K$ extends to a functional from $V$ to $\mathbb{F}$. Hence
\[ \dim_\mathbb{F} V = \dim_K W < \dim_K W^* \leq \dim_{\mathbb{F}} V^* \]
using $\dim_{K}W \geq |K| \geq \aleph_0$.
\end{enumerate}
\end{proof}


\begin{proposition}
Let $f\in\Hom(V,W)$ and $\mathcal{V}, \mathcal{W}$ bases of $V,W$. The
\[ (f^*)^{\mathcal{V}^*}_{\mathcal{W}^*} = ((f)^{\mathcal{W}}_{\mathcal{V}})^\transp. \] \label{transpDual}
\end{proposition}

\subsubsection{Annihilator subspace}
\begin{definition}
Let $U\subset V$ be a subspace. The \udef{annihilator} of $U$, denoted $U^0$, is the set of functionals that are identically zero on $U$:
\[ U^0 = \left\{ \varphi\in V^*\;|\; \forall u\in U:\varphi(u) = 0 \right\}. \]
\end{definition}
\begin{proposition} \label{annihilatorSpace}
Let $U\subset V$ be a subspace and $T\in \Hom(V,W)$.
\begin{enumerate}
\item $U^0$ is a subspace of $\adual{V}$;
\item $\dim \adual{U} + \dim U^0 = \dim \adual{V}$;
\item $\ker T^t = (\im T)^0$
\item $T$ is surjective \textup{if and only if} $T^t$ is injective.
\end{enumerate}
\end{proposition}
\begin{proof}
\mbox{}
\begin{enumerate}
\item Elementary application of subspace criterion, proposition \ref{subspaceCriterion}.
\item Consider the inclusion $\iota: U\hookrightarrow V$. Then the dimension theorem \ref{dimensionLinearMaps} applied to $\iota'$ gives
\[ \dim \im \iota' + \dim \ker\iota' = \dim V^*. \]
Now $\dim \ker\iota'$ are $\varphi\in V^*$ such that $\varphi \circ \iota = 0$. These are exactly the elements of the annihilator. Any functional on $U$ can be extended to a functional on $V$, so $\iota'$ is surjective and $\dim \im \iota' = \dim U^*$.
\item There are two inclusions. First assume $\varphi \in \ker T'$, so $\forall v\in V$
\[ 0 = (\varphi\circ T)(v) = \varphi(Tv). \]
Thus $\varphi\in(\im T)^0$. The other inclusion uses the same equality.
\item $T\in\Hom(V,W)$ is surjective iff $\im T = W$ iff $(\im T)^0 = \{0\}$ iff $\ker T' = \{0\}$ iff $T'$ is injective.
\end{enumerate}
\end{proof}


\subsection{The transpose of a map}
\begin{definition}
Let $f:V\to W \in \Hom_{\mathbb{F}}(V,W)$. The \udef{dual map}\footnote{The dual map $f^t$ is often denoted $f^*$ or $f'$. We avoid this because it clashes with the notation of the Hilbert adjoint.} or \udef{transpose} $f^t$ is the linear map
\[ f^t:W^* \to V^*: l\mapsto f^t(l) = l\circ f. \]
\end{definition}
\begin{lemma}
Let $f\in \Hom(U,V)$ and $g\in \Hom(V,W)$.
\begin{itemize}
\item $(g\circ f)^t = f^t\circ g^t$;
\item $\id^t_V = \id_{\adual{V}}$;
\item $f$ is an isomorphism \textup{if and only if} $f^t$ is an isomorphism;
\item $(f^t)^{-1} = (f^{-1})^t$ 
\end{itemize}
\end{lemma}
TODO: merge
\begin{lemma}
Let $S,T\in\Hom(V,W)$ and $\alpha\in\mathbb{F}$. Then
\begin{enumerate}
\item $(S+T)^t = S^t+T^t$;
\item $(\alpha T)^t = \alpha T^t$
\item if $T$ is invertible, then $T^t$ is invertible and
\[ (T^t)^{-1} = (T^{-1})^t. \]
\end{enumerate}
\end{lemma}

\begin{proposition}
Let $U\subset V$ be a subspace and $T\in \Hom(V,W)$, where $V,W$ are \emph{finite-dimensional}.
\begin{enumerate}
\item $\dim\ker T^t = \dim \ker T + \dim W - \dim V$;
\item $\dim\im T^t = \dim \im T$;
\item $\im T^t = (\ker T)^0$
\item $T$ is injective \textup{if and only if} $T^t$ is surjective.
\end{enumerate}
\end{proposition}
\begin{proof}
\mbox{}
\begin{enumerate}
\item Using $\dim \adual{V} = \dim V$, we have
\begin{align*}
\dim \ker T^t &= \dim(\im T)^0 = \dim W-\dim \im T \\
&= \dim W - (\dim V - \dim \ker T) = \dim \ker T + \dim W - \dim V
\end{align*}
where the equalities come from proposition \ref{annihilatorSpace} and the dimension theorem for linear maps, theorem \ref{dimensionLinearMaps}.
\item Still using these results, we can calculate
\begin{align*}
\dim \im T^t &= \dim \adual{W} - \dim \ker T^t = \dim \adual{W} - \dim (\im T)^0 \\
&= \dim \adual{(\im T)} = \dim \im T.
\end{align*}
\item Take $\varphi = T^t(\psi) \in \im T^t$ where $\psi \in \adual{W}$. If $v\in \ker T$, then
\[ \varphi(v) = \left(T^t(\psi)\right)v = (\psi\circ T)(v) = \psi(Tv) = \psi(0) = 0. \]
Hence $\varphi \in (\ker T)^0$ and $\im T^t\subset (\ker T)^0$. We prove the equality by showing the dimensions are the same. Indeed:
\[ \dim \im T^t = \dim \im T = \dim V - \dim \ker T = \dim(\ker T)^0. \]
\item $T\in\Hom(V,W)$ is injective iff $\ker T = \{0\}$ iff $(\ker T)^0 = \adual{V}$ iff $\im T^t = \adual{V}$ iff $T^t$ is surjective.
\end{enumerate}
\end{proof}
\subsection{Bidual spaces}
\begin{definition}
Let $V$ be a vector space. The \udef{bidual space} is the dual of the dual $\abidual{V} = \adual{(\adual{V})}$.
\end{definition}
\begin{definition}
Let $V$ be a vector space over $\mathbb{F}$ and $v\in V$. The \udef{evaluation map} $\evalMap: V\to \abidual{V}: v\mapsto \evalMap_v$ is given by
\[ \evalMap_v: \adual{V} \to \mathbb{F}: l\mapsto l(v). \]
\end{definition}

\begin{lemma}
Let $V$ be a vector space. The evaluation map $\evalMap: V\to \abidual{V}: v\mapsto \evalMap_v$ is linear:
\[ \forall v,w\in V, a\in\mathbb{F}: \quad \evalMap_{av + w} = a\evalMap_v + \evalMap_w. \]
\end{lemma}
\begin{lemma}
Let $V$ be vector space over $\mathbb{F}$. The evaluation map is injective.
\end{lemma}
\begin{proof}
Assume $\evalMap_v = \evalMap_w$ for some $v,w\in V$. Then
\[ 0 = \evalMap_v - \evalMap_w  = \evalMap_{v-w}. \]
So $\forall l\in \adual{V}: \evalMap_{v-w}(l) = l(v-w) = 0$. Now define the sublinear functional by
\[ p(x) = \begin{cases}
\alpha & x = \alpha(v-w) \\
0 & \text{else}.
\end{cases} \]
Then the functional $f$ defined on $\Span\{v-w\}$ by $f(\alpha(v-w)) = \alpha$ is bounded by $p$ and can be extended to a functional on all $V$ by the Hahn-Banach theorem \ref{sublinearHahnBanach} if $v-w\neq 0$. Then $f(v-w) \neq 0$, which contradicts our assumptions. Thus $v=w$.
\end{proof}

\begin{proposition}
The mapping $\evalMap: V\to \abidual{V}: v\mapsto \evalMap_v$ is an isomorphism \textup{if and only if} $V$ is finite-dimensional.
\end{proposition}
\begin{proof}
Assume $V$ finite dimensional. As the evaluation map is injective, it is an isomorphism by \ref{invertibleFiniteDim}.
The other direction is a dimensional argument by proposition \ref{dualBasisDimension}.
\end{proof}

\subsection{Considerations of naturality}
TODO
Dual basis not natural, determined by inner product.

$V$ canonically embeddable in $\abidual{V}$.



\section{Topological duality}
\begin{definition}
Let $X$ be a normed space. Define
\[ \tdual{X} = \{  \omega \;|\; \omega:X\to \mathbb{F} \;\text{is a continuous map}\} \]
with the norm
\[ \norm{\omega} = \sup\{|\omega(x)|\;|\; x\in X, \norm{x}\leq 1\}. \]
Then $(\tdual{X},\norm{\cdot})$ is a normed space. We call $\tdual{X}$ the \udef{continuous dual} or \udef{topological dual} of $X$.
\end{definition}
\begin{lemma}
The topological dual $\tdual{X}$ is a subspace of the algebraic dual $\adual{X}$. If $X$ is finite-dimensional, then $\tdual{X}=\adual{X}$.
\end{lemma}
\begin{lemma} Let $X$ be a normed space and
let $x\in X$ $\omega\in \tdual{X}$ be a bounded linear functional. Then
\begin{align*}
\norm{\omega} &= \sup\setbuilder{|\omega(v)|}{\norm{v}=1 } \\
&= \sup\setbuilder{\frac{|\omega(v)|}{\norm{v}}}{v\neq 0} \\
&= \inf\setbuilder{c>0} {|\omega(v)|\leq c\norm{v}\forall v\in X}
\end{align*}
and
\begin{align*}
\norm{x} &= \sup\setbuilder{|\varphi(x)|}{ \norm{\varphi}=1} \qquad\qquad\quad\\ %TODO: fragile spacing!
&= \sup\setbuilder{\frac{|\varphi(x)|}{\norm{\varphi}}}{\varphi\neq 0}.
\end{align*}
\end{lemma}
\begin{proof}
We prove the third equality. Let $\alpha$ be the infimum. Let $\epsilon>0$, then by the definition $|\omega[(\norm{x}+\epsilon)^{-1}x]|\leq \norm{\omega}$. Hence $|\omega(x)|\leq \norm{\omega}(\norm{x}+\epsilon)$. Letting $\epsilon\to 0$ gives $|\omega(x)|\leq \norm{\omega}\norm{x}$ for all $x$. So $\alpha\leq \norm{\omega}$. On the other hand, $|\omega(x)|\leq c$ for all $x$ with $\norm{x}=1$. Hence $\norm{\omega}\leq \alpha$.
\end{proof}

\begin{proposition}
The continuous dual of $l^p(J)$ is $l^q(J)$ where $1<p,q<\infty$ satisfy $\frac{1}{p}+\frac{1}{q}$.
Also, the continuous dual of $l^1$ is $l^\infty$.
\end{proposition}

\subsection{The (topological) transpose of a map}
\begin{definition}
Let $T\in\Bounded(V,W)$. The dual map $T^t: \tdual{W}\to \tdual{V}$ is called the \udef{adjoint} or the \udef{transpose} of $T$.
\end{definition}
The notation $T^t$ is consistent for maps on both the algebraic and topological duals: if $T$ is bounded, $T^t:\adual{W}\to \adual{V}$ restricts to $T^t|_{\tdual{W}} = T^t:\tdual{W}\to \tdual{V}$.

\begin{proposition}
Let $T\in\Bounded(V,W)$. Then the transpose $T^t$ is a bounded operator in $\Bounded(W,V)$ with $\norm{T^t} = \norm{T}$.
\end{proposition}
\begin{proof}
The operator $T^t$ is linear since $\forall f_1,f_2\in \tdual{W}, \forall a\in\mathbb{F}, \forall x\in V:$
\[ (T^t(af_1 + f_2))(x) = (af_1 + f_2)(Tx) = af_1(Tx) + f_2(Tx) = a(T^tf_1)(x) + (T^tf_2)(x). \]
For the equality of norms, we prove two inequalities. First $\forall x\in V, f\in \tdual{W}$
\[ |f(Tx)|\leq \norm{f}\norm{Tx}\leq \norm{f}\norm{x}\norm{T} \implies \frac{|f(Tx)|}{\norm{x}} \leq \norm{f}\norm{T}. \]
taking the supremum over $x\in V$, we get $\norm{T^tf} = \norm{f\circ T}\leq \norm{f}\norm{T}$ and taking the supremum over $f\in \tdual{W}$ gives $\norm{T^t}\leq \norm{T}$. This shows that $T^t$ is bounded.

For the other inequality, we use corollary \ref{existenceBoundedFunctionalOfSameNorm} to the Hahn-Banach theorem: for every $x\in V$, there exists a bounded functional $\omega_x$ such that $\norm{\omega_x}=1$ and $\omega_x(x) = \norm{x}$. Then we can calculate:
\begin{align*}
\norm{Tx} = \omega_{Tx}(Tx) = (T^t\omega_{Tx})(x) \leq \norm{T^t\omega_{Tx}}\norm{x} \leq \norm{T^t}\norm{\omega_{Tx}}\norm{x} = \norm{T^t}\norm{x}
\end{align*}
So $\norm{T}\leq\norm{T^t}$. Combining gives $\norm{T^t}=\norm{T}$.
\end{proof}
\begin{corollary}
The map $T\mapsto T^t$ is an isometric isomorphism in $(\Bounded(X,Y)\to \Bounded(\tdual{Y}, \tdual{X}))$.
\end{corollary}

\begin{lemma}
Let $S,T\in\Bounded(V,W)$ and $\alpha\in\mathbb{F}$. Then
\begin{enumerate}
\item $(S+T)^t = S^t+T^t$;
\item $(\alpha T)^t = \alpha T^t$
\item if $T$ is invertible, then $T^t$ is invertible and
\[ (T^t)^{-1} = (T^{-1})^t. \]
\end{enumerate}
Let $T\in\Bounded(U,V)$ and $S\in\Bounded(V,W)$. Then
\begin{enumerate}
\setcounter{enumi}{3}
\item $(ST)^t = T^tS^t$
\end{enumerate}
\end{lemma}

\subsection{Bidual spaces}
Just like for algebraic duality, we can define a topological bidual space (or second dual space) $\tbidual{V}$.

\begin{proposition}
Let $V$ be a normed space. 
For each $v\in V$
\[ \evalMap_v: \tdual{V} \to \mathbb{F}: \omega \mapsto \omega(v) \]
is bounded and thus an element of $\tbidual{V}$.

The evaluation map $\evalMap: V \to \tbidual{V}$ is
\begin{enumerate}
\item isometric (and thus injective): $\norm{\evalMap_v} = \norm{v}$;
\item bounded with norm $\norm{\evalMap} = 1$.
\end{enumerate}
\end{proposition}
\begin{proof}
Let $v\in V$. Then
\[ \norm{\evalMap_v} = \sup\setbuilder{\norm{\evalMap_v(\omega)}}{\norm{\omega}=1} = \sup\setbuilder{\norm{\omega(v)}}{\norm{\omega}=1} \leq \sup\setbuilder{\norm{v}\;\norm{\omega}}{\norm{\omega}=1} = \norm{v}. \]

(1) Setting $\omega = \inner{v/\norm{v}, \cdot}$, we get
\[ \norm{\evalMap_v} \leq |\evalMap_v(\omega)| = |\inner{v/\norm{v}, v}| = \norm{v}. \]
Together with the calculation above, this gives $\norm{\evalMap_v} = \norm{v}$.

(2) $\norm{\evalMap} = \sup\setbuilder{\norm{\evalMap_v}}{\norm{v}=1} = \sup\setbuilder{\norm{v}}{\norm{v}=1} = 1$.
\end{proof}

\begin{lemma}
Let $V$ be normed space over $\mathbb{F}$ and $v\in V$. For each $v\in V$
\[ \evalMap_v: \tdual{V} \to \mathbb{F}: \omega \mapsto \omega(v) \]
is bounded with norm $\norm{v}$ and thus $\evalMap\in \tbidual{V}$ with $\norm{\evalMap} = 1$.
\end{lemma}

\subsubsection{Reflexive spaces}
\begin{definition}
A normed space $V$ is \udef{reflexive} if the evaluation map $\evalMap:V\to \tbidual{V}$ is surjective:
\[ \im\evalMap = \tbidual{V}. \]
\end{definition}
If $V$ is reflexive, then $\tbidual{V}$ is isometrically isomorphic to $V$. The converse is not necessarily true.

\begin{lemma}
Every finite-dimensional space is reflexive.
\end{lemma}

\begin{proposition}
A separable normed space $X$ with a non-separable dual space $\tdual{X}$ cannot be reflexive. 
\end{proposition}
\begin{proof}
TODO
\end{proof}
Thus $l^1$ is not reflexive.

\begin{proposition}
If the dual space $\tdual{X}$ of a  normed space $X$ is separable, then $X$ itself is separable. 
\end{proposition}
\begin{proof}
TODO
\end{proof}


\chapter{Vector spaces convergence}

\section{Generalities}
\begin{definition}
Let $\sSet{\F,V,+}$ be a vector space and $\xi$ a convergence on $V$. Then $\sSet{\F,V,+, \xi}$ is a \udef{convergence vector space} if
\begin{itemize}
\item vector addition $+: V\times V \to V$ is continuous;
\item scalar multiplication $\cdot: \F\times V \to V$ is continuous.
\end{itemize}
\end{definition}

\begin{lemma}
If $\sSet{\F,V,+, \xi}$ is a convergence vector space, then $\sSet{V,+, 0, \xi}$ is a convergence group.
\end{lemma}
\begin{proof}
We just need to show that $v\mapsto -v$ is continuous, but this scalar multiplication and thus continuous by assumption.
\end{proof}

\begin{proposition}
Let $V$ be a vector space, $\{V_i\}_{i\in I}$ a set of convergence vector spaces and $\{L_i: V \to V_i\}_{i\in I}$ a set of linear maps. Then the initial convergence on $V$ w.r.t. $\{L_i: V \to V_i\}_{i\in I}$ makes $V$ a convergence vector space.
\end{proposition}
\begin{proof}
Continuity of vector addition follows from \ref{initialConvergenceGroup}.

We verify continuity of scalar multiplication $m: \F\times V \to V: (\lambda, v) \mapsto \lambda v$. Using \ref{characteristicPropertyInitialFinalConvergence}, we need to verify that $L_i\circ m$ is continuous for all $i\in I$. Because the $L_i$ are linear, we have
\[ L_i(\lambda v) = \lambda L_i(v) \]
for all $\lambda \in \F, v \in V$. This means that $L_i\circ m = m_i \circ L_i$, where $m_i$ is scalar multiplication in $V_i$, and thus continuous. So $L_i \circ m$ is continuous.
\end{proof}

\begin{lemma} \label{vectorSpaceConvergenceConstruction}
Let $V$ be a vector space over a field $\F$. And $\mathcal{F} \subseteq \powerfilters(V)$ a family of filters. There exists a convergence $\xi$ on $V$ such that $\mathcal{F} = \lim^{-1}_\xi(0)$ \textup{if and only if}
\begin{enumerate}
\item $\pfilter{0} \in \mathcal{F}$;
\item if $F \in \mathcal{F}$ and $G\supseteq F$, then $G\in \mathcal{F}$;
\item if $F,G \in \mathcal{F}$, then $F + G\in \mathcal{F}$;
\item if $F\in \mathcal{F}$, then $\vicinity_\F(0)\cdot F \in \mathcal{F}$;
\item if $v\in V$, then $\vicinity_\F(0)\cdot v \in \mathcal{F}$;
\item if $F\in \mathcal{F}$ and $\lambda\in \F$, then $\lambda\cdot F \in \mathcal{F}$.
\end{enumerate}
\end{lemma}
Note the similarity with \ref{groupConvergenceConstruction} for convergence groups.
\begin{proof}
TODO!

Let $G \to (\lambda, v)$ and take $G \geq \vicinity_\F(\lambda)\otimes H \to (\lambda, v)$. Then
\begin{align*}
\vicinity_\F(\lambda)\cdot H &= (\vicinity_\F(0) + \lambda)\cdot((H - v) + v) \\
&\supseteq \lambda\cdot(H-v) + \lambda \cdot v + \vicinity_\F(0)\cdot (H-v) + \vicinity_\F(0)\cdot v.
\end{align*}
\end{proof}

\section{Topological vector spaces}
\begin{definition}
A \udef{topological vector space} (or TVS) is a vector space $V$ over a field $K$ equipped with a topology $\tau$ such that
\begin{itemize}
\item vector addition $V\times V \to V: (v, w) \mapsto v+w$ and
\item scalar multiplication $K\times V\to V: (\lambda, v) \mapsto \lambda \cdot v$
\end{itemize}
are continuous.
\end{definition}

Clearly the map $p_x: V\to V: v\mapsto v+x$ is a homeomorphism. Thus the topology of $V$ is determined by the neighbourhood filter of the origin $0$.

\begin{lemma} \label{closureSum}
Let $V$ be a TVS and $A,B$ subsets of $V$. Then
\[ \overline{A+B} \subseteq \overline{A} + \overline{B} \]
\end{lemma}
\begin{proof}
From the continuity of $+$ and \ref{continuity}.
\end{proof}

\begin{lemma}
Let $V$ be a TVS
\end{lemma}

\section{General duality theory}
\subsection{Paired spaces}
\begin{definition}
A \udef{pairing} is a triple $(V,W, b)$ where $V,W$ are vector spaces over $\mathbb{F}$ and $b: V\times W\to \mathbb{F}$ is a bilinear form. Often we will write the pairing as just $(V,W)$.

We say $W$ \udef{distinguishes} points of $V$ or is \udef{separating} on $V$ if
\[ \forall v\in V: \exists w\in W: b(v,w) \neq 0. \]

A \udef{dual system}, \udef{dual pair} or \udef{duality} over a field $\mathbb{F}$ is a pairing $(V,W,b)$ such that $V$ distinguishes points of $W$ and $W$ distinguishes points of $V$.
\end{definition}

\begin{lemma}
Let $(V,W, b)$ be a pairing. The curried map $w\mapsto b(\cdot, w)$ is injective \textup{if and only if} $V$ distinguishes points of $W$.
\end{lemma}
In this case $W$ is isomorphic with a space of linear functionals (the image of the curried function), so we can also say a dual system is a pair $(V,W)$ where $W$ is a space of linear functionals on $V$ that distinguishes points of $V$.

\begin{example}
\begin{itemize}
\item Let $V$ be a vector space. Then $(V,V^*, b)$ with $b:V\times V^*: (v,f)\mapsto f(v)$ is a dual pair.
\item Let $V$ be a locally convex Hausdorff space. Hahn-Banach implies $(V,V')$ is a dual pair.
\end{itemize}
\end{example}

\subsection{Weak topologies}
\begin{definition}
Let $(X,Y,b)$ be paired vector spaces. Then for each $y\in Y$, the map
\[ p_y: X\to \R_{\geq 0} x\mapsto |b(x,y)| \]
determines a seminorm on $X$.

The weakest topology on $X$ for which the seminorms $\{p_y\;|\;y\in Y\}$ are continuous is called the \udef{weak topology} $\sigma(X,Y)$ on $X$ for the pair $(X,Y)$.
\end{definition}

\begin{proposition}
Let $(X,Y,b)$ be a pairing. The following are equivalent:
\begin{enumerate}
\item $X$ distinguishes points of $Y$;
\item the map $Y\to X^*: y\mapsto y^*$ is injective, where $y^*$ is defined by
\[ y^*: X\to \mathbb{F}: x\mapsto b(x,y); \]
\item $\sigma(Y,X)$ is Hausdorff.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsubsection{Weak-$*$ topology}

\begin{proposition} \label{weak*continuousFunctional}
Let $X$ be a Banach space and let $X'$ have the weak-$*$ topology. Then a linear functional $\theta: X'\to \C$ is continuous \textup{if and only if}
\[ \exists x\in X: \forall \omega\in X': \quad \theta(\omega) = \omega(x). \]
\end{proposition}
\begin{proof}
TODO 9.2 in lecture notes.
\end{proof}

\subsection{Mackey topology}

\begin{theorem}[Mackey-Arens]
\end{theorem}

\section{Operators on topological vector spaces}
\subsection{Compact operators}
\begin{definition}
A linear operator $T:X\to Y$ between TVSs is \udef{compact} if it maps a neighbourhood of the origin to a precompact set, i.e.\ 
\[ \exists U \in \neighbourhoods(0): \;  \text{$\overline{T[U]}$ is compact.} \]
The set of compact linear operators in $(X\to Y)$ is denoted $\Compact(X,Y)$.
\end{definition}
TODO: doesn't the neighbourhood need to be bounded in some way?????

\begin{proposition}
Let $X$ be a normed space and $Y$ a TVS and $T:X\to Y$ a linear operator. Then the following are equivalent:
\begin{enumerate}
\item $T$ is a compact operator;
\item there exists a neighbourhood $U \subset X$ of the origin and a compact set $V\subset Y$ such that $T[U] \subset V$;
\item the image of the unit ball of $X$, $T[B(\vec{0},1)]$, is precompact in $Y$;
\item the image of any bounded set in $X$ is precompact in $Y$.
\end{enumerate}
If $Y$ is a normed space, these are also equivalent to
\begin{enumerate} \setcounter{enumi}{4}
\item for any bounded sequence $(x_{n})_{n\in \mathbb{N}}$ in $X$, the sequence $(Tx_{n})_{n\in \mathbb{N} }$ contains a converging subsequence.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}


\begin{lemma}
Let $X,Y$ be TVSs.
\begin{enumerate}
\item Then $\Compact(X, Y)$ is a vector space.
\item If $X,Y$ are normed spaces, then $\Compact(X, Y)$ is a subspace of $\Bounded(X, Y)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Let $K,K':X\to Y$ be compact operators. Then, by \ref{closureSum},
\[ \overline{K[B(0, 1)]+K'[B(0, 1)]} \subseteq \overline{K[B(0, 1)]}+\overline{K'[B(0, 1)]}, \qquad \overline{K[\lambda B(0, 1)]} = \lambda\overline{K[B(0, 1)]}. \]

(2) Let $K\in\Compact(X, Y)$. Then the image of the unit ball is precompact, meaning it is bounded. So $K$ is bounded by \ref{existenceOperatorNorm}.
\end{proof}

\begin{lemma}
Let $T:V\to W$ be a bounded operator. If $W$ has the Heine-Borel property, then $T$ is compact.
\end{lemma}
\begin{proof}
The set $T[B(\vec{0},1)]$ is bounded because $T$ is. By the Heine-Borel (TODO ref) property of $W$, $\overline{B(\vec{0},1)}$ is compact.
\end{proof}
\begin{corollary}
Bounded operators with as image a finite dimensional normed space are compact.
\end{corollary}
\begin{corollary}
The identity on a normed space $X$ is compact \textup{if and only if} $X$ is finite-dimensional.
\end{corollary}
\begin{proof}
TODO ref. 
\end{proof}

\begin{proposition}
Compact operators map weakly convergent sequences to strongly convergent sequences. TODO! + remove from Hilbert section.
\end{proposition}
\begin{corollary} \label{limitCompactImageOrthonormalSequence}
Let $V$ be an inner product space and $\seq{e_n}$ a sequence of orthonormal vectors in $V$. If $K$ is a compact operator, then $\lim_{n\to\infty}Ke_n = 0$.
\end{corollary}
\begin{proof}
Any sequence of orthonormal vectors $\seq{e_n}$ converges weakly to $0$. Because $K$ is compact, $\seq{Ke_n}$ converges strongly to zero. TODO ref.
\end{proof}
\begin{corollary}
If $V$ is infinite-dimensional and $K$ is invertible, then its inverse is unbounded.
\end{corollary}
\begin{proof}
Due to $\lim_{n\to\infty}Ke_n = 0$ the operator $K$ cannot be bounded below, so $K^{-1}$ is not bounded by \ref{boundedBelow}.
\end{proof}

\section{Continuity}
\url{https://en.wikipedia.org/wiki/Bilinear_map#Continuity_and_separate_continuity}







\chapter{Banach spaces}
\begin{definition}
\begin{itemize}
\item A \udef{Banach space} is a normed vector space that is complete as a metric space.
\item A \udef{Hilbert space} is an inner product space that is complete as a metric space.
\end{itemize}
\end{definition}

A finite-dimensional normed / inner product space is automatically a Banach / Hilbert space by proposition \ref{finiteDimComplete}.

Every proper subspace $U$ of a normed vector space $V$ has empty interior.
A nice consequence of this is that any closed proper subspace is necessarily nowhere dense. So if V is a Banach space, the Baire category theorem implies that V cannot be a countable union of closed proper subspaces. In particular, an infinite dimensional Banach space cannot be a countable union of finite dimensional subspaces. This means, for example, that a vector space of countable dimension (e.g\ the space of polynomials) cannot be equipped with a complete norm.

The space $\Bounded(V,W)$ is a Banach space.

TODO: quotient of Banach spaces.

\section{Function spaces}
\subsection{The spaces $\mathcal{L}^p(X,\diff{\mu})$}
\subsection{The spaces $L^p(X,\diff{\mu})$}
\begin{theorem}[Riesz-Fisher]
The space $L^p(X,\diff{\mu})$ is complete.
\end{theorem}

For $L^\infty$: essential supremum.

\subsubsection{Locally integrable spaces}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. The \udef{locally $L^p$} space is the space
\[ L^p_\text{loc}(\Omega) \defeq \setbuilder{f \in (\Omega\to\C)}{\text{$f\in L^p(K)$ for all compact $K\subset \Omega$}}. \]
The functions in $L^1_\text{loc}(\Omega)$ are called \udef{locally integrable} on $\Omega$.
\end{definition}
TODO: deal with equivalence classes??

\subsection{Sequence spaces}
TODO:  $L^p(A,\mu)$ with $\mu$ counting measure.

Let $J$ be a countable index set and $x:J\to \mathbb{F}$ a sequence indexed by $J$. We define
\[ \norm{x}_p := \left(\sum_{j\in J}|x(j)|^p\right)^{1/p} \qquad\text{and}\qquad \norm{x}_\infty = \sup_{j\in J}|x(j)|. \]
So $\norm{\cdot}_1$ is the standard norm on $\mathbb{F}^n$. For general sequences there is no guarantee that these norms do not diverge.
\begin{definition}
Let $J$ be an index set, $D$ a directed set and $p\geq 1$,
\begin{align*}
\ell^p(J) &= \setbuilder{x:J\to \F}{\norm{x}_p < +\infty},\\
\ell^\infty(J) &= \setbuilder{x:J\to \F}{\norm{x}_\infty < +\infty},\\
c_0(D) &= \setbuilder{x:D\to \F}{\lim_{n\to\infty}|x(n)| = 0}, \\
c_{00}(D) &= \setbuilder{x:D\to \F}{\setbuilder{n\in D}{x(n)\neq 0}\;\text{has finite cardinality}}.
\end{align*}
unless specified we equip $c_0$ and $c_{00}$ with the norm $\norm{\cdot}_\infty$.
\end{definition}

\begin{lemma}
$c_{00}$ is dense in $\ell^p$ if it is equipped with the norm $\norm{\cdot}_p$ and dense in $c_0$ if it is equipped with the norm $\norm{\cdot}_\infty$.
\end{lemma}

Let $1<p,q<\infty$ satisfy $\frac{1}{p}+\frac{1}{q}$. We have the inequalities
\begin{align*}
\norm{xy}_1 &\leq \norm{x}_p\norm{y}_q\qquad\text{(Hölder inequality)} \\
\norm{x+y}_p &\leq \norm{x}_p+\norm{y}_p\qquad\text{(Minkowski inequality)}
\end{align*}
which follow from the general cases (TODO ref) by applying the counting measure.

\subsubsection{Operators on sequence spaces}
TODO Gribanov's theorems

3.7.1, 3.7.2 of Hanson / Yakovlev.

\section{Series in Banach spaces}
TODO
\url{https://link.springer.com/content/pdf/10.1007%2F978-0-8176-4687-5_3.pdf}
\begin{definition}
Let $\seq{x_n}$ be a sequence in a Banach space $X$. As for series of scalars, we say a series $\sum_{n=1}^\infty x_n$ is
\begin{itemize}
\item \udef{unconditionally convergent} if $\sum_{n=1}^\infty x_{\sigma(n)}$ converges for every permutation $\sigma$ of $\N$;
\item \udef{absolutely convergent} if $\sum_{n=1}^\infty \norm{x_n} < \infty$.
\end{itemize}
\end{definition}

\begin{proposition} \label{absoluteUnconditionalConvergenceBanach}
Let $\seq{x_n}$ be a sequence in a Banach space $X$. If $\sum_{n=1}^\infty$ converges absolutely, then it converges unconditionally.
\end{proposition}
\begin{proof}
Assume absolute convergence, so $\sum\norm{x_i}<\infty$. Then (for $m< n$)
\[ \norm{\sum_{i=1}^n x_i - \sum_{i=1}^m x_i} = \norm{\sum_{i=m+1}^n x_i} \leq \sum_{i=m+1}^n\norm{x_i} = \sum_{i=1}^n \norm{x_i} - \sum_{i=1}^m \norm{x_i}, \]
and because $\sum\norm{x_i}$ converges, it is a Cauchy sequence and by the inequality so is $\sum x_i$. By completeness this sequence is convergent.

By (TODO ref) $\sum\norm{x_{\sigma(i)}}$ converges for any permutation $\sigma$ of $\N$. We can then repeat the argument to show $\sum x_{\sigma(i)}$ is also convergent and thus unconditionally convergent.
\end{proof}

\subsection{Fourier series}
TODO Sacks 7.1

\section{Completions and constructions}

\begin{proposition}
The completions of a space with respect to two different norms are isomorphic \textup{if and only if} the norms are equivalent.
\end{proposition}

TODO move down
\subsection{Tensor products}
TODO Ryan
\url{https://math.stackexchange.com/questions/2712906/does-mathcalb-mathcalh-mathcalh-otimes-mathcalh-in-infinite-dime}
\url{https://math.stackexchange.com/questions/35191/operator-norm-and-tensor-norms?noredirect=1&lq=1}

\subsection{Direct sums}

For arbitrary direct sums we can generalise: now that we have a concept of limits, we can relax the requirement that all but finitely many terms be zero. Instead we require that the sequence of norms is bounded in some way. This gives a whole family of related concepts of direct sum, named for which sequence space the sequence of norms belongs to.
\begin{definition}
Let $\{V_i\}_{i\in I}$ be an arbitrary family of Banach spaces over a field $\F$ and let $\ell(I,\F)$ be a space of sequences in $\F$ indexed by $I$. Then the \udef{$\ell$-direct sum} is the vector space with as field
\[ \bigoplus_{i\in I}^\ell V_i = \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad (\norm{v_i}_{V_i})_{i\in I}\in \ell(I,\F) }. \]
In particular we have, for all $1\leq p<\infty$, the \udef{$\ell^p$-direct sum}
\[ \bigoplus_{i\in I}^p V_i \defeq \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad \sqrt[p]{\sum_{i\in I}\norm{v_i}_{V_i}^p}<\infty} \]
and the \udef{$\ell^\infty$-direct sum}
\[ \bigoplus_{i\in I}^\infty V_i \defeq \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad \sup_{i\in I}\norm{v_i}_{V_i}<\infty}. \]
\end{definition}

\begin{proposition}
For any sequence space that is a Banach space the direct sum is a Banach space. TODO: in particular algebraic direct sum as $c_{00}$? (one possible norm)? and finite direct sums?
\end{proposition}

\subsubsection{Direct sum of identical spaces}
\begin{proposition}
Let $V$ be a Banach space over $\F$, $I$ an arbitrary index set and $\ell(I,\F)$ a banach sequence space.
\[ \bigoplus_{i\in I}^\ell V \cong \ell\otimes V \]
\end{proposition}


\section{Operators on Banach spaces}
\begin{proposition}[Bounded linear extension] \label{BLT}
Let $T:\dom(T)\subseteq X\to Y$ be a bounded operator between normed spaces. Then $T$ has a unique extension
\[ \widetilde{T}:\overline{\dom(T)}\to Y \]
where $\widetilde{T}$ is a bounded operator with $\norm*{\widetilde{T}} = \norm{T}$.
\end{proposition}
\begin{proof}
Normed vector spaces have the unique extension property because they are Hausdorff, \ref{uniqueExtensionHausdorff}. We just need to show the norm stays the same:

Clearly $\norm*{\tilde{T}} \geq \norm{T}$. For the converse take any $x\in X$. As $\overline{\dom(T)} = X$, there exists a sequence $\seq{x_i}\subset \dom(T)$ that converges to $x$. Then
\[ \norm*{\tilde{T}(x)}_Y = \norm{T\left(\lim_{i\to\infty}x_i\right)}_Y = \lim_{i\to\infty}\norm{T(x_i)}_Y \leq \lim_{i\to\infty}\norm{T}\;\norm{x_i}_X = \norm{T}\;\norm{x}_X. \]
\end{proof}

\section{Bounded operators}
\begin{proposition}
Let $V,W$ be normed spaces. The vector space $\Bounded(V,W)$ with the operator norm is a Banach space \textup{if and only if} $W$ is a Banach space.
\end{proposition}
\begin{corollary}
Let $V$ be a normed space. The continuous dual $X'$ is a Banach space.
\end{corollary}
\begin{corollary}
Topologically reflexive spaces are Banach spaces.
\end{corollary}

\begin{proposition} \label{boundedBelowClosedRange}
Let $T$ be a bounded operator with $\dom(T)$ a Banach space that is bounded below. Then the range $\im T$ is closed.
\end{proposition}
\begin{proof}
Let $y\in \overline{\im T}$ and take a sequence $(Tx_n)$ converging to $y$. Because $T$ is bounded below
\[ \norm{x_m-x_n} \leq \frac{1}{b}\norm{T(x_m-x_n)} = \frac{1}{b}\norm{Tx_m-Tx_n} \]
and by proposition \ref{CauchyCriterion} $(x_n)$ is Cauchy. By completeness this sequence has a limit $x$. By boundedness $\lim_n Tx_n = Tx$, meaning $y$ is in $\im T$.
\end{proof}

\subsection{Contractions}
A linear operator $T$ on a normed space is a contraction if and only if it is bounded and $\norm{T}<1$. 

\subsubsection{Neumann series}
\begin{lemma}
Let $T$ be a bounded linear operator on a normed space $X$ with $\norm{T}<1$. Then the series $\sum^\infty_{i=1}T^i(b)$ converges for all $b\in X$ and is the unique fixed point of $F(x) = T(x)+b$.
\end{lemma}
\begin{proof}
The function $F$ is a contraction if and only if $\norm{T}<1$. So it has a unique fixed point. Starting the fixed point iteration at $b$ yields the series:
\begin{align*}
F(b) &= Tb + b \\
F(Tb+b) &= T^2b + Tb + b \\
&\hdots.
\end{align*}
Alternatively we could have used the inequality $\norm{T^nb} \leq \norm{T}^n\norm{b}$, the convergence of the geometric series and \ref{absoluteUnconditionalConvergenceBanach} to prove convergence. Proving it is a fixed point is then elementary.
\end{proof}
\begin{corollary}[Neumann series] \label{operatorNeumannSeries}
Let $T$ be a bounded linear operator with $\norm{T}<1$. Then
\[ (\id - T)^{-1} = \sum_{i=1}^\infty T^i \]
with uniform convergence. Also
\[ \norm{(\id - T)^{-1}} \leq \frac{1}{1-\norm{T}}. \]
\end{corollary}
\begin{proof}
Let $x\in X$. Then set $(\id - T)^{-1}x = y$. This is equivalent to $x = y-Ty$ and means $y$ is the fixed point of $y\mapsto Ty+x$. So $y = \sum_{i=1}^\infty T^ix$.

The convergence is uniform by TODO ref.

Finally we have
\[ \norm{(\id - T)^{-1}} = \norm{\sum_{i=1}^\infty T^i} \leq \sum_{i=1}^\infty \norm{T^i} = \frac{1}{1-\norm{T}} \]
by the geometric series.
\end{proof}
TODO ref: uniform convergence if $\sum_i \norm{T_i} < \infty$??

\subsection{The uniform boundedness principle}
TODO: if a family of bounded operators on a Banach space is pointwise bounded, then it is uniformly bounded.
\begin{theorem}[Uniform boundedness principle]
Let $\mathcal{F}\subset \Bounded(X,Y)$ be a family of bounded operators where $X$ is a Banach space and $Y$ a normed space, such that
\[ \sup\setbuilder{\norm{Tx}}{T\in\mathcal{F}} < \infty \qquad \text{for all $x\in X$}. \]
Then $\sup\setbuilder{\norm{T}}{T\in\mathcal{F}} < \infty$.
\end{theorem}
\begin{proof}
The proof is an application of the Baire category theorem. Define the closed subsets $K_n$ as
\[ K_n = \setbuilder{x\in X}{\forall T\in\mathcal{F}: \norm{Tx}\leq n}. \]
These are closed because the functional $f_T: X\to \R: x\mapsto \norm{Tx}$ is bounded and
\[ K_n = \bigcap_{T\in\mathcal{F}}f_T^{-1}[\,[0,n]\,]. \]
By assumption, $X=\bigcup_{n\in\N} K_n$. As $X$ is a Banach space, and thus a complete metric space, we can apply the Baire category theorem, \ref{BaireCategory}, to conclude that there is a $K_n$ with non-empty interior (by contraposition of the Baire condition). Take $x_0\in K_n^\circ$, then $-x_0+K_n^\circ \subset K_{n2}$. So $\vec{0}\in (K_{2n})^\circ$ and we can find a $\rho$ such that $B(\vec{0},\rho)\subset K_{2n}$. By proposition \ref{existenceOperatorNorm} we have $\norm{T}\leq 2n/\rho$ for all $T\in\mathcal{F}$.
\end{proof}
\begin{corollary}[Banach-Steinhaus]
Let $X$ be a Banach space and $Y$ a normed space. Let $T_n: X\to Y$ be a sequence of bounded operators. If $T_n$ converges pointwise to $T:X\to Y: Tx = \lim_n T_n x$, then $\sup_n\norm{T_n} <\infty$ and thus $T$ is bounded.
\end{corollary}
\begin{proof}
Any convergent sequence in a normed space is bounded, so we can apply the uniform boundedness principle.
\end{proof}

\subsection{Open mapping and closed graph theorems}

\begin{proposition} \label{openUnitBall}
Let $X,Y$ be Banach spaces and $T:X\to Y$ a surjective bounded operator.  Then the image of the open unit ball $B(\vec{0},1)\subset X$ contains an open ball about $\vec{0}\in Y$.
\end{proposition}
\begin{proof}
We first prove $0\in \overline{T[B(\vec{0},r)]}^\circ$ for every $r>0$: (TODO: make computations lemma.)
\begin{itemize}
\item Using $X = \bigcup_{n=1}^\infty B(\vec{0},n)$, we see by surjectivity
\[ Y = T[X] = T\left[\bigcup_{n=1}^\infty B(\vec{0},n)\right] = \bigcup_{n=1}^\infty T[B(\vec{0},n)]. \]
Because $Y$ has the Baire property (theorem \ref{BaireCategory}) and $Y$ is both open and non-empty, it may not be meagre, by lemma \ref{BaireEquivalents}. So for some $n\in\N$, $T[B(\vec{0},n)]$ is non-rare, meaning that $\overline{T[B(\vec{0},n)]}$ has non-empty interior.
\item Because
\[ \overline{T[B(\vec{0},n)]} = \overline{2nT[B(\vec{0},1/2)]} = 2n\overline{T[B(\vec{0},1/2)]}, \]
$\overline{T[B(\vec{0},1/2)]}$ must have non-empty interior. Let $B(y_0,\epsilon)\subset \overline{T[B(\vec{0},1/2)]}$.
\item Note $B(0,\epsilon) = y_0 - B(y_0,\epsilon) \subset \overline{T[B(\vec{0},1)]}$ and thus $B(0,r\epsilon) \subset \overline{T[B(\vec{0},r)]}$.
\end{itemize}
We then prove $\overline{T[B(\vec{0},1/2)]} \subset T[B(\vec{0}, 1)]$, proving the proposition.
\begin{itemize}
\item Choose some $y_0\in \overline{T[B(\vec{0},1/2)]}$. Then every neighbourhood $B(y_0,\epsilon/4)$ intersects $T[B(\vec{0},1/2)]$.
\item Then
\[ B(y_0,\epsilon/4) = y_0 - B(\vec{0},\epsilon/4) \subset y_0 - \overline{T[B(\vec{0},1/4)]}, \]
so $y_0 - \overline{T[B(\vec{0},1/4)]}$ intersects $T[B(\vec{0},1/2)]$. Take a $y_1 \in \overline{T[B(\vec{0},1/4)]}$ such that $y_0-y_1$ is in this intersection. Then we have an $x_0\in B(\vec{0},1/2)$ such that $T(x_0) = y_0-y_1$.
\item We can continue recursively choosing $y_{n+1}\in \overline{T[B(\vec{0}, 2^{-(n+1)})]}$ and $x_n \in B(\vec{0}, 2^{-n})$ such that $y_n-y_{n+1} = T(x_n)$.
\item Consider the sequence $\sum_{k=0}^nx_k$. It is a Cauchy sequence in $X$. Call its limit $x$. Then $x\in B(\vec{0},1)$.
\item Because $\norm{y_n}\leq 2^{-n}\norm{T}$, $(y_n)$ converges to zero. Then
\[ \left( T\left(\sum^n_{k=1}x_k\right) \right)_{n\in\N} = \left( y_0-y_{n+1} \right)_{n\in\N} \]
converges to $y_0$. Thus $T(x) = y_0 \in T[B(\vec{0},1)]$.
\end{itemize}
\end{proof}

\begin{proposition} \label{zeroInInteriorOfImageImpliesOpen}
Let $X,Y$ be normed spaces and $T: X\to Y$ a linear map. If $\vec{0}$ lies in the interior of $T[B(\vec{0},r)]$ for some $r>0$, then $T$ is open.
\end{proposition}
\begin{proof}
TODO: make computations lemma.
Given the assumption, $0$ lies in the interior of $T[B(\vec{0},\epsilon)]$ for all $\epsilon>0$.
Because $T[B(x,\epsilon)] = T(x) + T[B(\vec{0},\epsilon)]$, $T(x)$ lies in the interior of $T[B(x,\epsilon)]$, for all $x\in X$.
Thus for all neighbourhoods $U(x)\subset X$, $T(x)\subset T[U]^\circ$ and so $T[U] \subset T[U]^\circ$, so $T[U]$ is open.
\end{proof}

\begin{theorem}[Open mapping]
Let $X,Y$ be Banach spaces and $T:X\to Y$ a surjective bounded operator. Then $T$ is an open map.
\end{theorem}
\begin{proof}
This is the consequence of propositions \ref{openUnitBall} and \ref{zeroInInteriorOfImageImpliesOpen}.
\end{proof}
\begin{corollary}[Bounded inverse theorem] \label{boundedInverse}
Let $X,Y$ be Banach spaces. If $T:X\to Y$ is a bounded and bijective linear map, then $T^{-1}$ is bounded as well.
\end{corollary}


\begin{proposition}
Let $T: \dom(T)\subset X\to Y$ be a bounded linear operator. Then
\begin{enumerate}
\item if $\dom(T)$ is a closed subset of $X$, then $T$ has closed graph;
\item if $T$ has closed graph and $Y$ is complete, then $\dom(T)$ is a closed subset of $X$.
\end{enumerate}
\end{proposition}
\begin{proof}
We use proposition \ref{closedGraphEquivalence} twice: First assume $(x_n)$ and $(Tx_n)$ converge to $x$ and $y$, respectively. Then $x\in\dom(T)$ by closure and $y = Tx$ by continuity.

Now assume $T$ has closed graph and $Y$ is complete. Take $x\in\overline{\dom(T)}$ and $(x_n)\subset \dom(T)$ converging to $x$. Since $T$ is bounded:
\[ \norm{Tx_n - Tx_m} = \norm{T(x_n-x_m)} \leq \norm{T}\norm{x_n-x_m}, \]
so $(Tx_n)$ is Cauchy by \ref{CauchyCriterion} and thus by completeness has a limit, say $y$. Then $Tx=y$ by continuity. Since $T$ has closed graph, $x\in\dom(T)$. So $\overline{\dom(T)}\subseteq \dom(T)$ and $\dom(T)$ is closed. 
\end{proof}

\begin{theorem}[Closed graph theorem] \label{closedGraphTheorem}
Let $X,Y$ be Banach spaces and $T: X\to Y$ a linear operator. Then $T$ is bounded \textup{if and only if} $T$ has closed graph.
\end{theorem}
\begin{proof}
Assume the graph of $T$ is closed. Then the graph, being a closed subset of a Banach space, is a Banach space (TODO: reference). Then the restriction of the bounded map $X\oplus Y \to X: (x,y)\mapsto x$ to the graph of $T$ is bijective. So by corollary \ref{boundedInverse} the inverse is bounded, so $T$ is bounded.
\end{proof}
Notice that it is important that the domain of $T$ be the whole of $X$.


\subsection{Compact operators}
\begin{proposition}
Let $L\in\Hom(V,W)$ with $V,W$ Banach spaces. Then $L$ is compact \textup{if and only if} the image of any bounded subset of $V$ under $L$ is totally bounded in $W$.
\end{proposition}
TODO proof



\subsubsection{Calkin algebra}
\begin{proposition}
Let $X$ be a Banach space. Then $\Compact(X)$ is a closed two-sided ideal in $\Bounded(X)$.
\end{proposition}
\begin{proof}
TODO + $*$-ideal for Hilbert spaces.
\end{proof}

\begin{definition}
Let $X$ be a Banach space. The \udef{Calkin algebra} is the quotient $\Bounded(X)/\Compact(X)$.
\end{definition}
TODO: quotient algebra ($[A][B] = [AB]$)

\begin{proposition}
Let $[T]\in\Bounded(X)/\Compact(X)$. Then the following are equivalent:
\begin{enumerate}
\item $[T]$ is invertible in the Calkin algebra;
\item $\exists S\in\Bounded(X):$ both $\vec{1}-TS$ and $\vec{1}-ST$ are compact;
\item $T$ has closed range and finite-dimensional kernel and cokernel. 
\end{enumerate}
\end{proposition}
\begin{proof}
Point 1. and 2. are easily equivalent: $[S]$ is an inverse of $[T]$ if and only if $[\vec{1}] = [S][T] = [ST]$ and $[\vec{1}] = [T][S] = [TS]$. Then
\[ [\vec{1}] = [ST] \iff [ST - \vec{1}] = [0] \qquad [\vec{1}] = [TS] \iff [TS - \vec{1}] = [0] \]
and $[F]=[0]$ if and only if $F$ is compact.

TODO
\end{proof}

\section{Unbounded operators}




\chapter{Hilbert spaces}

\section{Examples}
\subsection{The $\ell^2$ spaces}
Sequence spaces $\ell^p$ Hilbert iff $p=2$. (TODO: other sequence spaces?)

\subsection{Direct sum}
Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. By considering them as Banach spaces we can take the $\ell^2$-direct sum. (TODO: other sequence spaces?)
\begin{proposition}
Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. The $\ell^2$-direct sum is a Hilbert space.
\end{proposition}
This gives the conventional interpretation of the \udef{Hilbert space direct sum}: it is the $\ell^2$-direct sum of the summands as Banach spaces.


\section{Projectors and minimisation problems}
Every subspace is a convex, non-empty subset.
\begin{theorem}[Hilbert projection theorem]
Let $\mathcal{H}$ be a Hilbert space, $K$ a closed, convex, non-empty subset of $\mathcal{H}$.
\begin{enumerate}
\item There exists a unique element of $K$ of least norm. i.e.\ there exists a unique $k_0\in K$ such that
\[ \norm{k_0} = \inf\setbuilder{\norm{k}}{k\in K}. \]
i.e.\ $\min\setbuilder{\norm{k}}{k\in K}$ exists.
\item For any $h\in\mathcal{H}$ there exists a unique point $k_0$ in $K$ such that
\[ \norm{h-k_0} = \inf\{\norm{h-k}\;|\; k\in K\}. \]
We use this to define the distance $d(h,K) \defeq \norm{h-k_0}$.
\item If $K$ is a (closed) subspace, then $k_0$ is also the unique point in $K$ such that $(h-k_0)\perp K$.
\end{enumerate}
\end{theorem}
The idea for the first part of the proof is to take a sequence $\seq{\norm{k_i}}\to \inf\setbuilder{\norm{k}}{k\in K}$. By the parallelogram law $\seq{k_i}$ is Cauchy and by completeness it has a limit $k_0$.
\begin{proof}
(1) We can find a sequence $\seq{k_i}$ in $K$ such that $\norm{k_i}$ converges to $d = \inf\setbuilder{\norm{k}}{k\in K}$ by \ref{sequenceToSupInf}. By the parallelogram law
\begin{align*}
\norm{k_i-k_j}^2 &= 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4\norm{\frac{1}{2}(k_i+k_j)}^2 \\
&\leq 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4d^2
\end{align*}
the sequence $\seq{k_i}$ is Cauchy. So it converges to some $k_0$ in $K$ because $K$ is a closed subset of a complete space.

To prove uniqueness, take another $k_0'\in K$ such that $\norm{k_0'}=d$. By convexity $\tfrac{1}{2}(k_0 +k_0')\in K$, hence
\[ d\leq \norm{\tfrac{1}{2}(k_0+k_0')}\leq \tfrac{1}{2}(\norm{k_0}+\norm{k_0'}) = d. \]
So $\norm{\tfrac{1}{2}(k_0+k_0')} = d$. The parallelogram law gives
\[ d^2 = \norm{\frac{k_0+k_0'}{2}}^2 = d^2- \norm{\frac{k_0-k_0'}{2}}^2; \]
hence $\norm{k_0 - k_0'}^2 = 0$ and thus $h_0=k_0$.

(2) The element $k_0$ considered in point 1. is the point closest to a particular choice for $h$, namely $h=0$. For other $h$ consider the set $K-h$, which is again closed and convex.

(3) For all $k\in K$ and $a\in \mathbb{F}$, we have
\[ \norm{h-k_0}\leq \norm{h-k_0+ak} \]
and thus, by lemma \ref{orthogonality}, $(h-k_0)\perp k$, meaning $(h-k_0)\perp K$.

For the converse (i.e.\ uniqueness), suppose $f_0\in K$ such that $(h-f_0)\perp K$. Then for all $f\in K$ we have $(h-f_0)\perp (f_0 -f)$ so that
\begin{align*}
\norm{h-f}^2 &= \norm{(h-f_0) + (f_0-f)}^2 \\
&= \norm{h-f_0}^2 + \norm{f_0 - f}^2 \geq \norm{h-f_0}^2.
\end{align*}
So $\norm{h-f_0}=\inf\{\norm{h-k}\;|\; k\in K\} = d(h,K)$ and thus $f_0=k_0$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed vector subspace. Then $\mathcal{H} = K^\perp \oplus K$.
\end{corollary}
\begin{proof}
We need to prove every vector $x\in \mathcal{H}$ has a unique decomposition of the form
\[ x = y+z \qquad y\in K,\; z\in K^\perp. \]

Such a decomposition exists: we can take $y=k_0$ and $z = x-k_0$. We have already proved uniqueness. We can also give another argument for uniqueness: assume another such decomposition $x=y'+z'$. Then $y-y'= z-z'$ where the left side is in $K$ and the right in $K^\perp$. The only element in $K\cap K^\perp$ is $0$, so $y=y'$ and $z=z'$.
\end{proof}
The ability to make such decompositions in general is unique to Hilbert spaces, see theorem \ref{criterionHilbertSpace}.

\subsection{Orthogonal projection and decomposition}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. Given a subspace $K$ and an element $x \in \mathcal{H}$, we call the unique element $y\in K$ of the decomposition $K\oplus K^\perp$ the \udef{orthogonal projection} of $x$ on $K$. It is denoted $P_K(x)$. This defines a function $P_K:\mathcal{H}\to K$ called the \udef{orthogonal projection} on $K$.
\end{definition}

\begin{proposition}
Let $P$ be the orthogonal projection on a closed subspace $K$. Then
\begin{enumerate}
\item $P$ is a linear operator on $\mathcal{H}$;
\item $\norm{Px}\leq \norm{x}$ for all $x\in\mathcal{H}$;
\item $P^2 = P$;
\item $\ker P = K^\perp$ and $\im P = K$;
\item $\id_\mathcal{H} - P$ is the orthogonal projection of $\mathcal{H}$ onto $K^\perp$.
\end{enumerate}
\end{proposition}
\begin{proof}
These are mostly direct results of the decomposition. In particular 5. follows if we know $K^\perp$ is closed, which it is by proposition \ref{orthogonalComplementClosed}.
\end{proof}
\begin{corollary} \label{HilbertClosedSpaceOrthogonalDecomposition}
Let $\mathcal{H}$ ba a Hilbert space and $K$ a closed subspace, then $\mathcal{H} = K\oplus K^\perp$.
\end{corollary}
\begin{proof}
Let $P$ be the orthogonal projection on $K$. Then by \ref{directSumKernelImageIdempotent}
\[ \mathcal{H} = \im P \oplus \ker P = K\oplus K^\perp. \]
\end{proof}
\begin{corollary} \label{doubleComplementClosure}
Let $\mathcal{H}$ be a Hilbert space.
\begin{enumerate}
\item If $K$ is a subspace, then $(K^\perp)^\perp = \overline{K}$ is the closure of $K$.
\item If $A$ is a subset, then $(A^\perp)^\perp$ is the closed linear span of $A$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Assume $K$ is closed. Then using $0=(I-P_K)x\;\; \Leftrightarrow \;\; x=P_Kx$, we see
\[ (K^\perp)^\perp = \ker(I-P_K) = \im P_K = K. \]
Then, if $K$ is not closed, $(K^\perp)^\perp = (\overline{K}^\perp)^\perp = \overline{K}$, by proposition \ref{orthogonalComplementClosed}.

(2) Using \ref{OrthogonalComplementProperties} we calculate $(A^\perp)^\perp = (\Span(A)^\perp)^\perp = \overline{\Span(A)}$.
\end{proof}
\begin{corollary} \label{denseZeroComplement}
Let $A$ be a subset of a Hilbert space $\mathcal{H}$. Then $\Span(A)$ is dense in $\mathcal{H}$ \textup{if and only if} $A^\perp = \{0\}$.
\end{corollary}
\begin{proof}
The subspace $\Span(A)$ is dense in $\mathcal{H}$ iff $\overline{\Span(A)} = \mathcal{H}$ iff $(\Span(A)^\perp)^\perp = (A^\perp)^\perp = \mathcal{H}$ iff $A^\perp = \{0\}$.

In the last step we have used that $A^\perp$ is closed so that $((A^\perp)^\perp)^\perp = \overline{A^\perp} = A^\perp$, see \ref{orthogonalComplementClosed}.
\end{proof}

\subsubsection{Existence of orthonormal bases}
\begin{corollary}
Let $D$ be an orthonormal subset of a Hilbert space $\mathcal{H}$, then $D$ is an ortonormal basis \textup{if and only if} it is maximal.
\end{corollary}
\begin{proof}
This is a restatement of the previous corollary in the language of \ref{characterisationMaximalOrthonormalSet}.
\end{proof}
\begin{corollary}
Every Hilbert space has an orthonormal basis.
\end{corollary}
\begin{proof}
Every inner product space has a maximal orthonormal set by \ref{exitenceMaximalOrthonormalSet}. This maximal orthonormal set is an orthonormal set by the proposition.
\end{proof}
\begin{corollary} \label{HilbertOnBasisMaximal}
An orthonormal subset of a Hilbert space is an orthonormal basis \textup{if and only if} it is maximal.
\end{corollary}

\begin{lemma}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed subspace. Let $\{e_i\}_{i\in I}$ be an orthonormal basis of $K$. Then
\[ P_K(x) = \sum_{i\in I} \inner{e_i,x}e_i. \]
\end{lemma}
\begin{proof}
We can extend $\{e_i\}_{i\in I}$ to an orthonormal basis $\{e_i\}_{i\in J}$ of $\mathcal{H}$. Then
\[ x = \sum_{i\in J}\inner{e_i,x}e_i = \sum_{i\in I}\inner{e_i,x}e_i + \sum_{i\notin I}\inner{e_i,x}e_i, \]
which is clearly a decomposition in $K\oplus K^\perp$. This is unique, so we have found $P_K(x)$.
\end{proof}

\subsubsection{When are inner product spaces complete?}
Notice that some of the results obtained for Hilbert spaces have one direction that is generally true for inner product spaces: in any inner product space we have
\begin{itemize}
\item $\overline{K}\subset (K^\perp)^\perp$;
\item $\Span(A)$ dense in $\mathcal{H}$ implies $A^\perp = \{0\}$;
\item if $D$ is an orthonormal basis, then it is maximal.
\end{itemize}
See \ref{orthogonalComplementClosed}, \ref{orthogonalComplementDenseSpace} and \ref{characterisationMaximalOrthonormalSet}.

The converses are only true for Hilbert spaces.
\begin{theorem} \label{criterionHilbertSpace}
Let $H$ be an inner product space. If any of the following hold, $H$ is a Hilbert space:
\begin{enumerate}
\item For any orthonormal set $D$,
\[ \text{$D$ is maximal} \quad\implies\quad \text{$D$ is an orthonormal basis.} \]
\item For any subset $A$, $A^\perp = \{0\}$ implies $\Span(A)$ is dense in $H$.
\item For any subspace $K$, we have $(K^\perp)^\perp = \overline{K}$.
\item For all closed subspaces $K$ we can decompose $H = K\oplus K^\perp$.
\end{enumerate}
\end{theorem}
\begin{proof}
We prove the first statement implies $H$ is a Hilbert space. The other three imply the first and thus that $H$ is a Hilbert space.
\begin{enumerate}
\item We prove the contrapositive: assume $H$ is not complete, we wish to show that 1. does not hold, i.e.\ there exists a maximal orthonormal subset of $H$ that is not an orthonormal basis.

Let $\mathcal{H}$ be the completion of $H$ and take a unit vector $v\in \mathcal{H}\setminus H$. Now working in the completion, we have the decomposition $\Span\{v\}\oplus \Span\{v\}^\perp$. Consider the subspace $\Span\{v\} + H = \Span\{v\}\oplus(H\cap \Span\{v\}^\perp)$. We can extend $\{v\}$ to a maximal orthonormal set $\{v\}\cup D$ by \ref{exitenceMaximalOrthonormalSet}.

We claim $D$ is the orthonormal set we want:

Firstly it is maximal.
Assume, towards a contradiction, that $D$ is not maximal in $H$, so there exists an orthonormal set $D'\supsetneq D$. Take $w\in D'\setminus D$ and let $w'$ be the normalisation of $w - \inner{v,w}v$. Then $w' \perp v$ and $w' \perp D$, so $\{v\}\cup D\cup\{w'\}$ is an orthonormal set in $\Span\{v\} + H$, which contradicts the maximality of $\{v\}\cup D$.

Secondly it cannot be total. Indeed if $\closure_H(\Span(D)) = H\cap\overline{\Span(D)}$ were equal to $H$, then $H \subseteq \overline{\Span(D)}$ and thus $\mathcal{H} = \overline{H} \subseteq \overline{\Span(D)} \subseteq \mathcal{H}$, meaning $\overline{\Span(D)} = \mathcal{H}$. But $v\notin \overline{\Span(D)}$, so $\overline{\Span(D)} \neq \mathcal{H}$.

\item 2. clearly implies 1. We can also adapt the proof above to show 2. implies $H$ is a Hilbert space:
Assume $H$ is not complete and let $\mathcal{H}$ be the completion of $H$. There exists a $v\in \mathcal{H}\setminus H$. All orthogonal complements are taken in the completion.
The set
\[ U \defeq H\cap\{v\}^\perp \]
is not dense in $\mathcal{H}$ for the same reason $D$ was not total above. We claim that the orthogonal complement of $U$ in $H$ is $\{0\}$:
\[ U^\perp\cap H = \{0\}. \]
First we claim $U$ is dense in $\{v\}^\perp$: take a $w\in \{v\}^\perp$ and let $(x_n)_{n\in\N}\subseteq H$ converge to $w$ (this is possible because $w\in\mathcal{H}$ and $H$ is dense in $\mathcal{H}$). Fix some $x\in H$ such that $\inner{x,v}\neq 0$, then we have the following sequence in $U$ that converges to $w$:
\[ n\mapsto x_n - \inner{x_n,v}\frac{x}{\inner{x,v}}. \]
Then because $U$ is dense in $\{v\}^\perp$,
\[ U^\perp\cap H = \overline{U}^\perp\cap H = (\{v\}^\perp)^\perp \cap H = \Span\{v\}\cap H = \{0\}. \]
\item Assume 3. Let $D$ be a maximal orthonormal set. Then
\[ \overline{\Span(D)} = (\Span(D)^\perp)^\perp = (D^\perp)^\perp = \{0\}^\perp = H, \]
so $D$ is an orthonormal basis.
\item Assume 4. Let $D$ be a maximal orthonormal set. Then $D^\perp$ is a closed subspace, so
\[ H  = D^\perp \oplus (D^\perp)^\perp = \{0\} \oplus (D^\perp)^\perp = (\Span(D)^\perp)^\perp = \overline{\Span(D)}. \]
\end{enumerate}
\end{proof}

\subsubsection{Orthogonal decomposition}
\begin{theorem}
 A Banach space such all of its closed subspaces are complemented is isomorphic to a Hilbert space.
\end{theorem}
\begin{proof}
TODO Lindestrauss and Tzafriri in 1971. Only real??
\end{proof}

\begin{proposition} \label{directSumOrthogonalClosed}
Let $\mathcal{H}$ be a Hilbert space and let $\{V_i\}_{i\in I}$ be a family of closed, (pairwise) orthogonal subspaces. Then
\[ \bigoplus_{i\in I}V_i \qquad \text{is a closed subspace of $\mathcal{H}$.} \]
\end{proposition}
\begin{proof}
Let $(v_n)$ be a Cauchy sequence in $\bigoplus_{i\in I}V_i$ which converges to $w$. Let $v_{i,n}$ be the component of $v_n$ in $V_i$. By orthogonality we have
\[ \norm{v_n-v_m}^2 = \sum_{i\in I}\norm{v_{i,n}-v_{i,m}}^2. \]
Then
\[ \norm{v_{i,n}-v_{i,m}} \leq \norm{v_n-v_m} \]
which implies $(v_{i,n})_n$ is a Cauchy sequence in the closed space $V_i$ which therefore converges to $w_i\in V_i$. Now there are only a finite number of $i$ for which there exist non-zero $v_{i,n}$ (TODO proof!!!!). So then
\[ \lim_n v_n = \lim_n \sum_{i\in I}v_{i,n} = \sum_{i\in I}w_i \in \bigoplus_{i\in I}V_i \]
where the interchange of limits and last equality follow because the sums are finite.
\end{proof}

\begin{lemma} \label{cancellationOminus}
Let $\mathcal{H}$ be a Hilbert space and $A\supseteq B \supseteq C$ subspaces with $B$ closed. Then
\[ (A\ominus B)\oplus (B\ominus C) = A\ominus C.\]
\end{lemma}
\begin{proof}
Take $v\in(A\ominus B)\oplus (B\ominus C)$. Then either $\{v\}\perp C$ or $\{v\}\perp B$, but this implies $\{v\}\perp C$, so $v\in A\ominus C$.

Take $v\in A\ominus C$. We can uniquely write $v = v_1 + v_2 \in (A\ominus B)\oplus B = A$. We just need to show that $v_2\in B\ominus C$. Indeed assume $\inner{c,v_2}\neq 0$ for some $c\in C$. Then
\[ \inner{c, v} = \inner{c, v_1+v_2} = \inner{c,v_1}+\inner{c,v_2} = \inner{c,v_2} \neq 0, \]
so $v\notin A\ominus C$, a contradiction.
\end{proof}

\subsection{Projection and minimisation in finite-dimensional spaces}

\begin{lemma}
Let $K$ be a subspace of $\F^n$ spanned by the orthonormal basis $\{\vec{u}_i\}_{i=1}^k$. Then
\[ P_K = QQ^* \qquad\text{where}\qquad Q = \begin{bmatrix}
\vec{u}_1 & \vec{u}_2 & \hdots & \vec{u}_k
\end{bmatrix}. \]
\end{lemma}
\begin{proof}
$P_K(\vec{x}) = \sum_{i=1}^k\vec{u}_i\inner{\vec{u}_i,\vec{x}} = \sum_{i=1}^k\vec{u}_i \vec{u}_i^*\vec{x} = \left(\sum_{i=1}^k\vec{u}_i \vec{u}_i^*\right)\vec{x} = QQ^*\vec{x}$.
\end{proof}
\begin{corollary}
For any matrix $A$ with QR factorisation $A=QR$, we have
\[ P_{\Col(A)} = QQ^*. \]
\end{corollary}
In general $P_{\Col(A)} = A(A^*A)^{-1}A^*$.

\begin{proposition}[Normal equations]
Let $\{\vec{v}_i\}_{i=1}^k$ be linearly independent set of vectors in $\F^n$. Set $K = \Span\{\vec{v}_i\}_{i=1}^k$. Then for all $\vec{x}\in \F^n$
\[ P_K(\vec{x}) = \sum_{i=1}^k c_i \vec{v}_i, \]
where $\begin{bmatrix}
c_1 & c_2 & \hdots & c_k
\end{bmatrix}^\transp$ is the solution of
\[ \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix}\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{x}} \\ \inner{\vec{v}_2,\vec{x}} \\ \vdots \\ \inner{\vec{v}_k,\vec{x}}
\end{bmatrix}. \]
This system of linear equations is consistent, yielding a unique solution.
\end{proposition}
The equations in this proposition are known as \udef{normal equations} and the matrix
\[ G(\vec{v}_1, \ldots, \vec{v}_k) \defeq \begin{bmatrix}
\vec{v}_1^* \\ \vec{v}_2^* \\ \vdots \\ \vec{v}_k^*
\end{bmatrix}\begin{bmatrix}
\vec{v}_1 & \vec{v}_2 & \hdots & \vec{v}_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix} \]
is known as the \udef{Gram matrix} or \udef{Grammian}.
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A\in\F^{m\times n}$, $\vec{b}\in\F^m$ and $\vec{x}_0\in\F^n$. Then
\[ \min_{\vec{x}\in\F^n}\norm{\vec{b}-A \vec{x}} = \norm{\vec{b} - A \vec{x}_0} \]
if and only if
\[ A^*A \vec{x}_0 = A^* \vec{b}. \]
\end{proposition}
We regard $\vec{x}_0$ as the ``best approximate solution'' to the (not necessarily consistent) system $A \vec{x} = \vec{b}$.
\begin{proof}
TODO
\end{proof}

\subsection{Riesz representation}
\begin{theorem}[Riesz-Fréchet representation theorem] \label{rieszRepresentation}
Let $\mathcal{H}$ be a Hilbert space. For every continuous linear functional $\omega\in \mathcal{H}'$, there exists a unique $v_\omega\in\mathcal{H}$ such that
\[ \omega(x) = \inner{v_\omega, x} \qquad \forall x\in\mathcal{H}. \]
Moreover, $\norm{v_\omega}_\mathcal{H} = \norm{\omega}_{\mathcal{H}'}$.
\end{theorem}  

The idea of the proof is as follows: consider $\mathcal{H} \cong \ker\omega \oplus \im\omega$. So we can find a subspace $U\subseteq \mathcal{H}$ such that $\mathcal{H} = \ker\omega\oplus U$. Clearly $\dim U = \dim\im\omega = \dim\F = 1$. Between $1$-dimensional spaces there can only be one linear map, up to rescaling. This map is given by $x\mapsto \inner{v,x}$ for some $v\in U$, where the scaling determines the $v$. So we choose $v$ such that $\omega|_U = x\mapsto \inner{v,x}$.

Now we want extend this form of $\omega|_U$ to the whole of $\mathcal{H}$. This works exactly if $v\in(\ker\omega)^\perp$. So we need $U=(\ker\omega)^\perp$ which is true if and only if $\mathcal{H} = \ker\omega\oplus U = \ker\omega\oplus (\ker\omega)^\perp$, which only works in general if $\ker\omega$ is closed and $\mathcal{H}$ is a Hilbert space. Now $\ker\omega$ is closed if and only if it is continuous, by \ref{continuousMapCriterion}.

With this idea we give a full proof:
\begin{proof}
If $\ker\omega = \mathcal{H}$, we can take $v_\omega = 0$.

Assume $\ker\omega\neq \mathcal{H}$, then $(\ker\omega)^\perp\neq \{0\}$ by \ref{denseZeroComplement}, because $\ker\omega$ is closed (\ref{continuousMapCriterion}). So we can take a non-zero $u\in (\ker\omega)^\perp$. We can choose it such that $\omega(u) = 1$, by rescaling. Now let $h\in\mathcal{H}$. We can write $h = (h - \omega(h)u)+\omega(h)u\in\ker\omega\oplus (\ker\omega)^\perp$, because $\omega(h - \omega(h)u) = 0$. So
\[ 0 = \inner{u,h - \omega(h)u} = \inner{u,h} - \omega(u)\norm{u}^2. \]
If $v_\omega = \norm{u}^{-2}u$, then $\omega(h) = \inner{v_\omega, h}$ for all $h\in\mathcal{H}$.

For uniqueness: assume we can find two vectors $v_\omega,v_\omega'$ such that for all $h\in\mathcal{H}$ we have $\omega(h) = \inner{v_\omega, h} = \inner{v_\omega', h}$. Then $v_\omega - v_\omega'\perp \mathcal{H}$, so $v_\omega - v_\omega'= 0$.
\end{proof}
Together with lemma \ref{innerBoundedFunctionals} this gives:
\begin{corollary} \label{RieszIsometry}
The map $C_\mathcal{H}:\mathcal{H}\to \tdual{\mathcal{H}}: v\mapsto \inner{v,\cdot}$ is a bijective anti-linear isometry.
\end{corollary}
\begin{corollary}
Every Hilbert space is reflexive.
\end{corollary}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Every bounded functional defined on a closed subspace of $\mathcal{H}$ can be extended to a functional on $\mathcal{H}$ with the same norm.
\end{corollary}
\begin{proof}
The functional on the closed subspace, say $K$, can be represented as $x\mapsto \inner{v,x}_K$ for some $v\in K$. The extended functional is then simply given by $x\mapsto \inner{v,x}_\mathcal{H}$.
\end{proof}

\begin{proposition}[Representation of sesquilinear forms] \label{sesquilinearRepresentation}
Let $\mathcal{H}_1,\mathcal{H}_2$ be Hilbert spaces over $\mathbb{F}$ and $h:\mathcal{H}_1,\mathcal{H}_2\to\mathbb{F}$ a bounded sesquilinear form. Then there exists a unique bounded operator $S:\mathcal{H}_1 \to \mathcal{H}_2$ such that
\[ h(x,y) = \inner{Sx,y}. \]
This operator has the property $\norm{S} = \norm{h}$.
\end{proposition}
\begin{proof}
For fixed $x$, $y\mapsto h(x,y)$ is a bounded linear functional, so by the Riesz representation theorem \ref{rieszRepresentation} this can be represented by a unique $v_x$. Let $S$ be the function $x\mapsto v_x$. Then $h(x,y) = \inner{Sx,y}$.

To prove this function $S$ is linear, take arbitrary $x_1,x_2\in \mathcal{H}_1;y\in \mathcal{H}_2$ and $\lambda \in \mathbb{F}$. Then
\begin{align*}
\inner{S(\lambda x_1+ x_2),y} &= h(\lambda x_1+ x_2, y) = \overline{\lambda} h(x_1,y)+h(v,y_2) \\
&= \overline{\lambda} \inner{Sx_1, y} + \inner{Sx_2, y} = \inner{\lambda Sx_1 + Sx_2,y},
\end{align*}
so $S$ is linear by lemma \ref{elementaryOrthogonality}.

The equality of norms follows from
\begin{align*}
\norm{h} = \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{|\inner{Sx,y}|}{\norm{x}\norm{y}} &\geq \sup_{\substack{x\neq 0 \\ Sx\neq 0}}\frac{|\inner{Sx,Sx}|}{\norm{x}\norm{Sx}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S} \\
&\leq \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{\norm{Sx}\norm{y}}{\norm{x}\norm{y}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S}
\end{align*}
where the second inequality is Cauchy-Schwarz.
\end{proof}

\section{Orthonormal bases}

Hamel basis / Schauder basis / Hilbert basis

Every Hilbert basis is Schauder basis if $V$ is separable.

Hamel basis too big in Banach space??

Necessity of completeness for existence of complete orthonormal system, i.e.\ orthonormal system $\{a_i\}_{i\in I}$ (so $a_i \cdot a_j = \delta_{ij}$) with
\[ v = \sum_{i\in I}(a_i \cdot v)a_i \]
for all $v$. This is equivalent with
\[ v \cdot w = \sum_{i\in I}(v\cdot a_i)(a_i \cdot w) \]
for all $v,w$.


\begin{theorem}[Riesz-Fischer]
Let $\{e_i\}_{i\in I}$ be an orthonormal basis of a Hilbert space $H$ and $\alpha: I\to \C$ a net. Then
\[ \sum_{i\in I}\alpha_i e_i \]
converges \textup{if and only if} $\sum_{i\in I}|\alpha_i|^2 < \infty$. 
\end{theorem}
\begin{proof}
If $\sum_{i\in I}\alpha_i e_i$ converges, then $\sum_{i\in I}|\alpha_i|^2$ is bounded by the Bessel inequality \ref{BesselInequality}.

By monotone convergence, $\sum_{i\in I}|\alpha_i|^2 < \infty$ is equivalent to saying the sum converges. By (ref TODO) $\alpha$ has finite support. So $\sum_{i\in I}\alpha_i e_i$ can be expressed as the series
\[ \sum_{k\in \N}\alpha_{i_k} e_{i_k}. \]
By completeness it is enough to show that $\seq{s_n} = \seq{\sum_{k=0}^n\alpha_{i_k} e_{i_k}}$ is Cauchy. Let $n < m$, then
\[ \norm{s_n - s_m}^2 = \norm{\sum_{k=m+1}^n\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n\norm{\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n |\alpha_{i_k}|^2 = \sum_{k=0}^n|\alpha_{i_k}|^2 -\sum_{k=0}^m|\alpha_{i_k}|^2.  \]
Since $\seq{\sum_{k=0}^n |\alpha_{i_k}|^2}$ is convergent, it is Cauchy and thus so is $\seq{s_n}$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $D$ be an orthonormal basis of $\mathcal{H}$. Then $\mathcal{H}$ is isometrically isomorphic to $\ell^2(D)$.
\end{corollary}
\begin{corollary}
Hilbert spaces whose orthonormal bases have the same cardinality are isometrically isomorphic.
\end{corollary}

??
\begin{lemma}
Let $(\Omega,\mathcal{A}, \mu)$ be a measure space. Then $L^2(\Omega, \mu)$ is separable \textup{if and only if} $\mu$ is $\sigma$-finite.
\end{lemma}

\begin{lemma}
Let $\{\phi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Omega, \mu)$ and $\{\psi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Lambda, \nu)$, then $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis of $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{lemma}
\begin{proof}
The set $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is orthonormal:
\[ \iint_{\Omega\times\Lambda} \phi_n(x)\psi_m(y)\overline{\phi_{n'}(x)\psi_{m'}(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega\phi_n(x)\overline{\phi_{n'}(x)}\diff{\mu(x)} \cdot \int_\Lambda\psi_m(y)\overline{\psi_{m'}(y)}\diff{\nu(y)} = \delta_{n,n'}\delta_{m,m'}, \]
using Fubini's theorem and the Hölder inequality (TODO refs).

To show $D = \{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis, we verify point 5. of \ref{totalONBParsevalEquivalence}: if $f\perp D$, then $f = 0$.

If $f\perp D$, then for all $m,n\in \N$
\[ 0 = \inner{f, \phi_n\psi_m} = \iint_{\Omega\times\Lambda}f(x,y)\overline{\phi_n(x)\psi_m(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega \left(\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \right)\overline{\phi_n(x)}\diff{\mu(x)}.  \]
Using point 5. of \ref{totalONBParsevalEquivalence} in $L^2(\Omega,\mu)$, we see that for all $m$ the function $x\mapsto\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)}$ is $0$ as an element of $L^2(\Omega, \mu)$, i.e.\ it is $0$ a.e. as a function of $x$. Let
\[ E_m = \setbuilder{x\in\Omega}{ \int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \neq 0} \]
and set $E = \bigcup_{m\in\N}E_m$.
Then
\[ \mu(E) =  \mu\left(\bigcup_{m\in \N}E_m\right) \leq \sum_{m\in\N}\mu(E_m) = 0. \]

For $x\notin E$, we have $\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} = 0$, so by the same logic $f(x,y) = 0$ for almost all $y$. 

Now $|f|^2$ is integrable and
\[ \iint_{\Omega\times \Lambda}|f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = \int_{\Omega\setminus E}\int_\Lambda |f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = 0, \]
so $f=0$ in $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{proof}


\section{Adjoints of operators}
\begin{definition}
Let $H,K$ be Hilbert spaces and $T: H\not\to K$ an operator. An \udef{adjoint} of $T$ is an operator $S: K\not\to H$ such that
\[ \inner{w,Tv}_K = \inner{S w,v}_H \quad \forall v\in \dom(T),\; \forall w\in \dom(S). \]
\end{definition}

\begin{theorem}[Hellinger-Toeplitz]
Let $T: H\to K$ be an operator between Hilbert spaces (which is defined everywhere), then $T$ has an adjoint that is defined everywhere \textup{if and only if} it is bounded.
\end{theorem}
\begin{proof}
The ``if'' will be shown below by explicit construction. For the ``only if'', take such an operator $T$.

First we show $T$ has closed graph, by using proposition \ref{closedGraphEquivalence}: assume $(x_n)$ converges to $x$ and $(Tx_n)$ converges to $y$. Then
\[ \inner{z, Tx} = \inner{Sz,x} = \lim_n\inner{Sz, x_n} = \lim_n\inner{z, Tx_n} = \inner{z, y} \]
where we have used the boundedness of $x\mapsto\inner{z,x}$. By the non-degeneracy of the inner product, $Tx = y$. So the graph of $T$ is closed. Similarly the graph of $S$ is closed. Applying the closed graph theorem \ref{closedGraphTheorem}, yields the boundedness of $T$ and $S$.
\end{proof}
\begin{corollary}
Everywhere-defined symmetric operators are bounded.
\end{corollary}

\begin{example}
The adjoint of the left-shift operator
\[ S_L: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto (x_{n+1})_{n\in\N} \]
is the right-shift operator
\[ S_R: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto \left(\begin{cases}
x_{n-1} & (n\geq 1) \\ 0 &(n=0)
\end{cases}\right)_{n\in \N}. \]
\end{example}

\subsection{Densely defined operators}
\begin{lemma}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) - S_2(x) \in \dom(T)^\perp$.

Conversely, let $S$ be an adjoint of $T$ and $x\in\dom(S)$. Then for all $v\in \dom(T)^\perp$ there exists an adjoint $S'$ such that $S'(x) = S(x) + v$.
\end{lemma}
\begin{proof}
For all $u\in \dom(T)$ we have
\[ \inner{S_1(x) - S_2(x), u}_H = \inner{S_1(x), u}_H - \inner{S_2(x), u}_H = \inner{x, Tu}_K - \inner{x, Tu}_K = 0. \]
So $\{S_1(x) - S_2(x)\} \in \dom(T)^\perp$.

For the converse, set $S' = S + \frac{\inner{x,\cdot}_K}{\inner{x,x}_K}v$. This is an adjoint: for all $a\in \dom(T), b\in \dom(S') = \dom(S)$ we have
\[  \inner{S' b,a}_H = \inner{Sb, a}_H + \frac{\inner{x,b}_K}{\inner{x,x}_K}\inner{v,a}_H = \inner{Sb, a}_H = \inner{b,Ta}_K. \]
\end{proof}
\begin{corollary} \label{agreementAdjoints}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) = S_2(x)$.
\end{corollary}
\begin{proof}
We have $\dom(T)^\perp = \overline{\dom(T)}^\perp = H^\perp = \{0\}$. So $S_1(x) - S_2(x) = 0$.
\end{proof}
\begin{corollary} \label{maximalAdjointIsOperator}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}} \]
is the graph of an operator \textup{if and only if} $T$ is densely defined.
\end{corollary}

\begin{definition}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. We define the adjoint $T^*$ as
\[ \graph(T^*) \defeq \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}}. \]
\begin{itemize}
\item If $T^* = T$, we say $T$ is \udef{self-adjoint}.
\item If $T^* = -T$, we say $T$ is \udef{skew-adjoint}.
\end{itemize}
We denote the set of self-adjoint operators on $H$ by $\SelfAdjoints(H)$.
\end{definition}

\begin{proposition}[Algebraic properties of the adjoint] \label{adjointAlgebraicProperties}
Let $T,S$ be compatible densely defined operators between Hilbert spaces. Then
\begin{enumerate}
\item $\lambda T$ is densely defined for all $\lambda\in \C$ and $(\lambda T)^* = \overline{\lambda}T^*$;
\item if $ST$ is densely defined, then $(ST)^* \supseteq T^*S^*$;
\item if $S+T$ is densely defined, then $(S+T)^*\supseteq S^* + T^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) If $\lambda = 0$, the result is trivial. Assume $\lambda \neq 0$. First we show that $\overline{\lambda}T^*$ is an adjoint of $\lambda T$. Indeed for all $x\in\dom(T^*), y\in \dom(T)$, we have
\[ \inner{x, \lambda Ty} = \lambda\inner{x, Ty} = \lambda \inner{T^* x, y} = \inner{\overline{\lambda}T^*x, y}. \]
Then we note that $\overline{\lambda}T^*x$ must have maximal domain. Indeed, if it did not, then neither would $T^*$, which is a contradiction.

(2) By \ref{agreementAdjoints} it is enough to prove that $T^*S^*$ is an adjoint of $ST$. Indeed, take $x\in \dom(T^*S^*) = \dom(S^*)\cap (S^*)^{-1}[\dom(T^*)]$ and $y\in \dom (ST)$, then
\[ \inner{x, STy} = \inner{S^*x, Ty} = \inner{T^*S^*x, y}. \]

(3) Similarly, by \ref{agreementAdjoints} it is enough to prove that $S^*+T^*$ is an adjoint of $S+T$. Indeed, take $x\in \dom(S^*+T^*) = \dom(S^*)\cap \dom(T^*)$ and $y\in \dom (S+T)$, then
\[ \inner{x, (S+T)y} = \inner{x,Sy}+\inner{x, Ty} = \inner{S^*x, y} + \inner{T^*x, y} = \inner{(S^* + T^*)x, y}. \]
\end{proof}

\begin{lemma}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then
\[ \dom(T^*) = \setbuilder{x\in K}{\text{$u\mapsto \inner{x, Tu}$ is a bounded functional}}. \]
\end{lemma}
\begin{proof}
$\boxed{\subseteq}$ If $\omega: u\mapsto \inner{x, Tu}$ is bounded, then it has a Riesz vector $x^*$ such that $\omega = u\mapsto \inner{x^*, u}$. The anti-linear operator with domain $\Span\{x\}$ that maps $x$ to $x^*$ is then an adjoint.

$\boxed{\supseteq}$ If $x\in\dom(T^*)$, then, using the Cauchy-Schwarz inequality,
\[ |\inner{x,Tu}| = |\inner{T^*x,u}| \leq \norm{T^*x}\;\norm{u}, \]
so the functional $u\mapsto \inner{x, Tu}$ is bounded.
\end{proof}
\begin{corollary}
The domain $\dom(T^*)$ is a vector space and in particular contains $0$.
\end{corollary}

\begin{proposition}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\begin{align*}
\bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}} &= \left( \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T) \right)^\perp \\
&=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp.
\end{align*}
\end{proposition}
\begin{proof}
Take an adjoint $S$ and $(w, Sw)$ in $\graph(S)$. Then for all $v\in\dom(T)$:
\[ 0 = \inner{w, Tv}_K - \inner{Sw, v}_H = \inner{w, Tv}_K + \inner{Sw, -v}_H = \inner{(w, Sw), (Tv,-v)}_{K\oplus H}. \]
So $\graph(S) \perp (Tv,-v) = \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix} (v,Tv)$.

The final equality follows from \ref{perpUnderIsometry}, using the fact that $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ is a surjective isometry.
\end{proof}
\begin{corollary}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then
\[ \graph(T^*) = \left( \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T) \right)^\perp =  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp \]
and $T^*$ is a closed operator.
\end{corollary}
\begin{corollary}
Let $T: H\not\to K$ be a densely defined and closable operator between Hilbert spaces. Then $T^*$ is densely defined and $\overline{T} = T^{**}$.
\end{corollary}
\begin{proof}
From the proposition we have
\begin{align*}
\bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T^*$}} &=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T^*)^\perp \\
&=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\left(\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp\right)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}^2\graph(T)^{\perp\perp} = -\graph(T)^{\perp\perp} \\
&= \overline{\graph(T)} = \graph(\overline{T}).
\end{align*}
This is an operator because $T$ is assumed closable. By \ref{maximalAdjointIsOperator} this means $T^*$ is densely defined and so $T^{**} = \overline{T}$.
\end{proof}

\begin{proposition} \label{kernelImageAdjoint}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item $\ker(T) = \im(T^*)^\perp$;
\item $\ker(T^*) = \im(T)^\perp$.
\end{enumerate}
\end{proposition}
\begin{proof}
First take $v\in \ker(T^*)$, then $T^*(v) = 0$ which implies
\[ \forall x \in\dom(T): \inner{T^*(v), x} = 0 \;\implies\; \forall x \in\dom(T): \inner{v, T(x)} = 0 \;\implies\; v\perp \im(T).  \]
Next take $v\perp \im(T)$
\end{proof}
TODO: link with previous?

\begin{lemma}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Assume $T$ injective and $\im(T)$ dense in $K$. Then $T^*$ is also injective and
\[ (T^*)^{-1} = (T^{-1})^*. \]
\end{lemma}

\url{https://arxiv.org/pdf/1507.08418.pdf}
\url{https://link.springer.com/article/10.1007/s43036-020-00068-4}

\subsection{Bounded operators}
\begin{proposition}
Let $T\in\Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\[ T^* = C_H^{-1}T^tC_K: K\to H, \]
where $C_K$ is the Riesz isometry from \ref{RieszIsometry}.
is the unique adjoint of $T$ with $\dom(S) = K$.
\end{proposition}

\begin{lemma}
Let $T\in\Bounded(H,K)$. Then the adjoint $T^*$ is a bounded operator in $\Bounded(K,H)$ with $\norm{T^*} = \norm{T}$.
\end{lemma}
\begin{proof}
The function $(w,v)\mapsto \inner{w,Tv}_K$ is sesquilinear. By proposition \ref{sesquilinearRepresentation} the function $T^*$ must be the unique $S$ from the proposition, which is linear and bounded.

This can also be proved by a direct calculation using $(T^*)^* = T$ from \ref{HilbertAdjointLemma}.
\end{proof}

\begin{lemma}
The adjoint defines a map $*:\Bounded(H,K)\to \Bounded(K,H)$ that is anti-linear and continuous in the weak and uniform operator topologies. It is continuous in the strong operator topology \textup{if and only if} finite dimensional.
\end{lemma}
\begin{proof}
By the proposition the adjoint map is anti-linear. It is also bounded with norm $1$. Then by corollary \ref{boundedAntiLinearMaps} it must be bounded.

TODO
\end{proof}

\begin{lemma} \label{HilbertAdjointLemma}
Let $S,T\in\Bounded(H,K)$ and $\lambda \in \mathbb{F}$.
\begin{enumerate}
\item $(T^*)^* = T$;
\item $(S+T)^* = S^* + T^*$;
\item $(\lambda T)^* = \bar{\lambda}T^*$;
\item $\id_V^* = \id_V$.
\end{enumerate}
Let $T\in\Bounded(H_1,H_2), S\in\Bounded(H_2,H_3)$
\begin{enumerate}
\setcounter{enumi}{4}
\item $(ST)^* = T^*S^*$.
\end{enumerate}
\end{lemma}
\begin{proof}
These follow straight from \ref{adjointAlgebraicProperties} and the fact that the operators and adjoints are everywhere defined.
\end{proof}

\begin{note}
Useful exercise: The identities of \ref{HilbertAdjointLemma} can also be proven by elementary manipulations. For example, to prove 1., we take arbitrary $v\in H$ and $w\in K$, Then
\[ \inner{w,Tv} = \inner{T^*w,v} = \overline{\inner{v,T^*w}} = \overline{\inner{(T^*)^*v,w}} = \inner{w, (T^*)^*v}. \]
By lemma \ref{elementaryOrthogonality} we have $Tv = (T^*)^*v$ for all $v\in V$. 
\end{note}

\begin{proposition}
Let $H,K$ be Hilbert spaces and $T:H\to K$ a bijective bounded linear operator with bounded inverse. Then $(T^*)^{-1}$ exists and
\[ (T^*)^{-1} = (T^{-1})^*. \]
\end{proposition}
\begin{proof}
We prove $(T^{-1})^*$ is both a left- and a right-inverse of $T^*$: $\forall x\in H, y\in K$
\begin{align*}
\inner{T^*(T^{-1})^*x,y} &= \inner{x,T^{-1}Ty} = \inner{x,y} \\
\inner{x,(T^{-1})^*T^*y} &= \inner{TT^{-1}x,y} = \inner{x,y}
\end{align*}
So, by lemma \ref{elementaryOrthogonality}, $T^*(T^{-1})^* = \id_H$ and $(T^{-1})^*T^* = \id_K$.
\end{proof}

\begin{proposition}
Let $T\in\Bounded(H,K)$. Then
\[ \ker T = (\im T^*)^\perp \qquad \text{and thus} \qquad \overline{\im(T)} \subseteq \ker(T^*)^\perp. \]
\end{proposition}
\begin{proof}
\[ x\in \ker T \iff Tx = 0 \iff \forall y\in K: \inner{y, Tx}=0 \iff \forall y\in K: \inner{T^*y, x}=0 \iff x\perp T^*[K]. \]
\end{proof}
In particular $\im(T)$ is closed iff it is equal to $\ker(T^*)^\perp$. This is sometimes known as the closed range theorem. This is, e.g\, the case when $T$ is bounded below, see \ref{boundedBelowClosedRange}.

\begin{proposition} \label{normOfSquare}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\[ \norm{T^*T}= \norm{T}^2 = \norm{TT^*}. \]
\end{proposition}
\begin{proof}
For $\norm{T^*T}= \norm{T}^2$ first observe that
\[ \norm{T^*T} \leq \norm{T^*}\cdot\norm{T} = \norm{T}^2. \]
Conversely, $\forall x\in H$:
\[ \norm{T(x)}^2 = \inner{Tx,Tx} = \inner{T^*Tx,x} \leq \norm{T^*Tx}\cdot \norm{x} \leq \norm{T^*T}\cdot\norm{x}^2. \]
The other equality follows by applying the first to $T^*$ and using $\norm{T^*}=\norm{T}$.
\end{proof}

\subsection{Normal operators}
\begin{definition}
A densely defined linear operator $T$ on a Hilbert space $H$ is \udef{normal} if
\[ TT^* = T^*T. \]
\end{definition}
Self-adjoint and unitary operators are normal, but the converse is not true.

TODO 3.10 Self-Adjoint, Unitary and Normal Operators from Kreyszig.

\begin{lemma} \label{equalityKernelAdjointNormal}
If $T$ is a normal operator, then $\ker T = \ker T^* = (\im T)^\perp$.
\end{lemma}
\begin{proof}
\[ \norm{Tx}^2 = \inner{Tx,Tx} = \inner{T^*Tx, x} = \inner{TT^*x, x} = \inner{T^*x, T^*x} = \norm{T^*x}^2. \]
\end{proof}

\begin{proposition}
Let $T\in\Bounded(H)$ be a bounded operator. Then $T$ is normal \textup{if and only if} $\forall x\in H: \norm{Tx} = \norm{T^*x}$.
\end{proposition}
\begin{proof}
First, assume $T$ normal. Then
\[ \norm{Tx}^2 = |\inner{Tx,Tx}| = |\inner{T^*Tx,x}| = |\inner{TT^*x,x}| = |\inner{T^*x,T^*x}| = \norm{T^*x}^2. \]

For the converse, we have $\inner{Tx, Ty} = \inner{T^*x, T^* y}$ for all $x,y\in H$ by polarisation.
\end{proof}

\begin{lemma} \label{normalSpectralRadiusEqualsNorm}
For normal elements the spectral radius equals the norm.
\end{lemma}

\begin{lemma}
A normal operator on a Hilbert space is invertible \textup{if and only if} it is bounded below.
\end{lemma}

\subsection{Self-adjoint operators}
\subsubsection{Symmetric operators}
A symmetric operator $T$ is self-adjoint if and only if $\dom(T) = \dom(T^*)$.
\begin{lemma}
Let $T$ be a symmetric densely defined operator on a Hilbert space $H$. Then
\begin{enumerate}
\item $\dom(T) \subseteq \dom(T^*)$;
\item $T = T^*|_{\dom(T)}$.
\end{enumerate}
\end{lemma}
\begin{proof}
This is immediate from symmetry: $\inner{Tx,y} = \inner{x,Ty}$ for all $x,y\in\dom(T)$.
\end{proof}
It is possible for an unbounded,
symmetric operator to not have a self-adjoint extension, even if it is densely defined. (TODO example)

\begin{example}
Consider the operator
\[ T: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(T) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a) = 0 = f(b)}. \]
Then
\begin{align*}
\inner{g, Tf} &= \int_{a}^b \overline{g(x)}i\od{f(x)}{x}\diff{x} \\
&= \overline{g(b)}f(b) - \overline{g(a)}f(a) - \int_{a}^b \Big(i \od{}{x}\overline{g(x)}\Big)f(x)\diff{x} \\
&= \int_a^b \overline{i \od{g(x)}{x}} f(x) \diff{x} = \inner{Tg, f}.
\end{align*}
So $T$ is symmetric and $\dom(T^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)}$. We cannot extend $\dom(T)$ while keeping $\dom(T^*)$ the same, because $T$ would no longer be symmetric due to boundary terms.

There are, however, multiple ways we can extend $T$ to a self-adjoint operator (in each case $\dom(T^*)$ must shrink).

Let $T_\alpha$, for $\alpha\in \R$, be the operator $T$ with domain
\[ \dom(T) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(b) = e^{i\alpha}f(b)}. \]
We must have $\forall f\in \dom(T_\alpha)$ and $g\in\dom(T^*_\alpha)$ that
\[ \overline{g(b)}f(b) - \overline{g(a)}f(a) = f(a)\Big(e^{i\alpha}\overline{g(b)} - \overline{g(a)}\Big) = 0, \]
so we have $e^{-i\alpha}g(b) = g(a)$ and thus $g(b) = e^{i\alpha}g(a)$, which means $\dom(T_\alpha^*) = \dom(T_\alpha)$. So $T_\alpha$ is a self-adjoint extension of $T$ for all $\alpha\in \R$.

TODO: compare Aharonov-Bohm
\end{example}
Notice that the operator
\[ T: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(T) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)} \]
is not symmetric. In this case
\[  \dom(T^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a)=0=f(b)}, \]
so $\dom(T^*) \subseteq \dom(T)$.





\subsubsection{Bounded self-adjoint operators}
\begin{lemma}
Let $A, B\in\Bounded(H)$. Then
\begin{enumerate}
\item $A^*A, AA^*$ and $A+A^*$ are self-adjoint;
\item if $A,B$ are self-adjoint, then $AB$ is self-adjoint \textup{if and only if} $A,B$ commute.
\end{enumerate}
\end{lemma}
\begin{corollary}
Let $A\in\Bounded(H)$. Then there exist unique self-adjoint operators $S,T$ such that
\[ A = S+iT \qquad A^* = S-iT. \]
\end{corollary}
\begin{proof}
Indeed $S = (A+A^*)/2$ and $T = (A-A^*)/2i$ are self-adjoint.
\end{proof}
\begin{corollary}
The operator $A$ is normal \textup{if and only if} $S,T$ commute.
\end{corollary}
\begin{proof}
We calculate the commutator
\[ [S,T] = \left[\frac{A+A^*}{2}, \frac{A-A^*}{2i}\right] = \frac{A^*A - AA^*}{2i} = \frac{1}{2i}[A^*, A]. \]
\end{proof}

\begin{proposition}
The set of bounded self-adjoint operators forms an anti-lattice.
\end{proposition}
\begin{proof}
TODO + generalised to self-adjoint operators??
\end{proof}

\subsection{Orthogonal projections}
\url{https://planetmath.org/latticeofprojections}

\url{https://zfn.mpdl.mpg.de/data/Reihe_A/35/ZNA-1980-35a-0437.pdf}

We denote the set op projections on a Hilberts space $\mathcal{H}$ by $\Projections(\mathcal{H})$.

TODO: $\im(P) = \ker{P^*}^\perp$ shows that we need $P= P^*$ for orthogonality.

\begin{proposition}
Let $P$ be a bounded operator $P$ on a Hilbert space $\mathcal{H}$. Then the following are equivalent:
\begin{enumerate}
\item $P$ is an orthogonal projection onto a closed subspace of $\mathcal{H}$;
\item $P^2 = P$ and $P=P^*$;
\item $P^2 = P$ and $\norm{P}\in \{0,1\}$;
\item $P^2 = P$ and $\norm{P}\leq 1$;
\end{enumerate}
\end{proposition}
\begin{proof}
$\boxed{(1)\Rightarrow (2)}$  Suppose first that $P$ is the orthogonal projection operator onto a closed subspace $K$. Clearly $P^2 = P$. Let $x,y\in\mathcal{H}$ and write $x= x_1+x_2, y = y_1+y_2$ where $x_1,y_1\in K$ and $x_2,y_2\in K^\perp$. Then
\[ \inner{Px, y} = \inner{x_1, y_1+y_2} = \inner{x_1, y_1} + \inner{x_1,y_2} = \inner{x_1,y_1} = \inner{x_1+x_2, y_2} = \inner{x,Py}. \]
So $P = P^*$.

$\boxed{(2)\Rightarrow (3)}$ We calculate $\norm{P} = \norm{P^2} = \norm{P^*P} = \norm{P}^2$ using \ref{normOfSquare}. The solutions to this equation are $\{0,1\}$.

$\boxed{(3)\Rightarrow (4)}$ This is clear.

$\boxed{(4)\Rightarrow (1)}$ Define $K=\im P$, then $K$ is closed because $x\in K$ iff $Px=x$ and thus for any converging sequence $(x_n)_n\subset K$: $\lim x_n = \lim Px_n = P\left(\lim x_n\right)$, so the limit is in $K$.

We just need to show orthogonality: $Px \perp x- Px$. For this we use \ref{orthogonality}: for all $a\in\F$
\[ \norm{Px} = \norm{Px + aPx - aPx} = \norm{P(Px + a(x-Px))} \leq \norm{P}\cdot \norm{Px + a(x-Px)} \leq \norm{Px + a(x-Px)}. \]
We conclude $Px \perp x- Px$.
\end{proof}

\begin{proposition} \label{projectorOrthogonalComplement}
Let $\mathcal{H}$ be a Hilbert space and let $P$ be an orthogonal projector on a closed subspace $K$. Then $\id-P$ is the orthogonal projector on $K^\perp$.
\end{proposition}
\begin{proof}
Any $x\in \mathcal{H}$ can be uniquely decomposed as $x_1 + x_2\in K\oplus K^\perp$. If $Px = x_1$, then $(\id - P)x = x_1 +x_2 - x_1 = x_2$.
\end{proof}
\begin{corollary} \label{projectorsIn01}
The set of projectors $\Projections(\mathcal{H})$ is a subset of $[0,\id]$.
\end{corollary}
\begin{proof}
Let $P\in\Projections(\mathcal{H})$. Then $P\geq 0$ follows from $P = P^2 = P^*P$.
\end{proof}

\begin{proposition} \label{commutingProjectors}
Let $\mathcal{H}$ be a Hilbert space and $P,Q$ be projections. The following are equivalent:
\begin{enumerate}
\item $PQ = QP$;
\item $PQ$ is a projection;
\item $QP$ is a projection;
\item $P+Q-PQ$ is a projection;
\item $\im(PQP) = \im(P) \cap \im(Q)$;
\item $PQP = QP$;
\item $\mathcal{H} = \big(\im(P)\cap\im(Q)\big)\oplus \big(\im(P)\cap\im(Q)^\perp\big) \oplus \big(\im(P)^\perp\cap\im(Q)\big) \oplus \big(\im(P)^\perp\cap\im(Q)^\perp\big)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Points (1), (2), (3) are equivalent by the equation $(PQ)^* = Q^*P^* = QP$, and the fact that (1) implies $(PQ)^2 = PQPQ = PPQQ = PQ$.

(4) If $P,Q$ commute, then
\begin{align*}
(P+Q-PQ)^* &= P+Q-(PQ)^* = P+Q-Q^*P^* =P+Q-QP = P+Q-PQ \\
(P+Q-PQ)^2 &= P^2 + PQ -P^2Q + QP+Q^2 - QPQ - PQP -PQP +PQPQ \\
&= P + Q + 3PQ - 4PQ= P+Q-PQ.
\end{align*}
Assume (4), then $(P+Q-PQ)^* = P+Q-QP = P+Q-PQ$. This implies $PQ=QP$.

$\boxed{(1)\Rightarrow (5)}$ Clearly $\im(PQP) \subseteq \im(P) \cap \im(Q)$.
For the inverse inequality, take $x\in im(P)\cap\im(Q)$. Then $PQP(x) = PQ(x) = P(x) = x$, so $x\in\im(PQP)$.

$\boxed{(5)\Rightarrow (6)}$ We decompose $\mathcal{H} = \im(PQP) \oplus \ker(PQP)$ and show that the operators are the same on both parts. For all $x\in \mathcal{H}$ we have
\[ x\in \ker(PQP) \iff \inner{x,PQPx} = 0 \iff \inner{QPx,QPx} = 0 \iff \norm{QPx} = 0 \iff x\in\ker{QP}.  \]
Now let $x\in\im(PQP) = \im(P)\cap\im(Q)$. Then $QPx = Qx = x = PQPx$.

$\boxed{(6)\Rightarrow (3)}$ $PQP$ is always a projection.

$\boxed{(6)\Rightarrow (7)}$ Take some $x\in \mathcal{H}$. Then we can uniquely decompose $x = P(x) + (x-P(x)) = x_P + x_{P^\perp} \in \im(P)\oplus \im(P)^\perp$. We can then further decompose $x_P = x_{P,Q} + x_{P,Q^\perp}$ and $x_{P^\perp} = x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. In order to have the decomposition of the proposition, we need to show that $x_{P,Q},x_{P,Q^\perp}\in \im(P)$ and $x_{P^\perp, Q},x_{P^\perp, Q^\perp}\in\im(P)^\perp$.

First take $x_{P,Q} = QPx$. From (6) we have $P(QPx) = PQPx = QPx$, so $x_{P,Q}\in \im(P)$. For the others we have similar calculations (also using the identity $PQP = PQ$):
\begin{align*}
P(x_{P,Q^\perp}) &= P\big((\id-Q)P\big)x = Px - PQPx = Px - QPx = (\id-Q)Px = x_{P,Q^\perp} \\
(\id-P)(x_{P^\perp,Q}) &= (\id-P)\big(Q(\id-P)\big)x = (Q-QP-PQ+PQP)x = (Q-QP)x = Q(\id-P)x = x_{P^\perp,Q} \\
(\id-P)(x_{P^\perp,Q^\perp}) &= (\id-P)\big((\id-Q)(\id-P)\big)x = (\id-P-Q+QP-P+P+PQ-PQP)x \\
&= (\id-Q-P+QP)x = (\id-Q)(\id-P)x = x_{P^\perp,Q^\perp}.
\end{align*}
$\boxed{(7)\Rightarrow (1)}$ Take $x\in \mathcal{H}$ and decompose it as $x_{P,Q} + x_{P,Q^\perp} + x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. Then $PQx = P(x_{P,Q} + x_{P^\perp, Q}) = x_{P,Q}$ and $QP = Q(x_{P,Q} + x_{P, Q^\perp}) = x_{P,Q}$, so $PQ = QP$. 
\end{proof}

\begin{proposition} \label{perpendicularProjections} \label{subspaceProjections}
Let $P,Q$ be orthogonal projections onto subspaces $\im(P)$ and $\im(Q)$ of $\mathcal{H}$.
\begin{enumerate}
\item The following are equivalent to $\im(P) \perp \im(Q)$:
\begin{enumerate}
\item $QP = 0$;
\item $PQ = 0$;
\item $Q+P$ is an orthogonal projection.
\end{enumerate}
\item The following are equivalent to $\im(P) \subseteq \im(Q)$:
\begin{enumerate}
\item $QP = P$;
\item $PQ = P$;
\item $Q-P$ is an orthogonal projection;
\item $P\leq Q$;
\item $\norm{Px} \leq \norm{Qx}$ for all $x \in \mathcal{H}$.
\end{enumerate}
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \perp \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a, b)\Leftrightarrow (c)}$ We know $(P+Q)^* = P^*+Q^* =P+Q$ and we can write
\[ (P+Q)^2 = P^2 + Q^2 + PQ + QP = P+Q+ PQ+QP,  \]
So clearly (a) or (b) imply (c). Conversely, assume $PQ + QP = 0$, implying $PQ=-QP$. By left- and right-multiplication by $P$ this implies both
\[ PPQ = PQ = -PQP \qquad \text{and} \qquad PQP = -QPP = -QP. \]
So $PQ = -PQP = QP$, meaning $PQ = 1/2(PQ+QP) = 0$.

(2) We prove the following:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \subseteq \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a,b)\Rightarrow (c)}$ Obviously $(Q-P)^*= Q-P$. Also
\[ (Q-P)^2 = Q+P-PQ-QP= Q+P-2P = Q-P. \]

$\boxed{(c)\Rightarrow (a,b)}$ Now from
\[ Q-P = (Q-P)^2 = Q+P-PQ-QP \]
we obtain $2P = PQ+QP$. The result then follows if we can show that $PQ=QP$. This follows by multiplying the equality on the left and on the right by $P$ to obtain $QP = 2P-PQP$ and $PQ = 2P-PQP$, respectively. 

$\boxed{(c)\Rightarrow (d)}$ This follows because all projections are positive.

$\boxed{(d)\Rightarrow (a, b)}$ Assume, towards a contradiction, that $\im(P)\nsubseteq \im(Q)$. Then we can take $v\in\im(P)\setminus \im(Q)$. Then
\[ \inner{v,(Q-P)v} = \inner{v,Qv} - \inner{v,v} = \inner{Qv,Qv} - \inner{Qv,Qv} - \inner{v-Qv, v-Qv} = -\norm{v-Qv}^2. \]
Because $v\notin \im(Q)$, $\norm{v-Qv}$ is not zero and thus $Q-P$ is not positive.

$\boxed{(d)\Leftrightarrow (e)}$ By the equivalence
\[ \norm{Px} \leq \norm{Qx} \iff \inner{Px,Px} \leq \inner{Qx,Qx} \iff \inner{Px,x}\leq \inner{Qx,x} \iff \inner{(Q-P)x,x}\geq 0. \]
\end{proof}

We can generalise part 2(d) of the previous proposition to a slightly larger class of operators.
\begin{lemma} \label{comparisonSelfAdjointProjection}
Let $P\in \Projections(\mathcal{H})$ and $T \in [0,\id]$, then the following are equivalent:
\begin{enumerate}
\item $\im(T) \subseteq \im(P)$;
\item $T\leq P$.
\end{enumerate}
\end{lemma}
\begin{proof}
As $T$ is self-adjoint, we have $\norm{T} = \nr(T) \leq 1$ by \ref{normNumRadius}.

Assume (1) so that for all $x\in \mathcal{H}$ we get
\[ \inner{x,Tx} = \inner{x, PTx} = \inner{Px,PTx} \leq \norm{Px}^2\nr(T) \leq \norm{Px}^2 = \inner{Px,Px} = \inner{x,Px}. \]
So $\inner{x, (P-T)x}\geq 0$ and thus $T\leq P$.

Assume (2). The energy form associated with $T$ is a pre-inner product by \ref{positiveOperatorPositiveEnergyForm}. The Cauchy-Schwarz inequality \ref{CauchySchwarz} gives
\[ |\inner{v,Tw}|^2 \leq \inner{v,Tv}\inner{w,Tw} \leq \inner{v,Pv}\inner{w,Pw}. \]
So if $v\in\im(P)^\perp$, then $\inner{v,Tw} = 0$ for all $w\in \mathcal{H}$. So $\im(T)\perp \im(P)^\perp$, implying $\im(T)\subseteq \im(P)^{\perp\perp} = \im(P)$.
\end{proof}

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space. Let $\{P_i\}_{i\in I}$ be an arbitrary subset of $\Projections(\mathcal{H})$ and let $K_i = \im(P_i)$ for all $i\in I$. Then, as a subset of $[0,\id]$,
\begin{enumerate}
\item $\inf \{P_i\}_{i\in I} = P_M$ where $M = \bigcap_{i\in I}K_i$;
\item $\sup \{P_i\}_{i\in I} = P_N$ where $N = \bigcap\setbuilder{K \subseteq \mathcal{H}}{\text{$K$ is closed} \land \forall i\in I: K_i \subseteq K}$.
\end{enumerate}
The set of projections on $\mathcal{H}$ is thus a complete lattice as a subset of $[0,\id]$.

If $I$ is finite, then $N = \Span(\bigcup_{i\in I}K_i)$. TODO: always closure of this $N$????
\end{proposition}
In particular this means $\Projections(\mathcal{H})$ is a complete lattice as itself, with the same suprema and infima. It is not a lattice as a subset of $\SelfAdjoints(\mathcal{H})$ (TODO + example ??).
\begin{proof}
(1) By \ref{subspaceProjections} $P_M$ is a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. Let $T$ be a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. By \ref{comparisonSelfAdjointProjection} $\im(T)\subseteq K_i$ for all $i\in I$, so $\im(T)\subseteq M$ and thus $T\leq P$ again by \ref{comparisonSelfAdjointProjection}. This means $P$ is the greatest lower bound.

(2) The mapping $T\mapsto \id-T$ keeps $[0,\id]$ invariant and inverts the order. Then $\inf \{\id - P_i\}_{i\in I}$ is a projection due to the previous point and so $\sup \{P_i\}_{i\in I}$ is also a projection. The expression for $N$ is clear from \ref{subspaceProjections}.
\end{proof}

\subsubsection{Sets of pairwise disjoint projections}
TODO!

\subsubsection{Derivatives of orthogonal projections}



\begin{proposition}
Let $\{P_i\}_{i\in I}$ be a set of pairwise disjoint orthogonal projectors which have derivatives and take $i\neq j$ in $I$. Then
\begin{enumerate}
\item $P_i'P_j = - P_iP_j'$;
\item if $\id \in \upset \{P_i\}_{i\in I}$, then
\[ P_iP_i' = \sum_{j\neq i}P'_iP_j \qquad\text{and}\qquad P_i'P_i = \sum_{j\neq i}P_jP_i'. \]
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have $P_iP_j = 0$, so $0 = P_i'P_j + P_iP_j'$.

(2) We calculate, using $\id = \sum_{j\in I}P_j$ and \ref{derivativeIdempotent}:
\[ P_iP_i' = P_iP_i'\left(\sum_{j\in I}P_j\right) = P_iP_i'P_i + \sum_{j\neq i}P_iP_i'P_j = 0 - \sum_{j\neq i}P_iP_iP_j' = -\sum_{j\neq i}P_iP_j' = \sum_{j\neq i}P_i'P_j. \]
\end{proof}
\begin{corollary}
Let $P_1, P_2$ be orthogonal projections such that $P_1 + P_2 = \id$. Then
\[ P_1P_1'= P_1'P_2 \qquad \text{and}\qquad P_1'P_1 = P_2P_1'. \]
\end{corollary}


\subsection{Isometries}
We recall that isometries are injective and continuous. On Hilbert spaces they are also closed. See \ref{isometryInjective}, \ref{isometryContinuous} and \ref{isometryClosed}.

\begin{proposition} \label{isometryCharacterisation}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\begin{enumerate}
\item $T$ is an isometry \textup{if and only if} $T^*T = \id_H$;
\item $T$ is unitary \textup{if and only if} $T^*T = \id_H$ and $TT^* = \id_K$, i.e.\ $T^{-1} = T^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) For all $v,w\in H$ we have
\[ \inner{Tv,Tw} = \inner{T^*Tv,w}. \]
The left-hand side is equal to $\inner{v,w}$ iff $T$ is an isometry. The right-hand side is equal to $\inner{v,w}$ iff $T^*T = \id_H$, by \ref{equalityOfMapsInnerProductSpaces}.

(2) If $T$ is invertible, it must have a left and right inverse. By lemma \ref{leftRightInverse} they must be the same.
\end{proof}
\begin{corollary}
An isometry $T\in\Bounded(H)$ is unitary \textup{if and only if} it is normal.
\end{corollary}

\begin{lemma} \label{isometryRangeProjection}
Let $T$ be an isometry between Hilbert spaces $H$ and $K$. Then $TT^*$ is an orthogonal projection.
\end{lemma}
\begin{proof}
Clearly $(TT^*)^* = TT^*$. Also $(TT^*)^2 = T(T^*T)T^* = T\id_HT^* = TT^*$.
\end{proof}


\subsubsection{Wandering spaces and unilateral shifts}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear map. Then $\mathcal{V}$ is called a \udef{wandering space} for $T$ if $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$.
\end{definition}

\begin{lemma} \label{WoldLemma1}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear isometry.
\begin{enumerate}
\item $\mathcal{V}$ is a wandering space for $T$ \textup{if and only if} $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$;
\item $T[\mathcal{H}]^\perp$ is a wandering subspace for $T$;
\item if $\mathcal{V}$ is a wandering space for $T$, then $T^n[\mathcal{V}] \cong \mathcal{V}$ for all $n\in N$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) The direction $\Rightarrow$ is clear. For the converse, assume $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$. We need to show that $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$. WLOG we may assume $p\leq q$. Take arbitrary $x\in T^p[\mathcal{V}]$ and $y\in T^q[\mathcal{V}]$. Then
\[ \inner{x,y} = \inner{T^p(u), T^q(v)} = \inner{u, T^{q-p}(v)} = 0 \]
because $\mathcal{V} \perp T^{q-p}[\mathcal{V}]$.

(2) For all $n\geq 1$ we have
\[ T^{n}\big[T[\mathcal{H}]^\perp\big] \subset T^{n}[\mathcal{H}] = T\big[T^{n-1}[\mathcal{H}]\big] \subset T[\mathcal{H}] \perp T[\mathcal{H}]^\perp. \]

(3) For all $n\in \N$ the operator $T^n$ is an isometry. It is injective by \ref{isometryInjective}, and thus maps its domain bijectively to its image.
\end{proof}

\begin{definition}
An isometry $T$ on a Hilbert space $\mathcal{H}$ is called a \udef{unilateral shift} if there is a closed subspace $\mathcal{V}\subseteq \mathcal{H}$ that is wandering for $T$ such that
\[ \mathcal{H} = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
We call the subspace $\mathcal{V}$ \udef{generating} for $T$ and $\dim(\mathcal{V})$ the \udef{multiplicity} of $T$.
\end{definition}

By \ref{WoldLemma1}, we see that any isometry $T:\mathcal{H}\to\mathcal{H}$ is a unilateral shift when restricted to $\bigoplus_{n=0}^\infty T^n\big[T[\mathcal{H}]^\perp\big]$.



\begin{lemma} \label{WoldLemma2}
Let $T$ be an isometry on $\mathcal{H}$. If $T$ is a unilateral shift, then it is generated by $T[\mathcal{H}]^\perp$.
\end{lemma}
\begin{proof}
Let $\mathcal{V}$ be the generating subspace of the unilateral shift $T$. We calculate
\[ T[\mathcal{H}] = T\left[\bigoplus_{n=0}^\infty T^n[\mathcal{V}]\right] = \bigoplus_{n=1}^\infty T^n[\mathcal{V}] = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \ominus \mathcal{V} = \mathcal{H}\ominus \mathcal{V} = \mathcal{V}^\perp, \]
so $\mathcal{V} = T[\mathcal{H}]^\perp$.
\end{proof}

A unilateral shift is determined up to unitary equivalence by its multiplicity:
\begin{lemma}
Let $T: \mathcal{H}\to\mathcal{H}$ and $T':\mathcal{H}'\to\mathcal{H}'$ be unilateral shifts generated by $\mathcal{V}$ and $\mathcal{V}'$ such that $\dim(\mathcal{V}) = \dim(\mathcal{V}')$. Then there exists an unitary $U:\mathcal{H}'\to\mathcal{H}$ such that
\[ T' = U^*TU \]
\end{lemma}
\begin{proof}
Choose an isometric isomorphism $u:\mathcal{V}'\to\mathcal{V}$. Then any $x\in\mathcal{H}'$ can be written as $x = \sum_{n=0}^\infty T^n(x_n)$. Then define
\[ Ux = \sum_{n=0}^\infty T^n(ux_n). \]
\end{proof}

\begin{theorem}[Wold decomposition]
Let $\mathcal{H}$ be a Hilbert space and $T\in\Bounded(\mathcal{H})$ an isometry. Then $\mathcal{H}$ decomposes into an orthogonal sum $\mathcal{H} = \mathcal{H}_0\oplus \mathcal{H}_1$such that $\mathcal{H}_0, \mathcal{H}_1$ reduce $T$ and
\[ T|_{\mathcal{H}_0}\;\text{is unitary} \quad\text{and}\quad T|_{\mathcal{H}_1}\;\text{is a unilateral shift}. \]
This decomposition is uniquely determined and given by
\[ \mathcal{H}_0 = \bigcap_{n=0}^\infty T^n[\mathcal{H}] \qquad\text{and}\qquad \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \qquad\text{where}\qquad \mathcal{V} = T[\mathcal{H}]^\perp. \]
\end{theorem}
\begin{proof}
The subspace $\mathcal{V} = T[\mathcal{H}]^\perp$ is wandering by \ref{WoldLemma1}. Then $T$ is a unilateral shift in the subspace
\[ \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
Now $v\in\mathcal{H}_0 = \mathcal{H}_1^\perp$ if and only if it is perpendicular to $\bigoplus_{i=0}^n T^i[\mathcal{V}]$ for all $n$ and we have
\begin{align*}
\bigoplus_{i=0}^n T^i[\mathcal{V}] &= \bigoplus_{i=0}^n T^i[\mathcal{H}\ominus T[\mathcal{H}]] = \bigoplus_{i=0}^n T^i[\mathcal{H}]\ominus T^{i+1}[\mathcal{H}] \\
&= (\mathcal{H}\ominus T[\mathcal{H}])\oplus(T[\mathcal{H}]\ominus T^2[\mathcal{H}])\oplus \ldots \oplus (T^n[\mathcal{H}]\ominus T^{n+1}[\mathcal{H}])  = \mathcal{H} \ominus T^{n+1}[\mathcal{H}] 
\end{align*}
using \ref{perpUnderIsometry} and \ref{cancellationOminus}, which is applicable because $T^i[\mathcal{V}]$ is closed by \ref{isometryClosed}. So $\mathcal{H}_0\subseteq T^n[\mathcal{H}]$ for all $n$.

Finally $T|_{\mathcal{H}_0}$ is unitary because it is an isometry and surjective on $\mathcal{H}_0$.
\end{proof}

\subsubsection{Partial isometries}
\begin{definition}
An operator $T\in \Lin(H, H')$ is called a \udef{partial isometry} if there is a closed subspace $K\subseteq H$ such that
\begin{itemize}
\item $T|_K$ is an isometry;
\item $T|_{K^\perp} = 0$.
\end{itemize}
\end{definition}

Clearly every partial isometry is bounded.

\begin{lemma}
An operator $T\in \Lin(H, H')$ is a partial isometry \textup{if and only if} $T|_{\ker(T)^\perp}$ is an isometry.
\end{lemma}

\begin{proposition} \label{partialIsometryEquivalences}
Let $T\in \Bounded(H,H')$. The following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry;
\item $T^*TT^* = T^*$;
\item $TT^*T = T$;
\item $TT^*: H' \to H'$ is a projection;
\item $T^*T: H \to H$ is a projection;
\item $T^*$ is a partial isometry.
\end{enumerate}
Moreover,
\begin{enumerate}
\item $T^*T$ is the projection onto $\ker(T)^\perp$;
\item $\im(T)$ is closed and $TT^*$ is the projection onto $\im(T)$.
\end{enumerate}
\end{proposition}
\begin{proof}

$\boxed{(1)\Rightarrow (2)}$ By \ref{elementaryOrthogonality} it is enough to show that $\inner{T^*TT^*x,y} = \inner{T^*x,y}$ for all $x\in H', y\in H$. Take such $x,y$. We decompose $y = y_1\oplus y_2 \ker(T)\oplus \ker(T)^\perp$. Then
\[ \inner{T^*TT^*x, y_1} = \inner{TT^*x, Ty} = 0 = \inner{x,Ty_1} = \inner{T^*x, y_1} \]
and
\[ \inner{T^*TT^*x, y_2} = \inner{TT^*x, Ty_2} = \inner{T^*x,y_2}, \]
where we have used the fact that both $y_2$ and $T^*x$ are elements of $\ker(T)^\perp = \overline{\im(T^*)}$, and $T$ is an isometry on this space. In conclusion, we have
\[ \inner{T^*TT^*x,y} = \inner{T^*TT^*x,y_1} + \inner{T^*TT^*x,y_2} = \inner{T^*x,y_1} + \inner{T^*x,y_2} = \inner{T^*x,y} \]
for all $x\in H', y\in H$, so $T^*TT^* = T^*$.

$\boxed{(2) \Leftrightarrow (3)}$ By taking adjoints: $(TT^*T)^* = T^*TT^*$.

$\boxed{(2) \Rightarrow (4,5)}$ Clearly $T^*T$ and $TT^*$ are self-adjoint. We just need to show idempotency:
\[ (T^*T)^2 = (T^*T)(T^*T) = (T^*TT^*)T = T^*T \qquad (TT^*)^2 = (TT^*)(TT^*) = T(T^*TT^*) = TT^*. \]

$\boxed{(4) \Rightarrow (1)}$ Assume $TT^*$ a projection. Let $v\in \ker(T)^\perp = \overline{\im(T^*)}$. Then there exists a sequence $\seq{v_n}\in H^{\prime\N}$ such that $\lim_{n\to\infty}T^*v_n = v$. Then
\begin{align*}
\norm{Tv}^2 &= \lim_{n\to\infty}\norm{TT^*v_n}^2 = \lim_{n\to\infty}\inner{TT^*v_n,TT^*v_n} \\
&= \lim_{n\to\infty}\inner{(TT^*)^2v_n,v_n} = \lim_{n\to\infty}\inner{TT^*v_n,v_n} \\
&= \lim_{n\to\infty}\inner{T^*v_n,T^*v_n} = \lim_{n\to\infty}\norm{T^*v_n}^2 = \norm{v}^2,
\end{align*}
so $T$ is a partial isometry.

$\boxed{(5,6)}$ Applying the proposition to $T^*$ instead of $T$ yields the equivalences with $T=TT^*T$, and thus with the rest of the statements.

TODO + $\im(T^*) = \ker(T)^\perp$ means support and range are exchanged between $T$ and $T^*$.
\end{proof}

\begin{definition}
Let $T$ be a partial isometry. We call
\begin{itemize}
\item $T^*T$ the \udef{support projection} or \udef{initial projection} of $T$;
\item $TT^*$ the \udef{range projection} or \udef{final projection} of $T$.
\end{itemize}
\end{definition}

\begin{proposition}
Let $H,H'$ be Hilbert spaces with $K\subseteq H$ and $L\subseteq H'$ closed subspaces. Then the following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry with support $K$ and range $L$;
\item $(T,T^*)$ is a Galois connection between $\sSet{H, \perp_K}$ and $\sSet{H', \perp_L}$.
\end{enumerate}
Here $\perp_K$ is defined by
\[ x \perp_K y \quad\defequiv\quad P_K(x)\perp P_{K}(y). \]
\end{proposition}
\begin{proof}
The direction $(2) \Rightarrow (1)$ is immediate from \ref{partialIsometryEquivalences}, because $T,T^*$ are generalised inverses.

For the other direction, we first prove $T$ preserves the relational structure. Take arbitrary $x= x_1+x_2$ and $y=y_1+y_2$ in $K\oplus K^\perp$ such that $x\perp_K y$. Then
\[ \inner{T(x), T(y)} = \inner{T(x_1), T(y_1)} = \inner{x_1, y_1} = 0. \]
So $T(x)\perp T(y)$ and, because $T(x), T(y) \in L$, we have $T(x)\perp_L T(y)$. The argument for $T^*$ is similar.

For the Galois condition, we need to show that $T^*T(x)\perp_K y \implies x\perp_K y$. Indeed
\begin{align*}
T^*T(x)\perp_K y &\iff T^*T(x_1)\perp y_1 \\
&\iff 0= \inner{T^*T(x_1), y_1} = \inner{T(x_1), T(y_1)} = \inner{x_1,y_1} \\
&\iff P_K(x)\perp P_K(y).
\end{align*}
\end{proof}
\begin{corollary}
Let $T: H\to H'$ be a partial isometry with support $K$ and range $L$. Then
\[ T(x) \perp P_L(y) \iff P_K(x) \perp T^*(y) \]
for all $x\in H, y\in H'$.
\end{corollary}
\begin{proof}
This is the Galois identity \ref{GaloisIdentity}, although the direct proof is also very simple.
\end{proof}

\subsubsection{Unitaries}
\paragraph{Bilateral shifts}

\subsection{Friedrichs extension}
\begin{proposition}
Let $T: H\not\to K$ be a densely defined, positive, symmetric operator between Hilbert spaces. Then $T$ has positive self-adjoint extension.
\end{proposition}
\begin{proof}

\end{proof}


\section{Dirac notation}
\url{https://core.ac.uk/download/pdf/25263496.pdf}
\url{https://michael-herbst.com/talks/2014.07.22_Mathematical_Concept_Dirac_Notation.pdf}
\url{http://galaxy.cs.lamar.edu/~rafaelm/webdis.pdf}
\url{https://plato.stanford.edu/entries/qt-nvd/}
\url{file:///C:/Users/user/Downloads/Abdus%20Salam,%20E.P.%20Wigner%20(Ed.)%20-%20Aspects%20of%20Quantum%20Theory%20-%20Dedicated%20to%20Dirac%E2%80%99s%2070th%20Birthday-Cambridge%20University%20Press%20(1972).pdf}
\url{https://aip.scitation.org/doi/pdf/10.1063/1.1705001}

\begin{lemma}
\begin{enumerate}
\item $T\ketbra{\varphi}{\psi} = \ketbra{T\varphi}{\psi} = \ketbra{\varphi}{\psi}T = \ketbra{\varphi}{T^*\psi}$;
\item $\ketbra{\varphi}{\psi}\ketbra{\xi}{\eta} = \inner{\psi, \xi}\ketbra{\varphi}{\eta}$;
\item $(\ketbra{\varphi}{\psi})^* = \ketbra{\psi}{\varphi}$.
\end{enumerate}
\end{lemma}

\section{Bounded operators on Hilbert spaces}

\subsection{Finite-rank operators}
\begin{proposition}[Finite rank singular value decomposition] \label{finiteRankSingularValues}
Let $V$ be an inner product space and $T\in\Hom(V)$. Then $T$ is a finite-rank operator \textup{if and only if} $T$ can be written in the form
\[ T = \sum_{i=1}^N \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^N$ and $(w_i)_{i=1}^N$ are orthonormal sets of vectors and $(\lambda_i)_{i=1}^N$ are positive (non-zero) numbers.

The numbers $(\lambda_i)_{i=1}^N$ in this decomposition are uniquely determined by the operator.
\end{proposition}
The numbers $(\lambda_i)_{i=1}^N$ are called the \udef{singular values} of the operator.
\begin{proof}
Because $\im(T)$ is finite-dimensional, we can find an orthonormal basis $(v_i)_{i=1}^N$ for it. Then $T$ can be written as $T(x) = \sum_{i=1}^N c_i(x)v_i$ for some linear functionals $c_i$. By \ref{rieszRepresentation} these functionals can be uniquely written in the form $\lambda_i\bra{w_i}$ where $w_i$ is a unit vector and $\lambda_i$ is positive (it is the norm of the Riesz vector). This gives the claimed form of $T$. We just need to show that $(w_i)_{i=1}^N$ is an orthogonal set and the values of $(\lambda_i)_{i=1}^N$ do not depend on the chosen basis $(v_i)_{i=1}^N$.

For \undline{orthogonality} of $(w_i)_{i=1}^N$: let $i\neq j$, then orthogonality of $(v_i)_{i=1}^N$ implies $c_i(w_j) = 0$, which in turn implies $\inner{w_i,w_j} = 0$.

For \undline{uniqueness} of $(\lambda_i)_{i=1}^N$: we claim it is equal to $L = \setbuilder{\sqrt{\norm{T(v)}}}{v\in\im(T)\land\norm{v}=1}$, which is independent of the choice of $(v_i)_{i=1}^N$. TODO!!!!
\end{proof}
\begin{corollary}
Every finite rank operator on a Hilbert space is a finite sum of rank-1 operators.
\end{corollary}

\begin{lemma}
Let $H$ be Hilbert space. The set of finite rank operators on $H$ is a $*$-ideal in $H$.
\end{lemma}

\subsection{Compact operators}
\begin{proposition}
Let $T\in\Bounded(H)$. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item $T^*$ is compact;
\item there exists a sequence $(T_n)_{n\in\N}$ of finite rank operators such that $\norm{T-T_n}\to 0$.
\end{enumerate}
\end{proposition}
This is false in Banach spaces. (TODO Enflo)
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Any compact operator $T$ on a Hilbert space $\mathcal{H}$ can be written in the form
\[ T = \sum_{i=1}^\infty \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^\infty$ and $(w_i)_{i=1}^\infty$ are orthonormal sets and $(\lambda_i)_{i=1}^\infty$ is a sequence of positive numbers with $\lim_{i\to\infty}\lambda_i = 0$.
\end{corollary}
As in \ref{finiteRankSingularValues} for finite-rank operators we call $(\lambda_i)_{i=1}^\infty$ the \udef{singular values} of $T$. They are uniquely determined by the operator.
\begin{proof}
TODO
\end{proof}

\begin{lemma}
Let $H$ be a Hilbert space. Then the set of compact operators on $H$, $\Compact(H)$ is a $*$-ideal of $H$. 
\end{lemma}

\begin{proposition}
Let $H$ be a Hilbert space with orthonormal basis $(e_i)_{i\in I}$. If $T\in\Bounded(H)$ and
\[ \sum_{i\in I}\norm{Te_i}^2  < \infty, \]
then $T$ is a compact operator. + Converse??
\end{proposition}
\begin{proof}
TODO + weaken $T\in\Bounded(H)$?
\end{proof}
\begin{corollary}
An integral operator defined by a square integrable kernel $K\in L^2(A\times A, \mu)$ is compact.
\end{corollary}

\begin{proposition}
Let $T$ be an operator on a Hilbert space. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item for all sequences $\seq{x_n}$, weak convergence $x_n \overset{w}{\to} x$ implies the strong convergence $Ax_n \to Ax$;
\item for any two weakly convergent sequences $x_n\overset{w}{\to} x$ and $y_n\overset{w}{\to} y$ the energy form is continuous in both arguments:
\[ \lim_{n\to\infty}\inner{x_n,y_n}_T = \lim_{n\to\infty}\inner{x_n,Ty_n} = \inner{x,Ty} = \inner{x,y}_T. \]
\end{enumerate} 
\end{proposition}

\begin{lemma}
Let $H$ be a Hilbert space and $P\in\Projections(H)$. If $P$ is compact, then $P$ has finite rank.
\end{lemma}

\subsubsection{The real spectral theorem}
\subsubsection{The complex spectral theorem}

\subsection{Positive operators}

\subsubsection{Polar decomposition}
\begin{proposition}
Let $H$ be a Hilbert space and $T\in \Bounded(H)$. There exists a unique partial isometry $V$ such that $T = V|T|$ and $\ker(V) = \supp(T)$.
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{lemma}
There is only one positive operator $A$ such that $T = VA$ for some partial isometry.
\end{lemma}
\begin{proof}
TODO uniqueness positive squared root.
\end{proof}

\url{https://encyclopediaofmath.org/wiki/Polar_decomposition}

\section{Unbounded operators}

\section{Dilation theory}
\subsection{Dilations, $N$-dilations and power dilations}
\begin{definition}
Let $\mathcal{H} \subseteq \mathcal{H}'$ be Hilbert spaces and let $P_\mathcal{H}$ be the projection on $\mathcal{H}$. If a pair of linear maps $S: \mathcal{H}'\to\mathcal{H}'$ and $T: \mathcal{H}\to \mathcal{H}$ satisfy the relation
\[ T = P_\mathcal{H} S |_\mathcal{H} \]
then $T$ is called a \udef{compression} of $S$ and $S$ a \udef{dilation} of $T$. This is abbreviated $T\prec U$.

\begin{itemize}
\item Let $N\in\N$. If $T^k = P_\mathcal{H} S^k |_\mathcal{H}$ for all $k\leq N$, then $S$ is called an \udef{$N$-dilation}.
\item If this holds for all $k\in\N$, then $S$ is called a \udef{power dilation}.
\item If $T^* = P_\mathcal{H} S^* |_\mathcal{H}$, we call TODO??
\end{itemize}
We call $\mathcal{H}'$ \udef{minimal} if the only reducing subspace for $S$ that contains $\mathcal{H}$ is $\mathcal{H}'$.
\end{definition}

If $S$ is a dilation of $T$, then we clearly have $T = P_\mathcal{H} S P_\mathcal{H}|_\mathcal{H}$.

\begin{lemma}
Let $S:\mathcal{H}'\to\mathcal{H}'$ be an $N$-dilation of $T: \mathcal{H}\to \mathcal{H}$ and $p$ a polynomial of degree at most $N$. Then
\[ p(T) = P_\mathcal{H}p(S)|_\mathcal{H}. \]
\end{lemma}

Let $\mathcal{H}$ be a Hilbert space. We call $T\in\Bounded(\mathcal{H})$ a \udef{contraction} if $\norm{T}\leq 1$.
\begin{proposition} \label{dilationOfContraction}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
From $\norm{T}\leq 1$ (and the fact that $T^*T$ is normal), we have that $\vec{1}-T^*T\geq 0$ by spectral mapping. We can define $D_T = \sqrt{\vec{1}-T^*T}$. Then
\[ U = \begin{pmatrix}
T & D_{T^*} \\ D_T & -T^*
\end{pmatrix} \]
is a dilation of $T$ and it is unitary:
\begin{align*}
UU^* &= \begin{pmatrix}
TT^* + D_{T^*}^2 & TD_T^* - D_{T^*}T \\
D_TT^* - T^*D_{T^*}^* & D^2_{T} + T^*T
\end{pmatrix} = \begin{pmatrix}
\vec{1} & TD_T - D_{T^*}T \\
D_TT^* - T^*D_{T^*} & \vec{1}
\end{pmatrix} \\
U^*U &= \begin{pmatrix}
T^*T + D_{T}^2 & T^*D_{T^*} - D_{T}^*T^* \\
D_{T^*}^*T - TD_{T} & D^2_{T^*} + TT^*
\end{pmatrix} = \begin{pmatrix}
\vec{1} & T^*D_{T^*} - D_{T}T^* \\
D_{T^*}T - TD_{T} & \vec{1}.
\end{pmatrix}
\end{align*}
We have used that $D_T$ is self-adjoint for all contractions $T$. We just need to show that $TD_T = D_{T^*}T$. Clearly we have
\[ T(D_T)^2 = T(\vec{1} - T^*T) = T - TT^*T = (\vec{1} - TT^*)T = (D_{T^*}T)^2T. \]
By functional-like calculus (TODO!!) we have $TD_T = D_{T^*}T$.
\end{proof}
The operator $D_T$ in the previous proof is sometimes called the \udef{defect operator} of $T$. It measures in some sense how far $T$ is from being a unitary operator. If $T$ is unitary, then $D_T = 0 = D_{T^*}$. If $T$ is an isometry, then $D_T = 0$ (by \ref{isometryRangeProjection}) and $D_{T^*}$ is a projector ($TT^*$ is a projector by \ref{isometryCharacterisation}, so $\vec{1} - TT^*$ is too by \ref{projectorOrthogonalComplement} and $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$).

\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every isometry $T$ on $\mathcal{H}$ has a unitary power dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
Consider the unitary dilation of \ref{dilationOfContraction}. When $T$ is an isometry this reduces to
\[ U = \begin{pmatrix}
T & D_{T^*} \\ 0 & -T^*
\end{pmatrix} = \begin{pmatrix}
T & \vec{1}-TT^* \\ 0 & -T^*
\end{pmatrix}, \]
where we have used that $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$ is a projector.

Now for all $n\in\N$ we have $U^n = \begin{pmatrix}
T^n & * \\ 0 & (-T^*)^n
\end{pmatrix}$, so in particular $P_\mathcal{H}U^n|_\mathcal{H} = T^n$, meaning $U$ is a power dilation of $T$. 
\end{proof}

\begin{lemma}
Let $T$ a contraction on a Hilbert space $\mathcal{H}$. Then $V_T: \mathcal{H} \to \mathcal{H}\oplus\mathcal{H}: x\mapsto (Tx, D_Tx)$ is an isometry.
\end{lemma}
\begin{proof}
For all $x\in \mathcal{H}$ we have
\[ \norm{V_Tx} = \sqrt{\norm{Tx}^2 + \norm{D_Tx}^2} = \sqrt{\inner{Tx,Tx} + \inner{D_Tx,D_Tx}} = \sqrt{\inner{T^*Tx,x} + \inner{D_T^2x,x}} = \sqrt{\inner{x,x}} = \norm{x}. \]
\end{proof}

\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\}^N \subseteq \mathcal{H}^{N+1}$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary $N$-dilation $U$ on $\mathcal{H}^{N+1}$.
\end{proposition}
\begin{proof}
Let $U'$ be a unitary dilation of $T$ on $\mathcal{H}^2$. Let $C_1 = U'_{-,1}$ and $C_2 = U'_{-,2}$ denote the columns. Then
\[ U = \begin{pmatrix}
C_1 & \mathbb{0}^{2\times N-1} & C_2 \\
\mathbb{0}^{N-1\times 1} & \mathbb{1}^{N-1\times N-1} & \mathbb{0}^{N-1\times 1}
\end{pmatrix} \]
is unitary by
\[ \begin{pmatrix}
C_1^* & \mathbb{0} \\
\mathbb{0} & \mathbb{1} \\
C_2^* & \mathbb{0}
\end{pmatrix}\begin{pmatrix}
C_1 & \mathbb{0} & C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0}
\end{pmatrix} = \begin{pmatrix}
C_1^*C_1 & \mathbb{0} & C_1^*C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0} \\
C_2^*C_1 & \mathbb{0} & C_2^*C_2
\end{pmatrix} = \mathbb{1}^{N+1\times N+1}. \]
We just need to show that the (1,1)-component of $U^k$ is $T^k$ for all $k\in 1:N$. In order to perform the multiplication, we rewrite $U$ such that the row and column partitions are the same, i.e.\ $(2|(N-3)|2)\times (2|(N-3)|2)$:
\[ U = \begin{pmatrix}
\begin{bmatrix}
T & 0 \\ D_T & 0
\end{bmatrix} & \mathbb{0} & \begin{bmatrix}
0 & D_{T^*} \\ 0 & -T^*
\end{bmatrix} \\
\begin{bmatrix}
0 & 1 \\ \mathbb{0} & \mathbb{0}
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 0 \\ \mathbb{1} & \mathbb{0}
\end{bmatrix} & \mathbb{0} \\
\begin{bmatrix}
0 & 0 \\ 0 & 0
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 1 \\ \mathbb{0} & 0
\end{bmatrix} & \begin{bmatrix}
0 & 0 \\ 1 & 0
\end{bmatrix}
\end{pmatrix} \]
TODO
\end{proof}

\begin{proposition}[von Neumann's inequality]
Let $T$ be a contraction on some Hilbert space $\mathcal{H}$. Then, for every polynomial $p\in\C[z]$,
\[ \norm{p(T)}\leq \sup_{|z|=1}|p(z)|. \]
\end{proposition}
\begin{proof}
Suppose the degree of $p$ is $N$. Let $U$ be a unitary $N$-dilation of $T$. Then
\[ \norm{p(T)} = \norm{P_\mathcal{H}p(U)|_\mathcal{H}}\leq \norm{p(U)} = \sup_{z\in\sigma(U)}|p(z)| \leq \sup_{|z|=1}|p(z)| \]
since the spectrum of $U$ is contained in the unit circle.
\end{proof}

\begin{theorem}[Sz.-Nagy's dilation theorem]
Let $\mathcal{H} \subseteq \ell^2(\N)\otimes\mathcal{H}$ be Hilbert spaces. Every contraction on $\mathcal{H}$ has a unitary power dilation on $\ell^2(\N)\otimes\mathcal{H}$.
\end{theorem}




\section{Constructions}
\subsection{Direct sum}
\subsection{Tensor product}
\url{https://web.ma.utexas.edu/mp_arc/c/14/14-2.pdf}



\chapter{Types of operators}
\section{Fredholm operators}
\begin{definition}
An operator $T\in\Bounded(X,Y)$ between Banach spaces is called a \udef{Fredholm operator} if $T$ has a finite-dimensional kernel and cokernel.

The \udef{Fredholm index} of $T$ is defined as
\[ \Index T \defeq \dim\ker T - \dim\coker T.  \]

We denote the space of Fredholm operators from $X$ to $Y$ as $\Fred(X,Y)$. If $X=Y$, we write $\Fred(X)$.
\end{definition}

\begin{example}
\begin{enumerate}
\item If $X=Y$ is finite-dimensional, then all operators are Fredholm with index $0$.
\item The left shift $S_l:\ell^2(\N)\to\ell^2(\N): (x_n)_n\mapsto (x_{n+1})_n$ has index $1$.
\item The right shift $S_r = S_l^*$ has index $-1$.
\end{enumerate}
\end{example}

\begin{lemma}
A Fredholm operator has closed range.
\end{lemma}

\begin{lemma}
Let $T\in\Bounded(H)$ be a bounded operator on a Hilbert space. Then $\dim\coker T = \dim\ker T^*$.
\end{lemma}
\begin{proof}
TODO (is it correct?) $\ker(T^*) = \im(T)^\perp$.
\end{proof}


\begin{proposition}
Let $S,T\in\Fred(X)$, $\lambda\in\F$ and $K\in\Compact(X)$. Then
\begin{enumerate}
\item $\Index(ST) = \Index(S)+\Index(T)$;
\item $\Index(T+K) = \Index(T)$;
\item $\Index(\lambda T) = \Index(T)$, if $\lambda \neq 0$;
\item $\Index(T) = 0$ \textup{if and only if} $T=K'+L$ for some compact $K'$ and invertible $L$.
\end{enumerate}
Let $T\in\Fred(H)$ for some Hilbert space $H$. Then
\begin{enumerate} \setcounter{enumi}{4}
\item $\Index(T^*) = -\Index(T)$.
\end{enumerate}
\end{proposition}
TODO: integrate with corollary??

\begin{lemma}
Let the commutative diagram
\[ \begin{tikzcd}
0 \rar & X \dar{T} \rar & Y \dar{S} \rar & Z \dar{R} \rar & 0 \\
0 \rar & X \rar & Y \rar & Z \rar & 0
\end{tikzcd} \]
have short exact rows. If any two of $T,S,R$ are Fredholm, then so is the third and
\[ \Index S = \Index T + \Index R. \]
\end{lemma}
\begin{proof}
TODO snake lemma to obtain long exact
\[ 0\to \ker T \to \ker S\to \ker R \to \coker T \to \coker S \to \coker R \to 0. \]
\end{proof}
\begin{corollary} \mbox{}
\begin{enumerate} 
\item Let $T\in\Fred(X)$ and $S\in\Fred(Y)$ be Fredholm, then so is $T\oplus S$ with
\[ \Index(T\oplus S) = \Index(T)+\Index(S). \]
\item Let $T\in\Fred(X,Y)$ and $S\in\Fred(Y,Z)$ be Fredholm, then so is $ST$ with
\[ \Index(ST) = \Index(T)+\Index(S). \]
\item Let $K\in\Compact(X)$ be compact, then $\id_X+K$ is Fredholm with
\[ \Index(\id_X+K) = 0. \]
\end{enumerate}
\end{corollary}


\begin{lemma}[Fredholm alternative] \label{FredholmAlternative}
Let $T$ be a Fredholm operator of index zero. Then either $T$ is bijective, or it is neither injective nor surjective.
\end{lemma}
\begin{proof}
The operator $T$ is injective iff $\dim\ker(T) = 0$ and surjective iff $\dim\coker(T) = 0$.
\end{proof}


\section{Integral operators and transforms}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. Then an \udef{integral operator} or \udef{integral transform} is a map of the form
\[ T: U\subset (\Omega\to\C) \to (\Omega\to\C): f \mapsto \int_\Omega K(x,y)f(y) \diff{\mu(y)} \]
where $K\in (\Omega\times \Omega \to \C)$ is the \udef{kernel} or \udef{nucleus} of $T$.

The kernel is called
\begin{itemize}
\item \udef{symmetric} if $K(x,y) = \overline{K(y,x)}$;
\item \udef{Volterra} if $\Omega = \R$ and $K(x,y) = 0$ for $y>x$;
\item \udef{convolutional} if $\Omega$ is a group and $K(x,y) = F(x-y)$ for some function $F$;
\item \udef{Hilbert-Schmidt} if $K\in L^2(\Omega\times \Omega)$, i.e.\
\[ \int_{\Omega\times \Omega}|K(x,y)|^2\diff{x}\diff{y} < \infty; \]
\item \udef{singular} if $K(x,y)$ is unbounded on $\Omega\times \Omega$.
\end{itemize}
\end{definition}

\begin{lemma}
Hilbert-Schmidt integral operators are compact operators on $L^2(\Omega\times \Omega)$.
\end{lemma}
\begin{proof}
A Hilbert-Schmidt integral operator $T$ maps $L^2(\Omega)$ to $L^2(\Omega)$ functions:
\begin{align*}
\norm{Tu}^2_{L^2} &= \int_\Omega \left|\int_{\Omega} K(x,y)u(y)\diff{\mu(y)}\right|^2\diff{\mu(x)} \\
&\leq \int_\Omega \left(\int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg)\diff{\mu(x)} \\
&= \left(\int_\Omega \int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\diff{\mu(x)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg) < \infty
\end{align*}
where we have used the Cauchy-Schwarz inequality. This also immediately shows Hilbert-Schmidt integral operators are bounded.

TODO Compact
\end{proof}

\begin{proposition}
Let $T$ be an integral operator with kernel $K(x,y)$, then $T^*$ is the integral operator with kernel $\overline{K(y,x)}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A$ be a Borel set and $K:A\times A\to \C$ a measurable function such that the integral operator with kernel $K$ is bounded. Then the adjoint of the integral operator is again an integral operator with kernel $K^*(x,y) = \overline{K(y,x)}$.
\end{proposition}

\begin{proposition}
Let $T$ be a Volterra integral operator. Then $\sigma(T) = \sigma_\text{c}(T) = \{0\}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsection{Integral equations}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. An \udef{integral equation} is an equation containing an unknown function on $\Omega$ and an integral over $\Omega$.

An integral equation is 
\begin{itemize}
\item \udef{of the first kind} if it is of the form
\[ \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function and $u$ is the unknown function;
\item \udef{of the second kind} if it is of the form
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function, $\lambda$ is a scalar and $u$ is the unknown function.
\end{itemize}
\end{definition}

\begin{proposition}
Let
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x)\]
be an integral equation of the second kind. This integral equation has a unique solution $u$ if
\[ |\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}. \]
\end{proposition}
\begin{proof}
Let the map $T$ be defined by
\[ T(u) = x\mapsto \frac{1}{\lambda}\left(\int_\Omega K(x,y)u(y)\diff{\mu(y)} + f(x)\right) \]
so that solutions of the integral equation are exactly the fixed points of $T$. Then
\[ \norm{Tu-Tv}_\infty = \sup_{x\in\Omega} \frac{1}{|\lambda|} \left|\int_\Omega K(x,y)(u(y)- v(y))\diff{\mu(y)}\right| \leq \frac{1}{|\lambda|} \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)} \cdot \norm{u-v}_\infty. \]
So $T$ is a contraction if $|\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}$. The result follows from \ref{contractionFixedPoint}.
\end{proof}

\section{Convolution operators}




\chapter{Spectral theory}
\section{Invariant subspaces}
\begin{definition}
Let $L\in \Hom(V)$ be an endomorphism. A subspace $U$ of $V$ is \udef{invariant} under $L$ if $T|_U$ is an endomorphism on $U$. In other words, $u\in U$ implies $Tu\in U$.
\end{definition}
Clearly this definition only works for endomorphisms, not for linear maps in general. This is true for the rest of the theory about eigenvalues and eigenvectors.
\begin{example}
Let $L\in \Hom(V)$. The following are invariant under $L$:
\begin{itemize}
\item $\{0\}$;
\item $\ker L$;
\item $\im L$.
\end{itemize}
\end{example}

\section{The spectrum}
TODO: eigenvalue problem $Lx = \lambda x$

generalised eigenvalue problem $Lx = \lambda T x$

nonstandard eigenvalue problem $A(\gamma)x = 0$.

TODO: consistency $\lambda \id - L$, not $L-\lambda \id$.
TODO: everything is now in $\C$.

\begin{definition}
Let $L: \dom(L)\subset V \to V$ be an operator on a complex vector space $V$.

For $\lambda\in\C$ the \udef{resolvent} $R_\lambda(L)$ is defined as
\[ R_\lambda(L) \defeq (\lambda \id_V - L)^{-1}: \im(\lambda \id_V - L)\to\dom(L), \]
if this inverse exists (i.e.\ if $\lambda \id_V - L$ is injective).

The \udef{resolvent set} $\rho(L)$ is the set
\[ \rho(L) \defeq \setbuilder{\lambda\in \C}{\text{$R_\lambda(L)$ exists, has domain $V$ and is bounded}}. \]
\end{definition}

\begin{lemma}[Resolvent identity]
Let $T$ be a linear operator and $\lambda,\mu\in\C$ such that $R_\lambda(T), R_\mu(T)$ exist. Then $R_\lambda(T)$ and $R_\mu(T)$ commute and
\[ R_\lambda(T) - R_\mu(T) = (\mu-\lambda)R_\lambda(T)R_\mu(T). \]
\end{lemma}
\begin{proof}
The commutativity of the resolvents follows from \ref{commutationInverse}.

We calculate
\begin{align*}
R_\lambda(T) - R_\mu(T) &= R_\lambda(T)R_\mu(T)(\mu\id - T) - R_\mu(T)R_\lambda(T)(\lambda\id - T) \\
&= \mu R_\lambda(T)R_\mu(T) - R_\lambda(T)R_\mu(T) T - \lambda R_\mu(T)R_\lambda(T) + R_\mu(T)R_\lambda(T) T \\
&= (\mu - \lambda)R_\lambda(T)R_\mu(T).
\end{align*}
\end{proof}

\begin{definition}
Let $L: \dom(L)\subset V \to V$ be an operator on a complex vector space $V$.

The \udef{spectrum} of $L$ is the complement of the resolvent set: $\sigma(L) \defeq \C\setminus\rho(L)$.
\begin{itemize}
\item The \udef{point spectrum} or \udef{discrete spectrum} $\sigma_\text{p}(L)$ contains the values of $\lambda$ where $\lambda \id_V - L$ fails to be injective. These values are called the \udef{eigenvalues} of $L$.

We call $\ker(\lambda \id_V - L)$ the \udef{multiplicity space} and $\dim\ker(\lambda \id_V - L)$ the \udef{multiplicity} of $\lambda$.
\item The \udef{continuous spectrum} $\sigma_\text{c}(L)$ is the set of all values of $\lambda$ such that $\lambda \id_V - L$ is injective, $\overline{\im(\lambda \id_V - L)} = V$, but $\im(\lambda \id_V - L) \neq V$.
\item The \udef{residual spectrum} $\sigma_\text{r}(L)$ is the set of all values of $\lambda$ such that $\lambda \id_V - L$ is injective, but $\overline{\im(\lambda \id_V - L)} \neq V$.

We call $\im(\lambda \id_V - L)^\perp$ the \udef{deficiency subspace} and $\dim(\im(\lambda \id_V - L)^\perp)$ the \udef{deficiency} of $\lambda$.
\end{itemize}
The sets $\sigma_\text{p}(T), \sigma_\text{c}(T)$ and $\sigma_\text{r}(T)$ are disjoint.
\end{definition}
In finite dimensions we know that
\[ \text{$\lambda \id_V - L$ is surjective} \quad\iff\quad \text{$\lambda \id_V - L$ is injective} \]
and all linear operator are bounded.
So in this case there can only ever be a point spectrum.

\begin{proposition}
Let $X$ be a Banach space and $T$ a closed linear operator on $X$. Then $\lambda \in \sigma(T)$ \textup{if and only if} $\lambda \id_X - T: \dom(T) \to V$ is not bijective.
\end{proposition}
\begin{proof}
If $\lambda \id_X - T$ is not bijective, then clearly $\lambda \in \sigma(T)$.

Conversely, assume $\lambda \id_X - T$ is bijective. Then $(\lambda \id_X - T)^{-1}: X\to \dom(T)$ is closed by \ref{algebraClosedOperators} and has as domain a Banach space, so it is bounded by the closed graph theorem \ref{closedGraphTheorem}.
\end{proof}
TODO: redefine continuous spectrum?? Such that $\sigma(T) = \sigma_\text{p}(T) \cup \sigma_\text{c}(T) \cup \sigma_\text{r}(T)$ by definition.
\begin{corollary}
Let $T$ a closed linear operator on a Banach space. Then
\[ \sigma(T) = \sigma_\text{p}(T) \cup \sigma_\text{c}(T) \cup \sigma_\text{r}(T). \]
\end{corollary}
\begin{corollary}
Let $K$ be a compact operator on a Banach space. Then $\sigma(K) = \sigma_\text{p}(K)\setminus\{0\}$.
\end{corollary}
\begin{proof}
For all $\lambda\neq 0$, we have that $\lambda\id - K$ is Fredholm with index zero (and thus bounded). Then by the Fredholm alternative \ref{FredholmAlternative} $\lambda\id - K$ is either bijective or neither injective nor surjective, meaning $\lambda$ is either in $\rho(T)$ or in $\sigma_\text{p}(T)$. 
\end{proof}

\begin{proposition} \label{spectrumCompactOperator}
Let $K$ be compact. Then
\begin{enumerate}
\item $\ker(\lambda\id- K)$ is finite dimensional for all $\lambda\in\sigma(K)\setminus\{0\}$;
\item for $\alpha > 0$ the number of eigenvalues $\lambda$ such that $|\lambda|\geq \alpha$ is finite;
\item $0$ is the only accumulation point??
\end{enumerate}
TODO
\end{proposition}

\begin{proposition}
Let $T$ be a bounded operator on a Banach space $X$. For $|\lambda|>\norm{T}$ the resolvent $R_\lambda(T)$ is bounded and given by
\[ R_\lambda(T) = (\lambda \id_X - T)^{-1} = \sum_{n=0}^\infty\frac{T^n}{\lambda^{n+1}} \]
with uniform convergence. The norm is bounded by
\[ \norm{R_\lambda(T)} = \norm{(\lambda \id_X - T)^{-1}} \leq \frac{1}{|\lambda|-\norm{T}}. \]
\end{proposition}
\begin{proof}
The operator $T/\lambda$ is a contraction so the Neumann series \ref{operatorNeumannSeries} gives
\[ \left(\id - \frac{T}{\lambda}\right)^{-1} = \sum_{n=0}^\infty \left(\frac{T}{\lambda}\right)^n.  \]
Now $(\lambda \id_X - L)^{-1} = \frac{1}{\lambda}\left(\id - \frac{T}{\lambda}\right)^{-1} = \sum_{n=0}^\infty \frac{T^n}{\lambda^{n+1}}$. The Neumann series also gives us the bound on the norm.
\end{proof}
\begin{corollary}
Let $T$ be a bounded operator on a Banach space. Then $\sigma(T)\subset [-\norm{T}, \norm{T}]$.
\end{corollary}

For the point spectrum a simpler argument leads to $\sigma_\text{p}(T)\subset [-\norm{T}, \norm{T}]$: let $\lambda$ be an eigenvalue with eigenvector $x$. Then
\[ |\lambda|\;\norm{x} = \norm{\lambda x} = \norm{Tx} \leq \norm{T}\;\norm{x}. \]

\begin{proposition}
Let $T$ be a closed invertible linear operator on a Banach space. Let $S$ be a bounded operator with $\norm{S} < \norm{T^{-1}}^{-1}$, then $T+S$ is invertible.
\end{proposition}
\begin{proof}
Since $\norm{T^{-1}S} \leq \norm{T^{-1}}\;\norm{S} < 1$, it follows from the Neumann series \ref{operatorNeumannSeries} that $\id + T^{-1}S$ has a bounded inverse so that $(\id + T^{-1}S)^{-1}T^{-1}$ exists and is bounded. Then $(\id + T^{-1}S)^{-1}T^{-1} = (T+S)^{-1} = T^{-1}(\id + ST^{-1})$.
\end{proof}
\begin{corollary}
Let $T$ be a closed linear operator on a Banach space. Then $\sigma(T)$ is closed and $\rho(T)$ is open.
\end{corollary}
\begin{proof}
Let $\lambda \in \rho$ such that $(\lambda\id - T)^{-1}$ is bounded. For all $\epsilon \in B(0, 1/\norm{(\lambda\id - T)^{-1}})$ we have that $(\lambda\id - T) + \epsilon\id = (\lambda +\epsilon)\id -T$ has bounded inverse, so $\lambda + \epsilon \in \rho(T)$.
\end{proof}
\begin{corollary}
Let $T$ be a bounded linear operator on a Banach space. Then $\sigma(T)$ is compact.
\end{corollary}

\begin{lemma}
Let $L$ be an operator on a vector space $V$. If $\lambda \id_V - T$ is not bounded from below, then $\lambda \in \sigma(T)$.
\end{lemma}
\begin{proof}
If $\lambda \notin \sigma(T)$, then $R_\lambda(L)$ is bijective and bounded. By lemma \ref{boundedBelow}, $\lambda \id_V - T$ is bounded below.
\end{proof}


\begin{lemma}
Let $T:X\to X$ be an operator on a Banach space and $\lambda\in\sigma_\text{c}$, then $R_\lambda(T)$ is unbounded.
\end{lemma}
\begin{proof}
If $R_\lambda(T)$ is bounded, $\lambda \id_V - T$ then is bounded below by lemma \ref{boundedBelow} and has closed range by proposition \ref{boundedBelowClosedRange}.
\end{proof}

\begin{proposition}
Let $T$ be an invertible operator. Then for all $\lambda\neq 0$
\begin{enumerate}
\item $\lambda\in\rho(T) \iff \lambda^{-1}\in \rho(T^{-1})$;
\item $\lambda\in\sigma_\text{p}(T) \iff \lambda^{-1}\in \sigma_\text{p}(T^{-1})$.
\end{enumerate}
\end{proposition}
\begin{proof}
We calculate
\[ \lambda \id - A^{-1} = A^{-1}(\lambda A - \id) = \lambda A^{-1}(A - \frac{\id}{\lambda}). \]
\end{proof}

\subsection{Parts of the spectrum}
\subsubsection{The point spectrum: eigenvalue and eigenvectors}
In this section we study invariant subspaces with dimension $1$, i.e.\ subspaces $U= \Span\{v\}$ such that
\[ Lv = \lambda v. \]
\begin{definition}
Suppose $L\in \Hom_{\mathbb{F}}(V)$.
\begin{itemize}
\item  A scalar $\lambda\in \mathbb{F}$ is called an \udef{eigenvalue} of $L$ if there exists a $v\in V$ such that $v\neq 0$ and $Lv = \lambda v$.
\item Such a vector $v$ is called an \udef{eigenvector}.
\item The set of all eigenvectors associated with an eigenvalue $\lambda$ is called the \udef{eigenspace} $E_\lambda(L)$. Because
\[ E_\lambda(L) = \ker(L-\lambda \id_V) \]
it is indeed a vector space.

The dimension of $E_\lambda(L)$ is the \udef{geometric multiplicity} of $\lambda$.
\end{itemize}
\end{definition}
\begin{proposition}
Let $L\in \Hom_\mathbb{F}(V)$ and $\lambda\in \mathbb{F}$, then
\[ \text{$\lambda$ is an eigenvalue of $L$} \qquad \iff \qquad \text{$\lambda$ is in the point spectrum $\sigma_p(L)$.} \]
\end{proposition}
\begin{proof}
The equation $Lv = \lambda v$ is equivalent to $(L-\lambda \id_V)v = 0$.
\end{proof}

\begin{proposition}
Let $L\in\Hom(V)$ be an operator on some vector space. Suppose $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $L$ and $v_1,\ldots, v_m$ are corresponding eigenvectors. Then $\{v_1,\ldots, v_m\}$ is linearly independent.
\end{proposition}
\begin{proof}
The proof goes by contradiction. Assume $\{v_1,\ldots, v_m\}$ is linearly dependent. Let $k$ be the smallest positive integer such that
\[ v_k \in \Span\{v_1,\ldots, v_{k-1}\}. \]
So there exists a nontrivial linear combination
\[ v_k = a_1v_1+\ldots +a_{k-1}v_{k-1}. \]
Applying $L$ to both sides gives
\[ \lambda_kv_k = a_1\lambda_kv_1+\ldots +a_{k-1}\lambda_kv_{k-1}. \]
Multipliying the previous combination by $\lambda_k$ and subtracting both equations gives
\[ 0= a_1(\lambda_k-\lambda_1)v_1 +\ldots + a_{k-1}(\lambda_k - \lambda_{k-1})v_{k-1}. \]
By assumption of linear independence of $\{v_1,\ldots, v_{k-1}\}$ this combination must be trivial, however none of the $(\lambda_k-\lambda_i)$ can be zero, so all the $a_i$ must be zero. This is a contradiction with the assumption of linear dependence.
\end{proof}
\begin{corollary}
For each operator on $V$, the set of distinct eigenvalues has at most cardinality $\dim V$.
\end{corollary}
\begin{corollary}
Let $L\in\Hom(V)$. Suppose $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $L$. Then
\[ E_{\lambda_1}(L) \oplus \ldots \oplus E_{\lambda_m}(L) \]
is a direct sum. Furthermore, the sum of geometric multiplicities is less than or equal to the dimension of $V$:
\[ \dim E_{\lambda_1}(L) + \ldots + \dim E_{\lambda_m}(L) \leq \dim V. \]
\end{corollary}

\subsubsection{Approximate spectrum}
\begin{definition}
The set of all $\lambda$ such that $T-\lambda \id_V$ is not bounded from below is called the \udef{approximate point spectrum} $\sigma_\text{ap}$.

If $\lambda\in\sigma_\text{ap}(T)$, then $\lambda$ is an \udef{approximate eigenvalue} of $T$.
\end{definition}
\begin{lemma}
Let $T$ be an operator on a vector space $V$. Then $\lambda \in \sigma_\text{ap}(T)$ \textup{if and only if} there exists a sequence of unit vectors $(e_n)_{n\in\N}$ for which
\[ \lim_{n\to\infty}\norm{Te_n - \lambda e_n} = 0. \]
\end{lemma}
\begin{proof}
Assume there is such a sequence $(e_n)_{n\in\N}$. Then for all $\epsilon>0$, we can find a unit  vector $e_k$ such that $\norm{(T - \lambda \id_V)e_n} \leq \epsilon = \epsilon \norm{e_n}$. This is clearly not bounded below.

This other direction is just an inversion of this argument.
\end{proof}

\begin{lemma}
Let $T$ be an operator. Then $\sigma_\text{p}(T)\cup\sigma_\text{c}(T)\subset\sigma_\text{ap}(T) \subset \sigma(T)$.
\end{lemma}
\begin{proof}
First assume $\lambda\notin \sigma_\text{ap}(T)$, 
so $T-\lambda \id_V$ is bounded below. Then $T-\lambda \id_V$ is injective by \ref{boundedBelow} and $\lambda\notin\sigma_\text{p}(T)$. By proposition \ref{boundedBelowClosedRange} the range $\im(T-\lambda \id_V)$ is closed, so it cannot be a proper dense subset of $X$ and $\lambda\notin\sigma_\text{c}(T)$.

Now assume $\lambda \notin \sigma(T)$. Then $(T-\lambda \id_V)^{-1}$ is bounded, so its inverse $T-\lambda \id_V$ is bounded below by \ref{boundedBelow} and $\lambda\in \sigma_\text{ap}(T)$.
\end{proof}

\subsubsection{Residual spectrum}
\begin{proposition}
Let $L$ be a densely defined linear operator on a Hilbert space. If $\lambda$ is in the residual spectrum of $L$ with deficiency $m$, then $\overline{\lambda}$ is in the point spectrum of $L^*$ with multiplicity $m$.
\end{proposition}
\begin{proof}
By \ref{kernelImageAdjoint} we have
\[ \im(\lambda \id - L)^\perp = \ker(\lambda\id - L)^* = \ker(\overline{\lambda}\id - L^*). \]
\end{proof}

\subsubsection{Compression spectrum}
\begin{definition}
The set of $\lambda$ for which $T-\lambda I$ does not have dense range is the \udef{compression spectrum} $\sigma_\text{cp}(T)$ of $T$.
\end{definition}
Then $\sigma_\text{r}(T) = \sigma_\text{cp}(T)\setminus\sigma_\text{p}(T)$.

\subsubsection{The essential spectrum}
TODO \url{https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)#Classification_of_points_in_the_spectrum}


\subsection{The spectral radius}
\begin{definition}
The \udef{spectral radius} $r_\sigma(T)$ of a operator $T$ is given by
\[ r_\sigma(T) \defeq \sup_{\lambda\in\sigma(T)}|\lambda|. \]
\end{definition}


\subsection{The spectrum of operators on Hilbert spaces}
\begin{proposition}
Let $T \in \Bounded(H)$ for some Hilbert space $H$. Then
\begin{enumerate}
\item $\sigma(T) \neq \emptyset$;
\item $\rho(T) = \overline{\rho(T^*)}$, where the bar denotes complex conjugation.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Let $x,y\in H$ and define
\[ f(\lambda) = \inner{x,R_\lambda(T)y}. \]
If $\sigma(T) = \emptyset$, then $f$ is an entire function. Now
\[ \norm{R_\lambda(T)} \leq \frac{1}{|\lambda| - \norm{T}} \to 0 \quad\text{as}\quad |\lambda| \to \infty. \]
By Liouville's theorem (TODO ref) we must have $f\equiv 0$. Because the $x,y$ we arbitrary we must have $R_\lambda(T)y = 0$ for all $y\in H$, such that $R_\lambda(T)$ is not injective, which is impossible as it is an inverse.

(2) Take $\lambda\in\rho(T)$. Then
\[ ((\lambda\id - A)^{-1})^* = (\overline{\lambda}\id - A^*)^{-1} \]
so $\overline{\lambda}\in\rho(T^*)$ iff $((\lambda\id - A)^{-1})^*$ is bounded iff $(\lambda\id - A)^{-1}$ is bounded iff $\lambda\in \rho(T)$.
\end{proof}

\begin{lemma} \label{eigenspaceOrthogonalAdjoint}
Let $L$ be a densely defined operator on a Hilbert space $H$. Take $\lambda\in \sigma_\text{p}(L)$ and $\mu\in\sigma_\text{p}(L^*)$. If $\lambda \neq \overline{\mu}$, then
\[ \ker(\lambda\id - L)\perp \ker(\mu\id - L^*). \]
\end{lemma}
\begin{proof}
Take non-zero eigenvectors $x,y$ such that $Ax = \lambda x$ and $A^*y = \mu y$. Then
\[ \lambda \inner{y,x} = \inner{y,\lambda x} = \inner{y, Ax} = \inner{A^*y,x} = \inner{\mu y,x} = \overline{\mu}\inner{y,x}. \]
So we have $(\lambda - \overline{\mu})\inner{y,x} = 0$.
\end{proof}

\begin{proposition} \label{adjointSpectrumNoResidual}
Let $L$ be a densely defined operator on a Hilbert space $H$. Then the following are equivalent:
\begin{enumerate}
\item the residual spectrum of $L$ is empty;
\item $\overline{\sigma_\text{p}(L^*)} \subseteq \sigma_\text{p}(L)$;
\end{enumerate}
as are the following:
\begin{enumerate}
\item the residual spectrum of $L^*$ is empty;
\item $\sigma_\text{p}(L) \subseteq \overline{\sigma_\text{p}(L^*)}$.
\end{enumerate}
In particular all these statements hold if $L$ is normal.
\end{proposition}
\begin{proof}
Consider, for all $x\in \dom(L), y\in\dom(L^*)$, the equality
\[ \inner{(\lambda\id-L)x,y} = \inner{x,(\overline{\lambda}\id-L^*)y}. \]
We can make the following inferences:
\begin{itemize}
\item If $\lambda\in \overline{\sigma_\text{p}(L^*)}$, then the equality holds in particular for all eigenvectors $y$. This implies $\inner{(\lambda\id-L)x,y} = 0$. By \ref{perpToDenseSet} $\im(\lambda\id-L)$ may then not be dense, so it cannot be injective because the residual spectrum of $L$ is empty.
\item Assume $\lambda\id-L$ injective and take  $y\perp \im(\lambda\id-L)$. Then by the equality $\inner{x, (\overline{\lambda}\id - L^*)y} = 0$ for all $x\in\dom(L)$, which is dense. So $(\overline{\lambda}\id - L^*)y = 0$ by \ref{perpToDenseSet}. Now $\lambda\notin \sigma_\text{p}(L)$, so $\overline{\lambda}\notin \sigma_\text{p}(L^*)$. Thus $y = 0$ and $\im(\lambda\id-L)^\perp = \{0\}$, meaning $\im(\lambda\id-L)$ is dense.
\end{itemize}
The arguments for the second set of statements are similar.

If $L$ is normal, then $\ker(\lambda \id - L) = \ker{\overline{\lambda}\id -L^*}$ by \ref{equalityKernelAdjointNormal}, so $\sigma_\text{p}(L) = \overline{\sigma_\text{p}(L^*)}$.
\end{proof}

\begin{proposition}
Let $T$ be a closed, densly defined operator on a Hilbert space.
\begin{enumerate}
\item If $\lambda\in\rho(T)$, then $\overline{\lambda}\in\rho(T^*)$.
\item If $\lambda\in\sigma_\text{r}(T)$, then $\overline{\lambda}\in\sigma_\text{p}(T^*)$.
\item If $\lambda\in\sigma_\text{p}(T)$, then $\overline{\lambda}\in\sigma_\text{r}(T^*)\cup\sigma_\text{p}(T^*)$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO Compare with \ref{adjointSpectrumNoResidual}. CLosure necessary?
\end{proof}

\begin{proposition}
Let $T$ be a densely defined self-adjoint operator. Then
\begin{enumerate}
\item $\sigma(T) \subset \R$;
\item $\sigma_r(T) = \emptyset$;
\item let $\lambda_1,\lambda_2 \in \sigma_\text{p}(T)$ and $\lambda_1\neq \lambda_2$, then 
\[ \Null(\lambda_1\id - T) \perp \Null(\lambda_2 \id - T). \]
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $T$ be a unitary operator. Then
\begin{enumerate}
\item $\sigma_\text{r}(T) = \emptyset$;
\item $\sigma(T) \subset \setbuilder{\lambda\in\C}{|\lambda| = 1}$.
\end{enumerate}
\end{proposition}
TODO: move to more general place??

\begin{lemma}
The eigenvalues of a bounded dissipative linear operator
lie in the half-plane $\Im\lambda \geq 0$.
\end{lemma}

\subsubsection{Rayleigh quotient}
\begin{lemma}
Let $L$ be an operator on a Hilbert space. If $x$ is an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \lambda. \]
\end{lemma}
\begin{proof}
Let $x$ be an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \frac{\inner{x,Lx}}{\inner{x,x}} = \lambda \frac{\inner{x,x}}{\inner{x,x}} = \lambda. \]
\end{proof}

\begin{proposition}
If $U$ is unitary, then $\sigma(U)\subset \mathbb{T}$.
\end{proposition}

\subsubsection{Symmetric and self-adjoint operators}
\begin{proposition}
Let $T$ be a symmetric operator on a Hilbert space $H$. Then
\begin{enumerate}
\item the eigenvalues of $T$ are real;
\item the eigenvectors corresponding to distinct eigenvalues are orthogonal.
\end{enumerate}
\end{proposition}
\begin{proof}
This is an application of \ref{eigenspaceOrthogonalAdjoint} and \ref{adjointSpectrumNoResidual}.
\end{proof}

\subsubsection{Compact self-adjoint operators}
\begin{proposition}
Every compact self-adjoint operator $L$ on a nontrivial Hilbert space has an eigenvalue $\lambda$ with $|\lambda| = \norm{L}$.
\end{proposition}

\begin{proposition}
Let $A$ be a compact self-adjoint operator. Then the only possible accumulation point of $\sigma(A)$ is $0$.
\end{proposition}
TODO self-adjoint not necessary? See \ref{spectrumCompactOperator}?
\begin{proof}
Assume $\sigma(A)$ is infinite. Then take $\seq{\lambda_n}\subset \sigma(A)$. Any associated sequence $\seq{x_n}$ of eigenvectors is orthogonal. We can take it to be orthonormal. By \ref{limitCompactImageOrthonormalSequence} we have
\[ 0 = \lim_{n\to\infty} \norm{Ax_n}^2 = \lim_{n\to\infty}\inner{Ax_n,Ax_n} = \lim_{n\to\infty}\lambda_n^2\inner{x_n,x_n} = \lim_{n\to\infty}\lambda_n^2, \]
so $\seq{\lambda_n}$ converges to $0$.
\end{proof}

\begin{theorem}
Every spectral value $\lambda\neq 0$ of a compact self-adjoint linear
operator $A : H \to H$ is an eigenvalue of finite multiplicity that can only
accumulate at $\lambda = 0$. Conversely, a self-adjoint operator having these
properties is compact.
\end{theorem}
\begin{proof}
TODO See \ref{spectrumCompactOperator}
\end{proof}

\section{Multiplication operators}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. A \udef{multiplication operator} is an operator of the form
\[ T: L^p(\Omega, \mu) \to L^p(\Omega, \mu): u(x) \mapsto a(x)u(x) \]
for some $a\in L^\infty(\Omega,\mu)$
\end{definition}

\begin{proposition}
Let $T: L^p(\Omega, \mu) \to L^p(\Omega, \mu): u \mapsto a\cdot u$ be a multiplication operator. Then
\[ \norm{T} = \norm{a}_{L^\infty}. \]
\end{proposition}
\begin{proof}
From the inequality $\norm{Tu}_{L^p}\leq \norm{a}_{L^\infty}\norm{u}_{L^p}$ we get $\norm{T} \leq \norm{a}_{L^\infty}$.

TODO
\end{proof}

\begin{lemma}
Let $T: L^2(\Omega, \mu) \to L^2(\Omega, \mu): u \mapsto a\cdot u$ be a multiplication operator with $a\in L^\infty(\Omega,\mu)$. Then $T^*$ is the multiplication operator
\[ T^*: L^2(\Omega, \mu) \to L^2(\Omega, \mu): u \mapsto \overline{a}\cdot u. \]
\end{lemma}
\begin{proof}
From 
\[ \inner{Tu,v} = \int_\Omega a\cdot u \cdot \overline{v}\diff{\mu} = \int_\Omega u \cdot \overline{\overline{a}\cdot v}\diff{\mu} \]
it follows that $T^*v = \overline{a}\cdot v$.
\end{proof}
\begin{corollary}
Then
\begin{enumerate}
\item $T$ is self-adjoint if $a$ is real-valued;
\item $T$ is skew-adjoint if $a$ is purely imaginary;
\item $T$ is unitary if $|a(x)| \equiv 1$.
\end{enumerate}
\end{corollary}

Let $E_\lambda$ be the level set
\[ E_\lambda = \setbuilder{x\in\Omega}{a(x) = \lambda} \]

\begin{proposition}
Let $T: L^2(\Omega, \mu) \to L^2(\Omega, \mu): u\mapsto a\cdot u$ be a multiplication operator with $a\in \cont(\Omega)$. Then
\begin{enumerate}
\item $\sigma_\text{p}(T) = \setbuilder{\lambda\in \im(a)}{\mu(E_\lambda)>0}$;
\item $\sigma_\text{c}(T) = \setbuilder{\lambda\in \overline{\im(a)}}{\mu(E_\lambda) = 0}$;
\item $\sigma_\text{r}(T) = \emptyset$;
\item $\rho(T) = \C\setminus \overline{\im(T)}$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}

\section{The spectral theorem}
\url{https://link.springer.com/content/pdf/10.1007%2F978-1-4614-7116-5.pdf}

\url{http://individual.utoronto.ca/jordanbell/notes/SVD.pdf}
\url{https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=2133&context=etdr}



\chapter{Fourier transforms}

\section{Types of Fourier transform}

\subsection{Discrete Fourier transform}
\begin{definition}
Then $N$-dimensional \udef{discrete Fourier transform} (DFT) is the linear transformation $\C^N \to \C^N$ defined by the matrix $DFT_N$ with components
\[ [DFT_N]_{j,k} = \frac{1}{\sqrt{N}}\omega_N^{(j-1)(k-1)}, \]
where $\omega_N$ is the $N^\text{th}$ root of unity.
\end{definition}

\begin{lemma} \mbox{}
\begin{enumerate}
\item The $DFT_N$ matrix is the Vandermonde matrix of the roots of unity, up to the normalisation factor $1/\sqrt{N}$.
\item The $DFT_N$ matrix is unitary.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Just an observation.

(2) We calculate
\[ [DFT_N\cdot DFT_N]_{j,l} = \frac{1}{N}\sum_{k=1}^N\omega_N^{jk}\overline{\omega_N}^{kl} = \frac{1}{N}\sum_{k=1}^N\omega_N^{k(j-l)} = \delta_{j,l}. \]
\end{proof}
