\chapter{Vector space convergence}

\section{Vector space convergence}
\begin{definition}
Let $\sSet{\F,V,+}$ be a vector space and $\xi$ a convergence on $V$. Then $\sSet{\F,V,+, \xi}$ is a \udef{convergence vector space} if
\begin{itemize}
\item vector addition $+: V\times V \to V$ is continuous;
\item scalar multiplication $\cdot: \F\times V \to V$ is continuous.
\end{itemize}
\end{definition}

\begin{lemma}
If $\sSet{\F,V,+, \xi}$ is a convergence vector space, then $\sSet{V,+, 0, \xi}$ is a convergence group.
\end{lemma}
\begin{proof}
We just need to show that $v\mapsto -v$ is continuous, but this scalar multiplication and thus continuous by assumption.
\end{proof}

\begin{lemma} \label{continuityLemmaVectorConvergence}
If $\sSet{\F,V,+, \xi}$ is a convergence vector space, then
\begin{enumerate}
\item the function $\F \to \Span\{v\}: \lambda \mapsto \lambda\cdot v$ is a homeomorphism for all $v\in V$;
\item the function $V \to V: v \mapsto \lambda\cdot v$ is a homeomorphism (?) for all $\lambda\in \F$.
\end{enumerate}
\end{lemma}
\begin{proof}
The functions $\lambda \mapsto (\lambda, v)$ and $v \mapsto (\lambda, v)$ are continuous by \ref{continuousEmbeddingProduct}. Composition with the continuous scalar product gives the result by continuity of composition (\ref{continuityComposition}).

They are both clearly invertible (for the first, note that the kernel is $\{0\}$). The inverse of the second is of the same form and thus immediately continuous.

For the inverse of the first TODO!?!
\end{proof}

\begin{proposition}
Let $V$ be a vector space, $\{V_i\}_{i\in I}$ a set of convergence vector spaces and $\{L_i: V \to V_i\}_{i\in I}$ a set of linear maps. Then the initial convergence on $V$ w.r.t. $\{L_i: V \to V_i\}_{i\in I}$ makes $V$ a convergence vector space.
\end{proposition}
\begin{proof}
Continuity of vector addition follows from \ref{initialConvergenceGroup}.

We verify continuity of scalar multiplication $m: \F\times V \to V: (\lambda, v) \mapsto \lambda v$. Using \ref{characteristicPropertyInitialFinalConvergence}, we need to verify that $L_i\circ m$ is continuous for all $i\in I$. Because the $L_i$ are linear, we have
\[ L_i(\lambda v) = \lambda L_i(v) \]
for all $\lambda \in \F, v \in V$. This means that $L_i\circ m = m_i \circ L_i$, where $m_i$ is scalar multiplication in $V_i$, and thus continuous. So $L_i \circ m$ is continuous.
\end{proof}

\begin{proposition} \label{vectorSpaceConvergenceConstruction}
Let $V$ be a vector space over a field $\F$. And $\mathcal{F} \subseteq \powerfilters(V)$ a family of filters. There exists a vector space convergence $\xi$ on $V$ such that $\mathcal{F} = \lim^{-1}_\xi(0)$ \textup{if and only if}
\begin{enumerate}
\item if $F \in \mathcal{F}$ and $G\supseteq F$, then $G\in \mathcal{F}$;
\item if $F,G \in \mathcal{F}$, then $F + G\in \mathcal{F}$;
\item if $F\in \mathcal{F}$, then $\vicinity_\F(0)\cdot F \in \mathcal{F}$;
\item if $v\in V$, then $\vicinity_\F(0)\cdot v \in \mathcal{F}$;
\item if $F\in \mathcal{F}$ and $\lambda\in \F$, then $\lambda\cdot F \in \mathcal{F}$.
\end{enumerate}
\end{proposition}
Note the similarity with \ref{groupConvergenceConstruction} for convergence groups. A group convergence is completely determined by $\lim^{-1}_\xi(0)$ due to the translation homeomorphisms \ref{shiftHomeomorphism}.
\begin{proof}
Assume first that $\mathcal{F} = \lim^{-1}_\xi(0)$ for some vector space convergence $\xi$.
\begin{enumerate}
\item This is just the monotonicity of the convergence.
\item If $F,G\to 0$, then $F\otimes G \to (0,0)$ by \ref{convergenceFiniteProductFilter}. By continuity of addition we have $F+G\to 0$.
\item The convergence on the scalar field is pretopological, so $\vicinity_\F(0)\to 0$. By \ref{convergenceFiniteProductFilter} $\vicinity_\F(0)\otimes F \to (0,0)$ and by continuity of the scalar multiplication $\vicinity_\F(0)\cdot F \to 0$.
\item By \ref{continuityLemmaVectorConvergence}.
\item By \ref{continuityLemmaVectorConvergence}.
\end{enumerate}

Now assume the five points hold. Define the convergence $\xi$ by $F\to v$ iff $F-v \in \mathcal{F}$. We need to show that this is a convergence and that it makes both the vector addition and scalar multiplication continuous.

Monotonicity is guaranteed by (1). To show the convergence is centered, note that $\mathcal{F} \neq \emptyset$ by (4), so long as $V\neq \emptyset$. Then for any $F\in \mathcal{F}$, $\big\{\{0\}\big\} = 0\cdot F \in \mathcal{F}$ by (5).

To show that the vector addition is continuous, take $F\to (v_1, v_2)$. Then $p_1^{\imf\imf}(F) = F_1\to v_1$ and $p_2^{\imf\imf}(F) = F_2 \to v_2$, i.e.\ $F_1-v_1 \in \mathcal{F}$ and $F_2-v_2 \in \mathcal{F}$. By (1), $(F_1-v_1) + (F_2-v_2) = (F_1+F_2) - (v_1 + v_2) \in \mathcal{F}$, so $F_1+F_2 \to v_1 + v_2$. Thus by \ref{filterFactorisationInequality}, $F_1+F_2 = +^{\imf\imf}[F_1\otimes F_2] \subseteq +^{\imf\imf}[F] \to v_1+v_2$ and the addition is continuous.

Let $G \to (\lambda, v)$. Then $G_1 = p_1^{\imf\imf}(G) \to \lambda$ and $G_2 = p_2^{\imf\imf}(G) \to v$, so $G_1 \supseteq \vicinity_\F(\lambda)$. We have
\begin{align*}
\cdot^{\imf\imf}[G] - \lambda\cdot v &\supseteq \cdot^{\imf\imf}[G_1\otimes G_2] - \lambda\cdot v = G_1\cdot G_2 - \lambda\cdot v \\
&\supseteq \vicinity_\F(\lambda) \cdot G_2 - \lambda\cdot v \\
&= (\vicinity_\F(0) + \lambda)\cdot((G_2 - v) + v) - \lambda\cdot v \\
&\supseteq \lambda\cdot (G_2 - v) + \vicinity_\F(0)\cdot(G_2-v) + \lambda\cdot v + \vicinity_\F(0)\cdot v - \lambda\cdot v \\
&= \lambda\cdot (G_2 - v) + \vicinity_\F(0)\cdot(G_2-v) + \vicinity_\F(0)\cdot v \in \mathcal{F}.
\end{align*}
So $\cdot^{\imf\imf}[G] \to \lambda\cdot v$, making the scalar multiplication continuous. Note the last inclusion is not an equality because we go from one instance of $G_2$ and $\vicinity_\F(0)$ to two!
\end{proof}

\begin{proposition} \label{initialVectorConvergenceLinearFunctions}
Let initial convergence w.r.t. a set of linear functions is a vector space convergence.
\end{proposition}

\begin{proposition}
Let $\sSet{V,\xi}$ be a vector space convergence and $A\subseteq V$. Then
\begin{enumerate}
\item if $A$ is balanced, then $\adh_\xi(A)$ is balanced;
\item if $A$ is convex, then $\adh_\xi(A)$ is convex;
\item if $A$ is a subspace, then $\adh_\xi(A)$ is a subspace.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We use \ref{productAdherence} and \ref{adherenceContinuity} to compute
\begin{align*}
\cball(0,1)\cdot \adh_\xi(A) &= \cdot^\imf[\adh_\F(\cball(0,1))\times \adh_\xi(A)] = \cdot^\imf[\adh_{\F\otimes \xi}(\cball(0,1)\times A)] \\
&\subseteq \adh_{\xi}\big(\cdot^\imf[\cball(0,1)\times A]\big) = \adh_{\xi}(\cball(0,1)\cdot A) = \adh_\xi(A).
\end{align*}
\textit{Alternative proof:}
Take $|\lambda|\leq 1$ and $v\in \adh_\xi(A)$, then we need to show that $\lambda v \in \adh_\xi(A)$. We have $A\in \vicinity_\xi(v)^{\mesh}$ and $A\subseteq \lambda^{-1}A$. So for all $B\in \vicinity_\xi(v)$:
\[ A\mesh B \quad\implies\quad \lambda^{-1}A\mesh B \quad\implies\quad A\mesh \lambda B. \]
Thus $A\in \vicinity_\xi(\lambda v)^{\mesh}$, which is what we needed to show by \ref{principalAdherenceInherence}.

(2) Take $0\leq r \leq 1$ and $v,w\in \adh_\xi(A)$, then we need to show that $rv + (1-r)w \in \adh_\xi(A)$. To that end, take some arbitrary $B\in \vicinity_\xi(rv+(1-r)w)$. Now $B - (rv+(1-r)w) \in \vicinity_\xi(0)$, so by \ref{vicinityFactorisation} we can find a $U\in\vicinity_\xi(0)$ such that $U+U \subseteq B - (rv+(1-r)w)$, which means $r\big(r^{-1}U + v\big) + (1-r)\big((1-r)^{-1}U + w\big) \subseteq B$. Now $r^{-1}U, (1-r)^{-1}U\in \vicinity_\xi(0)$ because $v\mapsto\lambda\cdot v$ is a homeomorphism (\ref{continuityLemmaVectorConvergence}) and \ref{homeomorphismPreservation}, so $r^{-1}U + v \mesh A$ and $(1-r)^{-1}U + w \mesh A$. We take $v'\in r^{-1}U + v \cap A$ and $w'\in (1-r)^{-1}U + w \cap A$. Then $rv'+(1-r)w'$ is in $A$ by convexity and in $B$ by construction, so $A\mesh B$.

(3) Clearly $\adh_\xi(A)$ is not empty. It is then enough to verify that $\adh_\xi(A)+\adh_\xi(A)\subseteq \adh_\xi(A)$ and $\F\cdot \adh_\xi(A) \subseteq \adh_\xi(A)$. We use \ref{productAdherence} and \ref{adherenceContinuity} to compute
\begin{align*}
\adh_\xi(A) + \adh_\xi(A) &= +^\imf[\adh_\xi(A)\times \adh_\xi(A)] = +^\imf[\adh_{\xi\otimes \xi}(A\times A)] \\
&\subseteq \adh_{\xi}\big(+^\imf[A\times A]\big) = \adh_{\xi}(A + A) = \adh_\xi(A)
\end{align*}
and
\begin{align*}
\F\cdot \adh_\xi(A) &= \cdot^\imf[\adh_\F(\F)\times \adh_\xi(A)] = \cdot^\imf[\adh_{\F\otimes \xi}(\F\times A)] \\
&\subseteq \adh_{\xi}\big(\cdot^\imf[\F\times A]\big) = \adh_{\xi}(\F\cdot A) = \adh_\xi(A).
\end{align*}
\end{proof}
\begin{corollary} \label{hyperplaneClosedDense}
A hyperplane in a convergence vector space is either closed or dense.
\end{corollary}
\begin{proof}
Let $H$ be a hyperplane in a vector space $V$. Then $H \subseteq \adh(H)$ and $\adh(H)$ is a subspace. Because $H$ is a coatom, we have either $\adh(H) = H$ or $\adh(H) = V$. In the first case $H$ is closed, in the second dense.
\end{proof}

\begin{lemma}
Let $V$ be a convergence vector space over a field $K$ and $F\subseteq K$ a subfield. Then the $F$-vector space $V_F$ with the same convergence structure is also a convergence vector space.
\end{lemma}
\begin{proof}
It is enough that the restriction of the scalar multiplication to $F$ remains continuous. Alternatively, we can use \ref{vectorSpaceConvergenceConstruction} and note that passing to the field $F$ simply represents a weakening of condition $(5)$.
\end{proof}

\subsection{Algebraic convergence}
\begin{definition}
Let $V$ be a vector space over a field $\F$. The \udef{algebraic convergence} on $V$ is the strongest vector space convergence on $V$. We denote this convergence $\mathfrak{a}$ and thus write $F \overset{\mathfrak{a}}{\longrightarrow} x$ and $x\in \lim_\mathfrak{a} F$.
\end{definition}
Note that the algebraic convergence is not just the discrete convergence because, for example, $\seq{n^{-1}v} \to 0$ for all $v\in V$ by continuity of the scalar product and the fact that $\pfilter{v} \to v$.


We need to show that the definition makes sense.
\begin{proposition} \label{algebraicConvergence}
Let $V$ be a vector space over a field $\F$. There exists a strongest vector space convergence on $V$ and this convergence is defined by
\[ \forall F\in \powerfilters(V): \qquad F\in {\lim}^{-1}(0) \iff \exists v\in V: \;F \supseteq \vicinity_\F(0)\cdot v.  \]
\end{proposition}
\begin{proof}
Using \ref{vectorSpaceConvergenceConstruction} it is easy to see that this convergence is a vector space convergence. Conversely, this is the strongest possible convergence by point (4) of \ref{vectorSpaceConvergenceConstruction}.
\end{proof}
Note that the algebraic convergence of a non-trivial vector space is not topological!

\begin{lemma} \label{constructionsInAlebraicConvergence}
Let $V$ be a vector space over a field $\F$, $v\in V$ and $A\subseteq V$ a subset. Then
\begin{enumerate}
\item $\begin{aligned}[t]
\vicinity_\mathfrak{a}(0) &= \bigcap_{v\in V} \upset \vicinity_\F(0)\cdot v \\
&= \setbuilder{B\in \powerset(V)}{\forall v\in V: \exists \Gamma_v\in \vicinity_\F(0):\; \Gamma_v\cdot v\subseteq B} \\
&= \setbuilder{\bigcup_{v\in V} \Gamma_v\cdot V}{\forall v\in V:\; \Gamma_v \in \vicinity_\F(0)};
\end{aligned}$
\item $\inh_\mathfrak{a}(A) = \setbuilder{x\in V}{\forall v\in V:\exists \Gamma_v \in \vicinity_\F(0):\; x + \Gamma_v\cdot v \subseteq A}$;
\item $\adh_\mathfrak{a}(A) = \setbuilder{x\in V}{\exists v\in V: \forall \Gamma\in\vicinity_\F(0):\; (x+\Gamma\cdot v)\mesh A}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) The first equality follows straight from \ref{algebraicConvergence}.

(2) We have $\inh_\mathfrak{a}(A) = \setbuilder{x}{A\in \vicinity_\mathfrak{a}(x)} = \setbuilder{x}{A-x\in \vicinity_\mathfrak{a}(0)}$. From (1) we get 
\begin{align*}
\inh_\mathfrak{a}(A) &= \setbuilder{x}{\forall v\in V: \exists \Gamma_v\in \vicinity_\F(0): \Gamma_v\cdot v \subseteq A-x} \\
&= \setbuilder{x}{\forall v\in V: \exists \Gamma_v\in \vicinity_\F(0): x + \Gamma_v\cdot v \subseteq A}.
\end{align*}

(3) We calculate
\begin{align*}
\adh_\mathfrak{a}(A) &= \big(\inh_\mathfrak{a}(A^c)\big)^c \\
&= \setbuilder{x\in V}{\exists v\in V:\forall \Gamma \in \vicinity_\F(0):\; \neq(x + \Gamma\cdot v \subseteq A^c)} \\
&= \setbuilder{x\in V}{\exists v\in V:\forall \Gamma \in \vicinity_\F(0):\; (x + \Gamma\cdot v) \mesh A}.
\end{align*}
\end{proof}

\begin{lemma}
Let $V$ be a vector space. Then every subspace $U\subseteq V$ is algebraically closed.
\end{lemma}
\begin{proof}
We need to show that $\adh_\mathfrak{a}(U)\subseteq U$. Take $x\in \adh_\mathfrak{a}(U)$. Then take a $v\in V$ such that $\forall \Gamma\in\vicinity_\F(0):\; (x+\Gamma\cdot v)\mesh U$.

Pick some $\Gamma\in\vicinity_\F(0)$. Then $x+\lambda v\in U$ for some $\lambda\in \Gamma$. Then take $\ball(0,|\lambda|/2)\in \vicinity_\F(0)$, so $x+\mu v\in U$ for some $\mu\in \ball(0,|\lambda|/2)$. In particular $\lambda \neq \mu$. If either $\lambda =0$ or $\mu = 0$, then $x\in U$ and we are done. Suppose $\lambda\neq 0 \neq \mu$. Then
\[ \lambda^{-1}(x+\lambda v) - \mu^{-1}(x+\mu v) = (\lambda^{-1} - \mu^{-1})x \in U. \]
So $x\in U$.
\end{proof}

\subsubsection{The algebraic interior or core}
\begin{definition}
Let $V$ be a vector space and $A\subseteq V$ a subset. Then algebraic inherence $\inh_\mathfrak{a}(A)$ is also called the \udef{algebraic interior} or \udef{core} of $A$.
\end{definition}

\begin{proposition} \label{coreProperties}
Let $V$ be a vector space and $A \subseteq V$ a subset. Then
\begin{enumerate}
\item $A$ is absorbing \textup{if and only if} $0\in \inh_\mathfrak{a}(A)$;
\item if $A$ is convex, then $\inh_\mathfrak{a}(A)$ is convex and open.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have that
\begin{align*}
\text{$A$ is absorbing} &\iff \forall v\in V: \exists \epsilon >0: \; \ball(0,\epsilon)\cdot v\subseteq A \\
&\iff \forall v\in V: \exists \Gamma \in \vicinity_\F(0): \; \Gamma\cdot v\subseteq A \\
&\iff 0\in \inh_\mathfrak{a}(A).
\end{align*}

(2) We first show convexity: take $x,y \in \inh_\mathfrak{a}(A)$. Then there exist relevant $v,w,\Gamma_v,\Gamma_w$ such that $x+ \Gamma_v\cdot v \subseteq A$ and $y+ \Gamma_w\cdot w \subseteq A$. By \ref{convexCriteria} we have $\lambda \big(x+ \Gamma_v\cdot v\big) + (1-\lambda)\big(y+ \Gamma_w\cdot w\big)\subseteq A$ for all $0\leq \lambda \leq 1$, so
\[ \lambda x+(1-\lambda)y + (\Gamma_v\cap\Gamma_w)\big(\lambda v+(1-\lambda)w\big) \subseteq \lambda x+(1-\lambda)y + \Gamma_v\cdot \lambda v+\Gamma_w\cdot (1-\lambda)w \subseteq A. \]

To show $\inh_\mathfrak{a}(A)$ is open, we use \ref{openClosedCriteria}. Take $x\in \inh_\mathfrak{a}(A)$. Then for all $v\in V$ we can find a $\Gamma_v\in\vicinity_\F(0)$ such that $x+\Gamma_v\cdot v \subseteq A$. This means $x+\bigcup_{v\in V}\Gamma_v\cdot v \subseteq A$. Because the convergence on $\F$ is topological, we may take the $\Gamma_v$ open. To conclude with \ref{openClosedCriteria} it is enough to show that $x+\bigcup_{v\in V}\Gamma_v\cdot v \subseteq \inh_\mathfrak{a}(A)$.

Pick some $y = x+ c_w w \in x+ \bigcup_{v\in V}\Gamma_v\cdot v \subseteq A$, meaning $c_w\in\Gamma_w$. We can find an $0<\epsilon_w<|c_w|$ such that $c_w + \ball(0,\epsilon_w) \subseteq \Gamma_w$ by \ref{openClosedCriteria}. Then for all $1-\frac{\epsilon_w}{|c_w|}<\delta<1$ we have $|c_w - \delta c_w| = (1-\delta)|c_w| < \frac{\epsilon_w}{|c_w|}|c_w| = \epsilon_w$ and so $x+ \delta c_w w \in A$.

Now pick an arbitrary $u\in V$. We have $x+\Gamma_u\cdot u \subseteq A$. By convexity we have
\[ \delta^{-1}(x+ \delta c_w w) + (1-\delta^{-1})\big(x+\Gamma_u\cdot u\big) = x + c_w w + (1-\delta^{-1})\Gamma_u\cdot u \subseteq A. \]
This means that for all $v\in V$ we have $y + (1-\delta^{-1})\Gamma_v\cdot v \subseteq A$ and thus $y\in \inh_\mathfrak{a}(A)$.
\end{proof}

\begin{proposition} \label{algebraicallyOpen}
Let $V$ be a vector space and $A \subseteq V$ an algebraically open subset. Then
\begin{enumerate}
\item $A+U$ is algebraically open for any subspace $U\subseteq V$;
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsection{Topological vector spaces}
\begin{definition}
A \udef{topological vector space} (or TVS) is a convergence vector space that is topological.
\end{definition}
As with convergence groups, any pretopological vector space convergence is topological, see \ref{pretopologicalGroupConvergence}.

\begin{lemma} \label{continuityToNormedSpace}
Let $V$ be a TVS and $W$ a normed space. A linear function $f: V\to W$ is continuous \textup{if and only if} there exists a neighbourhood $U\in \neighbourhood_V(0)$ such that $f$ is bounded on $U$.
\end{lemma}
\begin{proof}
Assume $f$ continuous, then $\ball(0,1)\in \neighbourhood_W(0) = \neighbourhood_W(f(0))$ implies $U = f^{\preimf}(\ball(0,1)) \in \neighbourhood_V(0)$ by \ref{pretopologicalContinuityVicinities} and $f$ is bounded on $U$ by construction.

Now assume there exists a neighbourhood $U\in \neighbourhood_V(0)$ such that $f$ is bounded on $U$. Then $f^{\imf}(U)\subseteq \ball_W(0,C)$. It is then enough to show that for all $\ball_W(0,\epsilon)$, $f^{\preimf}(\ball_W(0,\epsilon)) \in \neighbourhood_V(0)$. Indeed
\[ f^{\preimf}(\ball_W(0,\epsilon)) = \frac{\epsilon}{C} f^{\preimf}(\ball_W(0,C)) = \frac{\epsilon}{C}U \in \neighbourhood_V(0). \]
\end{proof}

\subsubsection{Neighbourhoods and base}
\begin{proposition} \label{TVSconstruction}
Let $V$ be a vector space and $N\in\powerfilters(V)$. Then $N = \neighbourhood_\xi(0)$ for some topological convergence on $V$ \textup{if and only if}
\begin{enumerate}
\item for all $A\in N$ and $\lambda\in \F$: $\lambda A\in N$;
\item for all $A\in N$, there exists some $B\in N$ such that $B+B\subseteq A$;
\item each $A \in N$ is absorbent;
\item $N$ has a balanced base.
\end{enumerate}
\end{proposition}
\begin{proof}
We adapt \ref{vectorSpaceConvergenceConstruction} to the present situation.

First assume $N$ has a balanced and absorbent base. We check the five conditions for $\mathcal{F} = \pfilter{N}$.

\begin{enumerate}
\item Immediate because $\mathcal{F} = \pfilter{N}$.
\item Take $F,G\in \pfilter{N}$. We need to show that $\upset (F + G) \supseteq N$, which means that for all $A \in N$ there exist $B\in F$ and $C\in G$ such that $B+C\subseteq A$. We can take $B = C$ equal to the $B$ of point (2).
\item Take $F\in \pfilter{N}$. We need to show that $\upset(\vicinity_\F(0)\cdot F) \supseteq N$, which means that for all $A\in N$ there exists a $\Gamma\in \vicinity_\F(0)$ and $B\in F$ such that $\Gamma \cdot B\subseteq A$. We can take $B = \balancedCore(A) \in N \subseteq F$ and $\Gamma = B(0,1)$.
\item Take $v\in V$. We need to show that all $A\in N$ contain $\Gamma\cdot v$ for some $\Gamma \in \vicinity_\F(0)$. Because $A$ is absorbent, there exists an $r>0$ such that $v\in cA$ for all $|c|\geq r$. Conversely $c^{-1}v \in A$ for all $|c^{-1}| \leq r^{-1}$. So $\ball(0,r^{-1})\cdot v \subseteq A$ and $B(0,r^{-1}) \in \vicinity_\F(0)$.
\item Take $F\in \pfilter{N}$ and $\lambda\in \F$. We need to show that for all $A\in N$ there exists a $B\in F$ such that $\lambda\cdot B\subseteq A$. We can take $B = \lambda^{-1}A \in N\subseteq F$.
\end{enumerate}

Now assume $\xi$ is a topological vector space convergence and $N = \neighbourhood_\xi(0)$.
\begin{enumerate}
\item By point (5) of \ref{vectorSpaceConvergenceConstruction}, we have that for all $\lambda\in \F$, $\lambda\cdot N \supseteq N$, so for all $A\in N$, there exists a $B\in N$ such that $A = \lambda B$. This means $\lambda^{-1}A \in N$ for all $\lambda\in \F, A\in N$.
\item By \ref{vicinityFactorisation};
\item For absorbence, take $A\in N$ and $v\in V$. Then there exists a $\Gamma \in \vicinity_\F(0)$ such that $\Gamma\cdot v \subseteq A$. Now we can find a $r>0$ such that $\ball(0,r)\subseteq \Gamma$, so for all $|c|\geq r^{-1}$ we have $v\in cA$.
\item By point (3) of \ref{vectorSpaceConvergenceConstruction}, $\vicinity_\F(0)\cdot N \supseteq N$. Take $A\in N$. Then there exists a $\Gamma\in\vicinity_\F(0)$ and $B\in N$ such that $\Gamma\cdot B \subseteq A$. We can find sume ball $\ball(0,\epsilon) \subseteq \Gamma$, so $\ball(0,1)\cdot \epsilon^{-1}B\subseteq \epsilon^{-1}B \subseteq A$. Thus $\epsilon^{-1}B$ is balanced and a neighbourhood by point(1). So every $A\in N$ contains a balanced set in $N$.
\end{enumerate}
\end{proof}


\subsubsection{Locally convex convergence}
\begin{definition}
A convergence vector space $\sSet{V,\xi}$ is called \udef{locally convex} if $\xi$ is topological and based in the convex sets.
\end{definition}

\begin{lemma} \label{locallyConvexNeighbourhoodsLemma}
Let $\sSet{V,\xi}$ be a TVS. Then the following are equivalent:
\begin{enumerate}
\item $\xi$ is locally convex;
\item $\neighbourhood_\xi(0)$ is based in the convex sets;
\item $\neighbourhood_\xi(0)$ is based in the absolutely convex sets.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Leftrightarrow (2)$ One direction is immediate, for the other it is enough to note that if $U$ is convex, then so is the translated set $x+U$ for all $x\in V$.

$(2) \Leftrightarrow (3)$ One direction is immediate, the other follows because the balanced core of a convex set is convex by \ref{balancedCoreConvexSet}.
\end{proof}

\begin{proposition}
Let $V$ be a vector space and $N\in\powerfilters(V)$. Then $N = \neighbourhood_\xi(0)$ for some topological convergence on $V$ \textup{if and only if}
\begin{enumerate}
\item for all $A\in N$ and $\lambda\in \F$: $\lambda A\in N$;
\item each $A \in N$ is absorbent;
\item $N$ has an absolutely convex base.
\end{enumerate}
\end{proposition}
\begin{proof}
This almost completely follows from \ref{TVSconstruction} and \ref{locallyConvexNeighbourhoodsLemma}. We just need to show that for all $A\in N$, there exists some $B\in N$ such that $B+B\subseteq A$. We may take $B = \frac{1}{2}A'$, where $A'$ is a convex subset of $A$, because for all $v,w\in A'$ we have $\frac{1}{2}v + \frac{1}{2}w \in A'$ by convexity.
\end{proof}


\section{Functionals}
\begin{definition}
Let $V$ be a vector space over a field $\mathbb{F}$.
\begin{enumerate}
\item A \udef{functional} on $V$ is a map $V\to \F$;
\item A \udef{linear functional} on $V$ is a linear map from $V$ to $\mathbb{F}$;
\item A \udef{real functional} on $V$ is a map $V\to \R$.
\end{enumerate}
\end{definition}

\begin{lemma} \label{continuityDominatedFunctional}
Let $V$ be a TVS and $f:V\to \F$ a continuous functional. If $g:V\to \F$ is a functional such that $|g(v)|\leq |f(v)|$ for all $v\in V$, then $g$ is continuous.
\end{lemma}
\begin{proof}
We use \ref{pretopologicalContinuityVicinities} to show continuity. To that end take $K\in \neighbourhood_\F(0)$. Then there exists $\epsilon >0$ such that $\ball(0,\epsilon)\subseteq K$ and so
\[ g^{\preimf}(K) \supseteq g^\preimf[\ball(0,\epsilon)] \supseteq f^\preimf[\ball(0,\epsilon)] \in \neighbourhood_V(0). \]
\end{proof}

\subsection{Linear functionals}
\begin{lemma} \label{kernelHyperplane}
Let $V$ be a vector space and $U\subseteq V$ a subspace. Then $U$ is a hyperplane \textup{if and only if} it is the kernel of a linear functional.
\end{lemma}

\begin{lemma} \label{functionalBoundedNeighbourhood}
Let $f: V\to \F$ be a linear functional and $x\notin \ker(f)$. Let $A\subseteq V$ be a balanced set. Then $(x+A)\perp \ker(f)$ \textup{if and only if} $A \subseteq f^{\preimf}(\ball(0,|f(x)|))$.
\end{lemma}
\begin{proof}
Suppose $A \subseteq f^{\preimf}(\ball(0,|f(x)|))$. Then for all $a\in A$: $f(x+a) = f(x) + f(a) \neq 0$.

Conversely, suppose $A \not\subseteq f^{\preimf}(\ball(0,|f(x)|))$, i.e.\ there exists $a\in A$ such that $|f(a)| \geq |f(x)|$. Then $v= -\frac{f(x)}{f(a)}a\in A$, because $A$ is balanced and so $f(x+ v) = f(x)-\frac{f(x)}{f(a)}f(a) = 0$ and so $(x+A) \mesh \ker(f)$.
\end{proof}

\begin{proposition} \label{linearFunctionalOpen}
Let $V$ be a convergence vector space and $f:V\to \F$ a non-zero linear functional. Then $f$ is an open map.
\end{proposition}
\begin{proof}
It is enough to show $f$ is open when $V$ is equipped with the algebraic convergence.

Let $A$ be an algebraically open map. We use \ref{openClosedCriteria} to show $f^\imf[A]$ is also open. Because $f$ is non-zero, there exists a $v\in V$ such that $f(v) \neq 0$. Take some $y\in f^\imf[A]$. Then there exists an $x\in A$ such that $f(x) = y$. Because $A$ is open, $x\in \inh_\mathfrak{x}(A)$ and there exists $x+ \Gamma_v\in \vicinity_\F(0)$ such that $x+\Gamma_v\cdot v \subseteq A$ by \ref{constructionsInAlebraicConvergence}.

Now $f^\imf[x+ \Gamma_v\in \vicinity_\F(0)] = y+\Gamma_v \cdot f(v) \subseteq f^\imf[A]$ and $y+\Gamma_v \cdot f(v)$ is a vicinity of $y$, so we are done.
\end{proof}

\begin{lemma} \label{complexRangeExtensionRealFunctional}
Let $V$ be a complex vector space and $g: V_\R\to \R$ a linear functional. Then there exists a unique linear functional $f: V\to \C$ such that $g = \Re(f)$.
\end{lemma}
\begin{proof}
We can write $f = g + ih$ for some function $h: V\to \R$. Then for all $x\in V$
\[ g(ix)+ih(ix) = f(ix) = if(x) = ig(x) - h(x). \]
Comparing real parts gives $h(x) = - g(ix)$. So $f$ must be given by $f(x) = g(x) - ig(ix)$. Clearly $f$ is real-linear. We just need to verify that this makes $f$ complex-linear. Indeed, take $\lambda = a +ib \in \C = \R+i\R$ arbitrarily. Then for all $v\in V$
\begin{align*}
f(\lambda v) &= f\big((a+ib)v\big) \\
&= af(v) + bf(iv) \\
&= af(v) + b\big(g(iv) - ig(i^2v)\big) \\
&= af(v) + b\big(g(iv) + ig(v)\big) \\
&= af(v) + ib\big(-ig(iv) + g(v)\big) \\
&= af(v) + ibf(v) = (a+ib)f(v) = \lambda f(v).
\end{align*}
\end{proof}

\begin{lemma}
Let $V$ be a vector space and $f_0,\ldots, f_n, f$ linear functionals in $(V\to \F)$. Then a linear funcctional $f: V\to \F$ is a linear combination of $f_0,\ldots, f_n$ \textup{if and only if} .
\end{lemma}
\begin{proof}
The direction $\Rightarrow$ is immediate: if $f_i(v) = 0$ for all $0\leq i\leq n$, then any linear combination is also zero.

Now assume 
\end{proof}

\begin{lemma} \label{linearDependenceLinearFunctionals}
Let $V$ be a vector space and $f_0,\ldots, f_n, f$ linear functionals in $(V\to \F)$. Then the following are equivalent:
\begin{enumerate}
\item $f$ is a linear combination of $f_0,\ldots, f_n$;
\item there exists a $C>0$ such that $f(v) \leq C \max_{0\leq i\leq n}|f_i(v)|$;
\item $\ker(f) \supseteq \bigcap_{0\leq i \leq n}\ker(f_i)$;
\end{enumerate}
\end{lemma}
\begin{proof}
The implications $(1) \Rightarrow (2) \Rightarrow (3)$ are clear.

Now assume $(3)$ holds and consider the linear function $f_0\times \ldots \times f_n: V\to \F^{n+1}$. Because of (3), the functional
\[ F: \im(f_0\times \ldots \times f_n) \to \F: x\mapsto \sunp\circ f^\imf\circ (f_0\times \ldots \times f_n)^\preimf(\{x\}) \]
is well-defined. Extend $F$ to the whole of $\F^{n+1}$. By \ref{ellIsomorphism} we can represent this as:
\[ F: \F^{n+1}\to \F: (u_0, \ldots, u_n)\mapsto \sum_{i=0}^n \alpha_iu_i. \]
From $f(v) = F\big((f_0\times \ldots \times f_n)(v)\big)$, we get
\[ f(v) = \sum_{i=0}^n \alpha_i f_i(v) \qquad \forall v\in V. \]
So $f = \sum_{i=0}^n \alpha_i f_i$.
\end{proof}
\begin{corollary}
Let $V$ be a vector space and $f_0,\ldots, f_n$ linearly independent linear functionals in $(V\to \F)$. Then there exist $v_0, \ldots, v_n$ such that $f_i(v_j) = \delta_{i,j}$.
\end{corollary}
\begin{proof}
The proof is by induction. The case $n=1$ is clear: if there was no such $a_1$, then $f_1$ would be zero and thus not linearly independent.

Suppose the statement holds for $n-1$ and take $f_0,\ldots, f_n$ linearly independent linear functionals with corresponding $v_0,\ldots, v_{n-1}$. By point (3) of the proposition we can find $v_n \in \bigcap_{0\leq i \leq n}\ker(f_i)\setminus \ker(f_n)$, which after rescaling can be taken to be such that $f_n(v_n) = 1$. By construction $f_i(v_n) = 0$ for $i<n$.

Now replace $v_i$ with $v_i-f_n(v_i)v_n$ and rescale.
\end{proof}

\subsection{The dual space}
\begin{definition}
Let $\sSet{V,\xi}$ be a convergence vector space over a field $\mathbb{F}$.

The \udef{dual} of $V$ is the vector space of all continuous linear functionals on $V$.

The dual is denoted $\sSet{V,\xi}^{*}$ (or just $V^*$ is the convergence is clear from the context).
\end{definition}

\begin{proposition} \label{continuityLinearFunctionals}
Let $V$ be a TVS and $f:V\to \F$ a linear functional on $V$. Then the following are equivalent:
\begin{enumerate}
\item $f\in V^{*}$, i.e.\ $f$ is continuous;
\item there exists a neighbourhood $U\in \neighbourhood_V(0)$ such that $f$ is bounded on $U$
\item $\ker(f)$ is closed;
\item $\ker(f)$ is not dense.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Leftrightarrow (2)$ By \ref{continuityToNormedSpace}.

$(1) \Rightarrow (3)$ Because $\ker(f) = f^{\preimf}(\{0\})$ and $\{0\}$ is closed in $\F$, $\ker(f)$ is closed by \ref{continuity}.

$(3) \Rightarrow (1)$ Now assume $\ker(f)$ closed. If $\ker(f) = V$, then $f$ is constant and thus continuous by \ref{continuityConstructions}. If $\ker(f) \neq V$, we can find some some $x\in \ker(f)^c$, which is open. Thus $\ker(f)^c - x$ is a neighbourhood of the origin, meaning we can take a balanced subset $A$ by \ref{TVSconstruction}. Now $(x+A)\perp \ker(f)$ by construction, so $f$ is bounded on $A$ by \ref{functionalBoundedNeighbourhood} and thus $f$ is continuous by \ref{continuityToNormedSpace}.

$(3) \Leftrightarrow (4)$ By \ref{kernelHyperplane} $\ker(f)$ is a hyperplane and by \ref{hyperplaneClosedDense} this hyperplane is either closed or dense.
\end{proof}


\begin{lemma} Let $X$ be a normed space and
let $x\in X$ $\omega\in \tdual{X}$ be a bounded linear functional. Then
\begin{align*}
\norm{\omega} &= \sup\setbuilder{|\omega(v)|}{\norm{v}=1 } \\
&= \sup\setbuilder{\frac{|\omega(v)|}{\norm{v}}}{v\neq 0} \\
&= \inf\setbuilder{c>0} {|\omega(v)|\leq c\norm{v}\forall v\in X}
\end{align*}
and
\begin{align*}
\norm{x} &= \sup\setbuilder{|\varphi(x)|}{ \norm{\varphi}=1} \qquad\qquad\quad\\ %TODO: fragile spacing!
&= \sup\setbuilder{\frac{|\varphi(x)|}{\norm{\varphi}}}{\varphi\neq 0}.
\end{align*}
\end{lemma}
TODO move??
\begin{proof}
We prove the third equality. Let $\alpha$ be the infimum. Let $\epsilon>0$, then by the definition $|\omega[(\norm{x}+\epsilon)^{-1}x]|\leq \norm{\omega}$. Hence $|\omega(x)|\leq \norm{\omega}(\norm{x}+\epsilon)$. Letting $\epsilon\to 0$ gives $|\omega(x)|\leq \norm{\omega}\norm{x}$ for all $x$. So $\alpha\leq \norm{\omega}$. On the other hand, $|\omega(x)|\leq c$ for all $x$ with $\norm{x}=1$. Hence $\norm{\omega}\leq \alpha$.
\end{proof}

\subsubsection{The algebraic dual}
\begin{definition}
Let $V$ be a vector space. The \udef{algebraic dual} of $V$ is the dual of $\sSet{V,\mathfrak{a}}$, where $\mathfrak{a}$ is the algebraic convergence.

If no convergence on $V$ has been mentioned, then $V^*$ means the algebraic dual.
\end{definition}

\begin{proposition} \label{algebraicDual}
Let $V$ be a vector space. Then the algebraic dual of $V$ is the set of all linear functionals: $V^* = \Lin(V,\F)$.

Thus $V^* \supseteq \sSet{V,\xi}^*$ for all vector space convergences $\xi$ on $V$.
\end{proposition}
\begin{proof}
We need to show that all linear functionals are continuous when $V$ is equipped with the algebraic convergence. Assume $F\overset{\mathfrak{a}}{\longrightarrow} x$. Then there exists a $v\in V$ such that $\vicinity_\F(0)\cdot v+x \subseteq F$ and so $\vicinity_\F(0)\cdot f(v)+f(x) \subseteq f^\imf[F]$, meaning $f^\imf[F] \overset{\F}{\longrightarrow} f(x)$. Thus $f$ is continuous.
\end{proof}


\begin{proposition} \label{dualBasisDimension}
Let $V$ be a vector space. Then $\dim V^* \geq \dim V$ and
\[ \dim V^* = \dim V \iff \text{$V$ is finite-dimensional}. \]
If $V$ is finite-dimensional with a basis $v_1, \ldots, v_n$, then the \udef{dual basis} $\varphi_1, \ldots, \varphi_n$ is the set of linear functionals on $V$ such that
\[ \varphi_j(v_k) = \begin{cases}
1 & (k=j), \\ 0 & (k\neq j)
\end{cases}. \]
This dual basis is indeed a basis of $V^*$.
\end{proposition}
\begin{proof}
We first assume $V$ is finite-dimensional and prove the dual basis is a basis, which proves $\dim V^* = \dim V$. We then assume $V$ is infinite-dimensional and prove $\dim V^* \neq \dim V$.\footnote{Reference: \url{https://mathoverflow.net/questions/13322/slick-proof-a-vector-space-has-the-same-dimension-as-its-dual-if-and-only-if-i}}
\begin{enumerate}
\item Assume $V$ is finite-dimensional. To show the dual basis spans $V^*$, take a linear functional $\varphi$. Now define $a_i = \varphi(v_i)$. It is clear that $\varphi = \sum_{i=1}^n a_i\varphi_i$. To show linear independence, take a combination
\[ b_1\varphi_1 + \ldots +b_n\varphi_n =0. \]
Filling in all basis vectors $v_i$ in turn, gives $b_i=0$ for all $i$.
\item Assume $V$ is infinite-dimensional. At first let us assume $\dim_{\mathbb{F}}V \geq |\mathbb{F}|$. Then we can apply lemma \ref{vsCardinality} to obtain $\dim_{\mathbb{F}}V = |V|$. Let $\beta$ be a basis for $V$. The elements of $V^*$ correspond bijectively to functions from $\beta$ to $\mathbb{F}$. Thus
\[ |V^*| = |\mathbb{F}^\beta| = |\mathbb{F}|^{|\beta|} > |\beta| = |V|. \]
Now we relax the condition $\dim_{\mathbb{F}}V \geq |\mathbb{F}|$. We first note that every field contains a subfield that is at most denumerable. Take such a field $K\subset \mathbb{F}$. We introduce the new vector space $W = \Span_K(\beta)$. Every functional from $W$ to $K$ extends to a functional from $V$ to $\mathbb{F}$. Hence
\[ \dim_\mathbb{F} V = \dim_K W < \dim_K W^* \leq \dim_{\mathbb{F}} V^* \]
using $\dim_{K}W \geq |K| \geq \aleph_0$.
\end{enumerate}
\end{proof}
\begin{corollary}
Let $\sSet{V,\xi}$ be a convergence vector space. If $V$ is finite-dimensional, then $\sSet{V,\xi}^* = \sSet{V,\mathfrak{a}}^*$.
\end{corollary}
\begin{proof}
We have $\sSet{V,\xi}^* \subseteq \sSet{V,\mathfrak{a}}^*$ by \ref{algebraicDual}. Because $V$ is finite-dimensional, we obtain equality equality of space from equality of dimension by \ref{vectorSpaceEquality}.
\end{proof}

\subsubsection{The bidual space}
TODO!
\begin{definition}
Let $V$ be a convergence vector space. The \udef{bidual space} is the dual of the dual $\abidual{V} = \adual{(\adual{V})}$.
\end{definition}
TODO continuous convergence!!

\begin{definition}
Let $V$ be a vector space over $\mathbb{F}$ and $v\in V$. The \udef{evaluation map} $\evalMap: V\to \abidual{V}: v\mapsto \evalMap_v$ is given by
\[ \evalMap_v: \adual{V} \to \mathbb{F}: l\mapsto l(v). \]
\end{definition}

\begin{lemma}
Let $V$ be a vector space. The evaluation map $\evalMap: V\to \abidual{V}: v\mapsto \evalMap_v$ is linear:
\[ \forall v,w\in V, a\in\mathbb{F}: \quad \evalMap_{av + w} = a\evalMap_v + \evalMap_w. \]
\end{lemma}
\begin{lemma}
Let $V$ be vector space over $\mathbb{F}$. The evaluation map is injective.
\end{lemma}
\begin{proof}
Assume $\evalMap_v = \evalMap_w$ for some $v,w\in V$. Then
\[ 0 = \evalMap_v - \evalMap_w  = \evalMap_{v-w}. \]
So $\forall l\in \adual{V}: \evalMap_{v-w}(l) = l(v-w) = 0$. Now define the sublinear functional by
\[ p(x) = \begin{cases}
\alpha & x = \alpha(v-w) \\
0 & \text{else}.
\end{cases} \]
Then the functional $f$ defined on $\Span\{v-w\}$ by $f(\alpha(v-w)) = \alpha$ is bounded by $p$ and can be extended to a functional on all $V$ by the Hahn-Banach theorem \ref{sublinearHahnBanach} if $v-w\neq 0$. Then $f(v-w) \neq 0$, which contradicts our assumptions. Thus $v=w$.
\end{proof}

\begin{proposition}
The mapping $\evalMap: V\to \abidual{V}: v\mapsto \evalMap_v$ is an isomorphism \textup{if and only if} $V$ is finite-dimensional.
\end{proposition}
\begin{proof}
Assume $V$ finite dimensional. As the evaluation map is injective, it is an isomorphism by \ref{invertibleFiniteDim}.
The other direction is a dimensional argument by proposition \ref{dualBasisDimension}.
\end{proof}


Just like for algebraic duality, we can define a topological bidual space (or second dual space) $\tbidual{V}$.

\begin{proposition}
Let $V$ be a normed space. 
For each $v\in V$
\[ \evalMap_v: \tdual{V} \to \mathbb{F}: \omega \mapsto \omega(v) \]
is bounded and thus an element of $\tbidual{V}$.

The evaluation map $\evalMap: V \to \tbidual{V}$ is
\begin{enumerate}
\item isometric (and thus injective): $\norm{\evalMap_v} = \norm{v}$;
\item bounded with norm $\norm{\evalMap} = 1$.
\end{enumerate}
\end{proposition}
\begin{proof}
Let $v\in V$. Then
\[ \norm{\evalMap_v} = \sup\setbuilder{\norm{\evalMap_v(\omega)}}{\norm{\omega}=1} = \sup\setbuilder{\norm{\omega(v)}}{\norm{\omega}=1} \leq \sup\setbuilder{\norm{v}\;\norm{\omega}}{\norm{\omega}=1} = \norm{v}. \]

(1) Setting $\omega = \inner{v/\norm{v}, \cdot}$, we get
\[ \norm{\evalMap_v} \leq |\evalMap_v(\omega)| = |\inner{v/\norm{v}, v}| = \norm{v}. \]
Together with the calculation above, this gives $\norm{\evalMap_v} = \norm{v}$.

(2) $\norm{\evalMap} = \sup\setbuilder{\norm{\evalMap_v}}{\norm{v}=1} = \sup\setbuilder{\norm{v}}{\norm{v}=1} = 1$.
\end{proof}

\begin{lemma}
Let $V$ be normed space over $\mathbb{F}$ and $v\in V$. For each $v\in V$
\[ \evalMap_v: \tdual{V} \to \mathbb{F}: \omega \mapsto \omega(v) \]
is bounded with norm $\norm{v}$ and thus $\evalMap\in \tbidual{V}$ with $\norm{\evalMap} = 1$.
\end{lemma}


\subsubsection{Reflexive spaces}
\begin{definition}
A normed space $V$ is \udef{reflexive} if the evaluation map $\evalMap:V\to \tbidual{V}$ is surjective:
\[ \im\evalMap = \tbidual{V}. \]
\end{definition}
If $V$ is reflexive, then $\tbidual{V}$ is isometrically isomorphic to $V$. The converse is not necessarily true.

\begin{lemma}
Every finite-dimensional space is reflexive.
\end{lemma}

\begin{proposition}
A separable normed space $X$ with a non-separable dual space $\tdual{X}$ cannot be reflexive. 
\end{proposition}
\begin{proof}
TODO
\end{proof}
Thus $l^1$ is not reflexive.

\begin{proposition}
If the dual space $\tdual{X}$ of a  normed space $X$ is separable, then $X$ itself is separable. 
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsubsection{Transposition}
\begin{definition}
Let $f:V\to W \in \Hom_{\mathbb{F}}(V,W)$. The \udef{dual map}\footnote{The dual map $f^t$ is often denoted $f^*$ or $f'$. We avoid this because it clashes with the notation of the Hilbert adjoint.} or \udef{transpose} $f^t$ is the linear map
\[ f^t:W^* \to V^*: l\mapsto f^t(l) = l\circ f. \]
\end{definition}
\begin{lemma}
Let $f\in \Hom(U,V)$ and $g\in \Hom(V,W)$.
\begin{itemize}
\item $(g\circ f)^t = f^t\circ g^t$;
\item $\id^t_V = \id_{\adual{V}}$;
\item $f$ is an isomorphism \textup{if and only if} $f^t$ is an isomorphism;
\item $(f^t)^{-1} = (f^{-1})^t$ 
\end{itemize}
\end{lemma}
TODO: merge
\begin{lemma}
Let $S,T\in\Hom(V,W)$ and $\alpha\in\mathbb{F}$. Then
\begin{enumerate}
\item $(S+T)^t = S^t+T^t$;
\item $(\alpha T)^t = \alpha T^t$
\item if $T$ is invertible, then $T^t$ is invertible and
\[ (T^t)^{-1} = (T^{-1})^t. \]
\end{enumerate}
\end{lemma}

\begin{proposition}
Let $U\subset V$ be a subspace and $T\in \Hom(V,W)$, where $V,W$ are \emph{finite-dimensional}.
\begin{enumerate}
\item $\dim\ker T^t = \dim \ker T + \dim W - \dim V$;
\item $\dim\im T^t = \dim \im T$;
\item $\im T^t = (\ker T)^0$
\item $T$ is injective \textup{if and only if} $T^t$ is surjective.
\end{enumerate}
\end{proposition}
\begin{proof}
\mbox{}
\begin{enumerate}
\item Using $\dim \adual{V} = \dim V$, we have
\begin{align*}
\dim \ker T^t &= \dim(\im T)^0 = \dim W-\dim \im T \\
&= \dim W - (\dim V - \dim \ker T) = \dim \ker T + \dim W - \dim V
\end{align*}
where the equalities come from proposition \ref{annihilatorSpace} and the dimension theorem for linear maps, theorem \ref{dimensionLinearMaps}.
\item Still using these results, we can calculate
\begin{align*}
\dim \im T^t &= \dim \adual{W} - \dim \ker T^t = \dim \adual{W} - \dim (\im T)^0 \\
&= \dim \adual{(\im T)} = \dim \im T.
\end{align*}
\item Take $\varphi = T^t(\psi) \in \im T^t$ where $\psi \in \adual{W}$. If $v\in \ker T$, then
\[ \varphi(v) = \left(T^t(\psi)\right)v = (\psi\circ T)(v) = \psi(Tv) = \psi(0) = 0. \]
Hence $\varphi \in (\ker T)^0$ and $\im T^t\subset (\ker T)^0$. We prove the equality by showing the dimensions are the same. Indeed:
\[ \dim \im T^t = \dim \im T = \dim V - \dim \ker T = \dim(\ker T)^0. \]
\item $T\in\Hom(V,W)$ is injective iff $\ker T = \{0\}$ iff $(\ker T)^0 = \adual{V}$ iff $\im T^t = \adual{V}$ iff $T^t$ is surjective.
\end{enumerate}
\end{proof}

\begin{proposition} \label{transpDual}
Let $f\in\Hom(V,W)$ and $\mathcal{V}, \mathcal{W}$ bases of $V,W$. The
\[ (f^*)^{\mathcal{V}^*}_{\mathcal{W}^*} = ((f)^{\mathcal{W}}_{\mathcal{V}})^\transp. \] 
\end{proposition}

\begin{definition}
Let $T\in\Bounded(V,W)$. The dual map $T^t: \tdual{W}\to \tdual{V}$ is called the \udef{adjoint} or the \udef{transpose} of $T$.
\end{definition}
The notation $T^t$ is consistent for maps on both the algebraic and topological duals: if $T$ is bounded, $T^t:\adual{W}\to \adual{V}$ restricts to $T^t|_{\tdual{W}} = T^t:\tdual{W}\to \tdual{V}$.

\begin{proposition}
Let $T\in\Bounded(V,W)$. Then the transpose $T^t$ is a bounded operator in $\Bounded(W,V)$ with $\norm{T^t} = \norm{T}$.
\end{proposition}
\begin{proof}
The operator $T^t$ is linear since $\forall f_1,f_2\in \tdual{W}, \forall a\in\mathbb{F}, \forall x\in V:$
\[ (T^t(af_1 + f_2))(x) = (af_1 + f_2)(Tx) = af_1(Tx) + f_2(Tx) = a(T^tf_1)(x) + (T^tf_2)(x). \]
For the equality of norms, we prove two inequalities. First $\forall x\in V, f\in \tdual{W}$
\[ |f(Tx)|\leq \norm{f}\norm{Tx}\leq \norm{f}\norm{x}\norm{T} \implies \frac{|f(Tx)|}{\norm{x}} \leq \norm{f}\norm{T}. \]
taking the supremum over $x\in V$, we get $\norm{T^tf} = \norm{f\circ T}\leq \norm{f}\norm{T}$ and taking the supremum over $f\in \tdual{W}$ gives $\norm{T^t}\leq \norm{T}$. This shows that $T^t$ is bounded.

For the other inequality, we use corollary \ref{existenceBoundedFunctionalOfSameNorm} to the Hahn-Banach theorem: for every $x\in V$, there exists a bounded functional $\omega_x$ such that $\norm{\omega_x}=1$ and $\omega_x(x) = \norm{x}$. Then we can calculate:
\begin{align*}
\norm{Tx} = \omega_{Tx}(Tx) = (T^t\omega_{Tx})(x) \leq \norm{T^t\omega_{Tx}}\norm{x} \leq \norm{T^t}\norm{\omega_{Tx}}\norm{x} = \norm{T^t}\norm{x}
\end{align*}
So $\norm{T}\leq\norm{T^t}$. Combining gives $\norm{T^t}=\norm{T}$.
\end{proof}
\begin{corollary}
The map $T\mapsto T^t$ is an isometric isomorphism in $(\Bounded(X,Y)\to \Bounded(\tdual{Y}, \tdual{X}))$.
\end{corollary}

\begin{lemma}
Let $S,T\in\Bounded(V,W)$ and $\alpha\in\mathbb{F}$. Then
\begin{enumerate}
\item $(S+T)^t = S^t+T^t$;
\item $(\alpha T)^t = \alpha T^t$
\item if $T$ is invertible, then $T^t$ is invertible and
\[ (T^t)^{-1} = (T^{-1})^t. \]
\end{enumerate}
Let $T\in\Bounded(U,V)$ and $S\in\Bounded(V,W)$. Then
\begin{enumerate}
\setcounter{enumi}{3}
\item $(ST)^t = T^tS^t$
\end{enumerate}
\end{lemma}


\subsection{Annihilator subspace}
\begin{definition}
Let $U\subset V$ be a subspace. The \udef{annihilator} of $U$, denoted $U^0$, is the set of functionals that are identically zero on $U$:
\[ U^0 = \left\{ \varphi\in V^*\;|\; \forall u\in U:\varphi(u) = 0 \right\}. \]
\end{definition}
\begin{proposition} \label{annihilatorSpace}
Let $U\subset V$ be a subspace and $T\in \Hom(V,W)$.
\begin{enumerate}
\item $U^0$ is a subspace of $\adual{V}$;
\item $\dim \adual{U} + \dim U^0 = \dim \adual{V}$;
\item $\ker T^t = (\im T)^0$
\item $T$ is surjective \textup{if and only if} $T^t$ is injective.
\end{enumerate}
\end{proposition}
\begin{proof}
\mbox{}
\begin{enumerate}
\item Elementary application of subspace criterion, proposition \ref{subspaceCriterion}.
\item Consider the inclusion $\iota: U\hookrightarrow V$. Then the dimension theorem \ref{dimensionLinearMaps} applied to $\iota'$ gives
\[ \dim \im \iota' + \dim \ker\iota' = \dim V^*. \]
Now $\dim \ker\iota'$ are $\varphi\in V^*$ such that $\varphi \circ \iota = 0$. These are exactly the elements of the annihilator. Any functional on $U$ can be extended to a functional on $V$, so $\iota'$ is surjective and $\dim \im \iota' = \dim U^*$.
\item There are two inclusions. First assume $\varphi \in \ker T'$, so $\forall v\in V$
\[ 0 = (\varphi\circ T)(v) = \varphi(Tv). \]
Thus $\varphi\in(\im T)^0$. The other inclusion uses the same equality.
\item $T\in\Hom(V,W)$ is surjective iff $\im T = W$ iff $(\im T)^0 = \{0\}$ iff $\ker T' = \{0\}$ iff $T'$ is injective.
\end{enumerate}
\end{proof}



\subsection{Real functionals}
\begin{definition}
Let $V$ be a real or complex vector space. Let $f: V\to \R$ be a real functional. We say
\begin{itemize}
\item $f$ is \udef{subadditive} or satisfies the \udef{triangle inequality} if $\forall x,y\in V: f(x+y) \leq f(x) + f(y)$;
\item $f$ is \udef{convex} if $\forall x,y\in V, \lambda\in[0,1]: f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$;
\item $f$ is \udef{positively homogeneous} if $\forall x\in V,\lambda\geq 0: f(\lambda x) = \lambda f(x)$;
\item $f$ is \udef{absolutely homogeneous} if $\forall x\in V,\lambda\in\F: f(\lambda x) = |\lambda| f(x)$;
\item $f$ \udef{separates points} if $\forall v\in V: f(v) = 0 \implies v = 0$.
\end{itemize}
We call $f$
\begin{itemize}
\item \udef{sublinear} if it is subadditive and positively homogeneous;
\item a \udef{seminorm} if it is subadditive and absolutely homogeneous;
\item a \udef{norm} if it is a point-separating seminorm.
\end{itemize}
\end{definition}
TODO general valued fields.

\begin{lemma}
Let $V$ be a real or complex vector space and $f: V\to \R$ be a real functional. Then
\begin{enumerate}
\item absolute homogeneity $\implies$ positive homogeneity;
\item subadditivity+positive homogeneity $\implies$ convexity $\implies$ subadditivity.
\end{enumerate}
\end{lemma}
Thus norms and seminorms are sublinear.

\begin{lemma}
A subadditive, absolutely homogenous function $f:V\to \R$ is non-negative:
\[ f: V\to \R_{\geq 0}. \]
Thus norms and seminorms are functions $V\to \R_{\geq 0}$.
\end{lemma}
\begin{proof}
For all $v\in V$ we have $0 = f(v-v) \leq f(v)+f(-v) = 2f(v)$, so $f(v) \geq 0$.
\end{proof}

\begin{proposition}[Reverse triangle inequality] \label{reverseTriangleInequality}
Let $V$ be a vector space and $\norm{\cdot}: V\to \R$ a function that satisfies the triangle inequality and has $\norm{-v} = \norm{v}$ for all $v\in V$. Then $\forall v,w\in V$:
\begin{enumerate}
\item $|\norm{v}-\norm{w}|\leq \norm{v-w}$;
\item $|\norm{v}-\norm{w}|\leq \norm{v+w}$.
\end{enumerate}
In particular this holds if $\norm{\cdot}$ is a norm or seminorm.
\end{proposition}
\begin{proof}
We calculate $\norm{v} = \norm{v-w+w} \leq \norm{v-w} + \norm{w}$, so $\norm{v}-\norm{w}\leq \norm{v-w}$. By swapping $v\leftrightarrow w$ we also get $-\norm{v}+\norm{w}\leq \norm{w-v} = \norm{v-w}$ and thus the first inequality is established.

For the second inequality, set $w\to -w$ and use $\norm{-w} = \norm{w}$.
\end{proof}

\subsubsection{Epigraphs}
\begin{definition}
Let $V$ be a vector space and $f: V\to \R$ a real functional on $V$. Then \udef{epigraph} of $f$ is defined as
\[ \epigraph(f) \defeq \setbuilder{(v,r)\in V\times \R}{f(v)\leq r}. \]
\end{definition}

\begin{lemma} \label{epigraphLemma}
Let $V$ be a vector space and $f: V\to \R$ a real functional on $V$. Then for all $v\in V$:
\[ f(v) = \inf\setbuilder{r}{(v,r)\in \epigraph(f)}. \]
\end{lemma}

\begin{proposition}
Let $V$ be a real vector space and $f: V\to \R$ a functional. Then
\begin{enumerate}
\item $f$ is convex \textup{if and only if} $\epigraph(f)$ is a convex subset of $V\oplus \R$;
\item $f$ is positively homogeneous \textup{if and only if} $\epigraph(f)$ is a cone in $V\oplus \R$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) First assume $f$ convex and pick $(v, s), (w,t)\in \epigraph(f)$ and $\lambda\in [0,1]$. Then we need to show that $(\lambda v + (1-\lambda)w, \lambda s + (1-\lambda)t) \in \epigraph(f)$. This is equivalent to saying $f(\lambda v + (1-\lambda)w) \leq \lambda s + (1-\lambda)t$. Indeed we have $f(\lambda v + (1-\lambda)w) \leq \lambda f(v) + (1-\lambda)f(w) \leq \lambda s + (1-\lambda)t$ by the convexity of $f$.

Conversely, assume $\epigraph(f)$ convex. Then $(v, f(v)), (w,f(w))\in \epigraph(f)$, $(\lambda v + (1-\lambda)w, \lambda f(v) + (1-\lambda)f(w)) \in \epigraph(f)$ for all $\lambda\in [0,1]$. This implies $f(\lambda v + (1-\lambda)w) \leq \lambda f(v) + (1-\lambda)f(w)$.

(2) First assume $f$ is positively homogeneous, take $(v,s)\in \epigraph(f)$ and $r>0$. Then we need to show that $r(v,s) = (rv,rs)\in \epigraph(f)$. This follows because of the implications $f(v)\leq s \implies rf(v) \leq rs \implies f(rv) \leq rs$.

Conversely, assume that $\epigraph(f)$ is a cone. Then $\lambda\cdot \epigraph(f) = \epigraph(f)$ for all $\lambda>0$ by \ref{coneEqualityLemma}. We then calculate using \ref{epigraphLemma}:
\begin{align*}
f(\lambda v) &= \inf\setbuilder{r}{(\lambda v,r)\in \epigraph(f)} \\
&= \inf\setbuilder{r}{(\lambda v,r)\in \lambda\cdot\epigraph(f)} \\
&= \inf\setbuilder{r}{\lambda(v,\lambda^{-1}r)\in \lambda\cdot\epigraph(f)} \\
&= \inf\setbuilder{r}{(v,\lambda^{-1}r)\in \epigraph(f)} \\
&= \inf\setbuilder{\lambda r}{(v,r)\in \epigraph(f)} = \lambda f(v).
\end{align*} 
\end{proof}
\begin{corollary}
A functional on a real vector space is sublinear \textup{if and only if} its epigraph is a convex cone.
\end{corollary}

\subsubsection{Convex functionals}

\begin{proposition}
Let $p: V\to\R$ be convex functional. Then
\[ P: V\to\R: x\mapsto \inf_{t>0} t^{-1}p(tx) \]
is sublinear and $P(x)\leq p(x)$.

Also, if $f:V\to \R$ is a linear functional, then $f\leq p \iff f\leq P$.
\end{proposition}
\begin{proof}
For sublinearity: let $x,y\in V$, then for all $s,t>0$
\[ P(x+y) \leq \frac{s+t}{st}p\left(\frac{st}{s+t}(x+y)\right) = \frac{s+t}{st}p\left(\frac{s}{s+t}(tx)+\frac{t}{s+t}(sy)\right) \leq t^{-1}p(tx) + s^{-1}p(sy). \]
This implies that $P(x+y)\leq P(x)+P(y)$.

For positive homogeneity: let $x\in V,\lambda\geq 0$
\[ P(\lambda x) = \inf_{t>0} t^{-1}p(t\lambda x) = \inf_{t\lambda>0} \lambda (t\lambda)^{-1}p(t\lambda x) = \inf_{t>0} \lambda (t)^{-1}p(tx) = \lambda P(x). \]

Finally we prove that $f\leq p \implies f\leq P$ for linear functionals $f$. For all $t>0$ we have $f(tx) \leq p(tx)$, which implies $f(x) = t^{-1}f(tx) \leq t^{-1}p(tx) \leq P(x)$. So $f\leq P$.
\end{proof}

\subsubsection{Seminorms}
\begin{lemma}
The kernel of a seminorm is a vector space.
\end{lemma}
Note this does not follow from \ref{kernelSubspace} because seminorms are not linear.
\begin{proof}
Let $p:V\to \R$ be a seminorm. We verify the subspace criterion \ref{subspaceCriterion}. First $0\in\ker(p)$ because $p(0) = p(0\cdot 0) = |0|p(0) = 0$.

Now take $v,w\in \ker(p)$ and $\lambda\in \F$. Then $0\leq p(v+\lambda w) \leq p(v)+|\lambda|p(w) = 0$, so $v+\lambda w\in\ker(p)$.
\end{proof}

\begin{proposition} \label{gaugeSeminorms}
Let $V$ be a vector space, $p: V\to \R$ a seminorm and $\lambda\in \F$. Then
\[ \setbuilder{v\in V}{p(v) < \lambda} \qquad\text{and}\qquad \setbuilder{v\in V}{p(v) \leq \lambda} \]
are absolutely convex and absorbent.
\end{proposition}
\begin{proof}
Take $|\mu| + |\nu| \leq 1$ and $v,w\in\setbuilder{v\in V}{p(v) \leq \lambda}$. Then $p(|\mu|v + |\nu|w)\leq |\mu|p(v) + |\nu|p(w) \leq (|\mu|+|\nu|)\lambda \leq \lambda$.

For absorbence, take $v\in V$. Then $\frac{\lambda}{2p(v)} v\in \setbuilder{v\in V}{p(v) < \lambda}$.
\end{proof}

\subsubsection{Gauges}
\begin{definition}
Let $V$ be a vector space and $A\subseteq V$ an absorbent subset. The function
\[ p_A: V\to \R^{\geq 0}: v\mapsto \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda A} \]
is called the \udef{gauge} or \udef{Minkowski functional} of $A$.
\end{definition}
The function $p_A$ is well-defined (i.e.\ $p_A(v)$ is finite for all $v\in V$) because $A$ is absorbent.

\begin{lemma} \label{gaugeLemma}
Let $V$ be a vector space, $A\subseteq V$ an absorbent subset and $\lambda\in \R^{> 0}$. Then
\begin{enumerate}
\item $\lambda > p_A(v) \implies \lambda^{-1}v\in A$;
\item $\lambda^{-1}v\in A \implies \lambda \geq p_A(v)$.
\end{enumerate}
In particular, we have
\[ p_A^{\preimf}[\ball(0,1)] = \setbuilder{v\in V}{p_A(v) < 1} \subseteq A \subseteq \setbuilder{v\in V}{p_A(v) \leq 1} = p_A^{\preimf}[\cball(0,1)]. \]
\end{lemma}
\begin{proposition} \label{gaugeInherenceAdherence}
Let $V$ be a vector space and $A\subseteq V$ an absorbent subset. Then
\begin{enumerate}
\item $p_A^{\preimf}[\ball(0,1)] = \inh_\mathfrak{a}(A)$;
\item $p_A^{\preimf}[\cball(0,1)] = \adh_\mathfrak{a}(A)$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}
We can summarise \ref{gaugeLemma} and \ref{gaugeInherenceAdherence} as
\[ \inh_\mathfrak{a}(A) = \setbuilder{v\in V}{p_A(v) < 1} \subseteq A \subseteq \setbuilder{v\in V}{p_A(v) \leq 1} = \adh_\mathfrak{a}(A). \]

\begin{proposition} \label{gaugeClassification}
Let $V$ be a vector space and $f: V\to \R^{\geq 0}$ a function.
Then the following are equivalent:
\begin{enumerate}
\item $f$ is positively homogenous;
\item for all $A\subseteq V$ such that $f^{\preimf}(\ball(0,1)) \subseteq A \subseteq f^{\preimf}(\cball(0,1))$, $A$ is absorbent and $f = p_A$;
\item $f = p_A$ for some absorbent subset $A$.
\end{enumerate}
\end{proposition}
Note that positive homogeneity is equivalent to strictly positive homogeneity.
\begin{proof}
$(1) \Rightarrow (2)$ To show absorbence, take some $v\in V$. Then for any $\epsilon>0$, $f\big((f(v)+\epsilon)^{-1}v\big) = \frac{f(v)}{f(v)+\epsilon} < 1$, so $(f(v)+\epsilon)^{-1}v\in A$.

Now take some $A$ and fix some arbitrary $v\in V$. We have $p_A(v) = \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda A}$, so
\[ \begin{aligned}
p_A(v) &\leq \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda f^{\preimf}(\ball(0,1))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in f^{\preimf}(\ball(0,\lambda))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{f(v) < \lambda} = f(v)
\end{aligned} \quad\text{and}\quad \begin{aligned}
p_A(v) &\geq \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda f^{\preimf}(\cball(0,1))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in f^{\preimf}(\cball(0,\lambda))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{f(v) \leq \lambda} = f(v).
\end{aligned} \]
We conclude that $f(v) = p_A(v)$.

$(2) \Rightarrow (3)$ Immediate.

$(3) \Rightarrow (1)$ We calculate for $t \geq 0$
\begin{align*}
f(tv) &= \inf\setbuilder{\lambda\in \R^{\geq 0}}{tv\in \lambda A} = \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in t^{-1}\lambda A} \\
&= \inf\setbuilder{t\lambda\in \R^{\geq 0}}{v\in \lambda A} = t\inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda A} = tf(v).
\end{align*}
\end{proof}

\begin{lemma} \label{gaugeZeroLemma}
Let $V$ be a vector space, $A\subseteq V$ an absorbent subset and $a\in A$. If there exists a subspace $U\subseteq A$ such that $a\in U$, then $p_A(a) = 0$.
\end{lemma}
\begin{proof}
For all $\epsilon > 0$, $\epsilon^{-1}a\in A$, so $a\in \epsilon A$.
\end{proof}

\begin{proposition} \label{gaugeProperties}
Let $V$ be a vector space and $A\subseteq V$ an absorbent subset. Then
\begin{enumerate}
\item $p_A$ is absolutely homogenous if $A$ is balanced;
\item $p_A$ is sublinear if $A$ is convex;
\item $p_A$ is point-separating if $A$ is balanced and contains only the trivial subspace.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) By \ref{balancedLemma} we have $\mu A = |\mu| A$ and thus
\begin{align*}
p_A(\mu\cdot v) &= \inf\setbuilder{\lambda\in \R^{\geq 0}}{\mu\cdot v\in \lambda A} = \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \frac{\lambda}{\mu} A} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \frac{\lambda}{|\mu|} A} = \inf\setbuilder{|\mu|\lambda\in \R^{\geq 0}}{v\in \lambda A} = |\mu|\cdot p_A(v).
\end{align*}

(2) We just need to show subadditivity. Positive homogeneity is automatic by \ref{gaugeClassification}. Take $v,w\in V$. Now take arbitrary $\epsilon > 0$, so $(p_A(v)+\epsilon)^{-1}v \in A$ and $(p_A(w)+\epsilon)^{-1}w \in A$ by \ref{gaugeLemma}. By convexity of $A$, we have
\[ \frac{v+w}{p_A(v)+p_A(w)+2\epsilon} = \frac{p_A(v)+\epsilon}{p_A(v)+p_A(w)+2\epsilon}(p_A(v)+\epsilon)^{-1}v + \frac{p_A(w)+\epsilon}{p_A(v)+p_A(w)+2\epsilon}(p_A(w)+\epsilon)^{-1}w \in A. \]
By \ref{gaugeLemma} this means $p_A(v)+p_A(w)+2\epsilon \geq p_A(v+w)$ and because $\epsilon$ was arbitrary, we conclude that $p_A(v+w) \leq p_A(v)+p_A(w)$.

(3) Assume $A$ contains only the trivial subspace. Then for all $v\in V$ there exists some $\lambda\in \F$ such that $\lambda\cdot v\notin A$. Now for all $|c|\geq |\lambda|$, $c\cdot v\notin A$ because $A$ is balanced. Then $p_A(2\lambda\cdot v) \neq 0$ and because $p_A$ is absolutely homogeneous we have $p_A(v) = (2\lambda)^{-1}p_A(2\lambda\cdot v) \neq 0$.
\end{proof}
\begin{corollary}
The gauge of an absolutely convex and absorbent subset is a seminorm. If the subset contains only the trivial subspace, then the gauge is a norm.
\end{corollary}

\begin{proposition} \label{gaugeConstructions}
Let $V$ be a vector space, $A,B\subseteq V$ absolutely convex and absorbent subsets and $\mathcal{E}$ a set of absolutely convex and absorbent subsets.
\begin{enumerate}
\item The gauge of $\lambda A$ is $|\lambda|^{-1}p_A$ for all balanced sets $A$ and $\lambda\in \F\setminus\{0\}$.
\item The gauge of $A\cap B$ is $\max\{p_A, p_B\}$ if $A,B$ are balanced.
\item If $A\subseteq B$, then $p_B \leq p_A$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Take $v\in V$. Then
\[ p_{\lambda A} = \inf\setbuilder{\mu\in\R^{\geq 0}}{v\in \mu\lambda A} = \inf\setbuilder{\mu \in\R^{\geq 0}}{\lambda^{-1}v\in \mu A} = p_A(\lambda^{-1}v) = |\lambda|^{-1}p_A(v), \]
where we have used that $p_A$ is absolutely homogeneous because $A$ is balanced.

(2) TODO
\end{proof}

\begin{proposition} \label{seminormContinuity}
Let $\sSet{V,\xi}$ be a TVS and $p_K:V\to \R^{\geq 0}$ the gauge of some absorbent set $K$. Then the following are equivalent:
\begin{enumerate}
\item $p_K\in \sSet{V,\xi}^*$;
\item $p_K$ is continuous at $0$;
\item $K\in \neighbourhood_\xi(0)$.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ Immediate.

$(2) \Rightarrow (3)$ We have $p_K^{\preimf}[\ball(0,1)] \subseteq K$ by \ref{gaugeClassification}. If $p_K$ is continuous at $0$, then $K$ is a neighbourhood of $0$ by \ref{pretopologicalContinuityVicinities}.

$(3) \Rightarrow (1)$ Assume $K$ a neighbourhood of $0$ and take some neighbourhood $\Gamma$ of $0$ in $\R$. Then $\Gamma$ contains a ball $\ball(0,\epsilon)$. By \ref{pretopologicalContinuityVicinities}, it is enough to show that $p_K^{\preimf}[\ball(0,\epsilon)]$ is a neighbourhood. Indeed
\[ p_K^{\preimf}[\ball(0,\epsilon)] \supseteq p_K^{\preimf}\left[\cball\left(0,\frac{\epsilon}{2}\right)\right] = p_K^{\preimf}\left[\frac{\epsilon}{2}\cball(0,1)\right] = \frac{\epsilon}{2} p_K^{\preimf}[\cball(0,1)] \supseteq \frac{\epsilon}{2} K, \]
and the result follows because $\frac{\epsilon}{2}K$ is a neighbourhood of $0$ by \ref{TVSconstruction}.
\end{proof}

\begin{proposition} \label{gaugeMajorisation}
Let $\sSet{V,\xi}$ be a TVS and $f:V\to \F$ a linear functional. Then $f\in V^*$ \textup{if and only if} $|f| \leq p_K$ for some $K\in \neighbourhood_\xi(0)$.
\end{proposition}
\begin{proof}
If $|f| \leq p_K$ for some $K\in \neighbourhood_\xi(0)$, then $f$ is bounded on $K$ and thus continuous by \ref{continuityToNormedSpace}.

Now assume $f\in V^*$. By \ref{continuityToNormedSpace} $f$ is bounded by some $C\in \R$ on some neighbourhood $M$. By \ref{continuity} we have that $f^\imf[\adh_\xi(M)] \subseteq \cball(0,C)$ and by \ref{inherenceAdherenceInclusion} and \ref{gaugeInherenceAdherence}, $f^\imf[p_M^\preimf(\cball(0,1))] = f^\imf[\adh_\mathfrak{a}(M)] \subseteq f^\imf[\adh_\xi(M)]$. This means that $\left|f\left(\frac{x}{p_M(x)}\right)\right| \leq C$ for all $x\in V$. Then $p_M(x)^{-1}|f(x)| \leq C$ and, using \ref{gaugeConstructions},
\[ |f(x)| \leq C p_M(x) = p_{C^{-1}\cdot M}(x). \]
Thus we can take $K = C^{-1}\cdot M$, which is a neighbourhood of the origin by \ref{TVSconstruction}.
\end{proof}


\subsection{Initial convergence w.r.t. seminorms}

\begin{proposition} \label{initialSeminormConvergence}
Let $V$ be a vector space and $S$ a set of seminorms on $V$. Let $\xi$ be the initial convergence on $V$ w.r.t. $S$. Then
\begin{enumerate}
\item $\xi$ is a topological vector space convergence;
\item $\begin{aligned}[t]
\neighbourhood_\xi(0) &= \mathfrak{F}\setbuilder{p^\preimf[\ball(0,\epsilon)]}{p\in S, \epsilon > 0} \\
&= \mathfrak{F}\setbuilder{p^\preimf[\cball(0,\epsilon)]}{p\in S, \epsilon > 0}
\end{aligned}$
\item $f\in \sSet{V, \xi}^*$ \textup{if and only if}
\begin{itemize}
\item $f\in \sSet{V,\mathfrak{a}}^*$;
\item there exists a finite subset $A\subseteq S$ and $C>0$ such that $|f(v)| \leq C\max_{g\in A} g(v)$ for all $v\in V$.
\end{itemize}
\end{enumerate}
\end{proposition}
\begin{proof}
(1, 2) That $\xi$ is topological follows from \ref{topologicalInitialFinalConvergence}. Point (2) follows from \ref{pretopologicalInitialFinalConvergence}.

To show $\xi$ is a vector space convergence, we verify the conditions in \ref{TVSconstruction}:
\begin{enumerate}
\item Take $\lambda\in \F$ and $U\in \neighbourhood_\xi(0)$. Then there exist $p\in S$ and $\epsilon > 0$ such that $p^\preimf[\ball(0,\epsilon)] \subseteq U$. Then
\[ \lambda U \subseteq \lambda p^\preimf[\ball(0,\epsilon)] = p^\preimf[|\lambda|\ball(0,\epsilon)] = p^\preimf[\ball(0,|\lambda|\cdot \epsilon)], \]
so $\lambda U\in \neighbourhood_\xi(0)$.
\item Take $U\in \neighbourhood_\xi(0)$, so there exist $p\in S$ and $\epsilon > 0$ such that $p^\preimf[\ball(0,\epsilon)] \subseteq U$. Then
\[ p^\preimf[\ball(0,\epsilon/2)] + p^\preimf[\ball(0,\epsilon/2)] \subseteq p^\preimf[\ball(0,\epsilon)] \subseteq U. \]
\item By \ref{coreProperties}.
\item We claim $p^\preimf[\ball(0,\epsilon)]$ is balanced for all $p\in S$ and $\epsilon > 0$. Indeed for all $|r|\leq 1$
\[ rp^\preimf[\ball(0,\epsilon)] = p^\preimf[|r|\cdot \ball(0,\epsilon)] = p^\preimf[\ball(0,|r|\cdot \epsilon)] \subseteq p^\preimf[\ball(0,\epsilon)]. \]
\end{enumerate}

(3) We have by \ref{continuityLinearFunctionals}
\[ f\in \sSet{V, \xi}^* \iff \exists D>0: \exists U\in \neighbourhood_\xi(0):\; f^\imf[U] \subseteq \cball(0,D). \]
Now $U\in \neighbourhood_\xi(0)$ iff there exists a finite $A = \{p_n^\preimf[\ball(0,\epsilon_n)]\}_{n=0}^N \subseteq \setbuilder{p^\preimf[\ball(0,\epsilon)]}{p\in S, \epsilon > 0}$ such that $\bigcap A\subseteq U$. So WLOG we may take $U$ of this form.
Now 
\begin{align*}
f^\imf\Big[\bigcap A\Big] \subseteq \cball(0,D) &\iff \forall v\in V: \; \Big(\forall n\leq N: p_n(v)\leq \epsilon_n \Big) \implies |f(v)|\leq D \\
&\iff \forall v\in V: \; \max_{n\leq N}\epsilon_n^{-1}p_n(v)\leq 1 \implies |f(v)|\leq D \\
&\iff \forall v\in V: \; \max_{n\leq N}\epsilon_n^{-1}p_n\left(\frac{\max_{n\leq N}\epsilon_n^{-1}p_n(v)}{\max_{n\leq N}\epsilon_n^{-1}p_n(v)} v\right)\leq 1 \implies |f(v)|\leq D \\
&\iff \forall v\in V: \; \left(\max_{n\leq N}\epsilon_n^{-1}p_n(v)\right)^{-1}\max_{n\leq N}\epsilon_n^{-1}p_n(v)\leq 1 \implies \left(\max_{n\leq N}\epsilon_n^{-1}p_n(v)\right)^{-1}|f(v)|\leq D \\
&\iff \forall v\in V: \; 1\leq 1 \implies |f(v)|\leq D\max_{n\leq N}\epsilon_n^{-1}p_n(v) \\
&\iff \forall v\in V: \; |f(v)|\leq D\max_{n\leq N}\epsilon_n^{-1}p_n(v).
\end{align*}
WLOG we may take all $\epsilon_n = \epsilon = \min_{n\leq N}\epsilon_n$. We may then take $C = D/\epsilon$.
\end{proof}

\begin{proposition} \label{locallyConvexSeminormTopology}
Let $V$ be a vector space. A convergence on $V$ is locally convex \textup{if and only if} it is the initial convergence w.r.t. some set $S$ of seminorms on $V$.
\end{proposition}
\begin{proof}
If $V$ has the initial convergence w.r.t. $S$, then $V$ is locally convex by \ref{initialSeminormConvergence} because $p^\preimf[\ball(0,\epsilon)]$ is convex.

Now let $V$ be a locally convex TVS, meaning there exists a convex base $\mathcal{B}$ of $\neighbourhood(0)$. Then $S = \setbuilder{p_B}{B\in \mathcal{B}}$ is a set of continuous seminorms, by \ref{seminormContinuity}. 

In order to show that the convergence on $V$ is initial w.r.t. $S$, we verify the form of $\neighbourhood(0)$ given in \ref{initialSeminormConvergence}.

We may take $\mathcal{B}$ to consist of algebraically open convex sets by replacing it with $\inh_\mathfrak{a}^\imf[\mathcal{B}]$, which contains open convex sets by \ref{coreProperties}. Then
\begin{align*}
\neighbourhood(0) &= \mathfrak{F}(\mathcal{B}) \\
&= \mathfrak{F}\setbuilder{p_B^\preimf[\ball(0,1)]}{B\in \mathcal{B}} \\
&= \mathfrak{F}\setbuilder{\epsilon^{-1} p_B^\preimf[\ball(0,1)]}{B\in \mathcal{B}} \\
&= \mathfrak{F}\setbuilder{p_B^\preimf[\ball(0,\epsilon)]}{B\in \mathcal{B}, \epsilon > 0} \\
&= \mathfrak{F}\setbuilder{p^\preimf(\ball(0,\epsilon))}{p\in S, \epsilon > 0},
\end{align*}
where we have used \ref{gaugeInherenceAdherence} and the fact that the convergence is a vector space convergence, so $\epsilon B \in \upset\mathcal{B}$.
\end{proof}

\begin{proposition}
Let $V$ be a vector space. The functions
\begin{align*}
\powerset\setbuilder{A\subseteq V}{\text{$A$ is convex}} &\to \setbuilder{p: V\to \R}{\text{$p$ is a seminorm}}: &&\mathcal{B}\mapsto \setbuilder{p_K}{K\in \mathcal{B}} \\
\setbuilder{p: V\to \R}{\text{$p$ is a seminorm}} &\to \powerset\setbuilder{A\subseteq V}{\text{$A$ is convex}}: &&S\mapsto \setbuilder{p^\preimf[U]}{p\in S, U\in \neighbourhood_\R(0)}
\end{align*}
form an antitone Galois connection, where we order the seminorms pointwise.
\end{proposition}
\begin{proof}
TODO + previous as corollary
\end{proof}

Note: metrisable is not equivalent to normable!






\subsection{Hahn-Banach extension theorems}
\begin{theorem}[Hahn-Banach majorised by convex functionals] \label{convexHahnBanach}
Let $V$ be a real vector space, $U\subset V$ a subspace and $p$ a convex functional on $V$. Let $f:U\to\R$ be a linear functional that is bounded by $p$:
\[ \forall u\in U: \quad f(u) \leq p(u). \]
Then $f$ has an extension $\tilde{f}: V\to \R$ such that $\tilde{f}$ is a linear functional on $V$ bounded by $p$:
\[ \forall v\in V: \tilde{f}(v) \leq p(v) \qquad \text{and} \qquad \forall u\in U: \tilde{f}(u) = f(u). \]
\end{theorem}
\begin{proof}
As a first step, we want to extend $f$ to a functional $g$ on a space that is one dimension larger than $U$. This means $g$ is of the form
\[ g: U\oplus\Span\{v_1\}\to\R: v + \alpha v_1 \mapsto f(v) + \alpha c \]
for some $v_1\in V\setminus U$.

If we want $g$ to be majorised by $p$, then we need to find a $c$ such that
\[ \forall v\in U: \forall \alpha\in\R: \; g(\alpha v_1 + v) = \alpha c + f(v) \leq p(\alpha v_1 + v) \]
this means that we need
\[ \forall v\in U: \forall \alpha\in\R:\; \frac{-p(v - |\alpha|v_1) + f(v)}{|\alpha|} \leq c \leq \frac{p(v + |\alpha|v_1) - f(v)}{|\alpha|} \]
and we can find such a $c$ if and only if
\[ \forall v\in U: \forall \alpha\in\R:\; -p(v - |\alpha|v_1) + f(v) \leq p(v + |\alpha|v_1) - f(v), \]
which is equivalent to $2f(v) \leq p(v+|\alpha|v_1)+p(v-|\alpha|v_1)$. This follows from
\begin{align*}
f(v) \leq p(v) &= p(\tfrac{1}{2}(v+|\alpha|v_1) + \tfrac{1}{2}(v-|\alpha|v_1)) \\
&\leq \tfrac{1}{2}p(v+|\alpha|v_1) + \tfrac{1}{2}p(v-|\alpha|v_1).
\end{align*}
So we can extend the domain of $f$ by one dimension such that it is still majorised by $p$.

We can iterate the construction to extend $f$ by multiple dimensions. Each extension can be viewed as a subset of $V\times \R$, by identifying it with its graph.
Consider the family of all such subsets that determine a majorised extension of $f$ (not just those obtained by iteration of the previous construction!). This is a family of finite character. We apply the Teichmller-Tukey lemma, \ref{ZornEquivalents}, to obtain a maximal element.

This maximal element has domain $V$, because if it did not, it could be extended and was not a maximal element.
\end{proof}
Clearly if $V$ has a well-ordered Hamel basis, we do not need choice as we can just take successive $v$s in the basis and find $c$s constructively.
\begin{corollary}[Hahn-Banach majorised by sublinear functionals] \label{sublinearHahnBanach}
Any majorant $p$ that is sublinear is also convex and can be used in the Hahn-Banach theorem.
\end{corollary}
\begin{corollary}[Hahn-Banach majorised by seminorms] \label{seminormHahnBanach}
Let $(\mathbb{F},V,+)$ be a real or complex vector space, $U\subset V$ a subspace and $p$ a seminorm on $V$. Let $f:U\to\mathbb{F}$ be a linear functional that is bounded by $p$:
\[ \forall u\in U: \quad |f(u)| \leq p(u). \]
Then $f$ has an extension $\tilde{f}: V\to \R$ such that $\tilde{f}$ is a linear functional on $V$ bounded by $p$:
\[ \forall v\in V: |\tilde{f}(v)| \leq p(v) \qquad \text{and} \qquad \forall u\in U: \tilde{f}(u) = f(u). \]
\end{corollary}
\begin{proof}
First assume $V$ is a \emph{real} vector space. Because every seminorm is a sublinear function, we can use \ref{sublinearHahnBanach} to find an extension $\tilde{f}$. We then just need to check it satisfies $\forall v\in V: |\tilde{f}(v)| \leq p(v)$.
From \ref{sublinearHahnBanach} we know $\forall v\in V: \tilde{f}(v) \leq p(v)$.
To prove $-\tilde{f}(v) \leq p(v)$, we calculate
\[ -\tilde{f}(v) = \tilde{f}(-v) \leq p(-v) = |-1|p(v) = p(v). \]

If $V$ is a \emph{complex} vector space, consider the realification $V_\R$ and apply the preceding proof to obtain a linear functional $g: V_\R \to \R$ that extends $f$ and is majorised by $p$. Then by \ref{complexRangeExtensionRealFunctional} we can find a complex-linear functional $\tilde{f}:V \to \C$ such that $\Re(\tilde{f}) = g$.

We just need to show that $f$ is bounded by $p$. Take arbitrary $v\in V$ and write $\tilde{f}(v) = |\tilde{f}(v)|e^{i\theta}$ then
\[ |\tilde{f}(v)| = \Re|\tilde{f}(v)| = \Re\Big(e^{-i\theta}\tilde{f}(v)\Big) = \Re\Big(\tilde{f}(e^{-i\theta}v)\Big) = g(e^{-i\theta}v) \leq p(e^{-i\theta}v) = |e^{-i\theta}|p(v) = p(v). \]
\end{proof}
\begin{corollary}
Let $V$ be a locally convex vector space, $U\subseteq V$ a subspace and $f:U\to \F$ a continuous functional. Then $f$ has a continuous extension to $V$.
\end{corollary}
\begin{proof}
We have that $|f|\leq p_K$ for some $K\in \neighbourhood_U(0)$ by \ref{gaugeMajorisation}. By continuity of the inclusion map, we can find an $M \in \neighbourhood_V(0)$ such that $M\cap U = K$. Then $|f|\leq p_M$ and $f$ can be extended by the Hahn-Banach extension theorem. By \ref{gaugeMajorisation} this extension is continuous.
\end{proof}
\begin{corollary} \label{locallyConvexTVSDualPair}
Let $V$ be a Hausdorff locally convex vector space and $v\in V$. If $f(v) = 0$ for all $f\in V^*$, then $v = 0$.
\end{corollary}
\begin{corollary}
Let $X$ be a normed space and $Z\subset X$ a subspace. Any bounded linear functional in $\tdual{Z}$ can be extended to a bounded linear functional in $\tdual{X}$ with the same norm.
\end{corollary}
\begin{proof}
Let $f:Z\to \mathbb{F}$ be such a functional. Extend $f$ by the previous theorem, \ref{seminormHahnBanach}, using $p(x) = \norm{f}_Z\norm{x}$.
\end{proof}
\begin{corollary} \label{existenceBoundedFunctionalOfSameNorm}
Let $X$ be a normed space and $x_0\neq 0$ an element of $X$. Then there exists a bounded linear functional $\omega_{x_0}$ such that
\[ \norm{\omega_{x_0}} = 1 \qquad \text{and} \qquad \omega_{x_0}(x_0)=\norm{x_0}. \]
\end{corollary}
\begin{proof}
Extend the functional $f: \Span\{x_0\}\to \mathbb{F}$ defined by
\[ f(x) = f(ax_0) = a\norm{x_0}. \]
\end{proof}
\begin{corollary}
Let $X$ be a normed space. Then $\forall x\in X:$
\[ \norm{x} = \sup_{\substack{f\in X' \\ f\neq 0}}\frac{|f(x)|}{\norm{f}}. \]
\end{corollary}
\begin{proof}
We calculate
\[ \norm{x} \geq \sup_{\substack{f\in X' \\ f\neq 0}}\frac{|f(x)|}{\norm{f}} \geq \frac{|\omega_{x}(x)|}{\norm{\omega_{x}}} = \frac{\norm{x}}{1} = \norm{x} \]
where the first inequality follows from $|f(x)|\leq \norm{f}\norm{x}$ for all $f\in X', x\in X$.
\end{proof}

\subsubsection{Hahn-Banach separation}

\begin{lemma} \label{gaugeSeparationLemma}
Let $V$ be a real vector space, $A$ an absorbent set and $x_0 \notin A$. Consider the functional $f_{x_0}: \Span\{x_0\}\to \F: tx_0 \mapsto t$. Then $f_{x_0}(x)\leq p_A(x)$ for all $x\in \Span\{x_0\}$.
\end{lemma}
\begin{proof}
Let $x = tx_0$. If $t\leq 0$, then the inequality is immediate. Suppose $t>0$. Because $p_A(x_0) \geq 1$ (by the converse of \ref{gaugeLemma}), we have
\[ f_{x_0}(x) = f_{x_0}(tx_0) = t \leq tp_A(x_0) = p_A(tx_0) = p_A(x)  \]
using positive homogeneity (\ref{gaugeClassification}).
\end{proof}

\begin{theorem}[Mazur] \label{MazurTheorem}
Let $V$ be a real or complex convergence vector space and $A$ an open and convex set. If $U$ is a subspace such that $A\perp U$, then there exists a closed hyperplane $H \supseteq U$ such that $A\perp H$.
\end{theorem}
\begin{proof}
First suppose $V$ is a \emph{real} vector space. Because $A$ is open, it is algebraically open. Take $a\in A$. Then $0\in a-A = \inh_{\mathfrak{a}}(a-A)$, so $a-A$ is absorbing by \ref{coreProperties}. 

Then we have
\[ U\perp A \iff 0\notin U-A \iff a \notin a-A+U. \]
Consider the functional $f_{a}$ of \ref{gaugeSeparationLemma}, which is majorised by the gauge $p_{a-A+U}$, which is sublinear by \ref{gaugeProperties}. Then $f_a$ can be extended as an $\R$-linear function to all $V$ by the Hahn-Banach extension theorem \ref{sublinearHahnBanach}.

We note that $U\subseteq \ker(f_a)$, because $p_{a-A+U}(u) = 0$ by \ref{gaugeZeroLemma}.

In order to conclude with \ref{functionalBoundedNeighbourhood}, we need to show that $A-a \subseteq f_a^{\preimf}(\ball(0,|f_a(a)|)) = f_a^{\preimf}(\ball(0,1))$.
Indeed $A-a \subseteq U+A-a = \inh_\mathfrak{a}(U+A-a) = p_{U+A-a}^\preimf[\ball(0,1)] \subseteq f_{a}^\preimf[\ball(0,1)]$ by \ref{algebraicallyOpen} and \ref{gaugeInherenceAdherence}.

Note that $\ker(f_a)^c$ contains the open set $A$ and thus $\ker(f_a)$ is not dense by \ref{openDensityLemma}. By \ref{hyperplaneClosedDense} this means that $\ker(f_a)$ is closed.

Now suppose $V$ is a \emph{complex} vector space. We can consider the realification $V_\R$ with the same convergence, which is a real convergence vector space by \ref. So we can use the preceding proof to find a real hyperplane $K$ in $V$. Then \ref{realComplexHyperplane} gives that $H = K\cap iK$ is a complex hyperplane in $V$. Now $H$ and $A$ must be disjoint because $K$ and $A$ are disjoint and $H \subseteq K$.

Also $H$ is closed because $K$ and $iK$ are closed (the first by the preceding proof, the second because multiplication by $i$ is a homeomorphism \ref{continuityLemmaVectorConvergence}) and the intersection of two closed sets is closed.
\end{proof}

\begin{theorem}[Hahn-Banach separation theorem]
Let $V$ be a convergence vector space. Suppose $A,B$ are disjoint, non-empty, convex sets and that $A$ is open. Then there exists a continuous linear functional $f:V\to \F$ such that $f^\imf[A]$ and $f^\imf[B]$ are disjoint.
\end{theorem}
\begin{proof}
The set $A-B = \bigcup_{b\in B}A-b$ is convex and a union of open sets and thus open by \ref{completeClosureTopology}.
The set $A-B$ and the vector space $\{0\}$ are disjoint, so by \ref{MazurTheorem} we can find a closed hyperplane that is disjoint with $A-B$.

By \ref{kernelHyperplane} and \ref{continuityLinearFunctionals} this is the kernel of a continuous linear functional $f$.
\end{proof}
\begin{corollary}
Let $V$ be a real or complex convergence vector space and $A,B$ as in the proposition. Then there exists a continuous linear functional $f:V\to \F$ and $t\in \R$ such that
\[ \Re f(a) < t \leq \Re f(b) \]
for all $a\in A$ and $b\in B$.
\end{corollary}
This means $A$ and $B$ are separated by a closed affine hyperplane $\ker(f)+v$, where $v \in f^\preimf[\{t\}]$.
\begin{proof}
Apply the proposition to the realification $V_\R$. This gives us an $\R$-linear functional $g: V\to \R$ such that $g^\imf[A]$ and $g^\imf[B]$ are disjoint convex sets. Additionally $g^{\imf}[A]$ is open in $\R$ by \ref{linearFunctionalOpen}.

Because $g^\imf[A]$ and $g^\imf[B]$ are convex, we either have $g^\imf[A]\leq g^\imf[B]$ or $g^\imf[A]\geq g^\imf[B]$. In the second case we simply replace $g$ by $-g$ to obtain the first case. We may take $t= \sup g^\imf[A]$. This is not in $g^\imf[A]$ because it is open.

If $V$ is a real vector space we take $f=g$ are done. If $V$ is complex, we can find a suitable $f$ by \ref{complexRangeExtensionRealFunctional}.
\end{proof}
\begin{corollary}
Let $V$ be a locally convex TVS. Suppose $A,B$ are disjoint, non-empty, convex sets and that $A$ is compact, $B$ is closed. Then there exists a continuous linear functional $f:V\to \F$ and $s,t\in \R$ such that
\[ \Re f(a) < t < s < \Re f(b) \]
for all $a\in A$ and $b\in B$.
\end{corollary}
\begin{proof}
TODO
\end{proof}

\subsubsection{Banach limits}
\begin{proposition}
There exists a linear map $L:l^\infty(\N) \to \C$ satisfying
\begin{enumerate}
\item $\displaystyle L(x) = \lim_{n\to \infty}x_n$ if the limit exists;
\item $L((x_{n+1})_{n\in\N}) = L((x_n)_{n\in\N})$;
\item if $\forall n\in\N:x_n\geq 0$, then $L(x) \geq 0$;
\item $\norm{L} = 1$.
\end{enumerate}
Such a linear map is called a \udef{Banach limit}.
\end{proposition}
\begin{proof}
TODO, after Cesro means.
\end{proof}

\subsection{Continuous functionals}

TODO???
\begin{proposition}
Let $V$ and $W$ be TVSs and $f: V\to W$ a linear function.
\begin{enumerate}
\item If $f$ is continuous and $W$ is Hausdorff, then $\ker(f)$ is closed.
\item If $f$ has closed kernel and finite-dimensional image, then $f$ is continuous.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Because $W$ is Hausdorff, it is also $T_1$ and thus $\{0\}$ is closed by \ref{FrechetCharacterisation}. Then $\ker(f) = f^{\preimf}(\{0\})$ is closed by \ref{continuity}.

(2) 
\end{proof}
??



\section{General duality theory}
\subsection{Paired spaces}
\begin{definition}
A \udef{pairing} is a triple $\sSet{V,W, \pair{\cdot,\cdot}}$ where $V,W$ are vector spaces over $\mathbb{F}$ and $\pair{\cdot,\cdot}: V\times W\to \mathbb{F}$ is a bilinear form. Often we will write the pairing as just $\sSet{V,W}$.

We say $W$ \udef{distinguishes} points of $V$ or is \udef{separating} on $V$ if $\pair{v,\cdot}$ is injective for all $v\in V$.

A \udef{dual system}, \udef{dual pair} or \udef{duality} over a field $\mathbb{F}$ is a pairing $\sSet{V,W, b}$ such that $V$ distinguishes points of $W$ and $W$ distinguishes points of $V$.
\end{definition}
We have that $W$ is separating on $V$ \textup{if and only if}
\[ \forall v\in V: \exists w\in W: \pair{v,w} \neq 0. \]

\begin{example}
\begin{itemize}
\item Let $V$ be a vector space. Then $\sSet{V^*, V, \pair{\cdot,\cdot}}$ with $\pair{\cdot,\cdot}:V^*\times V: (f,v)\mapsto \pair{f,v} = f(v)$ is a dual pair.
\item Let $\sSet{V,\xi}$ be a Hausdorff locally convex TVS. Then $\sSet{\sSet{V,\xi}^*, V, \pair{\cdot,\cdot}}$ is a dual pair by \ref{locallyConvexTVSDualPair}.
\end{itemize}
\end{example}

\begin{lemma}
Let $\sSet{V,W,\pair{\cdot,\cdot}}$ be a dual pair. Then $\sSet{W,V,\pair{\cdot,\cdot}^d}$ is also a dual pair.
\end{lemma}

\subsection{The weak topology}
\begin{definition}
Let $(X,Y,\pair{\cdot,\cdot})$ be paired vector spaces. Then for each $y\in Y$, the map
\[ \abspair{x,\cdot}: Y\to \R_{\geq 0}: y\mapsto \abspair{x,y} \]
determines a seminorm on $Y$.

The initial topology on $Y$ w.r.t. $\setbuilder{\abspair{x,\cdot}}{x\in X}$ is called the \udef{weak topology} $\sigma(X,Y)$ on $Y$ for the pair $\sSet{X,Y}$\footnote{What we denote $\sigma(X,Y)$ is usually denoted $\sigma(Y,X)$.}.
\end{definition}
\begin{lemma}
Let $\sSet{X,Y,\pair{\cdot,\cdot}}$ be a pairing. The weak topology $\sigma(X,Y)$ on $Y$ is locally convex and
\[ \neighbourhood_{\sigma(X,Y)}(0) = \mathfrak{F}\setbuilder{y\in Y}{\exists x\in X: \abspair{x,y} \leq 1}. \]
\end{lemma}
\begin{proof}
The weak topology is locally convex by \ref{locallyConvexSeminormTopology}. From this we also get the neighbourhood filter: it is enough to show that $\setbuilder{\abspair{x,\cdot}^{\preimf}(\,[0,\epsilon]\,)}{x\in X, \epsilon > 0} = \setbuilder{y\in Y}{\exists x\in X: \abspair{x,y} \leq 1}$. Then
\begin{align*}
y \in \setbuilder{\abspair{x,\cdot}^{\preimf}(\,[0,\epsilon]\,)}{x\in X, \epsilon > 0} &\iff \exists x\in X: \exists \epsilon > 0: \; \abspair{x,y} \leq \epsilon \\
&\iff \exists x\in X: \exists \epsilon > 0: \; \epsilon^{-1}\abspair{x,y} \leq 1 \\
&\iff \exists x\in X: \exists \epsilon > 0: \; \abspair{\epsilon^{-1}x,y} \leq 1 \\
&\iff \exists x\in X: \; \abspair{x,y} \leq 1 \\
&\iff y\in \setbuilder{y\in Y}{\exists x\in X: \abspair{x,y} \leq 1}.
\end{align*}
\end{proof}

\begin{lemma} \label{functionalContinuityWeakTopology}
Let $\sSet{X,Y,\pair{\cdot,\cdot}}$ be a pairing and $f: Y\to \F$ a linear functional. Then
\[ f\in \sSet{Y, \sigma(X,Y)}^* \iff \exists \{x_0,\ldots, x_n\}\subseteq X: \exists \{\alpha_0,\ldots, \alpha_n\}\subseteq \F: f = \sum_{i=0}^n \alpha_i \pair{x_i, \cdot}.  \]
\end{lemma}
\begin{proof}
By a comparison of \ref{linearDependenceLinearFunctionals} and point (3) of \ref{initialSeminormConvergence}.
\end{proof}

\begin{proposition}
Let $V$ be a locally convex TVS. Then $V^* = \sSet{V, \sigma(V^*, V)}^*$.
\end{proposition}
\begin{proof}
This follows easily from \ref{functionalContinuityWeakTopology}:

If $f\in V^*$, then $f = \pair{f,\cdot}$ and thus $f\in \sSet{V, \sigma(V^*, V)}^*$.

If $f\in \sSet{V, \sigma(V^*, V)}^*$, then $f$ is a linear combination of continuous functions and thus continuous, because the composition of linear functions is continuous.
\end{proof}

\subsubsection{Weak-$*$ topology}

\begin{proposition} \label{weak*continuousFunctional}
Let $X$ be a Banach space and let $X'$ have the weak-$*$ topology. Then a linear functional $\theta: X'\to \C$ is continuous \textup{if and only if}
\[ \exists x\in X: \forall \omega\in X': \quad \theta(\omega) = \omega(x). \]
\end{proposition}
\begin{proof}
TODO 9.2 in lecture notes.
\end{proof}

\subsection{Polar sets}
\begin{definition}
Let $\sSet{X,Y,\pair{\cdot,\cdot}}$ be a pairing and $B\subseteq Y$ a subset. The \udef{polar} of $B$ is the polar w.r.t. the relation $\pol$ on $(Y,X)$ defined by
\[ y\pol x \qquad\iff\qquad \abspair{x, y} \leq 1. \]
\end{definition}

\begin{lemma}
Let $\sSet{X,Y,\pair{\cdot,\cdot}}$ be a pairing and $B\subseteq Y$ a subset. Then
\[ B^\pol = \setbuilder{x\in X}{\sup_{y\in B}\abspair{x,y} \leq 1}. \]
\end{lemma}

\begin{lemma}
Let $\sSet{X,Y,\pair{\cdot,\cdot}}$ be a pairing and $B\subseteq Y$ a subset. Then
\begin{enumerate}
\item for all $\lambda \neq 0$: $(\lambda B)^\pol = |\lambda|^{-1}B^\pol$.
\end{enumerate}
\end{lemma}

\begin{proposition}
Let $\sSet{X,Y,\pair{\cdot,\cdot}}$ be a pairing and $B\subseteq Y$ a subset. Then $B^\pol$ is absolutely convex and $\sigma(X,Y)$-closed.
\end{proposition}

TODO $(\bigcup A)^\pol = \bigcap A^\pol$.



\subsubsection{Annihilator subspaces}
\begin{lemma}
Let $\sSet{X,Y,\pair{\cdot,\cdot}}$ be a pairing and $V\subseteq Y$ a subspace. Then
\[ V^\pol = \setbuilder{x\in X}{\forall y\in V:\;\pair{x,y} = 0}. \]
\end{lemma}


\subsection{The pair $(V^*, V)$}
\begin{definition}
Let $V$ be a vector space and $V^*$ the continuous dual under some convergence. Any convergence on $V$ such that the continuous dual is still $V^*$ is called a \udef{convergence of the dual pair}.
\end{definition}
Any property that only depends on continuous linear functionals is the same for ant convergence of the dual pair.

\begin{proposition}
Let $V$ be a vector space and $A\subseteq V$ a convex subset. Let $\xi,\zeta$ be two vector space convergences on $V$. Then
\[ \sSet{V,\xi}^* = \sSet{V,\zeta}^* \qquad\implies\qquad \adh_\xi(A) = \adh_\zeta(A). \]
\end{proposition}
TODO closures? or only topological convergences? 
\begin{proof}
TODO Robertson p34
\end{proof}


\subsection{Mackey topology}

\begin{theorem}[Mackey-Arens]
\end{theorem}

\section{Operators on topological vector spaces}

\subsection{Continuous operators}
\subsubsection{Closed graph theorem}

\begin{theorem}[Closed graph theorem] \label{closedGraphTheorem}
Let $f:X\to Y$ be a map from a topological space $X$ into a Hausdorff space $Y$.
\begin{enumerate}
\item if $f$ is continuous, then $f$ has closed graph;
\item if $X$ is compact, then the converse also holds.
\end{enumerate}
\end{theorem}
\begin{proof}
TODO
\end{proof}
TODO: for Banach spaces $X$ complete enough!!!!!!


\subsection{Compact operators}
\begin{definition}
A linear operator $T:X\to Y$ between TVSs is \udef{compact} if it maps a neighbourhood of the origin to a precompact set, i.e.\ 
\[ \exists U \in \neighbourhood(0): \;  \text{$\overline{T[U]}$ is compact.} \]
The set of compact linear operators in $(X\to Y)$ is denoted $\Compact(X,Y)$.
\end{definition}
TODO: doesn't the neighbourhood need to be bounded in some way?????

\begin{proposition}
Let $X$ be a normed space and $Y$ a TVS and $T:X\to Y$ a linear operator. Then the following are equivalent:
\begin{enumerate}
\item $T$ is a compact operator;
\item there exists a neighbourhood $U \subset X$ of the origin and a compact set $V\subset Y$ such that $T[U] \subset V$;
\item the image of the unit ball of $X$, $T[B(\vec{0},1)]$, is precompact in $Y$;
\item the image of any bounded set in $X$ is precompact in $Y$.
\end{enumerate}
If $Y$ is a normed space, these are also equivalent to
\begin{enumerate} \setcounter{enumi}{4}
\item for any bounded sequence $(x_{n})_{n\in \mathbb{N}}$ in $X$, the sequence $(Tx_{n})_{n\in \mathbb{N} }$ contains a converging subsequence.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}


\begin{lemma}
Let $X,Y$ be TVSs.
\begin{enumerate}
\item Then $\Compact(X, Y)$ is a vector space.
\item If $X,Y$ are normed spaces, then $\Compact(X, Y)$ is a subspace of $\Bounded(X, Y)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Let $K,K':X\to Y$ be compact operators. Then, by \ref{closureGroupOperation} (TODO opposite inclusion!),
\[ \overline{K[B(0, 1)]+K'[B(0, 1)]} \subseteq \overline{K[B(0, 1)]}+\overline{K'[B(0, 1)]}, \qquad \overline{K[\lambda B(0, 1)]} = \lambda\overline{K[B(0, 1)]}. \]

(2) Let $K\in\Compact(X, Y)$. Then the image of the unit ball is precompact, meaning it is bounded. So $K$ is bounded by \ref{existenceOperatorNorm}.
\end{proof}

\begin{lemma}
Let $T:V\to W$ be a bounded operator. If $W$ has the Heine-Borel property, then $T$ is compact.
\end{lemma}
\begin{proof}
The set $T[B(\vec{0},1)]$ is bounded because $T$ is. By the Heine-Borel (TODO ref) property of $W$, $\overline{B(\vec{0},1)}$ is compact.
\end{proof}
\begin{corollary}
Bounded operators with as image a finite dimensional normed space are compact.
\end{corollary}
\begin{corollary}
The identity on a normed space $X$ is compact \textup{if and only if} $X$ is finite-dimensional.
\end{corollary}
\begin{proof}
TODO ref. 
\end{proof}

\begin{proposition}
Compact operators map weakly convergent sequences to strongly convergent sequences. TODO! + remove from Hilbert section.
\end{proposition}
\begin{corollary} \label{limitCompactImageOrthonormalSequence}
Let $V$ be an inner product space and $\seq{e_n}$ a sequence of orthonormal vectors in $V$. If $K$ is a compact operator, then $\lim_{n\to\infty}Ke_n = 0$.
\end{corollary}
\begin{proof}
Any sequence of orthonormal vectors $\seq{e_n}$ converges weakly to $0$. Because $K$ is compact, $\seq{Ke_n}$ converges strongly to zero. TODO ref.
\end{proof}
\begin{corollary}
If $V$ is infinite-dimensional and $K$ is invertible, then its inverse is unbounded.
\end{corollary}
\begin{proof}
Due to $\lim_{n\to\infty}Ke_n = 0$ the operator $K$ cannot be bounded below, so $K^{-1}$ is not bounded by \ref{boundedBelow}.
\end{proof}

\section{Continuity}
\url{https://en.wikipedia.org/wiki/Bilinear_map#Continuity_and_separate_continuity}



















\chapter{Functionals on vector spaces}

\begin{theorem}[Riesz-Markov-Kakutani representation theorem]
Let $X$ be a locally compact Hausdorff space. For any positive linear functional $\psi$ on $C_c(X)$, there is a unique Radon measure $\mu$ on $X$ such that
\[ \forall f\in C_c(X): \quad \psi(f) = \int_X f(x)\diff{\mu(x)}. \]
\end{theorem}














\chapter{Banach spaces}
\begin{definition}
\begin{itemize}
\item A \udef{Banach space} is a normed vector space that is complete as a metric space.
\item A \udef{Hilbert space} is an inner product space that is complete as a metric space.
\end{itemize}
\end{definition}

A finite-dimensional normed / inner product space is automatically a Banach / Hilbert space by proposition \ref{finiteDimComplete}.

Every proper subspace $U$ of a normed vector space $V$ has empty interior.
A nice consequence of this is that any closed proper subspace is necessarily nowhere dense. So if V is a Banach space, the Baire category theorem implies that V cannot be a countable union of closed proper subspaces. In particular, an infinite dimensional Banach space cannot be a countable union of finite dimensional subspaces. This means, for example, that a vector space of countable dimension (e.g\ the space of polynomials) cannot be equipped with a complete norm.

The space $\Bounded(V,W)$ is a Banach space.

TODO: quotient of Banach spaces.

Complemented subspace problem: \url{https://arxiv.org/pdf/math/0501048v1.pdf}

\section{Function spaces}
\subsection{The spaces $L^p(X,\diff{\mu})$}

\subsubsection{The spaces $\mathcal{L}^p(X,\diff{\mu})$}
\begin{proposition}
Let $0< p<q \leq \infty$ and $\sSet{X,\mathcal{A},\mu}$ be a measure space. Then
\begin{enumerate}
\item $\mathcal{L}^p(X,\mu) \supseteq \mathcal{L}^q(X,\mu)$ \textup{if and only if} $X$ does not contain sets of finite but arbitrary large measure;
\item $\mathcal{L}^p(X,\mu) \subseteq \mathcal{L}^q(X,\mu)$ \textup{if and only if} $X$ does not contain sets of non-zero but arbitrary small measure.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO \url{https://en.wikipedia.org/wiki/Lp_space#Embeddings}
\end{proof}
\begin{corollary}
For all $0< p<q \leq \infty$ and $a,b\in\R$, we have $\mathcal{L}^p([a,b],\lambda) \supseteq \mathcal{L}^q([a,b],\lambda)$.
\end{corollary}

\subsubsection{Indentifying functions that are equal a.e.}

\begin{proposition}
TODO every equivalence class contains exactly one continuous functions (for Borel measures?)
\end{proposition}

\begin{theorem}[Riesz-Fisher]
The space $L^p(X,\diff{\mu})$ is complete.
\end{theorem}

For $L^\infty$: essential supremum.

\begin{proposition}
Let $X$ be a set. For all $1\leq p < \infty$, the set of $p$-integrable simple functions is dense in $L^p(\diff{\mu})$.
\end{proposition}


\begin{proposition}
Let $X$ be a locally compact Hausdorff space and $\mu$ a Radon measure on $X$. Then $\cont_c(X)$ is dense in $L^p(\diff{\mu})$ for all $1\leq p < \infty$.
\end{proposition}
\begin{proof}

\end{proof}


\subsubsection{Locally integrable spaces}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. The \udef{locally $L^p$} space is the space
\[ L^p_\text{loc}(\Omega) \defeq \setbuilder{f \in (\Omega\to\C)}{\text{$f\in L^p(K)$ for all compact $K\subset \Omega$}}. \]
The functions in $L^1_\text{loc}(\Omega)$ are called \udef{locally integrable} on $\Omega$.
\end{definition}
TODO: deal with equivalence classes??

\subsection{Sequence spaces}
TODO:  $L^p(A,\mu)$ with $\mu$ counting measure.

Let $J$ be a countable index set and $x:J\to \mathbb{F}$ a sequence indexed by $J$. We define
\[ \norm{x}_p := \left(\sum_{j\in J}|x(j)|^p\right)^{1/p} \qquad\text{and}\qquad \norm{x}_\infty = \sup_{j\in J}|x(j)|. \]
So $\norm{\cdot}_1$ is the standard norm on $\mathbb{F}^n$. For general sequences there is no guarantee that these norms do not diverge.
\begin{definition}
Let $J$ be an index set, $D$ a directed set and $p\geq 1$,
\begin{align*}
\ell^p(J) &= \setbuilder{x:J\to \F}{\norm{x}_p < +\infty},\\
\ell^\infty(J) &= \setbuilder{x:J\to \F}{\norm{x}_\infty < +\infty},\\
c_0(D) &= \setbuilder{x:D\to \F}{\lim_{n\to\infty}|x(n)| = 0}, \\
c_{00}(D) &= \setbuilder{x:D\to \F}{\setbuilder{n\in D}{x(n)\neq 0}\;\text{has finite cardinality}}.
\end{align*}
unless specified we equip $c_0$ and $c_{00}$ with the norm $\norm{\cdot}_\infty$.
\end{definition}

\begin{lemma}
$c_{00}$ is dense in $\ell^p$ if it is equipped with the norm $\norm{\cdot}_p$ and dense in $c_0$ if it is equipped with the norm $\norm{\cdot}_\infty$.
\end{lemma}

Let $1<p,q<\infty$ satisfy $\frac{1}{p}+\frac{1}{q}$. We have the inequalities
\begin{align*}
\norm{xy}_1 &\leq \norm{x}_p\norm{y}_q\qquad\text{(Hlder inequality)} \\
\norm{x+y}_p &\leq \norm{x}_p+\norm{y}_p\qquad\text{(Minkowski inequality)}
\end{align*}
which follow from the general cases (TODO ref) by applying the counting measure.

\begin{proposition}
The continuous dual of $l^p(J)$ is $l^q(J)$ where $1<p,q<\infty$ satisfy $\frac{1}{p}+\frac{1}{q}$.
Also, the continuous dual of $l^1$ is $l^\infty$.
\end{proposition}

\subsubsection{Operators on sequence spaces}
TODO Gribanov's theorems

3.7.1, 3.7.2 of Hanson / Yakovlev.

\section{Series in Banach spaces}
TODO
\url{https://link.springer.com/content/pdf/10.1007%2F978-0-8176-4687-5_3.pdf}
\begin{definition}
Let $\seq{x_n}$ be a sequence in a Banach space $X$. As for series of scalars, we say a series $\sum_{n=1}^\infty x_n$ is
\begin{itemize}
\item \udef{unconditionally convergent} if $\sum_{n=1}^\infty x_{\sigma(n)}$ converges for every permutation $\sigma$ of $\N$;
\item \udef{absolutely convergent} if $\sum_{n=1}^\infty \norm{x_n} < \infty$.
\end{itemize}
\end{definition}

\begin{proposition} \label{absoluteUnconditionalConvergenceBanach}
Let $\seq{x_n}$ be a sequence in a Banach space $X$. If $\sum_{n=1}^\infty$ converges absolutely, then it converges unconditionally.
\end{proposition}
\begin{proof}
Assume absolute convergence, so $\sum\norm{x_i}<\infty$. Then (for $m< n$)
\[ \norm{\sum_{i=1}^n x_i - \sum_{i=1}^m x_i} = \norm{\sum_{i=m+1}^n x_i} \leq \sum_{i=m+1}^n\norm{x_i} = \sum_{i=1}^n \norm{x_i} - \sum_{i=1}^m \norm{x_i}, \]
and because $\sum\norm{x_i}$ converges, it is a Cauchy sequence and by the inequality so is $\sum x_i$. By completeness this sequence is convergent.

By (TODO ref) $\sum\norm{x_{\sigma(i)}}$ converges for any permutation $\sigma$ of $\N$. We can then repeat the argument to show $\sum x_{\sigma(i)}$ is also convergent and thus unconditionally convergent.
\end{proof}

\subsection{Fourier series}
TODO Sacks 7.1

\section{Completions and constructions}

\begin{proposition}
The completions of a space with respect to two different norms are isomorphic \textup{if and only if} the norms are equivalent.
\end{proposition}

TODO move down
\subsection{Tensor products}
TODO Ryan
\url{https://math.stackexchange.com/questions/2712906/does-mathcalb-mathcalh-mathcalh-otimes-mathcalh-in-infinite-dime}
\url{https://math.stackexchange.com/questions/35191/operator-norm-and-tensor-norms?noredirect=1&lq=1}

\subsection{Direct sums}

For arbitrary direct sums we can generalise: now that we have a concept of limits, we can relax the requirement that all but finitely many terms be zero. Instead we require that the sequence of norms is bounded in some way. This gives a whole family of related concepts of direct sum, named for which sequence space the sequence of norms belongs to.
\begin{definition}
Let $\{V_i\}_{i\in I}$ be an arbitrary family of Banach spaces over a field $\F$ and let $\ell(I,\F)$ be a space of sequences in $\F$ indexed by $I$. Then the \udef{$\ell$-direct sum} is the vector space with as field
\[ \bigoplus_{i\in I}^\ell V_i = \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad (\norm{v_i}_{V_i})_{i\in I}\in \ell(I,\F) }. \]
In particular we have, for all $1\leq p<\infty$, the \udef{$\ell^p$-direct sum}
\[ \bigoplus_{i\in I}^p V_i \defeq \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad \sqrt[p]{\sum_{i\in I}\norm{v_i}_{V_i}^p}<\infty} \]
and the \udef{$\ell^\infty$-direct sum}
\[ \bigoplus_{i\in I}^\infty V_i \defeq \setbuilder{(v_i)_{i\in I}}{\forall i\in I: v_i\in V_i \quad\text{and}\quad \sup_{i\in I}\norm{v_i}_{V_i}<\infty}. \]
\end{definition}

\begin{proposition}
For any sequence space that is a Banach space the direct sum is a Banach space. TODO: in particular algebraic direct sum as $c_{00}$? (one possible norm)? and finite direct sums?
\end{proposition}

\subsubsection{Direct sum of identical spaces}
\begin{proposition}
Let $V$ be a Banach space over $\F$, $I$ an arbitrary index set and $\ell(I,\F)$ a banach sequence space.
\[ \bigoplus_{i\in I}^\ell V \cong \ell\otimes V \]
\end{proposition}


\section{Operators on Banach spaces}
\subsection{Closed operators}
\begin{proposition}
Let $X,Y$ be Banach spaces and $S,T\in \Lin(X,Y)$ with $\dom(S) = \dom(T)$. If $S$ is a closed operator and there exist $\alpha,\beta,\gamma\in \R^+$ such that $0 < \gamma \leq 1$ and $\beta < 1/\gamma$ and
\[ \norm{(S-T)u} \leq \alpha \norm{u} + \beta\norm{Su}^\gamma{u}^{1-\gamma} \qquad\text{for all $u\in \dom(S) = \dom(T)$,} \]
then $T$ is also closed.
\end{proposition}
\begin{proof}
TODO Jeribi.
\end{proof}

\section{Bounded operators}
\begin{proposition}
Let $V,W$ be normed spaces. The vector space $\Bounded(V,W)$ with the operator norm is a Banach space \textup{if and only if} $W$ is a Banach space.
\end{proposition}
\begin{corollary}
Let $V$ be a normed space. The continuous dual $X'$ is a Banach space.
\end{corollary}
\begin{corollary}
Topologically reflexive spaces are Banach spaces.
\end{corollary}

\begin{proposition}[Bounded linear extension] \label{BLT}
Let $T:\dom(T)\subseteq X\to Y$ be a bounded operator between normed spaces. Then $T$ has a unique extension
\[ \widetilde{T}:\overline{\dom(T)}\to Y \]
where $\widetilde{T}$ is a bounded operator with $\norm*{\widetilde{T}} = \norm{T}$.
\end{proposition}
\begin{proof}
Normed vector spaces have the unique extension property because they are Hausdorff, \ref{uniqueExtensionHausdorff}. We just need to show the norm stays the same:

Clearly $\norm*{\tilde{T}} \geq \norm{T}$. For the converse take any $x\in X$. As $\overline{\dom(T)} = X$, there exists a sequence $\seq{x_i}\subset \dom(T)$ that converges to $x$. Then
\[ \norm*{\tilde{T}(x)}_Y = \norm{T\left(\lim_{i\to\infty}x_i\right)}_Y = \lim_{i\to\infty}\norm{T(x_i)}_Y \leq \lim_{i\to\infty}\norm{T}\;\norm{x_i}_X = \norm{T}\;\norm{x}_X. \]
\end{proof}

\subsection{Contractions}
A linear operator $T$ on a normed space is a contraction if and only if it is bounded and $\norm{T}<1$. 

\subsubsection{Neumann series}
\begin{lemma}
Let $T$ be a bounded linear operator on a normed space $X$ with $\norm{T}<1$. Then the series $\sum^\infty_{i=1}T^i(b)$ converges for all $b\in X$ and is the unique fixed point of $F(x) = T(x)+b$.
\end{lemma}
\begin{proof}
The function $F$ is a contraction if and only if $\norm{T}<1$. So it has a unique fixed point. Starting the fixed point iteration at $b$ yields the series:
\begin{align*}
F(b) &= Tb + b \\
F(Tb+b) &= T^2b + Tb + b \\
&\hdots.
\end{align*}
Alternatively we could have used the inequality $\norm{T^nb} \leq \norm{T}^n\norm{b}$, the convergence of the geometric series and \ref{absoluteUnconditionalConvergenceBanach} to prove convergence. Proving it is a fixed point is then elementary.
\end{proof}
\begin{corollary}[Neumann series] \label{operatorNeumannSeries}
Let $T$ be a bounded linear operator with $\norm{T}<1$. Then
\[ (\id - T)^{-1} = \sum_{i=1}^\infty T^i \]
with uniform convergence. Also
\[ \norm{(\id - T)^{-1}} \leq \frac{1}{1-\norm{T}}. \]
\end{corollary}
\begin{proof}
Let $x\in X$. Then set $(\id - T)^{-1}x = y$. This is equivalent to $x = y-Ty$ and means $y$ is the fixed point of $y\mapsto Ty+x$. So $y = \sum_{i=1}^\infty T^ix$.

The convergence is uniform by TODO ref.

Finally we have
\[ \norm{(\id - T)^{-1}} = \norm{\sum_{i=1}^\infty T^i} \leq \sum_{i=1}^\infty \norm{T^i} = \frac{1}{1-\norm{T}} \]
by the geometric series.
\end{proof}
TODO ref: uniform convergence if $\sum_i \norm{T_i} < \infty$??

\subsection{The uniform boundedness principle}
TODO: if a family of bounded operators on a Banach space is pointwise bounded, then it is uniformly bounded.
\begin{theorem}[Uniform boundedness principle] \label{uniformBoundednessPrinciple}
Let $\mathcal{F}\subset \Bounded(X,Y)$ be a family of bounded operators where $X$ is a Banach space and $Y$ a normed space, such that
\[ \sup\setbuilder{\norm{Tx}}{T\in\mathcal{F}} < \infty \qquad \text{for all $x\in X$}. \]
Then $\sup\setbuilder{\norm{T}}{T\in\mathcal{F}} < \infty$.
\end{theorem}
\begin{proof}
The proof is an application of the Baire category theorem. Define the closed subsets $K_n$ as
\[ K_n = \setbuilder{x\in X}{\forall T\in\mathcal{F}: \norm{Tx}\leq n}. \]
These are closed because the functional $f_T: X\to \R: x\mapsto \norm{Tx}$ is bounded and
\[ K_n = \bigcap_{T\in\mathcal{F}}f_T^{-1}[\,[0,n]\,]. \]
By assumption, $X=\bigcup_{n\in\N} K_n$. As $X$ is a Banach space, and thus a complete metric space, we can apply the Baire category theorem, \ref{BaireCategory}, to conclude that there is a $K_n$ with non-empty interior (by contraposition of the Baire condition). Take $x_0\in K_n^\circ$, then $-x_0+K_n^\circ \subset K_{n2}$. So $\vec{0}\in (K_{2n})^\circ$ and we can find a $\rho$ such that $B(\vec{0},\rho)\subset K_{2n}$. By proposition \ref{existenceOperatorNorm} we have $\norm{T}\leq 2n/\rho$ for all $T\in\mathcal{F}$.
\end{proof}
\begin{corollary}[Banach-Steinhaus] \label{BanachSteinhaus}
Let $X$ be a Banach space and $Y$ a normed space. Let $T_n: X\to Y$ be a sequence of bounded operators. If $T_n$ converges pointwise to $T:X\to Y: Tx = \lim_n T_n x$, then $\sup_n\norm{T_n} <\infty$ and thus $T$ is bounded.
\end{corollary}
\begin{proof}
Any convergent sequence in a normed space is bounded, so we can apply the uniform boundedness principle.
\end{proof}

\subsection{Open mapping and closed graph theorems}

\begin{proposition} \label{openUnitBall}
Let $X,Y$ be Banach spaces and $T:X\to Y$ a surjective bounded operator.  Then the image of the open unit ball $B(\vec{0},1)\subset X$ contains an open ball about $\vec{0}\in Y$.
\end{proposition}
\begin{proof}
We first prove $0\in \overline{T[B(\vec{0},r)]}^\circ$ for every $r>0$: (TODO: make computations lemma.)
\begin{itemize}
\item Using $X = \bigcup_{n=1}^\infty B(\vec{0},n)$, we see by surjectivity
\[ Y = T[X] = T\left[\bigcup_{n=1}^\infty B(\vec{0},n)\right] = \bigcup_{n=1}^\infty T[B(\vec{0},n)]. \]
Because $Y$ has the Baire property (theorem \ref{BaireCategory}) and $Y$ is both open and non-empty, it may not be meagre, by lemma \ref{BaireEquivalents}. So for some $n\in\N$, $T[B(\vec{0},n)]$ is non-rare, meaning that $\overline{T[B(\vec{0},n)]}$ has non-empty interior.
\item Because
\[ \overline{T[B(\vec{0},n)]} = \overline{2nT[B(\vec{0},1/2)]} = 2n\overline{T[B(\vec{0},1/2)]}, \]
$\overline{T[B(\vec{0},1/2)]}$ must have non-empty interior. Let $B(y_0,\epsilon)\subset \overline{T[B(\vec{0},1/2)]}$.
\item Note $B(0,\epsilon) = y_0 - B(y_0,\epsilon) \subset \overline{T[B(\vec{0},1)]}$ and thus $B(0,r\epsilon) \subset \overline{T[B(\vec{0},r)]}$.
\end{itemize}
We then prove $\overline{T[B(\vec{0},1/2)]} \subset T[B(\vec{0}, 1)]$, proving the proposition.
\begin{itemize}
\item Choose some $y_0\in \overline{T[B(\vec{0},1/2)]}$. Then every neighbourhood $B(y_0,\epsilon/4)$ intersects $T[B(\vec{0},1/2)]$.
\item Then
\[ B(y_0,\epsilon/4) = y_0 - B(\vec{0},\epsilon/4) \subset y_0 - \overline{T[B(\vec{0},1/4)]}, \]
so $y_0 - \overline{T[B(\vec{0},1/4)]}$ intersects $T[B(\vec{0},1/2)]$. Take a $y_1 \in \overline{T[B(\vec{0},1/4)]}$ such that $y_0-y_1$ is in this intersection. Then we have an $x_0\in B(\vec{0},1/2)$ such that $T(x_0) = y_0-y_1$.
\item We can continue recursively choosing $y_{n+1}\in \overline{T[B(\vec{0}, 2^{-(n+1)})]}$ and $x_n \in B(\vec{0}, 2^{-n})$ such that $y_n-y_{n+1} = T(x_n)$.
\item Consider the sequence $\sum_{k=0}^nx_k$. It is a Cauchy sequence in $X$. Call its limit $x$. Then $x\in B(\vec{0},1)$.
\item Because $\norm{y_n}\leq 2^{-n}\norm{T}$, $(y_n)$ converges to zero. Then
\[ \left( T\left(\sum^n_{k=1}x_k\right) \right)_{n\in\N} = \left( y_0-y_{n+1} \right)_{n\in\N} \]
converges to $y_0$. Thus $T(x) = y_0 \in T[B(\vec{0},1)]$.
\end{itemize}
\end{proof}

\begin{proposition} \label{zeroInInteriorOfImageImpliesOpen}
Let $X,Y$ be normed spaces and $T: X\to Y$ a linear map. If $\vec{0}$ lies in the interior of $T[B(\vec{0},r)]$ for some $r>0$, then $T$ is open.
\end{proposition}
\begin{proof}
TODO: make computations lemma.
Given the assumption, $0$ lies in the interior of $T[B(\vec{0},\epsilon)]$ for all $\epsilon>0$.
Because $T[B(x,\epsilon)] = T(x) + T[B(\vec{0},\epsilon)]$, $T(x)$ lies in the interior of $T[B(x,\epsilon)]$, for all $x\in X$.
Thus for all neighbourhoods $U(x)\subset X$, $T(x)\subset T[U]^\circ$ and so $T[U] \subset T[U]^\circ$, so $T[U]$ is open.
\end{proof}

\begin{theorem}[Open mapping]
Let $X,Y$ be Banach spaces and $T:X\to Y$ a surjective bounded operator. Then $T$ is an open map.
\end{theorem}
\begin{proof}
This is the consequence of propositions \ref{openUnitBall} and \ref{zeroInInteriorOfImageImpliesOpen}.
\end{proof}
\begin{corollary}[Bounded inverse theorem] \label{boundedInverse}
Let $X,Y$ be Banach spaces. If $T:X\to Y$ is is continuous, linear and bijective, then $T$ is a homeomorphism.
\end{corollary}


\begin{proposition}
Let $T: \dom(T)\subset X\to Y$ be a bounded linear operator. Then
\begin{enumerate}
\item if $\dom(T)$ is a closed subset of $X$, then $T$ has closed graph;
\item if $T$ has closed graph and $Y$ is complete, then $\dom(T)$ is a closed subset of $X$.
\end{enumerate}
\end{proposition}
\begin{proof}
We use proposition \ref{closedGraphEquivalence} twice: First assume $(x_n)$ and $(Tx_n)$ converge to $x$ and $y$, respectively. Then $x\in\dom(T)$ by closure and $y = Tx$ by continuity.

Now assume $T$ has closed graph and $Y$ is complete. Take $x\in\overline{\dom(T)}$ and $(x_n)\subset \dom(T)$ converging to $x$. Since $T$ is bounded:
\[ \norm{Tx_n - Tx_m} = \norm{T(x_n-x_m)} \leq \norm{T}\norm{x_n-x_m}, \]
so $(Tx_n)$ is Cauchy by \ref{CauchyCriterion} and thus by completeness has a limit, say $y$. Then $Tx=y$ by continuity. Since $T$ has closed graph, $x\in\dom(T)$. So $\overline{\dom(T)}\subseteq \dom(T)$ and $\dom(T)$ is closed. 
\end{proof}

For closed graph theorem, see TVS.


\subsection{Compact operators}
\begin{proposition}
Let $L\in\Hom(V,W)$ with $V,W$ Banach spaces. Then $L$ is compact \textup{if and only if} the image of any bounded subset of $V$ under $L$ is totally bounded in $W$.
\end{proposition}
TODO proof


\subsubsection{Calkin algebra}
\begin{proposition}
Let $X$ be a Banach space. Then $\Compact(X)$ is a closed two-sided ideal in $\Bounded(X)$.
\end{proposition}
\begin{proof}
TODO + $*$-ideal for Hilbert spaces.
\end{proof}

\begin{definition}
Let $X$ be a Banach space. The \udef{Calkin algebra} is the quotient $\Bounded(X)/\Compact(X)$.
\end{definition}
TODO: quotient algebra ($[A][B] = [AB]$)

\begin{proposition}
Let $[T]\in\Bounded(X)/\Compact(X)$. Then the following are equivalent:
\begin{enumerate}
\item $[T]$ is invertible in the Calkin algebra;
\item $\exists S\in\Bounded(X):$ both $\vec{1}-TS$ and $\vec{1}-ST$ are compact;
\item $T$ has closed range and finite-dimensional kernel and cokernel. 
\end{enumerate}
\end{proposition}
\begin{proof}
Point 1. and 2. are easily equivalent: $[S]$ is an inverse of $[T]$ if and only if $[\vec{1}] = [S][T] = [ST]$ and $[\vec{1}] = [T][S] = [TS]$. Then
\[ [\vec{1}] = [ST] \iff [ST - \vec{1}] = [0] \qquad [\vec{1}] = [TS] \iff [TS - \vec{1}] = [0] \]
and $[F]=[0]$ if and only if $F$ is compact.

TODO
\end{proof}

\section{Unbounded operators}

\section{Bochner integration}

TODO: the Bochner integral is the unique extension of the integral of simple functions to the set of Bochner measurable functions???? (i.e.\ simple functions dense in Bochner space, with $L^1$ metric)
\begin{definition}
Let $(\Omega, \mathcal{A},\mu)$ be a measure space and $Y$ a normed vector space. Then a Bochner measurable function $f:\Omega\to Y$ is called \udef{Bochner integrable} if there exists a sequence of integrable simple functions $\seq{s_n}\subset\SF(\Omega,Y)$ such that
\[ \lim_{n\to\infty}\int_\Omega \norm{f-s_n}\diff{\mu} = 0. \]
Take such a sequence $\seq{s_n}$. The \udef{Bochner integral} of $f$ on $\Omega$ w.r.t. $\mu$ is defined as
\[ \int_\Omega f\diff{\mu} \defeq \lim_{n\to\infty}\int_\Omega s_n\diff{\mu}. \]
\end{definition}

\begin{lemma}
The Bochner integral is well-defined: let $\seq{s_n},\seq{t_n}\in \prescript{\N}{}{\SF(\Omega,Y)}$ be sequences such that
\[ \lim_{n\to\infty}\int_\Omega \norm{f-s_n}\diff{\mu} = 0 = \lim_{n\to\infty}\int_\Omega \norm{f-t_n}\diff{\mu}.  \]
Then
\begin{enumerate}
\item the limits $\lim_{n\to\infty}\int_\Omega s_n\diff{\mu}$ and $\lim_{n\to\infty}\int_\Omega t_n\diff{\mu}$ exist;
\item $\lim_{n\to\infty}\int_\Omega s_n\diff{\mu} = \lim_{n\to\infty}\int_\Omega t_n\diff{\mu}$.
\end{enumerate}
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{proposition}[Bochner integrability criterion] \label{BochnerIntegrabilityCondition}
Let $(\Omega, \mathcal{A},\mu)$ be a measure space and $Y$ a normed vector space.

A Bochner measurable function $f$ is Bochner integrable \textup{if and only if}
\[ \int_\Omega \norm{f} \diff{\mu} < \infty. \]
\end{proposition}

\begin{proposition}
Linearity and monotonicity.
\end{proposition}

\begin{theorem}[Hille's theorem] \label{HilleTheorem}
Let $(\Omega, \mathcal{A},\mu)$ be a measure space, $X,Y$ normed vector spaces and $T: X\not\to Y$ a closed operator. If $T\circ f$ is integrable, then
\[ \int_\Omega (T\circ f)\diff{\mu} = T\left(\int_\Omega f\diff{\mu}\right). \]
\end{theorem}
\begin{proof}
TODO
\end{proof}
\begin{corollary} \label{boundedOperatorUnderIntegral}
If $T$ is bounded, then $T\circ f$ is integrable and
\[ \int_\Omega (T\circ f)\diff{\mu} = T\left(\int_\Omega f\diff{\mu}\right). \]
\end{corollary}
\begin{proof}
TODO: show that $T\circ f$ is integrable!
\end{proof}

TODO Dominated convergence.

\subsection{Integration of bounded operators}
\begin{lemma} \label{integralBoundedOperator}
Let $X$ be a normed space and $(\Omega, \mathcal{A},\mu)$ a measure space. Let $T: \Omega \to \Bounded(X)$ be a function. If $T$ is integrable, then for all $x\in X$, $Tx$ is integrable and
\[ \left(\int_\Omega T\diff{\mu}\right)x = \int_\Omega Tx\diff{\mu}. \]
\end{lemma}
\begin{proof}
The evaluation map $\evalMap_x$ is linear and bounded by $\norm{x}$ for all $x\in X$, so we can use \ref{boundedOperatorUnderIntegral}.
\end{proof}


\chapter{Spectral theory and functional calculus}
\section{Invariant subspaces}
\begin{definition}
Let $L\in \Hom(V)$ be an endomorphism. A subspace $U$ of $V$ is \udef{invariant} under $L$ if $T|_U$ is an endomorphism on $U$. In other words, $u\in U$ implies $Tu\in U$.
\end{definition}
Clearly this definition only works for endomorphisms, not for linear maps in general. This is true for the rest of the theory about eigenvalues and eigenvectors.
\begin{example}
Let $L\in \Hom(V)$. The following are invariant under $L$:
\begin{itemize}
\item $\{0\}$;
\item $\ker L$;
\item $\im L$.
\end{itemize}
\end{example}

\section{The spectrum}
TODO: eigenvalue problem $Lx = \lambda x$

generalised eigenvalue problem $Lx = \lambda T x$

nonstandard eigenvalue problem $A(\gamma)x = 0$.

TODO: consistency $\lambda \id - L$, not $L-\lambda \id$.
TODO: everything is now in $\C$.

\begin{definition}
Let $L: \dom(L)\subset V \to V$ be an operator on a complex normed vector space $V$.

For $\lambda\in\C$ the \udef{resolvent} $R_L(\lambda): \im(\lambda \id_V - L)\to\dom(L)$ is the left inverse of $\lambda \id_V - L$, if this inverse exists (i.e.\ if $\lambda \id_V - L$ is injective).
\begin{itemize}
\item The \udef{resolvent set} $\res(L)$ is the set
\begin{align*}
\res(L) &\defeq \setbuilder{\lambda\in \C}{R_L(\lambda)\in\Bounded(V)} \\
&= \setbuilder{\lambda\in \C}{\text{$R_L(\lambda)$ exists, has domain $V$ and is bounded}}.
\end{align*}
\item The \udef{spectrum} of $L$ is the complement of the resolvent set: $\spec(L) \defeq \C\setminus\rho(L)$.
\item The \udef{spectral radius} $\spr(L)$ is $\sup_{\lambda\in\spec(L)} |\lambda|$.
\end{itemize}
\end{definition}

\begin{lemma} \label{elementResolventSetNormedSpace}
Let $T$ be an operator on a normed vector space $V$. Then $\lambda \in \res(T)$ \textup{if and only if} $\lambda \id_V - T$ is surjective and bounded from below.
\end{lemma}
\begin{proof}
By \ref{boundedBelow}, $\lambda \id_V - T$ has a bounded inverse $(\lambda \id_V - T)^{-1}: \im(\lambda \id_V - T)\to V$ if and only if it is bounded below. In order for $\lambda$ to be in the resolvent set, we need $(\lambda \id_V - T)^{-1}$ to be defined everywhere, i.e. $\im(\lambda \id_V - T) = V$.
\end{proof}



\subsection{The three-way classification of the spectrum}
\begin{definition}
Let $L: \dom(L)\subset V \to V$ be an operator on a complex vector space $V$.

\begin{itemize}
\item The \udef{point spectrum} or \udef{discrete spectrum} $\pspec(L)$ contains the values of $\lambda$ where $\lambda \id_V - L$ fails to be injective, so the resolvent fails to exist. These values are called the \udef{eigenvalues} of $L$.

We call
\begin{itemize}
\item $\ker(\lambda \id_V - L)$ the \udef{multiplicity space} or \udef{geometric eigenspace} of $\lambda$; and
\item $\dim\ker(\lambda \id_V - L)$ the \udef{(geometric) multiplicity} of $\lambda$.
\end{itemize}
\item The \udef{continuous spectrum} $\cspec(L)$ is the set of all values of $\lambda\in\spec(L)$ such that the resolvent $R_L(\lambda)$ exists and is densely defined.
\item The \udef{residual spectrum} $\rspec(L)$ is the set of all values of $\lambda\in\spec(L)$ such that the resolvent $R_L(\lambda)$ exists, but is not densely defined.

We call
\begin{itemize}
\item $\im(\lambda \id_V - L)^\perp$ the \udef{deficiency subspace} of $\lambda$; and 
\item $\dim(\im(\lambda \id_V - L)^\perp)$ the \udef{deficiency} of $\lambda$.
\end{itemize}
\end{itemize}
The sets $\pspec(T), \cspec(T)$ and $\rspec(T)$ are disjoint.
\end{definition}
In finite dimensions we know that
\[ \text{$\lambda \id_V - L$ is surjective} \quad\iff\quad \text{$\lambda \id_V - L$ is injective} \]
and all linear operator are bounded.
So in this case there can only ever be a point spectrum.

\begin{proposition}
If $T$ is an operator on a Banach space that is not closed, then $\spec(T) = \C$.
\end{proposition}
\begin{proof}
We can find a sequence $x_n \to x$ such that $Tx_n \to y$, but $Tx \neq y$. Then for all $\lambda\in\C$ we have $z_n = (\lambda\id - T)x_n \to \lambda x - y$. If $R_T(\lambda)$ was a bounded inverse of $(\lambda\id - T)$, then $R_T(\lambda)\circ(\lambda\id - T)x_n \to R_T(\lambda)(\lambda x - y)$. We need to show that $R_T(\lambda)(\lambda x - y) \neq x$. Indeed
\begin{align*}
R_T(\lambda)(\lambda x - y) &= R_T(\lambda)(\lambda x - Tx + Tx - y) \\
&= R_T(\lambda)(\lambda x - Tx) + R_T(\lambda)(Tx - y) \\
&= x + R_T(\lambda)(Tx - y),
\end{align*}
and $R_T(\lambda)(Tx - y) \neq 0$, because $Tx - y \neq 0$ and the kernel of $R_T(\lambda)$ is trivial because it is injective. 
\end{proof}

\begin{example}
Closed operators may also have empty resolvent set. \url{https://math.stackexchange.com/questions/3262168/closed-operator-with-trivial-resolvent-set}
\end{example}

So spectral theory is only interesting for closed operators. In this case the three-way classification exhausts the possibilities: (only on Banach spaces??)

\begin{proposition} \label{closedOperatorBanachSpaceSpectrumCriterion}
Let $X$ be a Banach space and $T$ a closed linear operator on $X$. Then $\lambda \in \spec(T)$ \textup{if and only if} $\lambda \id_X - T: \dom(T) \to V$ is not bijective.
\end{proposition}
\begin{proof}
If $\lambda \id_X - T$ is not bijective, then clearly $\lambda \in \spec(T)$.

Conversely, assume $\lambda \id_X - T$ is bijective. Then $(\lambda \id_X - T)^{-1}: X\to \dom(T)$ is closed by \ref{algebraClosedOperators} and has as domain a Banach space, so it is bounded by the closed graph theorem \ref{closedGraphTheorem}.
\end{proof}
\begin{corollary}
Let $T$ a closed operator on a Banach space. Then
\[ \spec(T) = \pspec(T) \cup \cspec(T) \cup \rspec(T). \]
\end{corollary}


\begin{proposition}
Let $T:X\to X$ be an operator on a Banach space and $\lambda\in\cspec$, then $R_\lambda(T)$ is unbounded.
\end{proposition}
\begin{proof}
If $R_\lambda(T)$ is bounded, $\lambda \id_V - T$ then is bounded below by lemma \ref{boundedBelow} and has closed range by proposition \ref{boundedBelowClosedRange}. Then because $\im(\lambda \id_V - T)$ is dense, this means $T$ is surjective, which is a contradiction because then $\lambda\in\res(T)$.
\end{proof}

\subsection{Resolvents}

\begin{proposition}[Resolvent identity]
Let $T$ be a linear operator and $\lambda,\mu\in\C$ such that $R_T(\lambda), R_T(\mu)$ exist. Then $R_T(\lambda)$ and $R_T(\mu)$ commute and
\[ \frac{R_T(\lambda) - R_T(\mu)}{\lambda-\mu} = -R_T(\lambda)R_T(\mu). \]
\end{proposition}
\begin{proof}
The commutativity of the resolvents follows from \ref{commutationInverse}.

We calculate
\begin{align*}
R_T(\lambda) - R_T(\mu) &= R_T(\lambda)R_T(\mu)(\mu\id - T) - R_T(\mu)R_T(\lambda)(\lambda\id - T) \\
&= \mu R_T(\lambda)R_T(\mu) - R_T(\lambda)R_T(\mu) T - \lambda R_T(\mu)R_T(\lambda) + R_T(\mu)R_T(\lambda) T \\
&= (\mu - \lambda)R_T(\lambda)R_T(\mu).
\end{align*}
\end{proof}
\begin{corollary}
Let $T$ ba a linear operator. Then
\begin{enumerate}
\item $R_T(\lambda)$ is holomorphic in $\rho(T)$;
\item $R_T'(\lambda) = -R_T(\lambda)^2$;
\item $R_T^{(n)}(\lambda) = n!(-1)^n R_T(\lambda)^{n+1}$ for all $n\in \N$.
\end{enumerate}
\end{corollary}
\begin{corollary} \label{firstNeumannSeries}
Let $T$ ba a linear operator and $\lambda_0\in \rho(T)$. Then the Taylor expansion of $R_T(\lambda)$ around $\lambda_0$ is
\begin{align*}
R_T(\lambda) &= \sum_{n=0}^\infty (\lambda-\lambda_0)^n(-1)^nR_T(\lambda_0)^{n+1} \\
&= R_T(\lambda_0) \sum_{n=0}^\infty \Big((\lambda_0-\lambda)R_T(\lambda_0)\Big)^n \\
&= \frac{R_T(\lambda_0)}{1+(\lambda-\lambda_0)R_T(\lambda_0)},
\end{align*}
with convergence radius $1/\norm{R_T(\lambda_0)}$.
\end{corollary}
\begin{proof}
If $|(\lambda_0-\lambda)| \leq 1/\norm{R_T(\lambda_0)}$, then $(\lambda_0-\lambda)R_T(\lambda_0)$ is a contraction and we can use the Neumann series expansion \ref{operatorNeumannSeries}, which gives the last equality.
\end{proof}
\begin{corollary} \label{resolventNormDistanceToSpectrum}
For all $\lambda\in\res(T)$, we have $d(\lambda, \spec(T)) \geq \norm{R_T(\lambda)}^{-1}$.
\end{corollary}
\begin{corollary}
The resolvent set $\res(T)$ is open.
\end{corollary}


\begin{proposition} \label{boundedPerturbationClosedOperator}
Let $S: \dom(S)\subseteq X\to X$ be a closed, bijective linear operator on a Banach space $X$. Let $T\in\Bounded(X)$ be a bounded operator with $\norm{T} < \norm{S^{-1}}^{-1}$, then $S-T$ is invertible and
\[ (S-T)^{-1} = S^{-1}\sum_{n=0}^\infty \left(S^{-1}T\right)^n. \]
Also $\dom((S-T)^{-1}) = X$.
\end{proposition}
The assumptions are enough to guarantee $S^{-1}\in \Bounded(X)$, so $\norm{S^{-1}}$ makes sense.
\begin{proof}
By injectivity, we can define $S^{-1}$. It is closed by \ref{algebraClosedOperators} and has closed domain, so it is bounded by the closed graph theorem \ref{closedGraphTheorem}. Since $\norm{S^{-1}T} \leq \norm{S^{-1}}\;\norm{T} < 1$, it follows that by \ref{operatorNeumannSeries} that $\id - S^{-1}T$  has bounded inverse and that we can expand its bounded inverse as a Neumann power series. So we can calculate
\[ (S-T)^{-1} = S^{-1}(\id - S^{-1}T)^{-1} = S^{-1}\sum_{n=0}^\infty \left(S^{-1}T\right)^n. \]
\end{proof}
\begin{corollary} \label{secondNeumannSeries}
Let $T$ be a bounded operator on a Banach space $X$. For $|\lambda|>\norm{T}$ the resolvent $R_T(\lambda)$ is bounded and given by
\[ R_T(\lambda) = \sum_{n=0}^\infty\frac{T^n}{\lambda^{n+1}} \]
with uniform convergence. The norm is bounded by
\[ \norm{R_T(\lambda)} \leq \frac{1}{|\lambda|-\norm{T}}. \]
\end{corollary}
\begin{proof}
We use the proposition with $S = \lambda\id$. The norm bound follows from the Neumann series expansion \ref{operatorNeumannSeries}.
\end{proof}
\begin{corollary}
Let $T$ be a bounded linear operator on a Banach space. Then
\begin{enumerate}
\item $\spec(T)\subset [-\norm{T}, \norm{T}]$;
\item $\spec(T)$ is compact.
\end{enumerate}
\end{corollary}
For the point spectrum a simpler argument also leads to $\pspec(T)\subset [-\norm{T}, \norm{T}]$: let $\lambda$ be an eigenvalue with eigenvector $x$. Then
\[ |\lambda|\;\norm{x} = \norm{\lambda x} = \norm{Tx} \leq \norm{T}\;\norm{x}. \]

\begin{proposition}
Let $T$ be an injective operator with dense range. Then for all $\lambda\neq 0$
\[ R_{T^{-1}}(\lambda^{-1}) = -\lambda T R_{T}(\lambda) = \lambda -\lambda^2 R_T(\lambda). \]
\end{proposition}
\begin{proof}
This is a reformulation of the calculation
\[ \frac{1}{\lambda^{-1} - T^{-1}} = \frac{\lambda T}{\lambda T}\frac{1}{\lambda^{-1} - T^{-1}} = \frac{\lambda T}{T - \lambda} = \frac{\lambda T - \lambda^2 + \lambda^2}{T - \lambda} = \frac{\lambda\cancel{(T - \lambda)}}{\cancel{T - \lambda}} + \frac{\lambda^2}{T - \lambda} = \lambda - \lambda^2 R_T(\lambda). \]
TODO: make rigourous!!
\end{proof}
\begin{corollary}
Let $T$ be an injective operator with dense range. Then for all $\lambda\neq 0$
\begin{enumerate}
\item $\spec(T^{-1})\setminus\{0\} = (\spec(T)\setminus \{0\})^{-1}$;
\item $\pspec(T^{-1})\setminus\{0\} = (\pspec(T)\setminus \{0\})^{-1}$.
\end{enumerate}
\end{corollary}

\subsection{Approximate spectrum and Weyl sequences}
\begin{definition}
The set of all $\lambda$ such that $T-\lambda \id_V$ is not bounded from below is called the \udef{approximate point spectrum} $\apspec$.

If $\lambda\in\apspec(T)$, then $\lambda$ is an \udef{approximate eigenvalue} of $T$.
\end{definition}

\begin{proposition} \label{approximateSpectrum}
Let $T$ be an operator. Then
\begin{enumerate}
\item $\apspec(T) \subset \spec(T)$;
\item if $T$ is closed, then $\pspec(T)\cup\cspec(T)\subset\apspec(T)$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Assume $\lambda \notin \spec(T)$. Then $(T-\lambda \id_V)^{-1}$ is bounded, so its inverse $T-\lambda \id_V$ is bounded below by \ref{boundedBelow} and $\lambda\in \apspec(T)$.

(2) Assume $\lambda\notin \apspec(T)$, 
so $T-\lambda \id_V$ is bounded below. Then $T-\lambda \id_V$ is injective by \ref{boundedBelow} and $\lambda\notin\pspec(T)$. By proposition \ref{boundedBelowClosedRange} the range $\im(T-\lambda \id_V)$ is closed, so it cannot be a proper dense subset of $X$ and $\lambda\notin\cspec(T)$.
\end{proof}

\begin{proposition}[Weyl sequences] \label{WeylSequence}
Let $T$ be an operator on a normed vector space $V$. Then $\lambda \in \apspec(T)$ \textup{if and only if} there exists a sequence of unit vectors $(e_n)_{n\in\N}$ for which
\[ \lim_{n\to\infty}\norm{\lambda e_n - Te_n} = 0. \]
\end{proposition}
\begin{proof}
Assume there is such a sequence $(e_n)_{n\in\N}$. Then for all $\epsilon>0$, we can find a unit  vector $e_k$ such that $\norm{(\lambda \id_V - T)e_n} \leq \epsilon = \epsilon \norm{e_n}$. This is clearly not bounded below.

This other direction is just an inversion of this argument.
\end{proof}
A sequence as described in \ref{WeylSequence} is called a \udef{Weyl sequence} for $\lambda$. This gives meaning to the name ``approximate eigenvalue''.

\begin{corollary}
Let $T$ be an operator. Then $\sigma(T)\cap \overline{\res(T)} \subseteq \apspec(T)$.
\end{corollary}
\begin{proof}
Let $\lambda \in \sigma(T)\cap \overline{\res(T)}$. We show there is a Weyl sequence for $\lambda$.

We can find a sequence $\seq{\lambda_n}\subseteq \res(T)$ such that $\lambda_n \to \lambda$.
Now $d(\lambda_n, \spec(T)) \to 0$, so by \ref{resolventNormDistanceToSpectrum}, we can find a sequence of unit vectors $\seq{x_n}$ such that $\norm{R_T(\lambda_n)x_n} \to \infty$. Now we can rescale $\seq{x_n}$ such that $\norm{R_T(\lambda_n)x_n} = 1$.

Then $\norm{x_n}\to 0$, and hence
\begin{align*}
\norm{(\lambda\id - T)R_T(\lambda_n)x_n} &= \norm{\frac{\lambda\id - T}{\lambda_n\id - T}x_n} \\
&= \norm{\frac{(\lambda\id - T) + (\lambda_n\id - T) - (\lambda_n\id - T)}{\lambda_n\id - T}x_n} \\
&= \norm{\left(\id + \frac{(\lambda\id - T) - (\lambda_n\id - T)}{\lambda_n\id - T}\right)x_n} \\
&= \norm{\left(\id + \frac{\lambda\id - \lambda_n\id}{\lambda_n\id - T}\right)x_n} \\
&= \norm{x_n + (\lambda\id - \lambda_n\id)R_T(\lambda_n)x_n} \\
&\leq \norm{x_n} + |\lambda - \lambda_n|\;\norm{R_T(\lambda_n)x_n} \to 0.
\end{align*}
Thus $\seq{R_T(\lambda_n)x_n}$ is the kind of sequence we were looking for.
\end{proof}


\subsection{Parts of the spectrum}

\begin{example}
Operator with empty spectrum. TODO \url{https://math.stackexchange.com/questions/1344287/example-operator-with-empty-spectrum}.
\end{example}

\subsubsection{The point spectrum: eigenvalue and eigenvectors}
In this section we study invariant subspaces with dimension $1$, i.e.\ subspaces $U= \Span\{v\}$ such that
\[ Lv = \lambda v. \]
\begin{definition}
Suppose $L\in \Hom_{\mathbb{F}}(V)$.
\begin{itemize}
\item  A scalar $\lambda\in \mathbb{F}$ is called an \udef{eigenvalue} of $L$ if there exists a $v\in V$ such that $v\neq 0$ and $Lv = \lambda v$.
\item Such a vector $v$ is called an \udef{eigenvector}.
\item The set of all eigenvectors associated with an eigenvalue $\lambda$ is called the \udef{eigenspace} $E_\lambda(L)$. Because
\[ E_\lambda(L) = \ker(L-\lambda \id_V) \]
it is indeed a vector space.

The dimension of $E_\lambda(L)$ is the \udef{geometric multiplicity} of $\lambda$.
\end{itemize}
\end{definition}
\begin{proposition}
Let $L\in \Hom_\mathbb{F}(V)$ and $\lambda\in \mathbb{F}$, then
\[ \text{$\lambda$ is an eigenvalue of $L$} \qquad \iff \qquad \text{$\lambda$ is in the point spectrum $\pspec(L)$.} \]
\end{proposition}
\begin{proof}
The equation $Lv = \lambda v$ is equivalent to $(L-\lambda \id_V)v = 0$.
\end{proof}

\begin{proposition}
Let $L\in\Hom(V)$ be an operator on some vector space. Suppose $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $L$ and $v_1,\ldots, v_m$ are corresponding eigenvectors. Then $\{v_1,\ldots, v_m\}$ is linearly independent.
\end{proposition}
\begin{proof}
The proof goes by contradiction. Assume $\{v_1,\ldots, v_m\}$ is linearly dependent. Let $k$ be the smallest positive integer such that
\[ v_k \in \Span\{v_1,\ldots, v_{k-1}\}. \]
So there exists a nontrivial linear combination
\[ v_k = a_1v_1+\ldots +a_{k-1}v_{k-1}. \]
Applying $L$ to both sides gives
\[ \lambda_kv_k = a_1\lambda_kv_1+\ldots +a_{k-1}\lambda_kv_{k-1}. \]
Multipliying the previous combination by $\lambda_k$ and subtracting both equations gives
\[ 0= a_1(\lambda_k-\lambda_1)v_1 +\ldots + a_{k-1}(\lambda_k - \lambda_{k-1})v_{k-1}. \]
By assumption of linear independence of $\{v_1,\ldots, v_{k-1}\}$ this combination must be trivial, however none of the $(\lambda_k-\lambda_i)$ can be zero, so all the $a_i$ must be zero. This is a contradiction with the assumption of linear dependence.
\end{proof}
\begin{corollary}
For each operator on $V$, the set of distinct eigenvalues has at most cardinality $\dim V$.
\end{corollary}
\begin{corollary}
Let $L\in\Hom(V)$. Suppose $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $L$. Then
\[ E_{\lambda_1}(L) \oplus \ldots \oplus E_{\lambda_m}(L) \]
is a direct sum. Furthermore, the sum of geometric multiplicities is less than or equal to the dimension of $V$:
\[ \dim E_{\lambda_1}(L) + \ldots + \dim E_{\lambda_m}(L) \leq \dim V. \]
\end{corollary}


\subsubsection{Residual spectrum}
\begin{proposition}
Let $L$ be a densely defined linear operator on a Hilbert space. If $\lambda$ is in the residual spectrum of $L$ with deficiency $m$, then $\overline{\lambda}$ is in the point spectrum of $L^*$ with multiplicity $m$.
\end{proposition}
\begin{proof}
By \ref{kernelImageAdjoint} we have
\[ \im(\lambda \id - L)^\perp = \ker(\lambda\id - L)^* = \ker(\overline{\lambda}\id - L^*). \]
\end{proof}

\subsubsection{Compression spectrum}
\begin{definition}
The set of $\lambda$ for which $T-\lambda I$ does not have dense range is the \udef{compression spectrum} $\cpspec(T)$ of $T$.
\end{definition}
Then $\rspec(T) = \cpspec(T)\setminus\pspec(T)$.

\subsubsection{The essential spectrum}
TODO \url{https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)#Classification_of_points_in_the_spectrum}


\subsection{The spectral radius}
\begin{definition}
The \udef{spectral radius} $\spr(T)$ of a operator $T$ is given by
\[ \spr(T) \defeq \sup_{\lambda\in\spec(T)}|\lambda|. \]
\end{definition}


\subsection{The spectrum of operators on Hilbert spaces}
\begin{proposition}
Let $T \in \Bounded(H)$ for some Hilbert space $H$. Then
\begin{enumerate}
\item $\spec(T) \neq \emptyset$;
\item $\rho(T) = \overline{\rho(T^*)}$, where the bar denotes complex conjugation.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Let $x,y\in H$ and define
\[ f(\lambda) = \inner{x,R_\lambda(T)y}. \]
If $\spec(T) = \emptyset$, then $f$ is an entire function. Now
\[ \norm{R_\lambda(T)} \leq \frac{1}{|\lambda| - \norm{T}} \to 0 \quad\text{as}\quad |\lambda| \to \infty. \]
By Liouville's theorem (TODO ref) we must have $f\equiv 0$. Because the $x,y$ we arbitrary we must have $R_\lambda(T)y = 0$ for all $y\in H$, such that $R_\lambda(T)$ is not injective, which is impossible as it is an inver\begin{proposition}
Let $K$ be a compact operator on a Banach space. Then
\[ \spec(K)\setminus\{0\} = \pspec(K)\setminus\{0\}. \]
\end{proposition}
\begin{proof}
For all $\lambda\neq 0$, we have that $\lambda\id - K$ is Fredholm with index zero (and thus bounded). Then by the Fredholm alternative \ref{FredholmAlternative} $\lambda\id - K$ is either bijective or neither injective nor surjective, meaning $\lambda$ is either in $\rho(T)$ or in $\pspec(T)$. 
\end{proof}

\begin{proposition} \label{spectrumCompactOperator}
Let $K$ be a compact operator on a Banach space $X$. Then
\begin{enumerate}
\item for all $\lambda\in\spec(K)\setminus\{0\}$ there exists a least $m$ such that $\ker(\lambda\id- K)^m = \ker(\lambda\id- K)^{m+1}$. This space is finite dimensional and reducing for $K$;
\item for $\alpha > 0$ the number of eigenvalues $\lambda$ such that $|\lambda|\geq \alpha$ is finite;
\item $0$ is the only accumulation point; if $X$ is infinite dimensional, then $0\in\spec(K)$;
\item $\spec(K)$ is at most countably infinite;
\item every $\lambda \in \spec(K)\setminus \{0\}$ is a pole of the resolvent $R_K$.
\end{enumerate}
\end{proposition}
\begin{proof}
\url{https://en.wikipedia.org/wiki/Spectral_theory_of_compact_operators}
\end{proof}
TODO: if $K$ is a self-adjoint compact operator on a Hilbert space $H$, then $H$ has an orthonormal basis of eigenvectors of $K$.
se.

(2) Take $\lambda\in\rho(T)$. Then
\[ ((\lambda\id - A)^{-1})^* = (\overline{\lambda}\id - A^*)^{-1} \]
so $\overline{\lambda}\in\rho(T^*)$ iff $((\lambda\id - A)^{-1})^*$ is bounded iff $(\lambda\id - A)^{-1}$ is bounded iff $\lambda\in \rho(T)$.
\end{proof}

\begin{lemma} \label{eigenspaceOrthogonalAdjoint}
Let $L$ be a densely defined operator on a Hilbert space $H$. Take $\lambda\in \pspec(L)$ and $\mu\in\pspec(L^*)$. If $\lambda \neq \overline{\mu}$, then
\[ \ker(\lambda\id - L)\perp \ker(\mu\id - L^*). \]
\end{lemma}
\begin{proof}
Take non-zero eigenvectors $x,y$ such that $Ax = \lambda x$ and $A^*y = \mu y$. Then
\[ \lambda \inner{y,x} = \inner{y,\lambda x} = \inner{y, Ax} = \inner{A^*y,x} = \inner{\mu y,x} = \overline{\mu}\inner{y,x}. \]
So we have $(\lambda - \overline{\mu})\inner{y,x} = 0$.
\end{proof}

\begin{proposition} \label{adjointSpectrumNoResidual}
Let $L$ be a densely defined operator on a Hilbert space $H$. Then the following are equivalent:
\begin{enumerate}
\item the residual spectrum of $L$ is empty;
\item $\overline{\pspec(L^*)} \subseteq \pspec(L)$;
\end{enumerate}
as are the following:
\begin{enumerate}
\item the residual spectrum of $L^*$ is empty;
\item $\pspec(L) \subseteq \overline{\pspec(L^*)}$.
\end{enumerate}
In particular all these statements hold if $L$ is normal.
\end{proposition}
\begin{proof}
Consider, for all $x\in \dom(L), y\in\dom(L^*)$, the equality
\[ \inner{(\lambda\id-L)x,y} = \inner{x,(\overline{\lambda}\id-L^*)y}. \]
We can make the following inferences:
\begin{itemize}
\item If $\lambda\in \overline{\pspec(L^*)}$, then the equality holds in particular for all eigenvectors $y$. This implies $\inner{(\lambda\id-L)x,y} = 0$. By \ref{perpToDenseSet} $\im(\lambda\id-L)$ may then not be dense, so it cannot be injective because the residual spectrum of $L$ is empty.
\item Assume $\lambda\id-L$ injective and take  $y\perp \im(\lambda\id-L)$. Then by the equality $\inner{x, (\overline{\lambda}\id - L^*)y} = 0$ for all $x\in\dom(L)$, which is dense. So $(\overline{\lambda}\id - L^*)y = 0$ by \ref{perpToDenseSet}. Now $\lambda\notin \pspec(L)$, so $\overline{\lambda}\notin \pspec(L^*)$. Thus $y = 0$ and $\im(\lambda\id-L)^\perp = \{0\}$, meaning $\im(\lambda\id-L)$ is dense.
\end{itemize}
The arguments for the second set of statements are similar.

If $L$ is normal, then $\ker(\lambda \id - L) = \ker{\overline{\lambda}\id -L^*}$ by \ref{equalityKernelAdjointNormal}, so $\pspec(L) = \overline{\pspec(L^*)}$.
\end{proof}

\begin{proposition}
Let $T$ be a closed, densly defined operator on a Hilbert space.
\begin{enumerate}
\item If $\lambda\in\rho(T)$, then $\overline{\lambda}\in\rho(T^*)$.
\item If $\lambda\in\rspec(T)$, then $\overline{\lambda}\in\pspec(T^*)$.
\item If $\lambda\in\pspec(T)$, then $\overline{\lambda}\in\rspec(T^*)\cup\pspec(T^*)$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO Compare with \ref{adjointSpectrumNoResidual}. CLosure necessary?
\end{proof}


\begin{proposition}
Let $T$ be a unitary operator. Then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item $\spec(T) \subset \setbuilder{\lambda\in\C}{|\lambda| = 1}$.
\end{enumerate}
\end{proposition}
TODO: move to more general place??

\begin{lemma}
The eigenvalues of a bounded dissipative linear operator
lie in the half-plane $\Im\lambda \geq 0$.
\end{lemma}

\subsubsection{Rayleigh quotient}
\begin{lemma}
Let $L$ be an operator on a Hilbert space. If $x$ is an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \lambda. \]
\end{lemma}
\begin{proof}
Let $x$ be an eigenvector with eigenvalue $\lambda$, then
\[ J_L(x) = \frac{\inner{x,Lx}}{\inner{x,x}} = \lambda \frac{\inner{x,x}}{\inner{x,x}} = \lambda. \]
\end{proof}

\begin{proposition}
If $U$ is unitary, then $\spec(U)\subset \mathbb{T}$.
\end{proposition}

\section{Spectral theory for types of operators}
\subsection{Compact operators}

\begin{proposition}
Let $K$ be a compact operator on a Banach space. Then
\[ \spec(K)\setminus\{0\} = \pspec(K)\setminus\{0\}. \]
\end{proposition}
\begin{proof}
For all $\lambda\neq 0$, we have that $\lambda\id - K$ is Fredholm with index zero (and thus bounded). Then by the Fredholm alternative \ref{FredholmAlternative} $\lambda\id - K$ is either bijective or neither injective nor surjective, meaning $\lambda$ is either in $\rho(T)$ or in $\pspec(T)$. 
\end{proof}

\begin{proposition} \label{spectrumCompactOperator}
Let $K$ be a compact operator on a Banach space $X$. Then
\begin{enumerate}
\item for all $\lambda\in\spec(K)\setminus\{0\}$ there exists a least $m$ such that $\ker(\lambda\id- K)^m = \ker(\lambda\id- K)^{m+1}$. This space is finite dimensional and reducing for $K$;
\item for $\alpha > 0$ the number of eigenvalues $\lambda$ such that $|\lambda|\geq \alpha$ is finite;
\item $0$ is the only accumulation point; if $X$ is infinite dimensional, then $0\in\spec(K)$;
\item $\spec(K)$ is at most countably infinite;
\item every $\lambda \in \spec(K)\setminus \{0\}$ is a pole of the resolvent $R_K$.
\end{enumerate}
\end{proposition}
\begin{proof}
\url{https://en.wikipedia.org/wiki/Spectral_theory_of_compact_operators}
\end{proof}
TODO: if $K$ is a self-adjoint compact operator on a Hilbert space $H$, then $H$ has an orthonormal basis of eigenvectors of $K$.


\subsection{Multiplication operators}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. A \udef{multiplication operator} is an operator of the form
\[ T: L^p(\Omega, \mu) \to L^p(\Omega, \mu): u(x) \mapsto a(x)u(x) \]
for some $a\in L^\infty(\Omega,\mu)$
\end{definition}

\begin{proposition}
Let $T: L^p(\Omega, \mu) \to L^p(\Omega, \mu): u \mapsto a\cdot u$ be a multiplication operator. Then
\[ \norm{T} = \norm{a}_{L^\infty}. \]
\end{proposition}
\begin{proof}
From the inequality $\norm{Tu}_{L^p}\leq \norm{a}_{L^\infty}\norm{u}_{L^p}$ we get $\norm{T} \leq \norm{a}_{L^\infty}$.

TODO
\end{proof}

\begin{lemma}
Let $T: L^2(\Omega, \mu) \to L^2(\Omega, \mu): u \mapsto a\cdot u$ be a multiplication operator with $a\in L^\infty(\Omega,\mu)$. Then $T^*$ is the multiplication operator
\[ T^*: L^2(\Omega, \mu) \to L^2(\Omega, \mu): u \mapsto \overline{a}\cdot u. \]
\end{lemma}
\begin{proof}
From 
\[ \inner{Tu,v} = \int_\Omega a\cdot u \cdot \overline{v}\diff{\mu} = \int_\Omega u \cdot \overline{\overline{a}\cdot v}\diff{\mu} \]
it follows that $T^*v = \overline{a}\cdot v$.
\end{proof}
\begin{corollary}
Then
\begin{enumerate}
\item $T$ is self-adjoint if $a$ is real-valued;
\item $T$ is skew-adjoint if $a$ is purely imaginary;
\item $T$ is unitary if $|a(x)| \equiv 1$.
\end{enumerate}
\end{corollary}

Let $E_\lambda$ be the level set
\[ E_\lambda = \setbuilder{x\in\Omega}{a(x) = \lambda} \]

\begin{proposition}
Let $T: L^2(\Omega, \mu) \to L^2(\Omega, \mu): u\mapsto a\cdot u$ be a multiplication operator with $a\in \cont(\Omega)$. Then
\begin{enumerate}
\item $\pspec(T) = \setbuilder{\lambda\in \im(a)}{\mu(E_\lambda)>0}$;
\item $\cspec(T) = \setbuilder{\lambda\in \overline{\im(a)}}{\mu(E_\lambda) = 0}$;
\item $\rspec(T) = \emptyset$;
\item $\rho(T) = \C\setminus \overline{\im(T)}$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsection{Dissipative operators}
\begin{definition}
Let $T\in \Lin(V, W)$ be a linear operator between Banach spaces. Then $T$ is called \udef{dissipative} if $\lambda\id-T$ is bounded below by $\lambda$ for all $\lambda>0$:
\[ \norm{(\lambda\id-T)x} \geq \lambda\norm{x} \]
for all $x\in\dom(T)$.
\end{definition}

\begin{lemma} \label{dissipativeResolventBound}
Let $T\in \Lin(V, W)$ be an operator between Banach spaces. Then $T$ is dissipative \textup{if and only if} for all $\lambda>0$ the resolvent $R_T(\lambda): \im(T)\to V$ exists and is bounded by $\norm{R_T(\lambda)} \leq \lambda^{-1}$.
\end{lemma}
\begin{proof}
If $T$ is dissipative, then the result is given by \ref{boundedBelow}.

Assume $R_T(\lambda): \im(T)\to V$ exists. Then
\[ \lambda\norm{x} = \lambda\norm{R_T(\lambda)(\lambda\id-T)x} \leq \lambda \norm{R_T(\lambda)}\,\norm{(\lambda\id-T)x} \leq \lambda\lambda^{-1}\norm{(\lambda\id-T)x} = \norm{(\lambda\id-T)x}. \]
\end{proof}

Thus $\lambda>0$ is in $\res(T)$ if and only if $\lambda\id - T$ is surjective.

\begin{proposition} \label{spectrumDissipativeOperator}
Let $T\in \Lin(V, W)$ be a dissipative operator between Banach spaces. Then either $]0,+\infty[\,\perp \res(T)$ or $]0,+\infty[\,\subseteq \res(T)$.
\end{proposition}
\begin{proof}
We need to show that $\lambda\in\res(T)$ for some $\lambda >0$, then $\lambda\id - T$ is surjective for all $\lambda>0$.

Assume $\lambda\in\res(T)$ for some $\lambda >0$. Then \ref{dissipativeResolventBound} and \ref{firstNeumannSeries} combine the give $]0, 2\lambda[ \subseteq \res(T)$. We can repeat this to cover the whole of $]0,+\infty[$.
\end{proof}

\begin{proposition} \label{closureDissipativeOperator}
Let $T\in \Lin(V, W)$ be a dissipative operator between Banach spaces. Then the following are equivalent:
\begin{enumerate}
\item $T$ is closed;
\item $\im(\lambda\id - T)$ is closed for some $\lambda > 0$;
\item $\im(\lambda\id - T)$ is closed for all $\lambda > 0$.
\end{enumerate}
\end{proposition}
\begin{proof}
For all $\lambda\in\R$, we have that $T$ is closed iff $\lambda\id - T$ is closed iff $(\lambda\id - T)^{-1}: \im(\lambda\id - T) \to V$ is closed by \ref{algebraClosedOperators}.

Now closedness of $\lambda\id - T$ implies $\im(\lambda\id - T)$ is closed by \ref{boundedBelowClosedRange}. Conversely, if $\im(\lambda\id - T)$ is closed, then $\lambda\id - T$ is closed by the closed graph theorem \ref{closedGraphTheorem}.
\end{proof}

\begin{proposition}
Let $T\in \Lin(V)$ be a dissipative operator on a Banach space $V$. If $\im(T)\subseteq \overline{\dom(T)}$, then
\begin{enumerate}
\item $T$ is closable;
\item its closure $\overline{T}$ is dissipative;
\item $\im(\lambda\id - \overline{T}) = \overline{\im(\lambda\id - T)}$ for all $\lambda >0$.
\end{enumerate}
\end{proposition}
In particular $\im(T)\subseteq \overline{\dom(T)}$ holds whenever $T$ is densely defined.
\begin{proof}
(1) We use \ref{closableCriterion}. Assume $\seq{x_n}\to 0$ and $\seq{Tx_n}\to v$. We need to show that $v=0$. Because $T$ is dissipative, we have
\[ \norm{\lambda(\lambda\id-T)x_n + (\lambda\id-T)w} = \norm{(\lambda\id-T)(\lambda x_n -w)} \geq \lambda\norm{\lambda x_n + w} \]
for all $w\in \in\dom(T)$ and all $\lambda>0$. Taking the limit $n\to \infty$ gives
\[ \norm{-\lambda v +(\lambda\id- T)w} \geq \lambda\norm{w}, \qquad\text{and hence}\qquad \norm{w - v - \frac{1}{\lambda}Tw} \geq w. \]
Taking the limit $\lambda \to \infty$ gives $\norm{w-v}\geq \norm{w}$. Now $y\in \overline{\im(T)} \subseteq \overline{\dom(T)}$. Thus we can find a sequence $\seq{w_n}\to y$ in $\dom(T)$. This sequence then satisfies $\norm{w_n-y} \geq \norm{w_n}$. Taking the limit gives $0\geq \norm{y}$, so $y = 0$.

(2) For all $x\in\dom(\overline{T})$ there exists a sequence $\seq{x_n}\to x$ in $\dom(T)$ such that $\seq{Tx_n} \to \overline{T}x$ by \ref{graphNormConvergenceLemma}. Now for all $n\in \N$,
\[ \norm{(\lambda\id-T)x_n} \geq \lambda\norm{x_n}. \]
Taking the limit $n\to\infty$ gives $\norm{(\lambda\id-\overline{T})x} \geq \lambda\norm{x}$, meaning $\overline{T}$ is dissipative.

(3) By \ref{domImClosureOperator}, $\im(\lambda\id - T)$ is dense in $\im(\lambda\id-\overline{T})$ and by \ref{closureDissipativeOperator}, $\im(\lambda\id-\overline{T})$ is closed.
\end{proof}



\section{The spectral theorem}
\url{https://link.springer.com/content/pdf/10.1007%2F978-1-4614-7116-5.pdf}

\url{http://individual.utoronto.ca/jordanbell/notes/SVD.pdf}
\url{https://digitalcommons.mtu.edu/cgi/viewcontent.cgi?article=2133&context=etdr}

\url{https://web.ma.utexas.edu/mp_arc/c/09/09-32.pdf}


\section{Functional calculus}
\subsection{Holomorphic functional calculus}

\begin{theorem}[Holomorphic functional calculus] 
\label{holomorphicFunctionalCalculus} \label{holomorphicSpectralMapping}
Let $A$ be a Banach algebra and $x\in A$. Consider the function
\[ \Phi_x: \cont^\infty(\spec(x),\C) \to A: f\mapsto f(x)\defeq \oint_\Gamma f(z)R_x(z)\diff{z}. \]
Here $\Gamma$ is any simple Jordan curve that contains $\spec(x)$ such that $f$ is holomorphic in a region that contains $\Gamma$ and its interior. Then
\begin{enumerate}
\item $\Phi_x$ is well-defined: it does not depend on the particular curve $\Gamma$;
\item $\Phi_x$ is a homomorphism;
\item for any polynomial $p\in \C[X]$, we have $\Phi_x(p) = p(x)$; \\
in particular $\Phi_x(\id_\C) = x$ and $\Phi_x(\underline{1}) = \id_A$;
\item $\spec(\Phi_x(f)) = f[\spec(x)]$;
\item $\Phi_x$ is continuous if $\cont^\infty(\spec(x),\C)$ is equipped with continuous convergence (?).
\end{enumerate}
\end{theorem}
TODO: $\cont^\infty(\spec(x))$ should be the space of functions that are analytic in some neighbourhood of $\spec(x)$. Is it??
\begin{proof}
TODO
\end{proof}

TODO unbounded operators

\subsubsection{Riesz eigenprojections}
Holomorphic functional calculus applied to
\[ \chi_{S,\delta}: A\to \{0,1\}: x\mapsto \begin{cases}
1 & d(x,S) \leq \delta \\
0 & \text{otherwise}.
\end{cases} \]

TODO: spectral measure with only disconnected parts in $\sigma$-algebra??

TODO: $P_\Delta$ and $E_\Delta \defeq \im P_\Delta$.

\begin{lemma}
$\spec(T|_{E_\Delta}) = \spec(T)\cap\Delta$.
\end{lemma}

\begin{definition}
We call $\dim E_\lambda$ the \udef{algebraic multiplicity} of $\lambda$.
\end{definition}

\subsubsection{Frobenius covariants}
TODO $P_\lambda$ is a Frobenius covariant. \url{https://en.wikipedia.org/wiki/Frobenius_covariant}

TODO cfr. Lagrange polynomial??

\section{Jordan decomposition}
\subsection{Eigennilpotent}
\begin{definition}
Let $a$ be a finite element in a semisimple Banach algebra and $\lambda\in \spec(a)$. The \udef{eigennilpotent operator} of $a$ at $\lambda$ is defined as
\[ D_{\lambda} \defeq (a-\lambda)P_{\lambda}. \]
\end{definition}
This definition works because we can find a $\delta < d(\lambda, \spec(a)\setminus\{\lambda\})$.

\begin{lemma}
Let $a$ be a finite element in a semisimple Banach algebra and $\lambda\in \spec(a)$. The eigennilpotent operator $D_\lambda$ is nilpotent.
\end{lemma}
\begin{proof}
By spectral mapping \ref{holomorphicSpectralMapping}, $D_\lambda$ is quasinilpotent. Because $a$ is finite, it is nilpotent by \ref{nilpotentQuasinilpotent}.
\end{proof}



\subsection{Jordan vectors}
\begin{definition}
Let $V$ be a finite dimensional vector space and $T$ an operator on $V$. A \udef{Jordan vector} of $T$ belonging to the eigenvalue $\lambda$ is a vector $x\in V$ such that
\[ (\lambda\id_V - T)^kx = 0 \]
for some $k\in \N$. The least such $k$ is called the \udef{degree} of $x$ and is denoted $\deg_J(x)$.
\end{definition}
Eigenvectors are Jordan vectors of degree $1$.

\begin{proposition}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$ $\lambda\in\spec(T)$ and $x\in V$. Then $x$ is a Jordan vector of $T$ belonging to the eigenvalue $\lambda$ \textup{if and only if} $x\in E_\lambda$.
\end{proposition}
\begin{proof}
Let $x\in E_\lambda$. Then $x = P_\lambda x$ and thus
\[ (\lambda\id_V - T)^kx = (\lambda\id_V - T)^kP_\lambda x = \big((\lambda\id_V - T)P_\lambda\big)^k x = D_\lambda^k x, \]
which is zero for some $k$ because $D_\lambda$ is nilpotent.

Conversely, assume $x$ is a Jordan vector of $T$ belonging to the eigenvalue $\lambda$. We can write $x = x_1+x_2 \in E_{\lambda}\oplus E_{\C\setminus\{\lambda\}}$.
Then (because $E_\lambda$ is reducing for $T-\lambda\id_V$)
\[ 0 = (\lambda\id_V - T)^kx = (\lambda\id_V - T)^kx_1 + (\lambda\id_V - T)^kx_2 \in E_{\lambda}\oplus E_{\C\setminus\{\lambda\}} \]
Thus we have $(\lambda\id_V - T)^kx_1 = 0$ and $(\lambda\id_V - T)^kx_2 = 0$ separately.
Now $T-\lambda\id_V$ is invertible on $E_{\C\setminus\{\lambda\}}$, so $x_2 = 0$ (TODO ref). This means that $x = x_1 \in E_\lambda$.
\end{proof}

\begin{definition}
Let $m = \deg_N(D_\lambda)$. Then we have
\[ \{0\} \subsetneq \ker(\lambda\id_V - T) \subsetneq \ker(\lambda\id_V - T)^2 \subsetneq \ldots \subsetneq \ker(\lambda\id_V - T)^{m-1} \subsetneq \ker(\lambda\id_V - T)^m = V. \]
We define $E^k_\lambda \defeq \ker(\lambda\id_V - T)^k$. In particular
\begin{itemize}
\item $E^1_\lambda$ is the \udef{geometric eigenspace};
\item $E^{m-1}_\lambda$ is the \udef{algebraic eigenspace}.
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$, $\lambda\in\spec(T)$ and $x\in E_\lambda$. Then
\begin{enumerate}
\item $1 \leq \dim\ker(\lambda\id_V - T) \leq \dim E_\lambda$;
\item $1 \leq \deg_J(x) \leq \dim_E\lambda$.
\end{enumerate}
\end{lemma}
The lemma says the geometric multiplicity is smaller than the algebraic multiplicity.
\begin{proof}
Every eigenvector is a Jordan vector, so $\ker(\lambda\id_V - T) \subseteq E_\lambda$.

For all $k\in\N$ smaller then the degree of $x$, $(\lambda\id_V - T)^kx$ is a Jordan vector and thus in $E_\lambda$. TODO all $(\lambda\id_V - T)^kx$ are linearly independent (like in \ref{nilpotentQuasinilpotent})
\end{proof}

\begin{definition}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$ and $\lambda\in\spec(T)$. The eigenvalue $\lambda$ is called
\begin{itemize}
    \item \udef{simple} if the algebraic multiplicity is $1$;
    \item \udef{semisimple} if every Jordan vector in $E_\lambda$ has degree $1$;
    \item \udef{prime} if the geometric multiplicity is $1$.
\end{itemize}
If all eigenvalues of $T$ are semisimple, then $T$ is called a \udef{diagonal operator}.
\end{definition}

\begin{lemma}
An operator $T$ is diagonal iff $T$ is of the form $\sum_j a_jP_j$, where $a_j\in \F$ and $P_j$ are projectors that commute pairwise.
\end{lemma}

\subsection{Characteristic polynomial and equation}
\begin{definition}
Let $V$ be a finite dimensional vector space and $T$ an operator on $V$. The \udef{characteristic polynomial} $p_T(x)$ of $T$ is the polynomial
\[ p_T(x) \defeq \det(x\id_V - T). \]
\end{definition}

\begin{proposition}
Let $V$ be a finite dimensional vector space, $T$ an operator on $V$ and $\spec(T) = \{\lambda_j\}_{j=1}^r$. Then
\[ p_T(x) = \prod_{j=1}^r(x - \lambda_j)^{\dim E_{\lambda_j}}. \]
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
A number $\lambda\in \C$ is an eigenvalue of $T$ \textup{if and only if} it is a root of $p_T(x)$.
\end{corollary}

\begin{definition}
The equation $p_T(x) = 0$ is the \udef{characteristic equation} of $T$.
\end{definition}

\subsection{Spectral representation}
\begin{proposition}
Let $V$ be a finite dimensional complex vector space and $T$ an operator on $V$. There exists a unique decomposition $T = S + D$ such that
\begin{itemize}
\item $S$ is diagonal;
\item $D$ is nilpotent;
\item $SD = DS$.
\end{itemize}
If $\spec(T) = \{\lambda_j\}_{j=1}^r$, this decomposition is given by
\[ T = \sum_{j=1}^r \lambda_r P_{\lambda_r} + \sum_{j=1}^r D_{\lambda_r}. \]
\end{proposition}

\subsection{Partial fraction decomposition of the resolvent}
For any operator $T$ on a vector space $V$ with eigenvalue $\lambda_0$, the resolvent $R_T(\lambda)$ has a pole at $\lambda_0$.

\begin{proposition}
Let $T$ be an operator on a finite dimensional vector space $V$ and $\lambda_0\in\spec(T)$. Then the Laurent expansion of $R_T(\lambda)$ around $\lambda_0$ is of the form
\[ R_T(\lambda) = \frac{P_0}{\lambda-\lambda_0} + \sum_{n=1}^{\deg_N(D_0)-1}\frac{D_0^{n}}{(\lambda - \lambda_0)^{n+1}} + \sum_{n=0}^\infty(-1)^n S_0^{n+1}(\lambda - \lambda_0)^n, \]
where $P_0\defeq P_{\lambda_0}, D_0\defeq D_{\lambda_0}$ and $S_0$ is some fixed operator.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{definition}
The holomorphic part of the Laurent expansion of $R_T(\lambda)$ at $\lambda_0$ is called the \udef{reduced resolvent} of $T$ w.r.t. $\lambda_0$:
\[ S_{T,\lambda_0}(\lambda) \defeq \sum_{n=0}^\infty(-1)^n S_0^{n+1}(\lambda - \lambda_0)^n = R_T(\lambda) - \left(\frac{P_0}{\lambda-\lambda_0} + \sum_{n=1}^{\deg_N(D_0)-1}\frac{D_0^{n}}{(\lambda - \lambda_0)^{n+1}}\right). \]
\end{definition}

\begin{proposition}
Let $T$ be an operator on a finite dimensional vector space $V$ and $\lambda_0\in\spec(T)$. Then
\[ R_{T|_{(\id_V-P_0)}}(\lambda) = S_{T,\lambda_0}|_{\id_V-P_0}(\lambda). \]
\end{proposition}

\begin{proposition}
Let $T$ be an operator on a finite dimensional vector space $V$ with $\spec(T) = \{\lambda_j\}_{j=1}^r$. The partial fraction decomposition of $R_T(\lambda)$ is given by
\[ R_T(\lambda) = \sum_{j=1}^r\left(\frac{P_{\lambda_j}}{\lambda - \lambda_j} +\sum_{n=1}^{\deg_N(D_{\lambda_j})-1}\frac{D_{\lambda_j}^n}{(\lambda - \lambda_j)^{n+1}}\right). \]
The partial fraction decomposition of $S_{T,\lambda_k}(\lambda)$ is given by
\[ S_{T,\lambda_k}(\lambda) = \sum_{\substack{j=1 \\ j\neq k}}^r\left(\frac{P_{\lambda_j}}{\lambda - \lambda_j} +\sum_{n=1}^{\deg_N(D_{\lambda_j})-1}\frac{D_{\lambda_j}^n}{(\lambda - \lambda_j)^{n+1}}\right). \]
\end{proposition}
\begin{proof}
The poles of $R_T(\lambda)$ are exactly the eigenvalues of $T$. There are finitely many of them, so we can use partial fraction decomposition, \ref{partialFractionDecomposition}. We just need to show that the holomorphic part is zero. For that we note that $\lim_{\lambda \to \infty} R_T(\lambda) = 0$ and all principal parts tend to $0$ at infinity as well. Thus the holomorphic part also tends to $0$, making it bounded. By Liouville's theorem, \ref{liouvilleTheoremAnalysis}, we get that it is identically zero.
\end{proof}
\begin{corollary}[Sylvester-Lagrange formula]
Let $f$ be a holomorphic function on an open set that contains $\spec(T)$. Then
\[ f(T) = \sum_{j=1}^r\left(f(\lambda_j)P_{\lambda_j} +\sum_{n=1}^{\deg_N(D_{\lambda_j})-1}\frac{f^{(n)}(\lambda_j)D_{\lambda_j}^n}{n!}\right). \] 
\end{corollary}
\begin{proof}
We have
\[ f(T) = \oint_\Gamma f(\lambda)R_T(\lambda)\diff{\lambda} = 2\pi i\sum_{j=1}^r \Res_{\lambda_j}f(\lambda)R_T(\lambda) \]
by the residue theorem (TODO ref for operators).
\end{proof}
\begin{corollary}[Cayley-Hamilton]
Let $p_T(x)$ be the characteristic polynomial of $T$. Then $p_T(T) = 0$.
\end{corollary}
\begin{proof}
Since $p_T(x) = \prod_{j=1}^r(x - \lambda_j)^{\dim E_{\lambda_j}}$ and $\dim E_{\lambda_j} \geq \deg_N(D_{\lambda_j})$, we see that $p_T(\lambda)R_T(\lambda)$ has no poles and is holomorphic, meaning that $oint_\Gamma f(\lambda)R_T(\lambda)\diff{\lambda} = 0$ by Cauchy's theorem (TODO ref for operators).
\end{proof}

\subsection{Normal operators}


\begin{proposition}
If $T$ is a normal operator, then $P_\lambda = P^*_\lambda$ and $D_\lambda = D^*_\lambda = 0$.
\end{proposition}
This means normal operators are diagonalisable.
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Let $V$ be a finite dimensional complex vector space and $T$ a normal operator
on $V$ with $\spec(T) = \{\lambda_j\}_{j=1}^r$.
\begin{enumerate}
\item We have the spectral decompositions
\[ T = \sum_{j=1}^r \lambda_r P_{\lambda_r} \qquad\text{and}\qquad T^* = \sum_{j=1}^r \overline{\lambda_r} P_{\lambda_r}. \]
\item We have
\[ R_T(\lambda) = \sum_{j = 1}^r \frac{P_{\lambda_j}}{\lambda - \lambda_j} \qquad \text{and} \qquad S_{T,\lambda_k}(\lambda) = \sum_{\substack{j = 1 \\ j\neq k}}^r \frac{P_{\lambda_j}}{\lambda - \lambda_j} \]
\end{enumerate}
\end{corollary}

\subsection{Jordan decomposition}
TODO matrix representation + matrix representation of Lagrange-Sylvester. See Baumgrtel







\chapter{Hilbert spaces}

\section{Examples}
\subsection{The $\ell^2$ spaces}
Sequence spaces $\ell^p$ Hilbert iff $p=2$. (TODO: other sequence spaces?)

\subsection{Direct sum}
Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. By considering them as Banach spaces we can take the $\ell^2$-direct sum. (TODO: other sequence spaces?)
\begin{proposition}
Let $(V_i)_{i\in I}$ be a family of Hilbert spaces. The $\ell^2$-direct sum is a Hilbert space.
\end{proposition}
This gives the conventional interpretation of the \udef{Hilbert space direct sum}: it is the $\ell^2$-direct sum of the summands as Banach spaces.

\section{Strong and weak convergence}
\subsection{Weak convergence of vectors}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. The \udef{weak convergence} on $\mathcal{H}$ is the initial convergence w.r.t.
\[ \setbuilder{\inner{x,-}: \mathcal{H}\to \F}{x\in\mathcal{H}}. \]
Thus $F \overset{w}{\longrightarrow} v$ \textup{if and only if} $\inner{x, F} \longrightarrow \inner{x,v}$ for all $x\in\mathcal{H}$.

We denote the weak convergence $\overset{w}{\longrightarrow}$ or $\lim^w$ and write $\mathcal{H}^w$ to denote the convergence space $\sSet{\mathcal{H},\lim^w}$.
\end{definition}
By \ref{initialVectorConvergenceLinearFunctions} we have that the weak convergence is a vector space convergence.

\begin{lemma} \label{normConvergenceFinerThenWeakConvergence}
Norm convergence is finer than weak convergence.
\end{lemma}
\begin{proof}
The functions in $\setbuilder{\inner{x,-}: \mathcal{H}\to \F}{x\in\mathcal{H}}$ are also continuous in the norm convergence.
\end{proof}

\begin{example}
Let $\seq{e_n}$ be an orthonormal basis. Then the Riemann-Lebesgue lemma, \ref{RiemannLebesgueLemma}, gives $e_n \overset{w}{\longrightarrow} 0$, even though clearly $e_n \not\to 0$.
\end{example}

So norm convergence is strictly finer.

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space, $x\in \mathcal{H}$ and $\seq{x_n}\subseteq \mathcal{H}$. Then
\begin{enumerate}
\item $x_n \overset{w}{\longrightarrow} x$ implies $\norm{x} \leq \liminf \norm{x_n}$;
\item $x_n \longrightarrow x$ \textup{if and only if} $x_n \overset{w}{\longrightarrow} x$ and $\norm{x_n}\to \norm{x}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have
\[ \norm{x}^2 = \inner{x,x} = \lim \inner{x,x_n} = \liminf\inner{x,x_n} \leq \norm{x}\liminf\norm{x_n}, \]
where the limit is in $\R$. We use that for convergent sequences, $\lim = \liminf$.

(2) The direction $\Rightarrow$ is clear form \ref{normConvergenceFinerThenWeakConvergence} and the continuity of the norm.

For the converse, we have
\[ \norm{x-x_n}^2 = \norm{x}-2\Re(\inner{x,x_n}) + \norm{x_n} \to 0, \]
because $\norm{x_n} \to \norm{x}$ and $\inner{x,x_n} \to \inner{x,x} = \norm{x}^2$.
\end{proof}

\url{https://math.stackexchange.com/questions/1461363/closed-unit-ball-of-hilbert-space-sequentially-compact-in-weak-topology}

\subsubsection{Weak Cauchy filters}
\begin{definition}
Cauchy filters in $\mathcal{H}^w$ are called \udef{weak Cauchy filters}.
\end{definition}


\begin{proposition}
Let $\mathcal{H}$ be a real or complex Hilbert space.
A filter $F$ in $\powerfilters(\mathcal{H})$ is a weak Cauchy filter \textup{if and only if} $\inner{x,F}$ converges in $\F$ for all $x$.
\end{proposition}
\begin{proof}
The following steps are equivalent:
\begin{itemize}
\item $F$ is weak Cauchy;
\item $F-F \overset{w}{\longrightarrow} 0$;
\item $\inner{x, F} - \inner{x,F} \to 0$ for all $x\in \mathcal{H}$;
\item $\inner{x, F}$ is Cauchy in the scalar field for all $x\in\mathcal{H}$;
\item $\inner{x, F}$ converges for all $x\in\mathcal{H}$.
\end{itemize}
The last equivalence follows because $\R$ and $\C$ are complete.
\end{proof}
\begin{corollary}
Every weak Cauchy filter is bounded in norm.
\end{corollary}
\begin{proof}
Consider the family of bounded linear operator $\inner{F,-}$. Then $\inner{F,-}$ converges pointwise, by the proposition. By Banach-Steinhaus, \ref{BanachSteinhaus}, we have $\sup_{y\in F}\norm{\inner{y,-}} = \sup_{y\in F}\norm{y} <\infty$.
\end{proof}

Note that the previous proposition does not mean a weak Cauchy filter is necessarily weakly convergent, as there may not exist a vector $v$ such that $\inner{x,F}$ converges to $\inner{x,v}$ for all $x\in\mathcal{H}$. However, we have the following proposition:

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space. Then $\mathcal{H}^w$ is Cauchy complete.
\end{proposition}
\begin{proof}
Let $F$ be a weak Cauchy filter. Let $\seq{e_i}_{i\in I}$ be an orthonormal basis of $\mathcal{H}$. Let $c_i = \lim \inner{e_i, F}$. Then $F \overset{w}{\longrightarrow} \sum_{i\in I}c_i e_i$.

First we check this sum is well-defined. TODO!!
\end{proof}

TODO: $\inner{F,F}$ weak convergent?????


\subsection{Strong and weak convergence of operators}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. Then the
\begin{itemize}
\item \udef{strong operator topology} is the topology of pointwise convergence on $\Lin(\mathcal{H})$;
\item \udef{weak operator topology} is the topology of pointwise convergence on $\Lin(\mathcal{H}^w)$.
\end{itemize}
We write $F \overset{SOT}{\longrightarrow}A$ and $F \overset{WOT}{\longrightarrow}A$ for the convergence of $F$ to $A$ in the strong and weak operator topologies.
\end{definition}


TODO introduce shifts earlier.
\begin{example}
Consider the left and right shifts $S_l$ and $S_r$ on $\ell^2(\N)$. Then
\begin{itemize}
\item $S_l \overset{SOT}{\longrightarrow} 0$, but $S_l \overset{norm}{\not\longrightarrow} 0$;
\item $S_r \overset{WOT}{\longrightarrow} 0$, but $S_r \overset{SOT}{\not\longrightarrow} 0$.
\end{itemize}
In general taking adjoints is not continuous w.r.t. strong operator convergence, because $S_l \overset{SOT}{\longrightarrow} 0$, but $S_r = S_l^* \overset{SOT}{\not\longrightarrow} 0$.
\end{example}

\url{https://math.stackexchange.com/questions/1054288/the-set-of-all-normal-operators-on-a-hilbert-space-is-not-strongly-closed}


Note normal operators not $SOT$-closed!
\begin{proposition}
If $\seq{A_n}$ is a sequence of normal operators that converges to a normal operator $A$ in the strong operator topology, then $A_n^* \overset{SOT}{\longrightarrow} A^*$.
\end{proposition}

\begin{proposition}
Let $\seq{A_n}$ be a sequence of bounded operators on a Hilbert space and $A\in\Lin(\mathcal{H})$. Then
\begin{enumerate}
\item if $A_n \overset{SOT}{\longrightarrow} A$, then $\norm{A}\leq \liminf\norm{A_n}$;
\item If $A_nx \longrightarrow Ax$ for all $x$ in a dense subset of $\mathcal{A}$ and $\seq{A_n}$ is a bounded sequence, then $A_n \overset{SOT}{\longrightarrow} A$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) For all unit vectors $x$ we have
\[ \norm{Ax} = \norm{\lim_{SOT}A_nx} = \lim_{n\to\infty}\norm{A_nx} \leq  \]
\end{proof}
TODO: same for WOT.

TODO: Cauchy sequences are bounded. Does this follow form general Banach theory?


\section{Tools to study operators}
\subsection{Spectrum}
\subsection{Numerical range}
\begin{lemma}
Let $T$ be an operator on a Hilbert space. If $\lambda\in\cspec(T)$, then there exists a Weyl sequence for $\lambda$. 
\end{lemma}
\begin{proof}
If $\lambda\in\cspec(T)$, then $R_T(\lambda)$ is densely defined.
\end{proof}

\begin{proposition}[Spectral inclusion property of numerical range] \label{spectralInclusionNumericalRange}
Let $T$ be an operator on a Hilbert space. Then
\begin{enumerate}
\item $\pspec(T)\subseteq \NumRange(T)$;
\item $\rspec(T) \subseteq \NumRange(T)$;
\item $\apspec(T) \subseteq \overline{\NumRange(T)}$.
\end{enumerate}
In particular $\cspec(T) \subseteq \overline{\NumRange(T)}$.
\end{proposition}
\begin{proof}
(1) Let $\lambda \in \pspec(T)$. Then $Tx = \lambda x$ for some unit vector $x$ and so
\[ \inner{x,Tx} = \inner{x,\lambda x} = \lambda \inner{x,x} = \lambda\norm{x}^2 = \lambda, \]
which means that $\lambda \in \NumRange(A)$.

(2) Let $\lambda \in \rspec(T)$. Then there exists a unit vector $x \in \im(\lambda\id - T)^\perp$, so
\[ 0 = \inner{x,(\lambda\id-T)x} = \lambda\inner{x}^2 - \inner{x,Tx} = \lambda - \inner{x,Tx}, \]
which means that $\lambda \in \NumRange(A)$.

(3) Let $\lambda \in \apspec(T)$. Then there exists a Weyl sequence $\seq{e_n}$ for $\lambda$ by \ref{WeylSequence}. Then
\[ \norm{(\lambda\id - T)e_n} = \norm{e_n}\;\norm{(\lambda\id - T)e_n} \geq |\inner{e_n, (\lambda\id - T)e_n}| = |\lambda - \inner{e_n,Te_n}| \to 0. \]
Thus $\lambda\in\overline{\NumRange(T)}$.

Finally we have $\cspec(T)\subseteq\apspec(T)$ by \ref{approximateSpectrum} and so $\cspec(T)\subseteq\overline{\NumRange(T)}$.
\end{proof}
\begin{corollary}
If $T$ is a closed operator on a Hilbert space, then $\spec(T)\subseteq \overline{\NumRange(T)}$.
\end{corollary}

\section{Projectors and minimisation problems}
Every subspace is a convex, non-empty subset.
\begin{theorem}[Hilbert projection theorem]
Let $\mathcal{H}$ be a Hilbert space, $K$ a closed, convex, non-empty subset of $\mathcal{H}$.
\begin{enumerate}
\item There exists a unique element of $K$ of least norm. i.e.\ there exists a unique $k_0\in K$ such that
\[ \norm{k_0} = \inf\setbuilder{\norm{k}}{k\in K}. \]
i.e.\ $\min\setbuilder{\norm{k}}{k\in K}$ exists.
\item For any $h\in\mathcal{H}$ there exists a unique point $k_0$ in $K$ such that
\[ \norm{h-k_0} = \inf\{\norm{h-k}\;|\; k\in K\}. \]
We use this to define the distance $d(h,K) \defeq \norm{h-k_0}$.
\item If $K$ is a (closed) subspace, then $k_0$ is also the unique point in $K$ such that $(h-k_0)\perp K$.
\end{enumerate}
\end{theorem}
The idea for the first part of the proof is to take a sequence $\seq{\norm{k_i}}\to \inf\setbuilder{\norm{k}}{k\in K}$. By the parallelogram law $\seq{k_i}$ is Cauchy and by completeness it has a limit $k_0$.
\begin{proof}
(1) We can find a sequence $\seq{k_i}$ in $K$ such that $\norm{k_i}$ converges to $d = \inf\setbuilder{\norm{k}}{k\in K}$ by \ref{sequenceToSupInf}. By the parallelogram law
\begin{align*}
\norm{k_i-k_j}^2 &= 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4\norm{\frac{1}{2}(k_i+k_j)}^2 \\
&\leq 2\norm{k_i}^2 + 2\norm{k_j}^2 - 4d^2
\end{align*}
the sequence $\seq{k_i}$ is Cauchy. So it converges to some $k_0$ in $K$ because $K$ is a closed subset of a complete space.

To prove uniqueness, take another $k_0'\in K$ such that $\norm{k_0'}=d$. By convexity $\tfrac{1}{2}(k_0 +k_0')\in K$, hence
\[ d\leq \norm{\tfrac{1}{2}(k_0+k_0')}\leq \tfrac{1}{2}(\norm{k_0}+\norm{k_0'}) = d. \]
So $\norm{\tfrac{1}{2}(k_0+k_0')} = d$. The parallelogram law gives
\[ d^2 = \norm{\frac{k_0+k_0'}{2}}^2 = d^2- \norm{\frac{k_0-k_0'}{2}}^2; \]
hence $\norm{k_0 - k_0'}^2 = 0$ and thus $h_0=k_0$.

(2) The element $k_0$ considered in point 1. is the point closest to a particular choice for $h$, namely $h=0$. For other $h$ consider the set $K-h$, which is again closed and convex.

(3) For all $k\in K$ and $a\in \mathbb{F}$, we have
\[ \norm{h-k_0}\leq \norm{h-k_0+ak} \]
and thus, by lemma \ref{orthogonality}, $(h-k_0)\perp k$, meaning $(h-k_0)\perp K$.

For the converse (i.e.\ uniqueness), suppose $f_0\in K$ such that $(h-f_0)\perp K$. Then for all $f\in K$ we have $(h-f_0)\perp (f_0 -f)$ so that
\begin{align*}
\norm{h-f}^2 &= \norm{(h-f_0) + (f_0-f)}^2 \\
&= \norm{h-f_0}^2 + \norm{f_0 - f}^2 \geq \norm{h-f_0}^2.
\end{align*}
So $\norm{h-f_0}=\inf\{\norm{h-k}\;|\; k\in K\} = d(h,K)$ and thus $f_0=k_0$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed vector subspace. Then $\mathcal{H} = K^\perp \oplus K$.
\end{corollary}
\begin{proof}
We need to prove every vector $x\in \mathcal{H}$ has a unique decomposition of the form
\[ x = y+z \qquad y\in K,\; z\in K^\perp. \]

Such a decomposition exists: we can take $y=k_0$ and $z = x-k_0$. We have already proved uniqueness. We can also give another argument for uniqueness: assume another such decomposition $x=y'+z'$. Then $y-y'= z-z'$ where the left side is in $K$ and the right in $K^\perp$. The only element in $K\cap K^\perp$ is $0$, so $y=y'$ and $z=z'$.
\end{proof}
The ability to make such decompositions in general is unique to Hilbert spaces, see theorem \ref{criterionHilbertSpace}.

\subsection{Orthogonal projection and decomposition}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space. Given a subspace $K$ and an element $x \in \mathcal{H}$, we call the unique element $y\in K$ of the decomposition $K\oplus K^\perp$ the \udef{orthogonal projection} of $x$ on $K$. It is denoted $P_K(x)$. This defines a function $P_K:\mathcal{H}\to K$ called the \udef{orthogonal projection} on $K$.
\end{definition}

\begin{proposition}
Let $P$ be the orthogonal projection on a closed subspace $K$. Then
\begin{enumerate}
\item $P$ is a linear operator on $\mathcal{H}$;
\item $\norm{Px}\leq \norm{x}$ for all $x\in\mathcal{H}$;
\item $P^2 = P$;
\item $\ker P = K^\perp$ and $\im P = K$;
\item $\id_\mathcal{H} - P$ is the orthogonal projection of $\mathcal{H}$ onto $K^\perp$.
\end{enumerate}
\end{proposition}
\begin{proof}
These are mostly direct results of the decomposition. In particular 5. follows if we know $K^\perp$ is closed, which it is by proposition \ref{orthogonalComplementClosed}.
\end{proof}
\begin{corollary} \label{HilbertClosedSpaceOrthogonalDecomposition}
Let $\mathcal{H}$ ba a Hilbert space and $K$ a closed subspace, then $\mathcal{H} = K\oplus K^\perp$.
\end{corollary}
\begin{proof}
Let $P$ be the orthogonal projection on $K$. Then by \ref{directSumKernelImageIdempotent}
\[ \mathcal{H} = \im P \oplus \ker P = K\oplus K^\perp. \]
\end{proof}
\begin{corollary} \label{doubleComplementClosure}
Let $\mathcal{H}$ be a Hilbert space.
\begin{enumerate}
\item If $K$ is a subspace, then $(K^\perp)^\perp = \overline{K}$ is the closure of $K$.
\item If $A$ is a subset, then $(A^\perp)^\perp$ is the closed linear span of $A$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Assume $K$ is closed. Then using $0=(I-P_K)x\;\; \Leftrightarrow \;\; x=P_Kx$, we see
\[ (K^\perp)^\perp = \ker(I-P_K) = \im P_K = K. \]
Then, if $K$ is not closed, $(K^\perp)^\perp = (\overline{K}^\perp)^\perp = \overline{K}$, by proposition \ref{orthogonalComplementClosed}.

(2) Using \ref{OrthogonalComplementProperties} we calculate $(A^\perp)^\perp = (\Span(A)^\perp)^\perp = \overline{\Span(A)}$.
\end{proof}
\begin{corollary} \label{denseZeroComplement}
Let $A$ be a subset of a Hilbert space $\mathcal{H}$. Then $\Span(A)$ is dense in $\mathcal{H}$ \textup{if and only if} $A^\perp = \{0\}$.
\end{corollary}
\begin{proof}
The subspace $\Span(A)$ is dense in $\mathcal{H}$ iff $\overline{\Span(A)} = \mathcal{H}$ iff $(\Span(A)^\perp)^\perp = (A^\perp)^\perp = \mathcal{H}$ iff $A^\perp = \{0\}$.

In the last step we have used that $A^\perp$ is closed so that $((A^\perp)^\perp)^\perp = \overline{A^\perp} = A^\perp$, see \ref{orthogonalComplementClosed}.
\end{proof}

\subsubsection{Existence of orthonormal bases}
\begin{corollary}
Let $D$ be an orthonormal subset of a Hilbert space $\mathcal{H}$, then $D$ is an ortonormal basis \textup{if and only if} it is maximal.
\end{corollary}
\begin{proof}
This is a restatement of the previous corollary in the language of \ref{characterisationMaximalOrthonormalSet}.
\end{proof}
\begin{corollary}
Every Hilbert space has an orthonormal basis.
\end{corollary}
\begin{proof}
Every inner product space has a maximal orthonormal set by \ref{exitenceMaximalOrthonormalSet}. This maximal orthonormal set is an orthonormal set by the proposition.
\end{proof}
\begin{corollary} \label{HilbertOnBasisMaximal}
An orthonormal subset of a Hilbert space is an orthonormal basis \textup{if and only if} it is maximal.
\end{corollary}

\begin{lemma}
Let $\mathcal{H}$ be a Hilbert space and $K$ a closed subspace. Let $\{e_i\}_{i\in I}$ be an orthonormal basis of $K$. Then
\[ P_K(x) = \sum_{i\in I} \inner{e_i,x}e_i. \]
\end{lemma}
\begin{proof}
We can extend $\{e_i\}_{i\in I}$ to an orthonormal basis $\{e_i\}_{i\in J}$ of $\mathcal{H}$. Then
\[ x = \sum_{i\in J}\inner{e_i,x}e_i = \sum_{i\in I}\inner{e_i,x}e_i + \sum_{i\notin I}\inner{e_i,x}e_i, \]
which is clearly a decomposition in $K\oplus K^\perp$. This is unique, so we have found $P_K(x)$.
\end{proof}

\subsubsection{When are inner product spaces complete?}
Notice that some of the results obtained for Hilbert spaces have one direction that is generally true for inner product spaces: in any inner product space we have
\begin{itemize}
\item $\overline{K}\subset (K^\perp)^\perp$;
\item $\Span(A)$ dense in $\mathcal{H}$ implies $A^\perp = \{0\}$;
\item if $D$ is an orthonormal basis, then it is maximal.
\end{itemize}
See \ref{orthogonalComplementClosed}, \ref{orthogonalComplementDenseSpace} and \ref{characterisationMaximalOrthonormalSet}.

The converses are only true for Hilbert spaces.
\begin{theorem} \label{criterionHilbertSpace}
Let $H$ be an inner product space. If any of the following hold, $H$ is a Hilbert space:
\begin{enumerate}
\item For any orthonormal set $D$,
\[ \text{$D$ is maximal} \quad\implies\quad \text{$D$ is an orthonormal basis.} \]
\item For any subset $A$, $A^\perp = \{0\}$ implies $\Span(A)$ is dense in $H$.
\item For any subspace $K$, we have $(K^\perp)^\perp = \overline{K}$.
\item For all closed subspaces $K$ we can decompose $H = K\oplus K^\perp$.
\end{enumerate}
\end{theorem}
\begin{proof}
We prove the first statement implies $H$ is a Hilbert space. The other three imply the first and thus that $H$ is a Hilbert space.
\begin{enumerate}
\item We prove the contrapositive: assume $H$ is not complete, we wish to show that 1. does not hold, i.e.\ there exists a maximal orthonormal subset of $H$ that is not an orthonormal basis.

Let $\mathcal{H}$ be the completion of $H$ and take a unit vector $v\in \mathcal{H}\setminus H$. Now working in the completion, we have the decomposition $\Span\{v\}\oplus \Span\{v\}^\perp$. Consider the subspace $\Span\{v\} + H = \Span\{v\}\oplus(H\cap \Span\{v\}^\perp)$. We can extend $\{v\}$ to a maximal orthonormal set $\{v\}\cup D$ by \ref{exitenceMaximalOrthonormalSet}.

We claim $D$ is the orthonormal set we want:

Firstly it is maximal.
Assume, towards a contradiction, that $D$ is not maximal in $H$, so there exists an orthonormal set $D'\supsetneq D$. Take $w\in D'\setminus D$ and let $w'$ be the normalisation of $w - \inner{v,w}v$. Then $w' \perp v$ and $w' \perp D$, so $\{v\}\cup D\cup\{w'\}$ is an orthonormal set in $\Span\{v\} + H$, which contradicts the maximality of $\{v\}\cup D$.

Secondly it cannot be total. Indeed if $\Closure_H(\Span(D)) = H\cap\overline{\Span(D)}$ were equal to $H$, then $H \subseteq \overline{\Span(D)}$ and thus $\mathcal{H} = \overline{H} \subseteq \overline{\Span(D)} \subseteq \mathcal{H}$, meaning $\overline{\Span(D)} = \mathcal{H}$. But $v\notin \overline{\Span(D)}$, so $\overline{\Span(D)} \neq \mathcal{H}$.

\item 2. clearly implies 1. We can also adapt the proof above to show 2. implies $H$ is a Hilbert space:
Assume $H$ is not complete and let $\mathcal{H}$ be the completion of $H$. There exists a $v\in \mathcal{H}\setminus H$. All orthogonal complements are taken in the completion.
The set
\[ U \defeq H\cap\{v\}^\perp \]
is not dense in $\mathcal{H}$ for the same reason $D$ was not total above. We claim that the orthogonal complement of $U$ in $H$ is $\{0\}$:
\[ U^\perp\cap H = \{0\}. \]
First we claim $U$ is dense in $\{v\}^\perp$: take a $w\in \{v\}^\perp$ and let $(x_n)_{n\in\N}\subseteq H$ converge to $w$ (this is possible because $w\in\mathcal{H}$ and $H$ is dense in $\mathcal{H}$). Fix some $x\in H$ such that $\inner{x,v}\neq 0$, then we have the following sequence in $U$ that converges to $w$:
\[ n\mapsto x_n - \inner{x_n,v}\frac{x}{\inner{x,v}}. \]
Then because $U$ is dense in $\{v\}^\perp$,
\[ U^\perp\cap H = \overline{U}^\perp\cap H = (\{v\}^\perp)^\perp \cap H = \Span\{v\}\cap H = \{0\}. \]
\item Assume 3. Let $D$ be a maximal orthonormal set. Then
\[ \overline{\Span(D)} = (\Span(D)^\perp)^\perp = (D^\perp)^\perp = \{0\}^\perp = H, \]
so $D$ is an orthonormal basis.
\item Assume 4. Let $D$ be a maximal orthonormal set. Then $D^\perp$ is a closed subspace, so
\[ H  = D^\perp \oplus (D^\perp)^\perp = \{0\} \oplus (D^\perp)^\perp = (\Span(D)^\perp)^\perp = \overline{\Span(D)}. \]
\end{enumerate}
\end{proof}

\subsubsection{Orthogonal decomposition}
\begin{theorem}
 A Banach space such all of its closed subspaces are complemented is isomorphic to a Hilbert space.
\end{theorem}
\begin{proof}
TODO Lindestrauss and Tzafriri in 1971. Only real??
\end{proof}

\begin{proposition} \label{directSumOrthogonalClosed}
Let $\mathcal{H}$ be a Hilbert space and let $\{V_i\}_{i\in I}$ be a family of closed, (pairwise) orthogonal subspaces. Then
\[ \bigoplus_{i\in I}V_i \qquad \text{is a closed subspace of $\mathcal{H}$.} \]
\end{proposition}
\begin{proof}
Let $(v_n)$ be a Cauchy sequence in $\bigoplus_{i\in I}V_i$ which converges to $w$. Let $v_{i,n}$ be the component of $v_n$ in $V_i$. By orthogonality we have
\[ \norm{v_n-v_m}^2 = \sum_{i\in I}\norm{v_{i,n}-v_{i,m}}^2. \]
Then
\[ \norm{v_{i,n}-v_{i,m}} \leq \norm{v_n-v_m} \]
which implies $(v_{i,n})_n$ is a Cauchy sequence in the closed space $V_i$ which therefore converges to $w_i\in V_i$. Now there are only a finite number of $i$ for which there exist non-zero $v_{i,n}$ (TODO proof!!!!). So then
\[ \lim_n v_n = \lim_n \sum_{i\in I}v_{i,n} = \sum_{i\in I}w_i \in \bigoplus_{i\in I}V_i \]
where the interchange of limits and last equality follow because the sums are finite.
\end{proof}

\begin{lemma} \label{cancellationOminus}
Let $\mathcal{H}$ be a Hilbert space and $A\supseteq B \supseteq C$ subspaces with $B$ closed. Then
\[ (A\ominus B)\oplus (B\ominus C) = A\ominus C.\]
\end{lemma}
\begin{proof}
Take $v\in(A\ominus B)\oplus (B\ominus C)$. Then either $\{v\}\perp C$ or $\{v\}\perp B$, but this implies $\{v\}\perp C$, so $v\in A\ominus C$.

Take $v\in A\ominus C$. We can uniquely write $v = v_1 + v_2 \in (A\ominus B)\oplus B = A$. We just need to show that $v_2\in B\ominus C$. Indeed assume $\inner{c,v_2}\neq 0$ for some $c\in C$. Then
\[ \inner{c, v} = \inner{c, v_1+v_2} = \inner{c,v_1}+\inner{c,v_2} = \inner{c,v_2} \neq 0, \]
so $v\notin A\ominus C$, a contradiction.
\end{proof}

\subsection{Projection and minimisation in finite-dimensional spaces}

\begin{lemma}
Let $K$ be a subspace of $\F^n$ spanned by the orthonormal basis $\{\vec{u}_i\}_{i=1}^k$. Then
\[ P_K = QQ^* \qquad\text{where}\qquad Q = \begin{bmatrix}
\vec{u}_1 & \vec{u}_2 & \hdots & \vec{u}_k
\end{bmatrix}. \]
\end{lemma}
\begin{proof}
$P_K(\vec{x}) = \sum_{i=1}^k\vec{u}_i\inner{\vec{u}_i,\vec{x}} = \sum_{i=1}^k\vec{u}_i \vec{u}_i^*\vec{x} = \left(\sum_{i=1}^k\vec{u}_i \vec{u}_i^*\right)\vec{x} = QQ^*\vec{x}$.
\end{proof}
\begin{corollary}
For any matrix $A$ with QR factorisation $A=QR$, we have
\[ P_{\Col(A)} = QQ^*. \]
\end{corollary}
In general $P_{\Col(A)} = A(A^*A)^{-1}A^*$.

\begin{proposition}[Normal equations]
Let $\{\vec{v}_i\}_{i=1}^k$ be linearly independent set of vectors in $\F^n$. Set $K = \Span\{\vec{v}_i\}_{i=1}^k$. Then for all $\vec{x}\in \F^n$
\[ P_K(\vec{x}) = \sum_{i=1}^k c_i \vec{v}_i, \]
where $\begin{bmatrix}
c_1 & c_2 & \hdots & c_k
\end{bmatrix}^\transp$ is the solution of
\[ \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix}\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{x}} \\ \inner{\vec{v}_2,\vec{x}} \\ \vdots \\ \inner{\vec{v}_k,\vec{x}}
\end{bmatrix}. \]
This system of linear equations is consistent, yielding a unique solution.
\end{proposition}
The equations in this proposition are known as \udef{normal equations} and the matrix
\[ G(\vec{v}_1, \ldots, \vec{v}_k) \defeq \begin{bmatrix}
\vec{v}_1^* \\ \vec{v}_2^* \\ \vdots \\ \vec{v}_k^*
\end{bmatrix}\begin{bmatrix}
\vec{v}_1 & \vec{v}_2 & \hdots & \vec{v}_k
\end{bmatrix} = \begin{bmatrix}
\inner{\vec{v}_1,\vec{v}_1} & \inner{\vec{v}_1,\vec{v}_2} & \hdots & \inner{\vec{v}_1,\vec{v}_k} \\
\inner{\vec{v}_2,\vec{v}_1} & \inner{\vec{v}_2,\vec{v}_2} & \hdots & \inner{\vec{v}_2,\vec{v}_k} \\
\vdots & \vdots & \ddots & \vdots \\
\inner{\vec{v}_k,\vec{v}_1} & \inner{\vec{v}_k,\vec{v}_2} & \hdots & \inner{\vec{v}_k,\vec{v}_k} \\
\end{bmatrix} \]
is known as the \udef{Gram matrix} or \udef{Grammian}.
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A\in\F^{m\times n}$, $\vec{b}\in\F^m$ and $\vec{x}_0\in\F^n$. Then
\[ \min_{\vec{x}\in\F^n}\norm{\vec{b}-A \vec{x}} = \norm{\vec{b} - A \vec{x}_0} \]
if and only if
\[ A^*A \vec{x}_0 = A^* \vec{b}. \]
\end{proposition}
We regard $\vec{x}_0$ as the ``best approximate solution'' to the (not necessarily consistent) system $A \vec{x} = \vec{b}$.
\begin{proof}
TODO
\end{proof}

\subsection{Riesz representation}
\begin{theorem}[Riesz-Frchet representation theorem] \label{rieszRepresentation}
Let $\mathcal{H}$ be a Hilbert space. For every continuous linear functional $\omega\in \mathcal{H}'$, there exists a unique $v_\omega\in\mathcal{H}$ such that
\[ \omega(x) = \inner{v_\omega, x} \qquad \forall x\in\mathcal{H}. \]
Moreover, $\norm{v_\omega}_\mathcal{H} = \norm{\omega}_{\mathcal{H}'}$.
\end{theorem}  

The idea of the proof is as follows: consider $\mathcal{H} \cong \ker\omega \oplus \im\omega$. So we can find a subspace $U\subseteq \mathcal{H}$ such that $\mathcal{H} = \ker\omega\oplus U$. Clearly $\dim U = \dim\im\omega = \dim\F = 1$. Between $1$-dimensional spaces there can only be one linear map, up to rescaling. This map is given by $x\mapsto \inner{v,x}$ for some $v\in U$, where the scaling determines the $v$. So we choose $v$ such that $\omega|_U = x\mapsto \inner{v,x}$.

Now we want extend this form of $\omega|_U$ to the whole of $\mathcal{H}$. This works exactly if $v\in(\ker\omega)^\perp$. So we need $U=(\ker\omega)^\perp$ which is true if and only if $\mathcal{H} = \ker\omega\oplus U = \ker\omega\oplus (\ker\omega)^\perp$, which only works in general if $\ker\omega$ is closed and $\mathcal{H}$ is a Hilbert space. Now $\ker\omega$ is closed if and only if it is continuous, by \ref{continuousMapCriterion}.

With this idea we give a full proof:
\begin{proof}
If $\ker\omega = \mathcal{H}$, we can take $v_\omega = 0$.

Assume $\ker\omega\neq \mathcal{H}$, then $(\ker\omega)^\perp\neq \{0\}$ by \ref{denseZeroComplement}, because $\ker\omega$ is closed (\ref{continuousMapCriterion}). So we can take a non-zero $u\in (\ker\omega)^\perp$. We can choose it such that $\omega(u) = 1$, by rescaling. Now let $h\in\mathcal{H}$. We can write $h = (h - \omega(h)u)+\omega(h)u\in\ker\omega\oplus (\ker\omega)^\perp$, because $\omega(h - \omega(h)u) = 0$. So
\[ 0 = \inner{u,h - \omega(h)u} = \inner{u,h} - \omega(u)\norm{u}^2. \]
If $v_\omega = \norm{u}^{-2}u$, then $\omega(h) = \inner{v_\omega, h}$ for all $h\in\mathcal{H}$.

For uniqueness: assume we can find two vectors $v_\omega,v_\omega'$ such that for all $h\in\mathcal{H}$ we have $\omega(h) = \inner{v_\omega, h} = \inner{v_\omega', h}$. Then $v_\omega - v_\omega'\perp \mathcal{H}$, so $v_\omega - v_\omega'= 0$.
\end{proof}
Together with lemma \ref{innerBoundedFunctionals} this gives:
\begin{corollary} \label{RieszIsometry}
The map $C_\mathcal{H}:\mathcal{H}\to \tdual{\mathcal{H}}: v\mapsto \inner{v,\cdot}$ is a bijective anti-linear isometry.
\end{corollary}
\begin{corollary}
Every Hilbert space is reflexive.
\end{corollary}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Every bounded functional defined on a closed subspace of $\mathcal{H}$ can be extended to a functional on $\mathcal{H}$ with the same norm.
\end{corollary}
\begin{proof}
The functional on the closed subspace, say $K$, can be represented as $x\mapsto \inner{v,x}_K$ for some $v\in K$. The extended functional is then simply given by $x\mapsto \inner{v,x}_\mathcal{H}$.
\end{proof}

\begin{proposition}[Representation of sesquilinear forms] \label{sesquilinearRepresentation}
Let $\mathcal{H}_1,\mathcal{H}_2$ be Hilbert spaces over $\mathbb{F}$ and $h:\mathcal{H}_1,\mathcal{H}_2\to\mathbb{F}$ a bounded sesquilinear form. Then there exists a unique bounded operator $S:\mathcal{H}_1 \to \mathcal{H}_2$ such that
\[ h(x,y) = \inner{Sx,y}. \]
This operator has the property $\norm{S} = \norm{h}$.
\end{proposition}
\begin{proof}
For fixed $x$, $y\mapsto h(x,y)$ is a bounded linear functional, so by the Riesz representation theorem \ref{rieszRepresentation} this can be represented by a unique $v_x$. Let $S$ be the function $x\mapsto v_x$. Then $h(x,y) = \inner{Sx,y}$.

To prove this function $S$ is linear, take arbitrary $x_1,x_2\in \mathcal{H}_1;y\in \mathcal{H}_2$ and $\lambda \in \mathbb{F}$. Then
\begin{align*}
\inner{S(\lambda x_1+ x_2),y} &= h(\lambda x_1+ x_2, y) = \overline{\lambda} h(x_1,y)+h(v,y_2) \\
&= \overline{\lambda} \inner{Sx_1, y} + \inner{Sx_2, y} = \inner{\lambda Sx_1 + Sx_2,y},
\end{align*}
so $S$ is linear by lemma \ref{elementaryOrthogonality}.

The equality of norms follows from
\begin{align*}
\norm{h} = \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{|\inner{Sx,y}|}{\norm{x}\norm{y}} &\geq \sup_{\substack{x\neq 0 \\ Sx\neq 0}}\frac{|\inner{Sx,Sx}|}{\norm{x}\norm{Sx}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S} \\
&\leq \sup_{\substack{x\neq 0 \\ y\neq 0}}\frac{\norm{Sx}\norm{y}}{\norm{x}\norm{y}} = \sup_{x\neq 0}\frac{\norm{Sx}}{\norm{x}} = \norm{S}
\end{align*}
where the second inequality is Cauchy-Schwarz.
\end{proof}

\section{Orthonormal bases}

Hamel basis / Schauder basis / Hilbert basis

Every Hilbert basis is Schauder basis if $V$ is separable.

Hamel basis too big in Banach space??

Necessity of completeness for existence of complete orthonormal system, i.e.\ orthonormal system $\{a_i\}_{i\in I}$ (so $a_i \cdot a_j = \delta_{ij}$) with
\[ v = \sum_{i\in I}(a_i \cdot v)a_i \]
for all $v$. This is equivalent with
\[ v \cdot w = \sum_{i\in I}(v\cdot a_i)(a_i \cdot w) \]
for all $v,w$.


\begin{theorem}[Riesz-Fischer]
Let $\{e_i\}_{i\in I}$ be an orthonormal basis of a Hilbert space $H$ and $\alpha: I\to \C$ a net. Then
\[ \sum_{i\in I}\alpha_i e_i \]
converges \textup{if and only if} $\sum_{i\in I}|\alpha_i|^2 < \infty$. 
\end{theorem}
\begin{proof}
If $\sum_{i\in I}\alpha_i e_i$ converges, then $\sum_{i\in I}|\alpha_i|^2$ is bounded by the Bessel inequality \ref{BesselInequality}.

By monotone convergence, $\sum_{i\in I}|\alpha_i|^2 < \infty$ is equivalent to saying the sum converges. By (ref TODO) $\alpha$ has finite support. So $\sum_{i\in I}\alpha_i e_i$ can be expressed as the series
\[ \sum_{k\in \N}\alpha_{i_k} e_{i_k}. \]
By completeness it is enough to show that $\seq{s_n} = \seq{\sum_{k=0}^n\alpha_{i_k} e_{i_k}}$ is Cauchy. Let $n < m$, then
\[ \norm{s_n - s_m}^2 = \norm{\sum_{k=m+1}^n\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n\norm{\alpha_{i_k} e_{i_k}}^2 = \sum_{k=m+1}^n |\alpha_{i_k}|^2 = \sum_{k=0}^n|\alpha_{i_k}|^2 -\sum_{k=0}^m|\alpha_{i_k}|^2.  \]
Since $\seq{\sum_{k=0}^n |\alpha_{i_k}|^2}$ is convergent, it is Cauchy and thus so is $\seq{s_n}$.
\end{proof}
\begin{corollary}
Let $\mathcal{H}$ be a Hilbert space and $D$ be an orthonormal basis of $\mathcal{H}$. Then $\mathcal{H}$ is isometrically isomorphic to $\ell^2(D)$.
\end{corollary}
\begin{corollary}
Hilbert spaces whose orthonormal bases have the same cardinality are isometrically isomorphic.
\end{corollary}

??
\begin{lemma}
Let $(\Omega,\mathcal{A}, \mu)$ be a measure space. Then $L^2(\Omega, \mu)$ is separable \textup{if and only if} $\mu$ is $\sigma$-finite.
\end{lemma}

\begin{lemma}
Let $\{\phi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Omega, \mu)$ and $\{\psi_n(x)\}^\infty_{n=0}$ be an orthonormal basis of $L^2(\Lambda, \nu)$, then $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis of $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{lemma}
\begin{proof}
The set $\{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is orthonormal:
\[ \iint_{\Omega\times\Lambda} \phi_n(x)\psi_m(y)\overline{\phi_{n'}(x)\psi_{m'}(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega\phi_n(x)\overline{\phi_{n'}(x)}\diff{\mu(x)} \cdot \int_\Lambda\psi_m(y)\overline{\psi_{m'}(y)}\diff{\nu(y)} = \delta_{n,n'}\delta_{m,m'}, \]
using Fubini's theorem and the Hlder inequality (TODO refs).

To show $D = \{\phi_n(x)\psi_m(y)\}^\infty_{n,m=0}$ is an orthonormal basis, we verify point 5. of \ref{totalONBParsevalEquivalence}: if $f\perp D$, then $f = 0$.

If $f\perp D$, then for all $m,n\in \N$
\[ 0 = \inner{f, \phi_n\psi_m} = \iint_{\Omega\times\Lambda}f(x,y)\overline{\phi_n(x)\psi_m(y)}\diff{\mu(x)}\diff{\nu(y)} = \int_\Omega \left(\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \right)\overline{\phi_n(x)}\diff{\mu(x)}.  \]
Using point 5. of \ref{totalONBParsevalEquivalence} in $L^2(\Omega,\mu)$, we see that for all $m$ the function $x\mapsto\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)}$ is $0$ as an element of $L^2(\Omega, \mu)$, i.e.\ it is $0$ a.e. as a function of $x$. Let
\[ E_m = \setbuilder{x\in\Omega}{ \int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} \neq 0} \]
and set $E = \bigcup_{m\in\N}E_m$.
Then
\[ \mu(E) =  \mu\left(\bigcup_{m\in \N}E_m\right) \leq \sum_{m\in\N}\mu(E_m) = 0. \]

For $x\notin E$, we have $\int_\Lambda f(x,y)\overline{\psi_m(y)}\diff{\nu(y)} = 0$, so by the same logic $f(x,y) = 0$ for almost all $y$. 

Now $|f|^2$ is integrable and
\[ \iint_{\Omega\times \Lambda}|f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = \int_{\Omega\setminus E}\int_\Lambda |f(x,y)|^2\diff{\mu(x)}\diff{\nu(y)} = 0, \]
so $f=0$ in $L^2(\Omega\times\Lambda, \mu\times\nu)$.
\end{proof}


\section{Adjoints of operators}
\begin{definition}
Let $H,K$ be Hilbert spaces and $T: H\not\to K$ an operator. An \udef{adjoint} of $T$ is an operator $S: K\not\to H$ such that
\[ \inner{w,Tv}_K = \inner{S w,v}_H \quad \forall v\in \dom(T),\; \forall w\in \dom(S). \]
\end{definition}

\begin{theorem}[Hellinger-Toeplitz]
Let $T: H\to K$ be an operator between Hilbert spaces (which is defined everywhere), then $T$ has an adjoint that is defined everywhere \textup{if and only if} it is bounded.
\end{theorem}
\begin{proof}
The ``if'' will be shown below by explicit construction. For the ``only if'', take such an operator $T$.

First we show $T$ has closed graph, by using proposition \ref{closedGraphEquivalence}: assume $(x_n)$ converges to $x$ and $(Tx_n)$ converges to $y$. Then
\[ \inner{z, Tx} = \inner{Sz,x} = \lim_n\inner{Sz, x_n} = \lim_n\inner{z, Tx_n} = \inner{z, y} \]
where we have used the boundedness of $x\mapsto\inner{z,x}$. By the non-degeneracy of the inner product, $Tx = y$. So the graph of $T$ is closed. Similarly the graph of $S$ is closed. Applying the closed graph theorem \ref{closedGraphTheorem}, yields the boundedness of $T$ and $S$.
\end{proof}
\begin{corollary}
Everywhere-defined symmetric operators are bounded.
\end{corollary}

\begin{example}
The adjoint of the left-shift operator
\[ S_L: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto (x_{n+1})_{n\in\N} \]
is the right-shift operator
\[ S_R: \ell^2(\N) \to \ell^2(\N): (x_n)_{n\in\N} \mapsto \left(\begin{cases}
x_{n-1} & (n\geq 1) \\ 0 &(n=0)
\end{cases}\right)_{n\in \N}. \]
\end{example}

\subsection{The adjoint as a relation}
\begin{lemma}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) - S_2(x) \in \dom(T)^\perp$.

Conversely, let $S$ be an adjoint of $T$ and $x\in\dom(S)$. Then for all $v\in \dom(T)^\perp$ there exists an adjoint $S'$ such that $S'(x) = S(x) + v$.
\end{lemma}
\begin{proof}
For all $u\in \dom(T)$ we have
\[ \inner{S_1(x) - S_2(x), u}_H = \inner{S_1(x), u}_H - \inner{S_2(x), u}_H = \inner{x, Tu}_K - \inner{x, Tu}_K = 0. \]
So $\{S_1(x) - S_2(x)\} \in \dom(T)^\perp$.

For the converse, set $S' = S + \frac{\inner{x,\cdot}_K}{\inner{x,x}_K}v$. This is an adjoint: for all $a\in \dom(T), b\in \dom(S') = \dom(S)$ we have
\[  \inner{S' b,a}_H = \inner{Sb, a}_H + \frac{\inner{x,b}_K}{\inner{x,x}_K}\inner{v,a}_H = \inner{Sb, a}_H = \inner{b,Ta}_K. \]
\end{proof}
\begin{corollary} \label{agreementAdjoints}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Let $S_1, S_2$ be adjoints of $T$ then for all $x\in \dom(S_1)\cap\dom(S_2)$ we have $S_1(x) = S_2(x)$.
\end{corollary}
\begin{proof}
We have $\dom(T)^\perp = \overline{\dom(T)}^\perp = H^\perp = \{0\}$. So $S_1(x) - S_2(x) = 0$.
\end{proof}
\begin{corollary} \label{maximalAdjointIsOperator}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}} \]
is the graph of an operator \textup{if and only if} $T$ is densely defined.
\end{corollary}

\begin{definition}
Let $T: H\not\to K$ be an operator between Hilbert spaces. We define the adjoint $T^*$ as the \emph{relation} on $(H,K)$ with graph
\[ \graph(T^*) \defeq \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}}. \]
\begin{itemize}
\item If $T^* = T$, we say $T$ is \udef{self-adjoint}.
\item If $T^* = -T$, we say $T$ is \udef{skew-adjoint}.
\end{itemize}
We denote the set of self-adjoint operators on $H$ by $\SelfAdjoints(H)$.
\end{definition}
Note that, by \ref{maximalAdjointIsOperator}, the adjoint is a function if and only if $T$ is densely defined.

\begin{lemma} \label{everywhereDefinedAdjointLemma}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. If $S$ is an adjoint of $T$ that is defined everywhere, then $T^* = S$.
\end{lemma}
\begin{corollary}
Let $H$ be a Hilbert space. Then $\id_H^* = \id_H$.
\end{corollary}

\begin{proposition} \label{adjointDomain}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\[ \dom(T^*) = \setbuilder{x\in K}{\text{$\dom(T)\to \F: u\mapsto \inner{x, Tu}$ is a bounded functional}}. \]
\end{proposition}
\begin{proof}
$\boxed{\subseteq}$ If $\omega_x: u\mapsto \inner{x, Tu}$ is bounded, then its domain can be extended by linearity to all of $H$ and it has a Riesz vector $x^*$ such that $\omega_x = u\mapsto \inner{x^*, u}$. The linear operator with domain $\Span\{x\}$ that maps $x$ to $x^*$ is then an adjoint.

$\boxed{\supseteq}$ If $x\in\dom(T^*)$, then, using the Cauchy-Schwarz inequality,
\[ |\inner{x,Tu}| = |\inner{T^*x,u}| \leq \norm{T^*x}\;\norm{u}, \]
so the functional $u\mapsto \inner{x, Tu}$ is bounded.
\end{proof}
\begin{corollary}
The domain $\dom(T^*)$ is a vector space and in particular contains $0$.
\end{corollary}
\begin{corollary} \label{HilbertAdjointAntitone}
Let $S,T: H\not\to K$ be operators between Hilbert spaces such that $S\subseteq T$. Then $T^* \subseteq S^*$.
\end{corollary}
\begin{corollary}
Let $H, K$ be Hilbert spaces. Then $(*,*)$ is a Galois connection between $\sSet{(H\not\to K), \subseteq}$ and $\sSet{(K\not\to H), \subseteq}$.
\end{corollary}
\begin{proof}
TODO
\end{proof}

\begin{proposition}[Algebraic properties of the adjoint] \label{adjointAlgebraicProperties}
Let $T,S$ be compatible operators between Hilbert spaces and $\lambda\in\C$. Then
\begin{enumerate}
\item $S^* + T^* \subseteq (S+T)^*$;
\item $S^*T^* \subseteq (TS)^*$;
\item $\begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*) \subseteq (\lambda T)^*$;
\item if $\lambda \neq 0$, then $\begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*) = (\lambda T)^*$;
\item $(T+\lambda\id)^* = T^*+\overline{\lambda}\id$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Let $f$ be an adjoint of $S$ and $g$ an adjoint of $T$. It is enough to see that $f+g$ is an adjoint of $S+T$. Indeed $\forall w\in \dom(f + g), v\in \dom(S+T)$
\[ \inner{(f + g)(w), v} = \inner{f(w),v} + \inner{g(w), Tv} = \inner{w,Sv} + \inner{w,Tv} = \inner{w,(S+T)v}. \]

(2) Let $f$ be an adjoint of $T$ and $g$ an adjoint of $S$. It is enough to see that $gf$ is an adjoint of $TS$. Indeed
\[ \inner{g\circ f(w), v} = \inner{f(w), Sv} = \inner{w,TSv} \qquad \forall w\in \dom(g\circ f), v\in \dom(TS). \] 

(3) Let $f$ be an adjoint of $T$. It is enough to see that $\overline{\lambda}f$ is an adjoint of $\lambda T$. Indeed
\[ \inner{\overline{\lambda}f(w), v} = \lambda\inner{f(w), v} = \lambda\inner{w,Tv} = \inner{w,\lambda Tv} \qquad \forall w\in \dom(f), v\in \dom(T). \]

(4) One inclusion has already been proved. For the other inclusion, let $f$ be an adjoint of $\lambda T$. It is enough to see that $\overline{\lambda^{-1}}f$ is an adjoint of $T$, because then $f = \overline{\lambda}\cdot\overline{\lambda^{-1}}f \subseteq \begin{pmatrix}
\id & 0 \\ 0 & \overline{\lambda}\id
\end{pmatrix} \graph(T^*)$. Indeed
\[ \inner{\overline{\lambda^{-1}}f(w), v} = \lambda^{-1}\inner{f(w),v} = \inner{w,\lambda^{-1}\lambda Tv} = \inner{w,Tv} \quad \forall w\in \dom(f), v\in \dom(T). \]

(5) From (1) and (3), we have $T^*+\overline{\lambda}\id \subseteq (T+\lambda\id)^*$. Conversely, let $f$ be an adjoint of $(T+\lambda\id)$. It is enough to see that $f-\overline{\lambda}\id$ is an adjoint of $T$. Indeed, $\forall w\in\dom(f-\overline{\lambda}\id), v\in \dom(T)$,
\begin{align*}
\inner{(f-\overline{\lambda}\id)(w),v} &= \inner{f(w),v}-\lambda \inner{w,v} \\
&= \inner{w,(T+\lambda\id)(v)}-\lambda \inner{w,v} \\
&= \inner{w,Tv} - \lambda\inner{w,v} + \lambda\inner{w,v} = \inner{w,Tv}.
\end{align*}
\end{proof}

\begin{proposition} \label{adjointGraph}
Let $T: H\not\to K$ be an operator between Hilbert spaces. Then
\begin{align*}
\graph(T^*) &= \left( \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T) \right)^\perp 
=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp.
\end{align*}
If $T$ is densely defined, then $T^*$ is a closed operator.
\end{proposition}
\begin{proof}
We have
\[ \graph(T^*) = \bigcup\setbuilder{\graph(S)}{\text{$S\in (K\not\to H)$ is an adjoint of $T$}}. \]
Take an adjoint $S$ and $(w, Sw)$ in $\graph(S)$. Then for all $v\in\dom(T)$:
\[ 0 = \inner{w, Tv}_K - \inner{Sw, v}_H = \inner{w, Tv}_K + \inner{Sw, -v}_H = \inner{(w, Sw), (Tv,-v)}_{K\oplus H}. \]
So $(Tv,-v) = \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix} (v,Tv) \in \graph(S)^\perp $.

The final equality follows from \ref{perpUnderIsometry}, using the fact that $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ is a surjective isometry.

If $T$ is densely defined, then $T^*$ is a function by \ref{maximalAdjointIsOperator}. It is closed by \ref{orthogonalComplementClosed}.
\end{proof}
\begin{corollary} \label{adjointDenselyDefinedClosable}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then $T^*$ is densely defined \textup{if and only if} $T$ is closable. In this case $\overline{T} = T^{**}$.
\end{corollary}
\begin{proof}
From the proposition we have
\begin{align*}
\graph(T^{**}) &=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T^*)^\perp 
=  \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\left(\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp\right)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}^2\graph(T)^{\perp\perp} = -\graph(T)^{\perp\perp}
= \overline{\graph(T)} = \graph(\overline{T}).
\end{align*}
The right-hand side is the graph of an operator iff $T$ is closable and the left-hand side is the graph of an operator iff $T^*$ is densely defined, by \ref{maximalAdjointIsOperator}.

By definition, the left-hand side is equal to $\graph(T^{**})$, so $T^{**} = \overline{T}$.
\end{proof}

\begin{proposition} \label{kernelImageAdjoint}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item $\ker(T^*) = \im(T)^\perp$;
\item $\ker(T) = \im(T^*)^\perp$.
\end{enumerate}
\end{proposition}
\begin{proof}
First take $v\in \ker(T^*)$, then $T^*(v) = 0$ which implies
\[ \forall x \in\dom(T): \inner{T^*(v), x} = 0 \;\implies\; \forall x \in\dom(T): \inner{v, T(x)} = 0 \;\implies\; v\perp \im(T).  \]
Next take $v\perp \im(T)$ TODO complete.
\end{proof}
TODO: link with previous? + Drop densely defined.

\begin{proposition} \label{adjointRangeCriterion}
Let $S: K\not\to H$ and $T: H\not\to K$ be linear operators between Hilbert spaces. If
\[ \im(S\cap T^*) = H \qquad\text{and}\qquad \im(T\cap S^*) = K, \]
then $S$ and $T$ are densely defined with $S^* = T$ and $T^* = S$.
\end{proposition}
\begin{proof}
Notice that $S\cap T^*$ and $T\cap S^*$ are linear operators that are adjoints of each other.

We claim that they are densely defined: take $x\in \dom(S\cap T^*)^\perp$. Then there exists some $y\in H$ such that $x = (T\cap S^*)y$ because of surjectivity. Now for all $z\in \dom(S\cap T^*)$
\[ 0 = \inner{z,x} = \inner{z, (T\cap S^*)y} = \inner{(S\cap T^*)z, y}, \]
so $\inner{z',y} = 0$ for all $z'\in H$, by surjectivity. This means, by \ref{elementaryOrthogonality}, that $y=0$ and thus also $x = (T\cap S^*)y = 0$. We conclude that $\dom(S\cap T^*)^\perp = \{0\}$, meaning $(S\cap T^*)$ is densely defined. The argument for $(T\cap S^*)$ is similar.

It follows that $S$ and $T$ must be densely defined. We have, by \ref{kernelImageAdjoint},
\[ \ker(S) = \im(S^*)^\perp \subseteq \im(T\cap S^*)^\perp = \{0\}. \]
Similarly $\ker(T) = \ker(S^*) = \ker(T^*) = \{0\}$.

So we have $\ker(S) = \ker(T^*)$, $\im(S)\subseteq \im(S\cap T^*)$ and $\im(T^*)\subseteq \im(S\cap T^*)$. The equality $S = T^*$ follows from \ref{partialFunctionSubset}. The equality $T = S^*$ is similar.
\end{proof}

\begin{proposition}
Let $T: H\not\to K$ be a densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item $\im(T)$ is dense in $K$ \textup{if and only if} $T^*$ is injective;
\item if $T$ and $T^*$ are injective, then $(T^*)^{-1} = (T^{-1})^*$;
\item if $T$ is closable and $\overline{T}$ is injective, then $\overline{T}^{\,-1} = \overline{T^{-1}}$.
\end{enumerate}
\begin{proof}
(1) This is immediate from \ref{kernelImageAdjoint} and \ref{injectivityKernelTriviality}:
\[ \text{$\im(T)$ is dense} \quad\iff\quad \{0\} = \im(T)^\perp = \ker(T^*). \]

(2) We have $\graph(T^{-1}) = \begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)$. Also note that $\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}$ and $\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}$ commute. Then we compute using \ref{adjointGraph}:
\begin{align*}
\graph((T^*)^{-1}) &= \begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\graph(T)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)^\perp \\
&= \begin{pmatrix}
0 & -\id \\ \id & 0
\end{pmatrix}\left(\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}\graph(T)\right)^\perp = \graph((T^{-1})^*).
\end{align*}
The penultimate equality follows from \ref{perpUnderIsometry}, using the fact that $\begin{pmatrix}
0 & \id \\ \id & 0
\end{pmatrix}$ is a surjective isometry.
\end{proof}

\end{proposition}

\url{https://arxiv.org/pdf/1507.08418.pdf}
\url{https://link.springer.com/article/10.1007/s43036-020-00068-4}

\subsection{Bounded operators}
\begin{proposition}
Let $T: H\to K$ be a closed densely defined operator between Hilbert spaces. Then
\begin{enumerate}
\item $T\in\Bounded(H,K)$ \textup{if and only if} $T^*\in\Bounded(K,H)$.
\end{enumerate}
In this case
\begin{enumerate} \setcounter{enumi}{1}
\item $\norm{T} = \norm{T^*}$;
\item $T^* = C_H^{-1}T^tC_K$, where $C_K$ is the Riesz isometry from \ref{RieszIsometry}.
\end{enumerate}
\end{proposition}
\begin{proof}
First assume $T\in\Bounded(H,K)$. Then $u\mapsto \inner{x,Tu}$ is a bounded functional for all $x\in K$, so $\dom(T^*) = K$ by \ref{adjointDomain}. Also $T^*$ is closed by \ref{adjointGraph}, so it is bounded by the closed graph theorem \ref{closedGraphTheorem}.

Now assume $T^*\in\Bounded(K,H)$. By the previous argument $T = \overline{T} = T^{**}\in\Bounded(H,K)$.

The function $(x,u)\mapsto \inner{x,Tu}$ is a bounded sesquilinear form. By proposition \ref{sesquilinearRepresentation}, $T^*$ must be the unique $S$ from the proposition, which has norm $\norm{T}$.

Finally we note that $C_H^{-1}T^tC_K$ is an adjoint with domain $K$ and conclude by \ref{everywhereDefinedAdjointLemma}.
\end{proof}

\begin{lemma}
The adjoint defines a map $*:\Bounded(H,K)\to \Bounded(K,H)$ that is anti-linear and continuous in the weak and uniform operator topologies. It is continuous in the strong operator topology \textup{if and only if} finite dimensional.
\end{lemma}
\begin{proof}
By the proposition the adjoint map is anti-linear. It is also bounded with norm $1$. Then by corollary \ref{boundedAntiLinearMaps} it must be bounded.

TODO
\end{proof}

\begin{lemma} \label{HilbertAdjointLemma}
Let $S,T\in\Bounded(H,K)$ and $\lambda \in \mathbb{F}$.
\begin{enumerate}
\item $(T^*)^* = T$;
\item $(S+T)^* = S^* + T^*$;
\item $(\lambda T)^* = \bar{\lambda}T^*$;
\item $\id_V^* = \id_V$.
\end{enumerate}
Let $T\in\Bounded(H_1,H_2), S\in\Bounded(H_2,H_3)$
\begin{enumerate}
\setcounter{enumi}{4}
\item $(ST)^* = T^*S^*$.
\end{enumerate}
\end{lemma}
\begin{proof}
These follow straight from \ref{adjointAlgebraicProperties} and the fact that the operators and adjoints are everywhere defined.
\end{proof}

\begin{note}
Useful exercise: The identities of \ref{HilbertAdjointLemma} can also be proven by elementary manipulations. For example, to prove 1., we take arbitrary $v\in H$ and $w\in K$, Then
\[ \inner{w,Tv} = \inner{T^*w,v} = \overline{\inner{v,T^*w}} = \overline{\inner{(T^*)^*v,w}} = \inner{w, (T^*)^*v}. \]
By lemma \ref{elementaryOrthogonality} we have $Tv = (T^*)^*v$ for all $v\in V$. 
\end{note}

\begin{proposition}
Let $H,K$ be Hilbert spaces and $T:H\to K$ a bijective bounded linear operator with bounded inverse. Then $(T^*)^{-1}$ exists and
\[ (T^*)^{-1} = (T^{-1})^*. \]
\end{proposition}
\begin{proof}
We prove $(T^{-1})^*$ is both a left- and a right-inverse of $T^*$: $\forall x\in H, y\in K$
\begin{align*}
\inner{T^*(T^{-1})^*x,y} &= \inner{x,T^{-1}Ty} = \inner{x,y} \\
\inner{x,(T^{-1})^*T^*y} &= \inner{TT^{-1}x,y} = \inner{x,y}
\end{align*}
So, by lemma \ref{elementaryOrthogonality}, $T^*(T^{-1})^* = \id_H$ and $(T^{-1})^*T^* = \id_K$.
\end{proof}

\begin{proposition}
Let $T\in\Bounded(H,K)$. Then
\[ \ker T = (\im T^*)^\perp \qquad \text{and thus} \qquad \overline{\im(T)} \subseteq \ker(T^*)^\perp. \]
\end{proposition}
\begin{proof}
\[ x\in \ker T \iff Tx = 0 \iff \forall y\in K: \inner{y, Tx}=0 \iff \forall y\in K: \inner{T^*y, x}=0 \iff x\perp T^*[K]. \]
\end{proof}
In particular $\im(T)$ is closed iff it is equal to $\ker(T^*)^\perp$. This is sometimes known as the closed range theorem. This is, e.g\, the case when $T$ is bounded below, see \ref{boundedBelowClosedRange}.

\begin{proposition} \label{normOfSquare}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\[ \norm{T^*T}= \norm{T}^2 = \norm{TT^*}. \]
\end{proposition}
\begin{proof}
For $\norm{T^*T}= \norm{T}^2$ first observe that
\[ \norm{T^*T} \leq \norm{T^*}\cdot\norm{T} = \norm{T}^2. \]
Conversely, $\forall x\in H$:
\[ \norm{T(x)}^2 = \inner{Tx,Tx} = \inner{T^*Tx,x} \leq \norm{T^*Tx}\cdot \norm{x} \leq \norm{T^*T}\cdot\norm{x}^2. \]
The other equality follows by applying the first to $T^*$ and using $\norm{T^*}=\norm{T}$.
\end{proof}

\subsection{Normal operators}
\begin{definition}
A densely defined linear operator $T$ on a Hilbert space $H$ is \udef{normal} if it is closed and $TT^* = T^*T$.
\end{definition}
Self-adjoint and unitary operators are normal.

TODO 3.10 Self-Adjoint, Unitary and Normal Operators from Kreyszig.


\begin{proposition}
Let $T: H\not\to H$ be a densely defined operator. Then $T$ is normal \textup{if and only if} $\dom(T) = \dom(T^*)$ and $\forall x\in H: \norm{Tx} = \norm{T^*x}$.
\end{proposition}
\begin{proof}
First, assume $T$ normal. Then
\[ \norm{Tx}^2 = |\inner{Tx,Tx}| = |\inner{T^*Tx,x}| = |\inner{TT^*x,x}| = |\inner{T^*x,T^*x}| = \norm{T^*x}^2. \]

For the converse, we have $\inner{Tx, Ty} = \inner{T^*x, T^* y}$ for all $x,y\in H$ by polarisation. From this we have $\inner{T^*Tx, y} = \inner{TT^*x, y}$ and normality follows from \ref{equalityOfMapsInnerProductSpaces}.

TODO question of domain.
\url{https://www.math.drexel.edu/faculty/mjz55/wp-content/uploads/sites/8/2017/01/normalnotes.pdf}.
\end{proof}
\begin{corollary} \label{equalityKernelAdjointNormal}
If $T$ is a normal operator, then $\ker T = \ker T^*$.
\end{corollary}
\begin{proof}
We have $x\in\ker(T) \iff \norm{Tx} = 0 \iff \norm{T^*x} = 0 \iff x\in\ker(T^*)$. 
\end{proof}
\begin{corollary}
If $T$ is a normal operatorm then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item $\spec(T) = \apspec(T)$.
\end{enumerate} 
\end{corollary}
\begin{proof}
If $T$ is normal, then so is $\lambda\id-T$. Now $\lambda\in\rspec(T)$ iff $\ker(\lambda\id - T) = \{0\}$ and $\im(\lambda\id-T)^\perp \neq \{0\}$, but $\im(\lambda\id-T)^\perp = \ker(\lambda\id-T)^* = \ker(\lambda\id-T)$. By \ref{kernelImageAdjoint} and the previous corollary. This is a contradiction.

(2) then follows straight from (1).
\end{proof}

\begin{theorem} \label{closureNumericRangeConvexHullSpectrum}
The closure of the numerical range of a normal operator is the
convex hull of its spectrum.
\end{theorem}
\begin{proof}
Normal operators $T$ are by definition closed, so $\spec(T)\subseteq \overline{\NumRange(T)}$ by \ref{spectralInclusionNumericalRange}. TODO
\end{proof}

\begin{lemma} \label{normalSpectralRadiusEqualsNorm}
For normal elements the spectral radius equals the norm.
\end{lemma}

\begin{lemma}
A normal operator on a Hilbert space is invertible \textup{if and only if} it is bounded below.
\end{lemma}

\subsection{Symmetric and self-adjoint operators}
\subsubsection{Domain related matters}
A symmetric operator $A$ is self-adjoint if and only if $\dom(A) = \dom(A^*)$.
\begin{lemma} \label{symmetricOperatorAdjointInclusion}
Let $A$ be a symmetric densely defined operator on a Hilbert space $H$. Then
\begin{enumerate}
\item $\dom(A) \subseteq \dom(A^*)$;
\item $A = A^*|_{\dom(A)}$;
\item $A$ is closable and $\overline{A} = A^{**}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1, 2) This is immediate from symmetry: $\inner{Ax,y} = \inner{x,Ay}$ for all $x,y\in\dom(A)$.

(3) From (1) we see that $A^*$ is densely defined, because the superset of a dense set is dense. The result follows by \ref{adjointDenselyDefinedClosable}.
\end{proof}
\begin{corollary}
A closed and densely defined symmetric operator is self-adjoint \textup{if and only if} $A^*$ is symmetric.
\end{corollary}
\begin{proof}
If $A^*$ is not symmetric, $A$ can clearly not be self-adjoint.

Assume $A^*$ is symmetric. Then $\dom(A) \subseteq \dom(A^*) \subseteq \dom(\overline{A}) = \dom(A)$.
\end{proof}

\begin{proposition} \label{selfAdjointMaximal}
A self-adjoint operator cannot have a proper symmetric extension.
\end{proposition}
\begin{proof}
Assume $A$ self-adjoint and $A\subseteq B$ for some symmetric operator $B$. Then
\[ A \subseteq B \subseteq B^* \subseteq A^* = A, \]
so $A = B$. We have used \ref{symmetricOperatorAdjointInclusion} and \ref{HilbertAdjointAntitone}.
\end{proof}
\begin{corollary}
Let $A$ be a densely defined symmetric operator. If $\overline{A}$ is self-adjoint, then it is the unique self-adjoint extension of $A$.
\end{corollary}
Note that $\overline{A}$ is always an operator by \ref{symmetricOperatorAdjointInclusion}.
\begin{proof}
Let $B$ be a self-adjoint extension of $A$. Then $\overline{A} = A^{**}\subseteq B^{**} = B$, by \ref{HilbertAdjointAntitone}. This means that $B$ is symmetric extension of the self-adjoint operator $\overline{A}$, which, by the proposition, implies $B = \overline{A}$.
\end{proof}
In general it is possible for an unbounded,
symmetric operator to not have a self-adjoint extension or have multiple self-adjoint extensions, even if it is densely defined. (TODO example)

\begin{definition}
Let $A$ be a densely defined symmetric operator whose closure is self-adjoint. We call $A$
\begin{itemize}
\item \udef{essentially self-adjoint};
\item a \udef{core} for $A$.
\end{itemize}
\end{definition}

\begin{example}
Consider the operator
\[ A: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(A) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a) = 0 = f(b)}. \]
Then
\begin{align*}
\inner{g, Af} &= \int_{a}^b \overline{g(x)}i\od{f(x)}{x}\diff{x} \\
&= \overline{g(b)}f(b) - \overline{g(a)}f(a) - \int_{a}^b \Big(i \od{}{x}\overline{g(x)}\Big)f(x)\diff{x} \\
&= \int_a^b \overline{i \od{g(x)}{x}} f(x) \diff{x} = \inner{Ag, f}.
\end{align*}
So $A$ is symmetric and $\dom(A^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)}$. We cannot extend $\dom(A)$ while keeping $\dom(A^*)$ the same, because $A$ would no longer be symmetric due to boundary terms.

There are, however, multiple ways we can extend $A$ to a self-adjoint operator (in each case $\dom(A^*)$ must shrink).

Let $A_\alpha$, for $\alpha\in \R$, be the operator $A$ with domain
\[ \dom(A_\alpha) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(b) = e^{i\alpha}f(b)}. \]
We must have $\forall f\in \dom(A_\alpha)$ and $g\in\dom(A^*_\alpha)$ that
\[ \overline{g(b)}f(b) - \overline{g(a)}f(a) = f(a)\Big(e^{i\alpha}\overline{g(b)} - \overline{g(a)}\Big) = 0, \]
so we have $e^{-i\alpha}g(b) = g(a)$ and thus $g(b) = e^{i\alpha}g(a)$, which means $\dom(A_\alpha^*) = \dom(A_\alpha)$. So $A_\alpha$ is a self-adjoint extension of $A$ for all $\alpha\in \R$.

TODO: compare Aharonov-Bohm
\end{example}
Notice that the operator
\[ T: L^2(a,b) \to L^2(a,b): f\mapsto i\od{f}{x} \]
with domain
\[ \dom(T) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b)} \]
is not symmetric. In this case
\[  \dom(T^*) = \setbuilder{f\in L^2(a,b)}{\od{f}{x}\in L^2(a,b),\; f(a)=0=f(b)}, \]
so $\dom(T^*) \subseteq \dom(T)$.

\subsubsection{Spectrum and related criteria}
TODO: $iA$ dissipative!
\begin{lemma}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. If $\exists z \in \C\setminus\R: \; \im(A+z\id) = H$, then $A$ is densely defined.
\end{lemma}
\begin{proof}
Let $A+z\id$ be surjective and suppose, towards a contradiction that there exists an $y\perp \dom(A)$. Then $y = (A+z\id)x$ for some $x\in\dom(A)$ by surjectivity. Then
\[ 0 = \Im\inner{x,y} = \Im\inner{x, (A+z\id)x} = \cancel{\Im\inner{x,Ax}} + \Im \inner{x,zx} = \Im(z)\norm{x}^2. \]
By assumption, $\Im(z) \neq 0$, so $x=0$, meaning $y = (A+z\id)x = 0$ and thus $\dom(A)^\perp = \{0\}$.
\end{proof}

\begin{proposition}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then $A + z\id_H$ is bounded below by $|\Im \lambda|$ for all $\lambda \in \C\setminus\R$.
\end{proposition}
\begin{proof}
We first calculate, $\forall x\in H$:
\[ \Im\inner{x, (A+ z\id_H)x} = \cancel{\Im\inner{x,Ax}} + \Im z\norm{x}^2. \]
Thus
\[ |\Im\lambda|\;\norm{x}^2 = |\Im\inner{x, (A + z\id_H)x}| \leq |\inner{x, (A + z\id_H)x}| \leq \norm{x}\;\norm{(A + z\id_H)x}, \]
which means that $\norm{(A + z\id_H)x} \geq |\Im\lambda|\;\norm{x}$, so $A + z\id_H$ is bounded below by $|\Im \lambda|$.
\end{proof}
\begin{corollary} \label{approximateSpectrumSymmetricOperator}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then $\apspec(A) \subseteq \R$.
\end{corollary}
\begin{corollary}
The eigenvalues of a symmetric operator are real.
\end{corollary}
\begin{proof}
This is immediate using $\pspec(A)\subseteq \apspec(A)$. We can also give a direct calculation:

Assume there exists an $x\in \ker(\lambda\id_H - A)\setminus\{0\}$. Then $Ax = \lambda x$ and thus
\[ \lambda\norm{x}^2 = \lambda\inner{x,x} = \inner{x, \lambda x} = \inner{x,Ax} = \inner{Ax,x} = \inner{\lambda x, x} = \overline{\lambda}\inner{x,x} = \overline{\lambda}\norm{x}^2. \]
Because $\norm{x}^2 \neq 0$, we have $\lambda = \overline{\lambda}$, meaning $\lambda$ is real.
\end{proof}
\begin{corollary} \label{symmetricResolvent}
Let $A$ be a symmetric operator on a complex Hilbert space $H$. Then for all $\lambda\in\C\setminus\R$, the resolvent $R_A(\lambda)$ well-defined and bounded by $\norm{R_A(\lambda)}\leq 1/|\Im \lambda|$.
\end{corollary}
Note this does not mean $\C\setminus\R\subseteq \res(A)$, as $\dom(R_A(\lambda))$ may not be all of $H$.
\begin{proof}
This is an application of \ref{boundedBelow}.
\end{proof}

\begin{proposition} \label{rangeSelfAdjointCriterion}
Let $A$ be a symmetric operator on a Hilbert space $H$. The following are equivalent:
\begin{enumerate}
\item $\exists z \in \C: \; \im(A+z\id) = H = \im(A+\overline{z}\id)$;
\item $A$ is self-adjoint;
\item $\rspec(A) = \emptyset$.
\end{enumerate}
In this case $\spec(A) = \apspec(A)$.
\end{proposition}
Notice that in (1) we include $\R$ and in (3) we exclude $\R$.
\begin{proof}
$(1) \Rightarrow (2)$ From \ref{symmetricOperatorAdjointInclusion}, we have $A\subseteq A^*$ and thus $A+z\id = (A^* + z\id)\cap(A+z\id)$. From point (5) of \ref{adjointAlgebraicProperties}, we have $A+z\id = (A^* + z\id)\cap(A+z\id) = (A+\overline{z}\id)^*\cap (A+z\id)$.

We then use \ref{adjointRangeCriterion} with $S = A+z\id$ and $T = A+\overline{z}\id$ to obtain $(A+z\id)^* = A+\overline{z}\id$. Subtracting $\overline{z}\id$ from each side yields the result.

$(2) \Rightarrow (3)$ Fix some $z \in \spec(A) \setminus\pspec(A)$ we need to show that $\overline{\im(\lambda\id - A)} = H$. Indeed
\[ \overline{\im(A+z\id)} = \ker(A^* + \overline{z}\id)^\perp = \ker(A+\overline{z}\id)^\perp = \{0\}^\perp = H. \]

$(3) \Rightarrow (1)$ We have $\spec(A) = \apspec(A)$. Because $\apspec\subseteq \R$, by \ref{approximateSpectrumSymmetricOperator}, we have that $A+z\id$ is surjective for all $\C\setminus\spec(A) = \C\setminus\apspec(A) \supseteq \C\setminus\R$.
\end{proof}
\begin{corollary}
Every surjective symmetric operator is self-adjoint.
\end{corollary}
\begin{proof}
Take $z=0$ in point (1).
\end{proof}

\begin{proposition}
Let $A$ be a closed symmetric operator. Then one of the following cases holds:
\begin{itemize}
\item $A$ is self-adjoint, in which case $\spec(A) \subseteq \R$;
\item $\spec(A) = \overline{\C^{\uparrow}}$;
\item $\spec(A) = \overline{\C^{\downarrow}}$;
\item $\spec(A) = \C$.
\end{itemize}
If $A$ is not densely-defined, then the last case holds.
\end{proposition}
We have denoted the closed upper half plane $\overline{\C^{\uparrow}}$ and the closed lower half plane $\overline{\C^{\downarrow}}$.
\begin{proof}
First assume $A$ self-adjoint, then $\spec(A)\subseteq \R$ by a combination of \ref{approximateSpectrumSymmetricOperator} and \ref{rangeSelfAdjointCriterion}.

Now note that if there exists a real $\lambda\in\R$ such that $\lambda \in \res(A)$, then in particular $\lambda\id -A$ is surjective, so $A$ is self-adjoint by \ref{rangeSelfAdjointCriterion}.

Now assume $A$ not self-adjoint and pick a $\lambda\in \C^{\uparrow}$. From \ref{rangeSelfAdjointCriterion} we must have either $\lambda\in\spec(A)$ or $\overline{\lambda}\in\spec(A)$ (or both).

If $\lambda\in \res(A)$, then $\C^\uparrow \subseteq \res(A)$ and if $\overline{\lambda}\in\res(A)$, then $\C^\downarrow \subseteq \res(A)$.

Indeed take some $\mu\in\C$.
By \ref{symmetricResolvent} we only need to check surjectivity of $\mu\id - A$. Now note that
\begin{align*}
(\mu\id - A)R_A(\lambda) &= (\mu\id -\lambda\id+\lambda\id - A)R_A(\lambda) \\
&= (\mu-\lambda)R_A(\lambda) + (\lambda\id-A)R_A(\lambda) \\
&= (\mu-\lambda)R_A(\lambda) + \id,
\end{align*}
which we can consider as a bounded perturbation of $\id$. Thus by \ref{boundedPerturbationClosedOperator}, $(\mu\id - A)R_A(\lambda)$ is surjective if $\norm{(\mu-\lambda)R_A(\lambda)}< 1$, i.e.\ $|\mu-\lambda| < |\Im(\lambda)|$ using \ref{symmetricResolvent}.

We can iterate this construction to cover the whole of $\C^\uparrow$. The argument for $\overline{\lambda}$ is similar.
\end{proof}

\begin{example}
Spectrum half plane TODO \url{https://math.stackexchange.com/questions/893899/spectrum-of-symmetric-non-selfadjoint-operator-on-hilbert-space}

\url{https://math.stackexchange.com/questions/925097/spectrum-of-self-adjoint-operator-on-hilbert-space-real}
\end{example}

\begin{proposition}
Let $A$ be a closed symmetric operator on a Hilbert space. Then $A$ is positive \textup{if and only if} $\spec(A)\subseteq [0,\infty[$.
\end{proposition}
\begin{proof}
\ref{closureNumericRangeConvexHullSpectrum}
\end{proof}

\begin{proposition}
Let $A$ be a self-adjoint operator. Then
\begin{enumerate}
\item $\inf \sigma(A) = \inf\NumRange(A)$;
\item $\sup \sigma(A) = \sup\NumRange(A)$.
\end{enumerate}
\end{proposition}
\begin{proof}
\ref{closureNumericRangeConvexHullSpectrum}
\end{proof}

\begin{proposition}
Let $T$ be a densely defined self-adjoint operator. Then
\begin{enumerate}
\item $\rspec(T) = \emptyset$;
\item let $\lambda_1,\lambda_2 \in \pspec(T)$ and $\lambda_1\neq \lambda_2$, then 
\[ \Null(\lambda_1\id - T) \perp \Null(\lambda_2 \id - T). \]
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}


\begin{proposition}
Let $T$ be a symmetric operator on a Hilbert space $H$. Then
\begin{enumerate}
\item the eigenvalues of $T$ are real;
\item the eigenvectors corresponding to distinct eigenvalues are orthogonal.
\end{enumerate}
\end{proposition}
\begin{proof}
This is an application of \ref{eigenspaceOrthogonalAdjoint} and \ref{adjointSpectrumNoResidual}.
\end{proof}

\subsubsection{Compact self-adjoint operators}
\begin{proposition}
Every compact self-adjoint operator $L$ on a nontrivial Hilbert space has an eigenvalue $\lambda$ with $|\lambda| = \norm{L}$.
\end{proposition}

\begin{proposition}
Let $A$ be a compact self-adjoint operator. Then the only possible accumulation point of $\spec(A)$ is $0$.
\end{proposition}
TODO self-adjoint not necessary? See \ref{spectrumCompactOperator}?
\begin{proof}
Assume $\spec(A)$ is infinite. Then take $\seq{\lambda_n}\subset \spec(A)$. Any associated sequence $\seq{x_n}$ of eigenvectors is orthogonal. We can take it to be orthonormal. By \ref{limitCompactImageOrthonormalSequence} we have
\[ 0 = \lim_{n\to\infty} \norm{Ax_n}^2 = \lim_{n\to\infty}\inner{Ax_n,Ax_n} = \lim_{n\to\infty}\lambda_n^2\inner{x_n,x_n} = \lim_{n\to\infty}\lambda_n^2, \]
so $\seq{\lambda_n}$ converges to $0$.
\end{proof}

\begin{theorem}
Every spectral value $\lambda\neq 0$ of a compact self-adjoint linear
operator $A : H \to H$ is an eigenvalue of finite multiplicity that can only
accumulate at $\lambda = 0$. Conversely, a self-adjoint operator having these
properties is compact.
\end{theorem}
\begin{proof}
TODO See \ref{spectrumCompactOperator}
\end{proof}

\subsubsection{Self-adjoint extensions of symmetric operators}
\paragraph{Cayley transform}
Consider the Mbius transform
\[ \C\setminus\{\overline{\lambda}\} \to \C: x\mapsto \frac{x - \lambda}{x-\overline{\lambda}} \qquad \text{for some $\lambda\in\C\setminus\R$.} \]
This transform maps
\begin{itemize}
\item the real line to $\T\setminus\{1\}$;
\item the half-plane above / below the real line containing $\lambda$ to the interior of the unit disk;
\item the half plane containing $\overline{\lambda}$ to the exterior of the unit disk;
\item in particular $\lambda \mapsto 0$ and $\overline{\lambda} \mapsto \infty$.
\end{itemize}
Conventional choice: $\lambda = i$.

\paragraph{Defect indices}
Or deficiency(?)
\url{https://link-springer-com.ezproxy.ulb.ac.be/content/pdf/10.1007/978-94-007-4753-1.pdf}

Cfr. dilation theory through Cayley transform.

\subsubsection{Positive self-adjoint extensions of symmetric operators}
\begin{theorem}[Friedrich's extension]
Let $A$ be a positive symmetric operator on a Hilbert space $H$. Then $A$ has a unique positive self-adjoint extension $\widetilde{A}$ with domain $\dom(\widetilde{A}) \subseteq \Closure_{\norm{\cdot}_{A+\id}}(\dom(A))$.
\end{theorem}
\begin{proof}
Set $H_A \defeq \Closure_{\norm{\cdot}_{A+\id}}(\dom(A))$. By \ref{energyNormTopology}, we have $H_A \subseteq \Closure_{\norm{\cdot}}(\dom(A))$.

For \undline{existence}, we can construct the operator $\widetilde{A}$ as follows:
\begin{align*}
\dom(\widetilde{A}) &\defeq \setbuilder{x\in H_A}{\exists x'\in H:\forall y\in H_A:\; \inner{y,x}_{A+\id} = \inner{y,x'}} \\
\widetilde{A}x &\defeq x' - x.
\end{align*}
Now $\widetilde{A}$ is an extension of $A$, because for all $x\in \dom(A)$, we can take $x' = Ax + x$. So $\widetilde{A}x = Ax$.

But $\dom(\widetilde{A})$ may be larger than $\dom(A)$, because we can extended $\inner{y,x}_{A+\id}$ to be defined on all of $H_A$ by continuity.

By construction $\dom(\widetilde{A}) \subseteq \Closure_{\norm{\cdot}_{A+\id}}(\dom(A))$.

Now we claim $\im(\widetilde{A} + \id) = H$. Indeed for any $x'\in H$, the functional $H_A \to H_A: y\mapsto \inner{y,x'}$ is bounded. By Riesz representiation \ref{rieszRepresentation}, we can find an $x\in H_A$ such that $\inner{y,x}_{A+\id} = \inner{y,x'}$. Thus $(\widetilde{A} + \id)x = x'$.

By \ref{rangeSelfAdjointCriterion} we conclude that $\widetilde{A}$ is self-adjoint. 

For \undline{uniqueness}, assume there exists a second such extension $\widehat{A}$. For all $y\in \dom(A)$ and $x\in \dom(\widehat{A})$, we have
\[ \inner{y, (\widehat{A}+\id)x} = \inner{(\widehat{A}+\id)y, x} = \inner{(A+\id)y, x} = \overline{\inner{x, (A+\id)y}} = \overline{\inner{x, y}_{A+\id}} = \inner{y, x}_{A+\id}. \]
By continuity this holds for all $y\in H_A$. And thus by definition $\widetilde{A}x = \widehat{A}x$ for all $x\in\dom(\widetilde{A})$. Thus $\widetilde{A} \subseteq \widehat{A}$, but self-adjoint operators are maximal by \ref{selfAdjointMaximal}, so $\widetilde{A} = \widehat{A}$.
\end{proof}

\subsubsection{Bounded self-adjoint operators}
\begin{lemma}
Let $A, B\in\Bounded(H)$. Then
\begin{enumerate}
\item $A^*A, AA^*$ and $A+A^*$ are self-adjoint;
\item if $A,B$ are self-adjoint, then $AB$ is self-adjoint \textup{if and only if} $A,B$ commute.
\end{enumerate}
\end{lemma}
\begin{corollary}
Let $A\in\Bounded(H)$. Then there exist unique self-adjoint operators $S,T$ such that
\[ A = S+iT \qquad A^* = S-iT. \]
\end{corollary}
\begin{proof}
Indeed $S = (A+A^*)/2$ and $T = (A-A^*)/2i$ are self-adjoint.
\end{proof}
\begin{corollary}
The operator $A$ is normal \textup{if and only if} $S,T$ commute.
\end{corollary}
\begin{proof}
We calculate the commutator
\[ [S,T] = \left[\frac{A+A^*}{2}, \frac{A-A^*}{2i}\right] = \frac{A^*A - AA^*}{2i} = \frac{1}{2i}[A^*, A]. \]
\end{proof}

\begin{proposition}
The set of bounded self-adjoint operators forms an anti-lattice.
\end{proposition}
\begin{proof}
TODO + generalised to self-adjoint operators??
\end{proof}

\subsection{Orthogonal projections}
\url{https://planetmath.org/latticeofprojections}

\url{https://zfn.mpdl.mpg.de/data/Reihe_A/35/ZNA-1980-35a-0437.pdf}

We denote the set op projections on a Hilberts space $\mathcal{H}$ by $\Projections(\mathcal{H})$.

TODO: $\im(P) = \ker{P^*}^\perp$ shows that we need $P= P^*$ for orthogonality.

\begin{proposition}
Let $P$ be a bounded operator $P$ on a Hilbert space $\mathcal{H}$. Then the following are equivalent:
\begin{enumerate}
\item $P$ is an orthogonal projection onto a closed subspace of $\mathcal{H}$;
\item $P^2 = P$ and $P=P^*$;
\item $P^2 = P$ and $\norm{P}\in \{0,1\}$;
\item $P^2 = P$ and $\norm{P}\leq 1$;
\end{enumerate}
\end{proposition}
\begin{proof}
$\boxed{(1)\Rightarrow (2)}$  Suppose first that $P$ is the orthogonal projection operator onto a closed subspace $K$. Clearly $P^2 = P$. Let $x,y\in\mathcal{H}$ and write $x= x_1+x_2, y = y_1+y_2$ where $x_1,y_1\in K$ and $x_2,y_2\in K^\perp$. Then
\[ \inner{Px, y} = \inner{x_1, y_1+y_2} = \inner{x_1, y_1} + \inner{x_1,y_2} = \inner{x_1,y_1} = \inner{x_1+x_2, y_2} = \inner{x,Py}. \]
So $P = P^*$.

$\boxed{(2)\Rightarrow (3)}$ We calculate $\norm{P} = \norm{P^2} = \norm{P^*P} = \norm{P}^2$ using \ref{normOfSquare}. The solutions to this equation are $\{0,1\}$.

$\boxed{(3)\Rightarrow (4)}$ This is clear.

$\boxed{(4)\Rightarrow (1)}$ Define $K=\im P$, then $K$ is closed because $x\in K$ iff $Px=x$ and thus for any converging sequence $(x_n)_n\subset K$: $\lim x_n = \lim Px_n = P\left(\lim x_n\right)$, so the limit is in $K$.

We just need to show orthogonality: $Px \perp x- Px$. For this we use \ref{orthogonality}: for all $a\in\F$
\[ \norm{Px} = \norm{Px + aPx - aPx} = \norm{P(Px + a(x-Px))} \leq \norm{P}\cdot \norm{Px + a(x-Px)} \leq \norm{Px + a(x-Px)}. \]
We conclude $Px \perp x- Px$.
\end{proof}

\begin{proposition} \label{projectorOrthogonalComplement}
Let $\mathcal{H}$ be a Hilbert space and let $P$ be an orthogonal projector on a closed subspace $K$. Then $\id-P$ is the orthogonal projector on $K^\perp$.
\end{proposition}
\begin{proof}
Any $x\in \mathcal{H}$ can be uniquely decomposed as $x_1 + x_2\in K\oplus K^\perp$. If $Px = x_1$, then $(\id - P)x = x_1 +x_2 - x_1 = x_2$.
\end{proof}
\begin{corollary} \label{projectorsIn01}
The set of projectors $\Projections(\mathcal{H})$ is a subset of $[0,\id]$.
\end{corollary}
\begin{proof}
Let $P\in\Projections(\mathcal{H})$. Then $P\geq 0$ follows from $P = P^2 = P^*P$.
\end{proof}

\begin{proposition} \label{commutingProjectors}
Let $\mathcal{H}$ be a Hilbert space and $P,Q$ be projections. The following are equivalent:
\begin{enumerate}
\item $PQ = QP$;
\item $PQ$ is a projection;
\item $QP$ is a projection;
\item $P+Q-PQ$ is a projection;
\item $\im(PQP) = \im(P) \cap \im(Q)$;
\item $PQP = QP$;
\item $\mathcal{H} = \big(\im(P)\cap\im(Q)\big)\oplus \big(\im(P)\cap\im(Q)^\perp\big) \oplus \big(\im(P)^\perp\cap\im(Q)\big) \oplus \big(\im(P)^\perp\cap\im(Q)^\perp\big)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Points (1), (2), (3) are equivalent by the equation $(PQ)^* = Q^*P^* = QP$, and the fact that (1) implies $(PQ)^2 = PQPQ = PPQQ = PQ$.

(4) If $P,Q$ commute, then
\begin{align*}
(P+Q-PQ)^* &= P+Q-(PQ)^* = P+Q-Q^*P^* =P+Q-QP = P+Q-PQ \\
(P+Q-PQ)^2 &= P^2 + PQ -P^2Q + QP+Q^2 - QPQ - PQP -PQP +PQPQ \\
&= P + Q + 3PQ - 4PQ= P+Q-PQ.
\end{align*}
Assume (4), then $(P+Q-PQ)^* = P+Q-QP = P+Q-PQ$. This implies $PQ=QP$.

$\boxed{(1)\Rightarrow (5)}$ Clearly $\im(PQP) \subseteq \im(P) \cap \im(Q)$.
For the inverse inequality, take $x\in im(P)\cap\im(Q)$. Then $PQP(x) = PQ(x) = P(x) = x$, so $x\in\im(PQP)$.

$\boxed{(5)\Rightarrow (6)}$ We decompose $\mathcal{H} = \im(PQP) \oplus \ker(PQP)$ and show that the operators are the same on both parts. For all $x\in \mathcal{H}$ we have
\[ x\in \ker(PQP) \iff \inner{x,PQPx} = 0 \iff \inner{QPx,QPx} = 0 \iff \norm{QPx} = 0 \iff x\in\ker{QP}.  \]
Now let $x\in\im(PQP) = \im(P)\cap\im(Q)$. Then $QPx = Qx = x = PQPx$.

$\boxed{(6)\Rightarrow (3)}$ $PQP$ is always a projection.

$\boxed{(6)\Rightarrow (7)}$ Take some $x\in \mathcal{H}$. Then we can uniquely decompose $x = P(x) + (x-P(x)) = x_P + x_{P^\perp} \in \im(P)\oplus \im(P)^\perp$. We can then further decompose $x_P = x_{P,Q} + x_{P,Q^\perp}$ and $x_{P^\perp} = x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. In order to have the decomposition of the proposition, we need to show that $x_{P,Q},x_{P,Q^\perp}\in \im(P)$ and $x_{P^\perp, Q},x_{P^\perp, Q^\perp}\in\im(P)^\perp$.

First take $x_{P,Q} = QPx$. From (6) we have $P(QPx) = PQPx = QPx$, so $x_{P,Q}\in \im(P)$. For the others we have similar calculations (also using the identity $PQP = PQ$):
\begin{align*}
P(x_{P,Q^\perp}) &= P\big((\id-Q)P\big)x = Px - PQPx = Px - QPx = (\id-Q)Px = x_{P,Q^\perp} \\
(\id-P)(x_{P^\perp,Q}) &= (\id-P)\big(Q(\id-P)\big)x = (Q-QP-PQ+PQP)x = (Q-QP)x = Q(\id-P)x = x_{P^\perp,Q} \\
(\id-P)(x_{P^\perp,Q^\perp}) &= (\id-P)\big((\id-Q)(\id-P)\big)x = (\id-P-Q+QP-P+P+PQ-PQP)x \\
&= (\id-Q-P+QP)x = (\id-Q)(\id-P)x = x_{P^\perp,Q^\perp}.
\end{align*}
$\boxed{(7)\Rightarrow (1)}$ Take $x\in \mathcal{H}$ and decompose it as $x_{P,Q} + x_{P,Q^\perp} + x_{P^\perp, Q} + x_{P^\perp, Q^\perp}$. Then $PQx = P(x_{P,Q} + x_{P^\perp, Q}) = x_{P,Q}$ and $QP = Q(x_{P,Q} + x_{P, Q^\perp}) = x_{P,Q}$, so $PQ = QP$. 
\end{proof}

\begin{proposition} \label{perpendicularProjections} \label{subspaceProjections}
Let $P,Q$ be orthogonal projections onto subspaces $\im(P)$ and $\im(Q)$ of $\mathcal{H}$.
\begin{enumerate}
\item The following are equivalent to $\im(P) \perp \im(Q)$:
\begin{enumerate}
\item $QP = 0$;
\item $PQ = 0$;
\item $Q+P$ is an orthogonal projection.
\end{enumerate}
\item The following are equivalent to $\im(P) \subseteq \im(Q)$:
\begin{enumerate}
\item $QP = P$;
\item $PQ = P$;
\item $Q-P$ is an orthogonal projection;
\item $P\leq Q$;
\item $\norm{Px} \leq \norm{Qx}$ for all $x \in \mathcal{H}$.
\end{enumerate}
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \perp \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a, b)\Leftrightarrow (c)}$ We know $(P+Q)^* = P^*+Q^* =P+Q$ and we can write
\[ (P+Q)^2 = P^2 + Q^2 + PQ + QP = P+Q+ PQ+QP,  \]
So clearly (a) or (b) imply (c). Conversely, assume $PQ + QP = 0$, implying $PQ=-QP$. By left- and right-multiplication by $P$ this implies both
\[ PPQ = PQ = -PQP \qquad \text{and} \qquad PQP = -QPP = -QP. \]
So $PQ = -PQP = QP$, meaning $PQ = 1/2(PQ+QP) = 0$.

(2) We prove the following:

$\boxed{(a)\Leftrightarrow (b) \Leftrightarrow \im(P) \subseteq \im(Q)}$ By \ref{commutingProjectors}.

$\boxed{(a,b)\Rightarrow (c)}$ Obviously $(Q-P)^*= Q-P$. Also
\[ (Q-P)^2 = Q+P-PQ-QP= Q+P-2P = Q-P. \]

$\boxed{(c)\Rightarrow (a,b)}$ Now from
\[ Q-P = (Q-P)^2 = Q+P-PQ-QP \]
we obtain $2P = PQ+QP$. The result then follows if we can show that $PQ=QP$. This follows by multiplying the equality on the left and on the right by $P$ to obtain $QP = 2P-PQP$ and $PQ = 2P-PQP$, respectively. 

$\boxed{(c)\Rightarrow (d)}$ This follows because all projections are positive.

$\boxed{(d)\Rightarrow (a, b)}$ Assume, towards a contradiction, that $\im(P)\nsubseteq \im(Q)$. Then we can take $v\in\im(P)\setminus \im(Q)$. Then
\[ \inner{v,(Q-P)v} = \inner{v,Qv} - \inner{v,v} = \inner{Qv,Qv} - \inner{Qv,Qv} - \inner{v-Qv, v-Qv} = -\norm{v-Qv}^2. \]
Because $v\notin \im(Q)$, $\norm{v-Qv}$ is not zero and thus $Q-P$ is not positive.

$\boxed{(d)\Leftrightarrow (e)}$ By the equivalence
\[ \norm{Px} \leq \norm{Qx} \iff \inner{Px,Px} \leq \inner{Qx,Qx} \iff \inner{Px,x}\leq \inner{Qx,x} \iff \inner{(Q-P)x,x}\geq 0. \]
\end{proof}

We can generalise part 2(d) of the previous proposition to a slightly larger class of operators.
\begin{lemma} \label{comparisonSelfAdjointProjection}
Let $P\in \Projections(\mathcal{H})$ and $T \in [0,\id]$, then the following are equivalent:
\begin{enumerate}
\item $\im(T) \subseteq \im(P)$;
\item $T\leq P$.
\end{enumerate}
\end{lemma}
\begin{proof}
As $T$ is self-adjoint, we have $\norm{T} = \nr(T) \leq 1$ by \ref{normNumRadius}.

Assume (1) so that for all $x\in \mathcal{H}$ we get
\[ \inner{x,Tx} = \inner{x, PTx} = \inner{Px,PTx} \leq \norm{Px}^2\nr(T) \leq \norm{Px}^2 = \inner{Px,Px} = \inner{x,Px}. \]
So $\inner{x, (P-T)x}\geq 0$ and thus $T\leq P$.

Assume (2). The energy form associated with $T$ is a pre-inner product by \ref{positiveOperatorPositiveEnergyForm}. The Cauchy-Schwarz inequality \ref{CauchySchwarz} gives
\[ |\inner{v,Tw}|^2 \leq \inner{v,Tv}\inner{w,Tw} \leq \inner{v,Pv}\inner{w,Pw}. \]
So if $v\in\im(P)^\perp$, then $\inner{v,Tw} = 0$ for all $w\in \mathcal{H}$. So $\im(T)\perp \im(P)^\perp$, implying $\im(T)\subseteq \im(P)^{\perp\perp} = \im(P)$.
\end{proof}

\begin{proposition}
Let $\mathcal{H}$ be a Hilbert space. Let $\{P_i\}_{i\in I}$ be an arbitrary subset of $\Projections(\mathcal{H})$ and let $K_i = \im(P_i)$ for all $i\in I$. Then, as a subset of $[0,\id]$,
\begin{enumerate}
\item $\inf \{P_i\}_{i\in I} = P_M$ where $M = \bigcap_{i\in I}K_i$;
\item $\sup \{P_i\}_{i\in I} = P_N$ where $N = \bigcap\setbuilder{K \subseteq \mathcal{H}}{\text{$K$ is closed} \land \forall i\in I: K_i \subseteq K}$.
\end{enumerate}
The set of projections on $\mathcal{H}$ is thus a complete lattice as a subset of $[0,\id]$.

If $I$ is finite, then $N = \Span(\bigcup_{i\in I}K_i)$. TODO: always closure of this $N$????
\end{proposition}
In particular this means $\Projections(\mathcal{H})$ is a complete lattice as itself, with the same suprema and infima. It is not a lattice as a subset of $\SelfAdjoints(\mathcal{H})$ (TODO + example ??).
\begin{proof}
(1) By \ref{subspaceProjections} $P_M$ is a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. Let $T$ be a lower bound of $\{P_i\}_{i\in I}$ in $[0,\id]$. By \ref{comparisonSelfAdjointProjection} $\im(T)\subseteq K_i$ for all $i\in I$, so $\im(T)\subseteq M$ and thus $T\leq P$ again by \ref{comparisonSelfAdjointProjection}. This means $P$ is the greatest lower bound.

(2) The mapping $T\mapsto \id-T$ keeps $[0,\id]$ invariant and inverts the order. Then $\inf \{\id - P_i\}_{i\in I}$ is a projection due to the previous point and so $\sup \{P_i\}_{i\in I}$ is also a projection. The expression for $N$ is clear from \ref{subspaceProjections}.
\end{proof}

\subsubsection{Sets of pairwise disjoint projections}
TODO!

\subsubsection{Derivatives of orthogonal projections}



\begin{proposition}
Let $\{P_i\}_{i\in I}$ be a set of pairwise disjoint orthogonal projectors which have derivatives and take $i\neq j$ in $I$. Then
\begin{enumerate}
\item $P_i'P_j = - P_iP_j'$;
\item if $\id \in \upset \{P_i\}_{i\in I}$, then
\[ P_iP_i' = \sum_{j\neq i}P'_iP_j \qquad\text{and}\qquad P_i'P_i = \sum_{j\neq i}P_jP_i'. \]
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We have $P_iP_j = 0$, so $0 = P_i'P_j + P_iP_j'$.

(2) We calculate, using $\id = \sum_{j\in I}P_j$ and \ref{derivativeIdempotent}:
\[ P_iP_i' = P_iP_i'\left(\sum_{j\in I}P_j\right) = P_iP_i'P_i + \sum_{j\neq i}P_iP_i'P_j = 0 - \sum_{j\neq i}P_iP_iP_j' = -\sum_{j\neq i}P_iP_j' = \sum_{j\neq i}P_i'P_j. \]
\end{proof}
\begin{corollary}
Let $P_1, P_2$ be orthogonal projections such that $P_1 + P_2 = \id$. Then
\[ P_1P_1'= P_1'P_2 \qquad \text{and}\qquad P_1'P_1 = P_2P_1'. \]
\end{corollary}


\subsection{Isometries}
We recall that isometries are injective and continuous. On Hilbert spaces they are also closed. See \ref{isometryInjective}, \ref{isometryContinuous} and \ref{isometryClosed}.

\begin{proposition} \label{isometryCharacterisation}
Let $T\in \Bounded(H,K)$ with $H,K$ Hilbert spaces. Then
\begin{enumerate}
\item $T$ is an isometry \textup{if and only if} $T^*T = \id_H$;
\item $T$ is unitary \textup{if and only if} $T^*T = \id_H$ and $TT^* = \id_K$, i.e.\ $T^{-1} = T^*$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) For all $v,w\in H$ we have
\[ \inner{Tv,Tw} = \inner{T^*Tv,w}. \]
The left-hand side is equal to $\inner{v,w}$ iff $T$ is an isometry. The right-hand side is equal to $\inner{v,w}$ iff $T^*T = \id_H$, by \ref{equalityOfMapsInnerProductSpaces}.

(2) If $T$ is invertible, it must have a left and right inverse. By lemma \ref{leftRightInverse} they must be the same.
\end{proof}
\begin{corollary}
An isometry $T\in\Bounded(H)$ is unitary \textup{if and only if} it is normal.
\end{corollary}

\begin{lemma} \label{isometryRangeProjection}
Let $T$ be an isometry between Hilbert spaces $H$ and $K$. Then $TT^*$ is an orthogonal projection.
\end{lemma}
\begin{proof}
Clearly $(TT^*)^* = TT^*$. Also $(TT^*)^2 = T(T^*T)T^* = T\id_HT^* = TT^*$.
\end{proof}


\subsubsection{Wandering spaces and unilateral shifts}
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear map. Then $\mathcal{V}$ is called a \udef{wandering space} for $T$ if $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$.
\end{definition}

\begin{lemma} \label{WoldLemma1}
Let $\mathcal{H}$ be a Hilbert space, $\mathcal{V}\subseteq \mathcal{H}$ a closed subspace and $T:\mathcal{H}\to \mathcal{H}$ a linear isometry.
\begin{enumerate}
\item $\mathcal{V}$ is a wandering space for $T$ \textup{if and only if} $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$;
\item $T[\mathcal{H}]^\perp$ is a wandering subspace for $T$;
\item if $\mathcal{V}$ is a wandering space for $T$, then $T^n[\mathcal{V}] \cong \mathcal{V}$ for all $n\in N$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) The direction $\Rightarrow$ is clear. For the converse, assume $T^n[\mathcal{V}]\perp \mathcal{V}$ for all $n\in\N$. We need to show that $T^p[\mathcal{V}]\perp T^q[\mathcal{V}]$ for every $p\neq q\in\N$. WLOG we may assume $p\leq q$. Take arbitrary $x\in T^p[\mathcal{V}]$ and $y\in T^q[\mathcal{V}]$. Then
\[ \inner{x,y} = \inner{T^p(u), T^q(v)} = \inner{u, T^{q-p}(v)} = 0 \]
because $\mathcal{V} \perp T^{q-p}[\mathcal{V}]$.

(2) For all $n\geq 1$ we have
\[ T^{n}\big[T[\mathcal{H}]^\perp\big] \subset T^{n}[\mathcal{H}] = T\big[T^{n-1}[\mathcal{H}]\big] \subset T[\mathcal{H}] \perp T[\mathcal{H}]^\perp. \]

(3) For all $n\in \N$ the operator $T^n$ is an isometry. It is injective by \ref{isometryInjective}, and thus maps its domain bijectively to its image.
\end{proof}

\begin{definition}
An isometry $T$ on a Hilbert space $\mathcal{H}$ is called a \udef{unilateral shift} if there is a closed subspace $\mathcal{V}\subseteq \mathcal{H}$ that is wandering for $T$ such that
\[ \mathcal{H} = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
We call the subspace $\mathcal{V}$ \udef{generating} for $T$ and $\dim(\mathcal{V})$ the \udef{multiplicity} of $T$.
\end{definition}

By \ref{WoldLemma1}, we see that any isometry $T:\mathcal{H}\to\mathcal{H}$ is a unilateral shift when restricted to $\bigoplus_{n=0}^\infty T^n\big[T[\mathcal{H}]^\perp\big]$.



\begin{lemma} \label{WoldLemma2}
Let $T$ be an isometry on $\mathcal{H}$. If $T$ is a unilateral shift, then it is generated by $T[\mathcal{H}]^\perp$.
\end{lemma}
\begin{proof}
Let $\mathcal{V}$ be the generating subspace of the unilateral shift $T$. We calculate
\[ T[\mathcal{H}] = T\left[\bigoplus_{n=0}^\infty T^n[\mathcal{V}]\right] = \bigoplus_{n=1}^\infty T^n[\mathcal{V}] = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \ominus \mathcal{V} = \mathcal{H}\ominus \mathcal{V} = \mathcal{V}^\perp, \]
so $\mathcal{V} = T[\mathcal{H}]^\perp$.
\end{proof}

A unilateral shift is determined up to unitary equivalence by its multiplicity:
\begin{lemma}
Let $T: \mathcal{H}\to\mathcal{H}$ and $T':\mathcal{H}'\to\mathcal{H}'$ be unilateral shifts generated by $\mathcal{V}$ and $\mathcal{V}'$ such that $\dim(\mathcal{V}) = \dim(\mathcal{V}')$. Then there exists an unitary $U:\mathcal{H}'\to\mathcal{H}$ such that
\[ T' = U^*TU \]
\end{lemma}
\begin{proof}
Choose an isometric isomorphism $u:\mathcal{V}'\to\mathcal{V}$. Then any $x\in\mathcal{H}'$ can be written as $x = \sum_{n=0}^\infty T^n(x_n)$. Then define
\[ Ux = \sum_{n=0}^\infty T^n(ux_n). \]
\end{proof}

\begin{theorem}[Wold decomposition]
Let $\mathcal{H}$ be a Hilbert space and $T\in\Bounded(\mathcal{H})$ an isometry. Then $\mathcal{H}$ decomposes into an orthogonal sum $\mathcal{H} = \mathcal{H}_0\oplus \mathcal{H}_1$such that $\mathcal{H}_0, \mathcal{H}_1$ reduce $T$ and
\[ T|_{\mathcal{H}_0}\;\text{is unitary} \quad\text{and}\quad T|_{\mathcal{H}_1}\;\text{is a unilateral shift}. \]
This decomposition is uniquely determined and given by
\[ \mathcal{H}_0 = \bigcap_{n=0}^\infty T^n[\mathcal{H}] \qquad\text{and}\qquad \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}] \qquad\text{where}\qquad \mathcal{V} = T[\mathcal{H}]^\perp. \]
\end{theorem}
\begin{proof}
The subspace $\mathcal{V} = T[\mathcal{H}]^\perp$ is wandering by \ref{WoldLemma1}. Then $T$ is a unilateral shift in the subspace
\[ \mathcal{H}_1 = \bigoplus_{n=0}^\infty T^n[\mathcal{V}]. \]
Now $v\in\mathcal{H}_0 = \mathcal{H}_1^\perp$ if and only if it is perpendicular to $\bigoplus_{i=0}^n T^i[\mathcal{V}]$ for all $n$ and we have
\begin{align*}
\bigoplus_{i=0}^n T^i[\mathcal{V}] &= \bigoplus_{i=0}^n T^i[\mathcal{H}\ominus T[\mathcal{H}]] = \bigoplus_{i=0}^n T^i[\mathcal{H}]\ominus T^{i+1}[\mathcal{H}] \\
&= (\mathcal{H}\ominus T[\mathcal{H}])\oplus(T[\mathcal{H}]\ominus T^2[\mathcal{H}])\oplus \ldots \oplus (T^n[\mathcal{H}]\ominus T^{n+1}[\mathcal{H}])  = \mathcal{H} \ominus T^{n+1}[\mathcal{H}] 
\end{align*}
using \ref{perpUnderIsometry} and \ref{cancellationOminus}, which is applicable because $T^i[\mathcal{V}]$ is closed by \ref{isometryClosed}. So $\mathcal{H}_0\subseteq T^n[\mathcal{H}]$ for all $n$.

Finally $T|_{\mathcal{H}_0}$ is unitary because it is an isometry and surjective on $\mathcal{H}_0$.
\end{proof}

\subsubsection{Left and right shifts on $\ell^2$}
\begin{definition}
Consider the space $\ell^2(\N)$ with o.n. basis $\seq{e_i}$. Then
\begin{itemize}
\item the \udef{right shift operator} $S_r$ is the operator that maps $e_i \mapsto e_{i+1}$;
\item the \udef{left shift operator} $S_l$ is the operator that maps $e_i \mapsto \begin{cases}
e_{i-1} & i \geq 1 \\ 0 & i = 0
\end{cases}$.
\end{itemize}
\end{definition}

\begin{lemma}
$S_r$ is a unilateral shift
\end{lemma}

\begin{proposition}
$S_r = S^*_l$ (also converse?)
\end{proposition}

\subsubsection{Partial isometries}
\begin{definition}
An operator $T\in \Lin(H, H')$ is called a \udef{partial isometry} if there is a closed subspace $K\subseteq H$ such that
\begin{itemize}
\item $T|_K$ is an isometry;
\item $T|_{K^\perp} = 0$.
\end{itemize}
\end{definition}

Clearly every partial isometry is bounded.

\begin{lemma}
An operator $T\in \Lin(H, H')$ is a partial isometry \textup{if and only if} $T|_{\ker(T)^\perp}$ is an isometry.
\end{lemma}

\begin{proposition} \label{partialIsometryEquivalences}
Let $T\in \Bounded(H,H')$. The following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry;
\item $T^*TT^* = T^*$;
\item $TT^*T = T$;
\item $TT^*: H' \to H'$ is a projection;
\item $T^*T: H \to H$ is a projection;
\item $T^*$ is a partial isometry.
\end{enumerate}
Moreover,
\begin{enumerate}
\item $T^*T$ is the projection onto $\ker(T)^\perp$;
\item $\im(T)$ is closed and $TT^*$ is the projection onto $\im(T)$.
\end{enumerate}
\end{proposition}
\begin{proof}

$\boxed{(1)\Rightarrow (2)}$ By \ref{elementaryOrthogonality} it is enough to show that $\inner{T^*TT^*x,y} = \inner{T^*x,y}$ for all $x\in H', y\in H$. Take such $x,y$. We decompose $y = y_1\oplus y_2 \ker(T)\oplus \ker(T)^\perp$. Then
\[ \inner{T^*TT^*x, y_1} = \inner{TT^*x, Ty} = 0 = \inner{x,Ty_1} = \inner{T^*x, y_1} \]
and
\[ \inner{T^*TT^*x, y_2} = \inner{TT^*x, Ty_2} = \inner{T^*x,y_2}, \]
where we have used the fact that both $y_2$ and $T^*x$ are elements of $\ker(T)^\perp = \overline{\im(T^*)}$, and $T$ is an isometry on this space. In conclusion, we have
\[ \inner{T^*TT^*x,y} = \inner{T^*TT^*x,y_1} + \inner{T^*TT^*x,y_2} = \inner{T^*x,y_1} + \inner{T^*x,y_2} = \inner{T^*x,y} \]
for all $x\in H', y\in H$, so $T^*TT^* = T^*$.

$\boxed{(2) \Leftrightarrow (3)}$ By taking adjoints: $(TT^*T)^* = T^*TT^*$.

$\boxed{(2) \Rightarrow (4,5)}$ Clearly $T^*T$ and $TT^*$ are self-adjoint. We just need to show idempotency:
\[ (T^*T)^2 = (T^*T)(T^*T) = (T^*TT^*)T = T^*T \qquad (TT^*)^2 = (TT^*)(TT^*) = T(T^*TT^*) = TT^*. \]

$\boxed{(4) \Rightarrow (1)}$ Assume $TT^*$ a projection. Let $v\in \ker(T)^\perp = \overline{\im(T^*)}$. Then there exists a sequence $\seq{v_n}\in H^{\prime\N}$ such that $\lim_{n\to\infty}T^*v_n = v$. Then
\begin{align*}
\norm{Tv}^2 &= \lim_{n\to\infty}\norm{TT^*v_n}^2 = \lim_{n\to\infty}\inner{TT^*v_n,TT^*v_n} \\
&= \lim_{n\to\infty}\inner{(TT^*)^2v_n,v_n} = \lim_{n\to\infty}\inner{TT^*v_n,v_n} \\
&= \lim_{n\to\infty}\inner{T^*v_n,T^*v_n} = \lim_{n\to\infty}\norm{T^*v_n}^2 = \norm{v}^2,
\end{align*}
so $T$ is a partial isometry.

$\boxed{(5,6)}$ Applying the proposition to $T^*$ instead of $T$ yields the equivalences with $T=TT^*T$, and thus with the rest of the statements.

TODO + $\im(T^*) = \ker(T)^\perp$ means support and range are exchanged between $T$ and $T^*$.
\end{proof}

\begin{definition}
Let $T$ be a partial isometry. We call
\begin{itemize}
\item $T^*T$ the \udef{support projection} or \udef{initial projection} of $T$;
\item $TT^*$ the \udef{range projection} or \udef{final projection} of $T$.
\end{itemize}
\end{definition}

\begin{proposition}
Let $H,H'$ be Hilbert spaces with $K\subseteq H$ and $L\subseteq H'$ closed subspaces. Then the following are equivalent:
\begin{enumerate}
\item $T$ is a partial isometry with support $K$ and range $L$;
\item $(T,T^*)$ is a Galois connection between $\sSet{H, \perp_K}$ and $\sSet{H', \perp_L}$.
\end{enumerate}
Here $\perp_K$ is defined by
\[ x \perp_K y \quad\defequiv\quad P_K(x)\perp P_{K}(y). \]
\end{proposition}
\begin{proof}
The direction $(2) \Rightarrow (1)$ is immediate from \ref{partialIsometryEquivalences}, because $T,T^*$ are generalised inverses.

For the other direction, we first prove $T$ preserves the relational structure. Take arbitrary $x= x_1+x_2$ and $y=y_1+y_2$ in $K\oplus K^\perp$ such that $x\perp_K y$. Then
\[ \inner{T(x), T(y)} = \inner{T(x_1), T(y_1)} = \inner{x_1, y_1} = 0. \]
So $T(x)\perp T(y)$ and, because $T(x), T(y) \in L$, we have $T(x)\perp_L T(y)$. The argument for $T^*$ is similar.

For the Galois condition, we need to show that $T^*T(x)\perp_K y \implies x\perp_K y$. Indeed
\begin{align*}
T^*T(x)\perp_K y &\iff T^*T(x_1)\perp y_1 \\
&\iff 0= \inner{T^*T(x_1), y_1} = \inner{T(x_1), T(y_1)} = \inner{x_1,y_1} \\
&\iff P_K(x)\perp P_K(y).
\end{align*}
\end{proof}
\begin{corollary}
Let $T: H\to H'$ be a partial isometry with support $K$ and range $L$. Then
\[ T(x) \perp P_L(y) \iff P_K(x) \perp T^*(y) \]
for all $x\in H, y\in H'$.
\end{corollary}
\begin{proof}
This is the Galois identity \ref{GaloisIdentity}, although the direct proof is also very simple.
\end{proof}

\subsubsection{Unitaries}
\paragraph{Bilateral shifts}


\section{Dirac notation}
\url{https://core.ac.uk/download/pdf/25263496.pdf}
\url{https://michael-herbst.com/talks/2014.07.22_Mathematical_Concept_Dirac_Notation.pdf}
\url{http://galaxy.cs.lamar.edu/~rafaelm/webdis.pdf}
\url{https://plato.stanford.edu/entries/qt-nvd/}
\url{file:///C:/Users/user/Downloads/Abdus%20Salam,%20E.P.%20Wigner%20(Ed.)%20-%20Aspects%20of%20Quantum%20Theory%20-%20Dedicated%20to%20Dirac%E2%80%99s%2070th%20Birthday-Cambridge%20University%20Press%20(1972).pdf}
\url{https://aip.scitation.org/doi/pdf/10.1063/1.1705001}

\begin{lemma}
\begin{enumerate}
\item $T\ketbra{\varphi}{\psi} = \ketbra{T\varphi}{\psi} = \ketbra{\varphi}{\psi}T = \ketbra{\varphi}{T^*\psi}$;
\item $\ketbra{\varphi}{\psi}\ketbra{\xi}{\eta} = \inner{\psi, \xi}\ketbra{\varphi}{\eta}$;
\item $(\ketbra{\varphi}{\psi})^* = \ketbra{\psi}{\varphi}$.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $H$ be a Hilbert space and $\seq{e_i}_{i\in I}$ a basis for $H$. Then
\[ \id_H = \sum_{i\in I}\ketbra{e_i}{e_i} \qquad\text{in the strong limit.} \]
\end{lemma}
\begin{proof}
TODO!!
\end{proof}
\begin{lemma} \label{operatorBraketExpansion}
Let $H$ be a Hilbert space, $\seq{e_i}_{i\in I}$ a basis for $H$ and $T$ an operator on $H$. Then
\[ T = \sum_{i,j\in I}\braket[T]{e_i}{e_j}\; \ketbra{e_i}{e_j}. \]
in the strong limit.
\end{lemma}
\begin{proof}
TODO!! Tannery.
\end{proof}

\section{Hilbert space ideals}

\subsection{Finite-rank operators}
Remember that finite-rank operators are bounded by definition (this is not automatic, cfr. \ref{continuousMapCriterion}).

\begin{proposition}[Finite rank singular value decomposition] \label{finiteRankSingularValues}
Let $V$ be an inner product space and $T\in\Hom(V)$. Then $T$ is a finite-rank operator \textup{if and only if} $T$ can be written in the form
\[ T = \sum_{i=1}^N \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^N$ and $(w_i)_{i=1}^N$ are finite sets of vectors and $(\lambda_i)_{i=1}^N$ are positive (non-zero) numbers.

The numbers $(\lambda_i)_{i=1}^N$ in this decomposition are uniquely determined by the operator.
\end{proposition}
The numbers $(\lambda_i)_{i=1}^N$ are called the \udef{singular values} of the operator.
\begin{proof}
Because $\im(T)$ is finite-dimensional, we can find an orthonormal basis $(v_i)_{i=1}^N$ for it. Then we can write
\begin{align*}
Tx &= \sum_{i=1}^N \ket{v_i}\braket{v_i}{Tx} = \sum_{i=1}^N \ket{v_i}\braket{T^*v_i}{Tx} = \sum_{i=1}^N \ket{v_i}\braket{\lambda_i w_i}{Tx}  = \sum_{i=1}^N \lambda_i\ket{v_i}\braket{w_i}{Tx}
\end{align*}
where $\lambda_i = \norm{T^*v_i}$ and $w_i = \frac{T^*v_i}{\lambda_i}$.

We just need to show that the $\lambda_i$ are independent of the chosen basis $(v_i)_{i=1}^N$. TODO!!!!
\end{proof}
\begin{corollary}
Every finite-rank operator on a Hilbert space is a finite sum of rank-1 operators.
\end{corollary}

\begin{lemma}
Let $H$ be Hilbert space. The set of finite rank operators on $H$ is a $*$-ideal in $H$.
\end{lemma}

\subsection{Compact operators}
\begin{proposition}
Let $T\in\Bounded(H)$. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item $T^*$ is compact;
\item there exists a sequence $(T_n)_{n\in\N}$ of finite rank operators such that $\norm{T-T_n}\to 0$.
\end{enumerate}
\end{proposition}
This is false in Banach spaces. (TODO Enflo, approximation property, goose problem)
\begin{proof}
TODO
\end{proof}
\begin{corollary}[Canonical expansion]
Any compact operator $T$ on a Hilbert space $\mathcal{H}$ can be written in the form
\[ T = \sum_{i=1}^\infty \lambda_i \ketbra{v_i}{w_i}, \]
where $(v_i)_{i=1}^\infty$ and $(w_i)_{i=1}^\infty$ are orthonormal sets and $(\lambda_i)_{i=1}^\infty$ is a monotonically decreasing sequence of positive numbers with $\lim_{i\to\infty}\lambda_i = 0$.
\end{corollary}
As in \ref{finiteRankSingularValues} for finite-rank operators we call $(\lambda_i)_{i=1}^\infty$ the \udef{singular values} of $T$. They are uniquely determined by the operator.
\begin{proof}
TODO (one way is with polar decomposition and spectral theorem. Are there others?)
\end{proof}
Compare with \ref{operatorBraketExpansion}.

\begin{lemma}
Let $H$ be a Hilbert space. Then the set of compact operators on $H$, $\Compact(H)$ is a two-sided $*$-ideal of $H$. 
\end{lemma}

\begin{proposition}
Let $H$ be a Hilbert space with orthonormal basis $(e_i)_{i\in I}$. If $T\in\Bounded(H)$ and
\[ \sum_{i\in I}\norm{Te_i}^2  < \infty, \]
then $T$ is a compact operator. + Converse??
\end{proposition}
\begin{proof}
TODO + weaken $T\in\Bounded(H)$?
\end{proof}
\begin{corollary}
An integral operator defined by a square integrable kernel $K\in L^2(A\times A, \mu)$ is compact.
\end{corollary}

\begin{proposition}
Let $T$ be an operator on a Hilbert space. Then the following are equivalent:
\begin{enumerate}
\item $T$ is compact;
\item for all sequences $\seq{x_n}$, weak convergence $x_n \overset{w}{\to} x$ implies the strong convergence $Ax_n \to Ax$;
\item for any two weakly convergent sequences $x_n\overset{w}{\to} x$ and $y_n\overset{w}{\to} y$ the energy form is continuous in both arguments:
\[ \lim_{n\to\infty}\inner{x_n,y_n}_T = \lim_{n\to\infty}\inner{x_n,Ty_n} = \inner{x,Ty} = \inner{x,y}_T. \]
\end{enumerate} 
\end{proposition}

\begin{lemma}
Let $H$ be a Hilbert space and $P\in\Projections(H)$. If $P$ is compact, then $P$ has finite rank.
\end{lemma}

\subsection{Positive operators}

\subsubsection{Polar decomposition}
\begin{proposition}
Let $H$ be a Hilbert space and $T\in \Bounded(H)$. There exists a unique partial isometry $V$ such that $T = V|T|$ and $\ker(V) = \supp(T)$.
\end{proposition}
TODO: should this be $\ker(V) = \ker(T)$??
\begin{proof}
TODO
\end{proof}
\begin{lemma}
There is only one positive operator $A$ such that $T = VA$ for some partial isometry.
\end{lemma}
\begin{proof}
TODO uniqueness positive squared root.
\end{proof}

\url{https://encyclopediaofmath.org/wiki/Polar_decomposition}

\subsection{Trace class operators}
TODO Simon


\section{Dilation theory}
\subsection{Dilations, $N$-dilations and power dilations}
\begin{definition}
Let $\mathcal{H} \subseteq \mathcal{H}'$ be Hilbert spaces and let $P_\mathcal{H}$ be the projection on $\mathcal{H}$. If a pair of linear maps $S: \mathcal{H}'\to\mathcal{H}'$ and $T: \mathcal{H}\to \mathcal{H}$ satisfy the relation
\[ T = P_\mathcal{H} S |_\mathcal{H} \]
then $T$ is called a \udef{compression} of $S$ and $S$ a \udef{dilation} of $T$. This is abbreviated $T\prec U$.

\begin{itemize}
\item Let $N\in\N$. If $T^k = P_\mathcal{H} S^k |_\mathcal{H}$ for all $k\leq N$, then $S$ is called an \udef{$N$-dilation}.
\item If this holds for all $k\in\N$, then $S$ is called a \udef{power dilation}.
\item If $T^* = P_\mathcal{H} S^* |_\mathcal{H}$, we call TODO??
\end{itemize}
We call $\mathcal{H}'$ \udef{minimal} if the only reducing subspace for $S$ that contains $\mathcal{H}$ is $\mathcal{H}'$.
\end{definition}

If $S$ is a dilation of $T$, then we clearly have $T = P_\mathcal{H} S P_\mathcal{H}|_\mathcal{H}$.

\begin{lemma}
Let $S:\mathcal{H}'\to\mathcal{H}'$ be an $N$-dilation of $T: \mathcal{H}\to \mathcal{H}$ and $p$ a polynomial of degree at most $N$. Then
\[ p(T) = P_\mathcal{H}p(S)|_\mathcal{H}. \]
\end{lemma}

Let $\mathcal{H}$ be a Hilbert space. We call $T\in\Bounded(\mathcal{H})$ a \udef{contraction} if $\norm{T}\leq 1$.
\begin{proposition} \label{dilationOfContraction}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
From $\norm{T}\leq 1$ (and the fact that $T^*T$ is normal), we have that $\vec{1}-T^*T\geq 0$ by spectral mapping. We can define $D_T = \sqrt{\vec{1}-T^*T}$. Then
\[ U = \begin{pmatrix}
T & D_{T^*} \\ D_T & -T^*
\end{pmatrix} \]
is a dilation of $T$ and it is unitary:
\begin{align*}
UU^* &= \begin{pmatrix}
TT^* + D_{T^*}^2 & TD_T^* - D_{T^*}T \\
D_TT^* - T^*D_{T^*}^* & D^2_{T} + T^*T
\end{pmatrix} = \begin{pmatrix}
\vec{1} & TD_T - D_{T^*}T \\
D_TT^* - T^*D_{T^*} & \vec{1}
\end{pmatrix} \\
U^*U &= \begin{pmatrix}
T^*T + D_{T}^2 & T^*D_{T^*} - D_{T}^*T^* \\
D_{T^*}^*T - TD_{T} & D^2_{T^*} + TT^*
\end{pmatrix} = \begin{pmatrix}
\vec{1} & T^*D_{T^*} - D_{T}T^* \\
D_{T^*}T - TD_{T} & \vec{1}.
\end{pmatrix}
\end{align*}
We have used that $D_T$ is self-adjoint for all contractions $T$. We just need to show that $TD_T = D_{T^*}T$. Clearly we have
\[ T(D_T)^2 = T(\vec{1} - T^*T) = T - TT^*T = (\vec{1} - TT^*)T = (D_{T^*}T)^2T. \]
By functional-like calculus (TODO!!) we have $TD_T = D_{T^*}T$.
\end{proof}
The operator $D_T$ in the previous proof is sometimes called the \udef{defect operator} of $T$. It measures in some sense how far $T$ is from being a unitary operator. If $T$ is unitary, then $D_T = 0 = D_{T^*}$. If $T$ is an isometry, then $D_T = 0$ (by \ref{isometryRangeProjection}) and $D_{T^*}$ is a projector ($TT^*$ is a projector by \ref{isometryCharacterisation}, so $\vec{1} - TT^*$ is too by \ref{projectorOrthogonalComplement} and $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$).

\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\} \subseteq \mathcal{H}\oplus \mathcal{H} = \mathcal{H}^2$ be a Hilbert space. Every isometry $T$ on $\mathcal{H}$ has a unitary power dilation $U$ on $\mathcal{H}^2$.
\end{proposition}
\begin{proof}
Consider the unitary dilation of \ref{dilationOfContraction}. When $T$ is an isometry this reduces to
\[ U = \begin{pmatrix}
T & D_{T^*} \\ 0 & -T^*
\end{pmatrix} = \begin{pmatrix}
T & \vec{1}-TT^* \\ 0 & -T^*
\end{pmatrix}, \]
where we have used that $D_{T^*} = \sqrt{\vec{1}-TT^*} = \sqrt{(\vec{1}-TT^*)^2} = \vec{1}-TT^*$ is a projector.

Now for all $n\in\N$ we have $U^n = \begin{pmatrix}
T^n & * \\ 0 & (-T^*)^n
\end{pmatrix}$, so in particular $P_\mathcal{H}U^n|_\mathcal{H} = T^n$, meaning $U$ is a power dilation of $T$. 
\end{proof}

\begin{lemma}
Let $T$ a contraction on a Hilbert space $\mathcal{H}$. Then $V_T: \mathcal{H} \to \mathcal{H}\oplus\mathcal{H}: x\mapsto (Tx, D_Tx)$ is an isometry.
\end{lemma}
\begin{proof}
For all $x\in \mathcal{H}$ we have
\[ \norm{V_Tx} = \sqrt{\norm{Tx}^2 + \norm{D_Tx}^2} = \sqrt{\inner{Tx,Tx} + \inner{D_Tx,D_Tx}} = \sqrt{\inner{T^*Tx,x} + \inner{D_T^2x,x}} = \sqrt{\inner{x,x}} = \norm{x}. \]
\end{proof}

\begin{proposition}
Let $\mathcal{H} \cong \mathcal{H}\oplus \{0\}^N \subseteq \mathcal{H}^{N+1}$ be a Hilbert space. Every contraction $T$ on $\mathcal{H}$ has a unitary $N$-dilation $U$ on $\mathcal{H}^{N+1}$.
\end{proposition}
\begin{proof}
Let $U'$ be a unitary dilation of $T$ on $\mathcal{H}^2$. Let $C_1 = U'_{-,1}$ and $C_2 = U'_{-,2}$ denote the columns. Then
\[ U = \begin{pmatrix}
C_1 & \mathbb{0}^{2\times N-1} & C_2 \\
\mathbb{0}^{N-1\times 1} & \mathbb{1}^{N-1\times N-1} & \mathbb{0}^{N-1\times 1}
\end{pmatrix} \]
is unitary by
\[ \begin{pmatrix}
C_1^* & \mathbb{0} \\
\mathbb{0} & \mathbb{1} \\
C_2^* & \mathbb{0}
\end{pmatrix}\begin{pmatrix}
C_1 & \mathbb{0} & C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0}
\end{pmatrix} = \begin{pmatrix}
C_1^*C_1 & \mathbb{0} & C_1^*C_2 \\
\mathbb{0} & \mathbb{1} & \mathbb{0} \\
C_2^*C_1 & \mathbb{0} & C_2^*C_2
\end{pmatrix} = \mathbb{1}^{N+1\times N+1}. \]
We just need to show that the (1,1)-component of $U^k$ is $T^k$ for all $k\in 1:N$. In order to perform the multiplication, we rewrite $U$ such that the row and column partitions are the same, i.e.\ $(2|(N-3)|2)\times (2|(N-3)|2)$:
\[ U = \begin{pmatrix}
\begin{bmatrix}
T & 0 \\ D_T & 0
\end{bmatrix} & \mathbb{0} & \begin{bmatrix}
0 & D_{T^*} \\ 0 & -T^*
\end{bmatrix} \\
\begin{bmatrix}
0 & 1 \\ \mathbb{0} & \mathbb{0}
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 0 \\ \mathbb{1} & \mathbb{0}
\end{bmatrix} & \mathbb{0} \\
\begin{bmatrix}
0 & 0 \\ 0 & 0
\end{bmatrix} & \begin{bmatrix}
\mathbb{0} & 1 \\ \mathbb{0} & 0
\end{bmatrix} & \begin{bmatrix}
0 & 0 \\ 1 & 0
\end{bmatrix}
\end{pmatrix} \]
TODO
\end{proof}

\begin{proposition}[von Neumann's inequality]
Let $T$ be a contraction on some Hilbert space $\mathcal{H}$. Then, for every polynomial $p\in\C[z]$,
\[ \norm{p(T)}\leq \sup_{|z|=1}|p(z)|. \]
\end{proposition}
\begin{proof}
Suppose the degree of $p$ is $N$. Let $U$ be a unitary $N$-dilation of $T$. Then
\[ \norm{p(T)} = \norm{P_\mathcal{H}p(U)|_\mathcal{H}}\leq \norm{p(U)} = \sup_{z\in\sigma(U)}|p(z)| \leq \sup_{|z|=1}|p(z)| \]
since the spectrum of $U$ is contained in the unit circle.
\end{proof}

\begin{theorem}[Sz.-Nagy's dilation theorem]
Let $\mathcal{H} \subseteq \ell^2(\N)\otimes\mathcal{H}$ be Hilbert spaces. Every contraction on $\mathcal{H}$ has a unitary power dilation on $\ell^2(\N)\otimes\mathcal{H}$.
\end{theorem}




\section{Constructions}
\subsection{Direct sum}
\subsection{Tensor product}
\url{https://web.ma.utexas.edu/mp_arc/c/14/14-2.pdf}



\chapter{Types of operators}
\section{Fredholm operators}
\begin{definition}
An operator $T\in\Bounded(X,Y)$ between Banach spaces is called a \udef{Fredholm operator} if $T$ has a finite-dimensional kernel and cokernel.

The \udef{Fredholm index} of $T$ is defined as
\[ \Index T \defeq \dim\ker T - \dim\coker T.  \]

We denote the space of Fredholm operators from $X$ to $Y$ as $\Fred(X,Y)$. If $X=Y$, we write $\Fred(X)$.
\end{definition}

\begin{example}
\begin{enumerate}
\item If $X=Y$ is finite-dimensional, then all operators are Fredholm with index $0$.
\item The left shift $S_l:\ell^2(\N)\to\ell^2(\N): (x_n)_n\mapsto (x_{n+1})_n$ has index $1$.
\item The right shift $S_r = S_l^*$ has index $-1$.
\end{enumerate}
\end{example}

\begin{lemma}
A Fredholm operator has closed range.
\end{lemma}

\begin{lemma}
Let $T\in\Bounded(H)$ be a bounded operator on a Hilbert space. Then $\dim\coker T = \dim\ker T^*$.
\end{lemma}
\begin{proof}
TODO (is it correct?) $\ker(T^*) = \im(T)^\perp$.
\end{proof}


\begin{proposition}
Let $S,T\in\Fred(X)$, $\lambda\in\F$ and $K\in\Compact(X)$. Then
\begin{enumerate}
\item $\Index(ST) = \Index(S)+\Index(T)$;
\item $\Index(T+K) = \Index(T)$;
\item $\Index(\lambda T) = \Index(T)$, if $\lambda \neq 0$;
\item $\Index(T) = 0$ \textup{if and only if} $T=K'+L$ for some compact $K'$ and invertible $L$.
\end{enumerate}
Let $T\in\Fred(H)$ for some Hilbert space $H$. Then
\begin{enumerate} \setcounter{enumi}{4}
\item $\Index(T^*) = -\Index(T)$.
\end{enumerate}
\end{proposition}
TODO: integrate with corollary??

\begin{lemma}
Let the commutative diagram
\[ \begin{tikzcd}
0 \rar & X \dar{T} \rar & Y \dar{S} \rar & Z \dar{R} \rar & 0 \\
0 \rar & X \rar & Y \rar & Z \rar & 0
\end{tikzcd} \]
have short exact rows. If any two of $T,S,R$ are Fredholm, then so is the third and
\[ \Index S = \Index T + \Index R. \]
\end{lemma}
\begin{proof}
TODO snake lemma to obtain long exact
\[ 0\to \ker T \to \ker S\to \ker R \to \coker T \to \coker S \to \coker R \to 0. \]
\end{proof}
\begin{corollary} \mbox{}
\begin{enumerate} 
\item Let $T\in\Fred(X)$ and $S\in\Fred(Y)$ be Fredholm, then so is $T\oplus S$ with
\[ \Index(T\oplus S) = \Index(T)+\Index(S). \]
\item Let $T\in\Fred(X,Y)$ and $S\in\Fred(Y,Z)$ be Fredholm, then so is $ST$ with
\[ \Index(ST) = \Index(T)+\Index(S). \]
\item Let $K\in\Compact(X)$ be compact, then $\id_X+K$ is Fredholm with
\[ \Index(\id_X+K) = 0. \]
\end{enumerate}
\end{corollary}


\begin{lemma}[Fredholm alternative] \label{FredholmAlternative}
Let $T$ be a Fredholm operator of index zero. Then either $T$ is bijective, or it is neither injective nor surjective.
\end{lemma}
\begin{proof}
The operator $T$ is injective iff $\dim\ker(T) = 0$ and surjective iff $\dim\coker(T) = 0$.
\end{proof}


\section{Integral operators and transforms}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. Then an \udef{integral operator} or \udef{integral transform} is a map of the form
\[ T: U\subset (\Omega\to\C) \to (\Omega\to\C): f \mapsto \int_\Omega K(x,y)f(y) \diff{\mu(y)} \]
where $K\in (\Omega\times \Omega \to \C)$ is the \udef{kernel} or \udef{nucleus} of $T$.

The kernel is called
\begin{itemize}
\item \udef{symmetric} if $K(x,y) = \overline{K(y,x)}$;
\item \udef{Volterra} if $\Omega = \R$ and $K(x,y) = 0$ for $y>x$;
\item \udef{convolutional} if $\Omega$ is a group and $K(x,y) = F(x-y)$ for some function $F$;
\item \udef{Hilbert-Schmidt} if $K\in L^2(\Omega\times \Omega)$, i.e.\
\[ \int_{\Omega\times \Omega}|K(x,y)|^2\diff{x}\diff{y} < \infty; \]
\item \udef{singular} if $K(x,y)$ is unbounded on $\Omega\times \Omega$.
\end{itemize}
\end{definition}

\begin{lemma}
Hilbert-Schmidt integral operators are compact operators on $L^2(\Omega\times \Omega)$.
\end{lemma}
\begin{proof}
A Hilbert-Schmidt integral operator $T$ maps $L^2(\Omega)$ to $L^2(\Omega)$ functions:
\begin{align*}
\norm{Tu}^2_{L^2} &= \int_\Omega \left|\int_{\Omega} K(x,y)u(y)\diff{\mu(y)}\right|^2\diff{\mu(x)} \\
&\leq \int_\Omega \left(\int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg)\diff{\mu(x)} \\
&= \left(\int_\Omega \int_{\Omega} |K(x,y)|^2\diff{\mu(y)}\diff{\mu(x)}\right) \bigg( |u(y)|^2\diff{\mu(y)}\bigg) < \infty
\end{align*}
where we have used the Cauchy-Schwarz inequality. This also immediately shows Hilbert-Schmidt integral operators are bounded.

TODO Compact
\end{proof}

\begin{proposition}
Let $T$ be an integral operator with kernel $K(x,y)$, then $T^*$ is the integral operator with kernel $\overline{K(y,x)}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $A$ be a Borel set and $K:A\times A\to \C$ a measurable function such that the integral operator with kernel $K$ is bounded. Then the adjoint of the integral operator is again an integral operator with kernel $K^*(x,y) = \overline{K(y,x)}$.
\end{proposition}

\begin{proposition}
Let $T$ be a Volterra integral operator. Then $\spec(T) = \cspec(T) = \{0\}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsection{Integral equations}
\begin{definition}
Let $(\Omega, \mathcal{A}, \mu)$ be a measure space. An \udef{integral equation} is an equation containing an unknown function on $\Omega$ and an integral over $\Omega$.

An integral equation is 
\begin{itemize}
\item \udef{of the first kind} if it is of the form
\[ \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function and $u$ is the unknown function;
\item \udef{of the second kind} if it is of the form
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x) \qquad x\in \Omega \]
where $f$ is a given function, $\lambda$ is a scalar and $u$ is the unknown function.
\end{itemize}
\end{definition}

\begin{proposition}
Let
\[ \lambda u(x) - \int_\Omega K(x,y)u(y)\diff{\mu(y)} = f(x)\]
be an integral equation of the second kind. This integral equation has a unique solution $u$ if
\[ |\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}. \]
\end{proposition}
\begin{proof}
Let the map $T$ be defined by
\[ T(u) = x\mapsto \frac{1}{\lambda}\left(\int_\Omega K(x,y)u(y)\diff{\mu(y)} + f(x)\right) \]
so that solutions of the integral equation are exactly the fixed points of $T$. Then
\[ \norm{Tu-Tv}_\infty = \sup_{x\in\Omega} \frac{1}{|\lambda|} \left|\int_\Omega K(x,y)(u(y)- v(y))\diff{\mu(y)}\right| \leq \frac{1}{|\lambda|} \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)} \cdot \norm{u-v}_\infty. \]
So $T$ is a contraction if $|\lambda| > \sup_{x\in \Omega} \int_{\Omega}|K(x,y)|\diff{\mu(y)}$. The result follows from \ref{contractionFixedPoint}.
\end{proof}

\section{Convolution operators}





\chapter{Fourier transforms}

\section{Types of Fourier transform}

\subsection{Discrete Fourier transform}
\begin{definition}
Then $N$-dimensional \udef{discrete Fourier transform} (DFT) is the linear transformation $\C^N \to \C^N$ defined by the matrix $DFT_N$ with components
\[ [DFT_N]_{j,k} = \frac{1}{\sqrt{N}}\omega_N^{(j-1)(k-1)}, \]
where $\omega_N$ is the $N^\text{th}$ root of unity.
\end{definition}

\begin{lemma} \mbox{}
\begin{enumerate}
\item The $DFT_N$ matrix is the Vandermonde matrix of the roots of unity, up to the normalisation factor $1/\sqrt{N}$.
\item The $DFT_N$ matrix is unitary.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Just an observation.

(2) We calculate
\[ [DFT_N\cdot DFT_N]_{j,l} = \frac{1}{N}\sum_{k=1}^N\omega_N^{jk}\overline{\omega_N}^{kl} = \frac{1}{N}\sum_{k=1}^N\omega_N^{k(j-l)} = \delta_{j,l}. \]
\end{proof}
