\section{Exploring the concept of change}
TODO: diffeomorphism

In physics how things change is quite important. Much of physics is concerned with the question of, given a particular system at a particular time, how that system will evolve.

We have not yet really introduced a mathematical construct that expresses an idea of change. We will do so here.

In particular we will consider ways to express how the output of a function changes if we (slightly) change its input.

To motivate the discussion below, consider the function represented by the graph in figure TODO. 

Locally at any one point the rate of change of the function can be described using the slope at that point. That makes intuitive sense; when walking up a mountain the slope is a measure for how quickly the altitude changes.

The slope between two points can be calculated by dividing the vertical distance by the horizontal distance. This definition of slope obviously depends on two points. We would quite like to be able to talk about the slope at a single point (the way we would intuitively when walking up a hill). To do that we can just bring both points very close together.

As can be seen on the picture this procedure gives the slope of the tangent line at that point (straight lines have a constant slope).

\subsection{Speed}

At this point we can give an important physical motivating example, namely the speed of an object. Say we throw an apple straight up into the air. Its vertical movement is plotted in figure TODO.

We may want to know its speed at different times. We can calculate speed by taking the displacement and dividing it by the time it takes traverse that distance. We can now make an important distinction between average speed (the slope between two distinct points) and instantaneous speed (the limit when we bring both points together).

\section{The derivative}
As motivated above, the rate of change of a (real) function is the difference in output divided by the difference in input of two points:
\[ \frac{f(y) - f(x)}{y-x}. \]
We conventionally call $h = y-x$. We can then write the above quantity (which is called the \udef{Newton quotient}) as
\[ \frac{f(x+h) - f(x)}{x+h-x} = \frac{f(x+h) - f(x)}{h}. \]
The \udef{derivative} of $f$ at $x$ is then just the limit of the Newton quotient with $h$ going to zero.

This limit does not always exist. If the limit exists for all $x$, the function is called \udef{differentiable}. A function may also be differentiable in some points and not in others.

We can now use the definition and properties of limits to calculate derivatives, such as in the following example. This process is slow and laborious even for relatively simple functions. Luckily the derivative has some important properties that lets us calculate the derivative of many functions with relative ease.

We can define a (real) function that, for any input, calculates the derivative of a particular fixed function $f$ at that point and gives that as its output. This new function is often called the derivative of the function $f$. 

There are many ways to write the derivative of $f$:
\[ \lim_{h\to 0} \frac{f(x+h)-f(h)}{h} \equiv f'(x) \equiv \od{f}{x} \equiv \od{f(x)}{x} \]
A mathematician would want me to emphasize that the expression $\od{f}{x}$ should be read as a whole and is technically \emph{not} a division, but a physicist would say that (in some situations) it can be viewed as such, where $\div{f}$ and $\div{x}$ are (the in this context relevant) infinitesimal variations of $f$ and $x$. Do not tell any mathematicians I said this.

\begin{example}
TODO derivative of polynomial function using limits.
\end{example}

In certain situations a dot is used to indicate a derivative with respect to time (i.e.\ the derivative of a quantity in function of time). So we might for example use $x(t)$ to denote the position in function of time (here $x$ is \emph{not} used to refer to a variable but to a function, the notation is standard and usually it clear from the context what $x$ refers to). We can then use the notation
\[ x'(t) \equiv \dot{x}(t). \]
In fact $\dot{x}(t)$, is just the speed.

When using the notation $\od{f}{x}$, this usually refers to the function that is the derivative of $f$. If we want to evaluate this function in a particular point (say $x_0$), we can write something like this
\[ \left.\od{f}{x}\right|_{x=x_0}. \]

\subsection{Slope of a curve}
\[ \text{slope of the normal} = \frac{-1}{\text{slope of the tangent}} \]

\subsection{Properties of the derivative}
Here we give some properties of the derivative:
\begin{itemize}
\item The derivative is a linear operation:
\[ (f+g)'(x) = f'(x) + g'(x) \]
and
\[ (c\cdot f)'(x) = c\cdot f'(x) \qquad \forall c \in \R \]
\item \ueig{Product rule}
\[ (f\cdot g)'(x) = f(x)\cdot g'(x) + f'(x)g(x) \]
\item The derivative of $\frac{1}{f(x)}$, assuming $f(x) \neq 0$:
\[ \left(\frac{1}{f(x)}\right)' = \frac{f'(x)}{f(x)^2}. \]
\item Combining the previous two properties, we get the quotient rule (assuming $g(x)\neq 0$)
\[ \left(\frac{f(x)}{g(x)}\right)' = \frac{g(x)f'(x) - f(x)g'(x)}{g(x)^2}. \]
\item Finally we have the very important \ueig{chain rule}. This tells us how to take the derivative of composite functions:
\[ (f \circ g)'(x) = f'(g(x))g'(x). \]
We can also write this as
\[ \od{f(g(x))}{x} = \od{f}{g}\od{g}{x}. \]
TODO example
\item Derivative of an inverse
\[ \od{f^{-1}(x)}{x} = \frac{1}{f'(f^{-1}(x))} \]
\end{itemize}

TODO Faà di Bruno

\subsection{Derivatives of some common functions}
Using the results below together with the properties above we can calculate the derivative of a large number of functions.

\begin{itemize}
\item Let $n$ be an integer larger than or equal to $1$ and let $f(x) = x^n$. Then
\[ f'(x) = n x^{n-1}. \]
Using this result together with the property of linearity, we can easily calculate the derivative of any polynomial function. TODO: general exponent
\item The derivatives of the trigonometric functions can be derived from
\[ \sin'(x) = \cos \qquad \text{and} \qquad \cos'(x) = -\sin(x) \]
TODO: list
\item TODO cyclometric
\item TODO hyperbolic
\end{itemize}

\subsubsection{The exponential and logarithm}
Define natural logarithm $\ln$ and \textit{the} exponential function $\exp$.
\[ \od{\ln x}{x} = \frac{1}{x} \]
\[ a^x = e^{x\ln a} \qquad (a>0, x\in \R) \]

\[ e^x = \lim_{n\to \infty}\left(1+\frac{x}{n}\right)^n \]
and growth.

\subsection{Applications of differentiation}
\subsubsection{Extreme values}
Link increasing, decreasing and derivatives. + derivative zero everywhere = constant.
critical points. singulat points. concavity and inflections
\subsubsection{Rolle's lemma}
\subsubsection{Mean-value theorem}
\subsubsection{L'Hôpital's rules}


\subsection{Higher order derivatives}
When we take the derivative of a function, we we get a new function. We can now take the derivative of this new function. This is called taking the second order derivative. This process can be repeated for as long as the derivatives exist. We write the $n$-th order derivative as
\[ f^{(n)}(x) = \od[n]{f}{n}. \]
So for example $f''(x) = f^{(2)}(x)$.

\subsection{Implicit differentiation}

\subsection{Partial derivatives}
\subsubsection{Definition}
\subsubsection{Geometric interpretation}

\subsection{Meaning of the differential $\div{}$}
TODO conventional use + examples with nabla

TODO: put series here!

\subsection{Generalisations and types of derivatives}
TODO: Liebnitz rule!!! + linear.

\section{Integration}
TODO intuition, solving strategies, solving intelligently 
SEE: The electric field (first write all quantities, then )

\subsection{Areas as limits of sums}
\subsubsection{Sums and sigma notation}
\subsubsection{Trapezoid rule}
\subsubsection{Midpoint rule}
\subsubsection{Simpson's rule}

\subsection{The definite integral}
\subsection{Computing different areas and volumes}
\subsubsection{Rotation bodies}
\subsubsection{Surface bounded by function of polar coordinate $\theta$}
\[ \frac{1}{2}\int_{\theta_1}^{\theta_2}[f(\theta)]^2\div{\theta} \]

\subsection{The fundamental theorem of calculus}
\subsubsection{Indefinite integrals}
anti-derivative $+C$
\subsubsection{Some elementary integrals}

\subsection{Properties of integrals}
\subsubsection{Linearity}
\subsubsection{Mean-value theorem}
\subsubsection{Integrals of piece-wise continuous functions}

\subsection{Techniques of integration}
\subsubsection{Integrals of rational functions}
\subsubsection{Substitutions}
+ inverse substitutions
\subsubsection{Integration by parts}


\subsection{Improper integrals}


\subsection{Different types of integrals}
\subsubsection{Riemann}
\subsubsection{Lebesgue}
\subsubsection{Stieltjes}
\subsubsection{Cauchy}

\subsection{From infinite sum to integral}
Using measure

\section{Complex analysis}
holomorphic functions, residue theorem

\subsection{Complex integration and analyticity}
\subsection{Laurent series and isolated singularities}
\subsection{Residue calculus}
\subsection{Conformal mapping}


TODO
Solving intelligently (later using physics): Green functions, method of mirrors (+ cfr. general section on equations)
charge distributions

separation of variables (Legendre polynomials)

going from discrete sum to integral (also opposite with dirac delta). volume int using $\mathcal{V}$ and surface $\mathcal{S}$

surface int goes to zero at infinity.

\section{Dirac delta}
\subsection{In one dimension}
\subsection{In three dimensions}
\subsection{Properties}

\begin{eigenschap}
Composition of the Dirac $\delta$ with a smooth, continuously differentiable function $g$ follows from the following relation
\[ \int_\R \delta(g(x))f(g(x))|g'(x)|\div{x} = \int_{g(\R)} \delta(u)f(u)\div{u} \]
Thus we say that
\[ \delta(g(x)) = \sum_i \frac{\delta(x-x_i)}{|g'(x_i)|}\]
Where $x_i$ are the simple roots of $g$.
\end{eigenschap}

\section{Silly integrals}
\[ \int x^{\diff x}-1 = x\ln(x) - x +c \]