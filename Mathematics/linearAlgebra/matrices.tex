\url{file:///C:/Users/user/Downloads/2013%20Matrix%20Computations%204th(1).pdf}
\url{file:///C:/Users/user/Downloads/(Cambridge%20mathematical%20textbooks)%20Garcia,%20Stephan%20Ramon_%20Horn,%20Roger%20A.%20-%20A%20Second%20Course%20in%20Linear%20Algebra-Cambridge%20University%20Press%20(2017)(1).pdf}


\chapter{Coordinates and matrices}

TODO Haynsworth inertia additivity formula

\section{Coordinates}
In this chapter we will purely be interested in finite-dimensional spaces. From proposition \ref{isomorphicCondition} we know that for any $n$-dimensional vector space $V$ over a field $\mathbb{F}$, $V\cong \mathbb{F}^n$. If we choose a basis $\beta$, we can explicitly give an isomorphism.
\begin{definition}
Let $V$ be an $n$-dimensional vector space with basis $\beta = \{e_1,\ldots, e_n\}$. Because $\beta$ is a basis, we can uniquely write every $v\in V$ as $a_1e_1+\ldots +a_ne_n$. Then we define the \udef{coordinate map w.r.t. $\beta$} as
\[ \co_\beta: V \to \mathbb{F}^n: v \mapsto \co_\beta(v) = \co_\beta(a_1e_1+\ldots +a_ne_n) = \begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}. \]
The vector $\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}$ is called a \udef{coordinate vector}.

The vector $\co_\beta(v)$ is also denoted $[v]_\beta$.
\end{definition}
This coordinate map is indeed an isomorphism.

We conventionally write coordinate vectors as column vectors in $\F^n$. We will represent such vectors in bold type: $\vec{v}\in\F^n$.

\begin{lemma}
Let $\mathcal{E}$ be the standard basis of $\F^n$. Then
\[ \co_\mathcal{E} = \id = \co_\mathcal{E}^{-1}. \]
\end{lemma}

\section{Matrices}
\begin{definition}
A matrix is a rectangular grid of numbers. If it has $m$ rows and $n$ columns, we call it a \udef{$(m\times n)$-matrix}.
If the numbers are elements of the field $\mathbb{F}$, we denote the set of $(m\times n)$-matrices as $\mathbb{F}^{m\times n}$.

If $m=n$,we call the matrix a \udef{square matrix}.
\end{definition}
\begin{example}
An example of a ($2\times 4$)-matrix:
\[ \begin{bmatrix}
1 &2 &3\\4 &5&6
\end{bmatrix} = \begin{pmatrix}
1 &2 &3\\4 &5&6
\end{pmatrix} \]
Sometimes square brackets are used, sometimes parentheses. It's just a matter of style.
\end{example}
Matrices are usually denoted using capital letters.

\begin{definition}
Let $n,m\in\N_0$.
\begin{itemize}
\item The \udef{zero matrix} of dimension $n\times m$ is the $(n\times m)-$matrix
\[ \mathbb{0}^{n\times m} = \begin{pmatrix}
0 & \hdots & 0 \\
\vdots & \ddots & \\
0 & \hdots & 0
\end{pmatrix}. \]
\item The \udef{matrix of ones} or \udef{all-ones matrix} of dimension $n\times m$ is the $(n\times m)-$matrix
\[ \mathbb{J}^{n\times m} = \begin{pmatrix}
1 & \hdots & 1 \\
\vdots & \ddots & \\
1 & \hdots & 1
\end{pmatrix}. \]
\end{itemize}
If $m=n$, we abbreviate these matrices as $\mathbb{0}_n$ and $\mathbb{J}_n$.
\end{definition}

\begin{lemma}
The $(m\times n)$-matrices in $\F^{m\times n}$ naturally form a vector space with point-wise addition and scalar multiplication. 
\end{lemma}

\begin{definition}
We call matrices $A,B$ \udef{conformal} for a certain operation if the operation is defined on these matrices. So $A,B$ are conformal for addition if they have the same dimensions. 
\end{definition}

\subsection{Components of matrices}
The element on the $i^\text{th}$ row and $j^\text{th}$ column of a matrix $A$ is denoted $[A]_{i,j}$ or $a_{i,j}$. These numbers are known as the \udef{components} of the matrix.

Vectors in $\F^n$ can be seen as matrices by writing them as column vectors. In this way we identify $\F^n$ with $\F^{n\times 1}$.

Let $\vec{v}\in \F^n$. The components of $\vec{v}$ are of the form $[\vec{v}]_{i,1}$. We abbreviate this to $[\vec{v}]_i$.

\begin{lemma}
The functions
\[ [-]_{ij}: A\mapsto [A]_{ij} \qquad \text{and} \qquad [-]_i: \vec{v}\mapsto [\vec{v}]_i  \]
are linear.
\end{lemma}

Conversely, consider a set of numbers $a_{i,j}$ where $i\in (1:m), j\in (1:n)$. Then by $[a_{i,j}]$ we mean the matrix consisting of those numbers.

We can also consider components after applying a function. For some linear map $f$, we often write
\[ f_i \defeq [-]_i\circ f. \]

\begin{definition}
Let $A$ be an $(m\times n)$-matrix.
\begin{itemize}
\item If $1\leq j\leq m$, then $[A]_{j,-}$ denotes the $(1\times n)$-matrix consisting of row $j$ of $A$.
\item If $1\leq k\leq n$, then $[A]_{-,k}$ denotes the $(m\times 1)$-matrix consisting of column $k$ of $A$.
\end{itemize}
\end{definition}

\begin{lemma}
Let $A,B\in \F^{m\times n}$ and $\lambda\in \F$, then
\begin{itemize}
\item $[A+B]_{ij} = [A]_{ij} + [B]_{ij}$;
\item $[\lambda A]_{ij} = \lambda[A]_{ij}$.
\end{itemize}
\end{lemma}

\begin{definition}
Let $A\in\F^{m\times n}$ be a matrix. A component $[A]_{i,j}$ is
\begin{itemize}
\item on the \udef{diagonal} if $i=j$;
\item \udef{off-diagonal} $i\neq j$;
\item on the $k^\text{th}$ \udef{superdiagonal} if $j = i+k$;
\item on the $k^\text{th}$ \udef{subdiagonal} if $j = i-k$.
\end{itemize}
\end{definition}

\subsubsection{Submatrices}
Let $A\in\F^{m\times n}$ be a matrix and $I\subseteq 1:m$ and $J\subseteq 1:n$ sets, then $[A]_{I,J}$ is the matrix consisting only of those entries whose row number is in $I$ and whose column number is in $J$. A matrix of this form is called a \udef{submatrix}.

\begin{example}
Let
\[ A = \begin{pmatrix}
1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12
\end{pmatrix}, \]
then
\[ [A]_{1:2,1:3} = \begin{pmatrix}
1 & 2 & 3 \\ 5 & 6 & 7
\end{pmatrix}. \]
\end{example}

\subsubsection{Types of matrices}
\begin{definition}
Let $A\in\F^{n\times n}$. We say
\begin{itemize}
\item $A$ is \udef{upper triangular} if $i>j \implies [A]_{i,j} = 0$;
\item $A$ is \udef{strictly upper triangular} if $i\geq j \implies [A]_{i,j} = 0$;
\item $A$ is \udef{lower triangular} if $i<j \implies [A]_{i,j} = 0$;
\item $A$ is \udef{strictly lower triangular} if $i\leq j \implies [A]_{i,j} = 0$;
\item $A$ is \udef{triangular} if it is upper or lower triangular.
\end{itemize}
We say
\begin{itemize}
\item $A$ is \udef{diagonal} if $i\neq j \implies [A]_{i,j} = 0$; in this case we write $A = \diag([A]_{11},\ldots, [A]_{nn})$;
\item $A$ is \udef{tridiagonal} if $|i\neq j| \geq 2 \implies [A]_{i,j} = 0$;
\item $A$ is \udef{bidiagonal} if it is tridiagonal and triangular.
\end{itemize}
We say
\begin{itemize}
\item $A$ is a \udef{permutation matrix} if exactly one entry in each row and in each column is 1; all other entries are 0.
\end{itemize}
\end{definition}

\subsection{Matrix multiplication}
Assume we have $n$ vectors $v_1,\ldots, v_n$ in $\F^m$. We may be interested in linear combinations of these vectors, say $a_1v_1+\ldots + a_nv_n$. We can collect the coefficients $a_i$ in a column vector in $\F^n$. The vectors $v_i$ can be written as columns and placed in a matrix.

Consider the action that pairs such a matrix of column vectors with the element in its column space determined by a column matrix. This action is called \udef{matrix multiplication} and is denoted by juxtaposing the matrix and the vector (sometimes separated by a dot).

\begin{example}
Let $v_1 = (1,3,4)$ and $v_2 = (2,5,6)$ be vectors in $\R^3$. These can be placed as columns in a matrix:
\[ \begin{pmatrix}
1 & 2 \\ 3 & 5 \\ 4 & 6
\end{pmatrix} \]
Consider the linear combination $2v_1 + v_2$, we can write this as the matrix multiplication
\[ 2v_1 + v_2 = \begin{pmatrix}
1 & 2 \\ 3 & 5 \\ 4 & 6
\end{pmatrix}\begin{pmatrix}
2 \\ 1
\end{pmatrix} = \begin{pmatrix}
2\cdot 1 + 1\cdot 2 \\
2\cdot 3 + 1\cdot 5 \\
2\cdot 4 + 1\cdot 6
\end{pmatrix} = \begin{pmatrix}
4 \\ 11 \\ 14
\end{pmatrix} \]
\end{example}
\begin{example}
A very important case (and one we will explore in more detail later) is given by systems of linear equations. We might have the following equations:
\[ \begin{cases}
2x + y -z = 3 \\
-x + y +3z = 2 \\
x+y = -2
\end{cases} \]
This can be rewritten as
\[ x\begin{pmatrix}
2 \\-1 \\ 1
\end{pmatrix} + y \begin{pmatrix}
1 \\1 \\ 1
\end{pmatrix} + z\begin{pmatrix}
-1 \\ 3 \\ 0
\end{pmatrix} = \begin{pmatrix}
3 \\2 \\ -2
\end{pmatrix} \]
where each row is an equation. In this case the coefficients are the unknowns $x,y,z$. So using the notation of matrix multiplication, the equations become
\[ \begin{pmatrix}
2 & 1 & -1 \\
-1 & 1 & 3 \\
1 & 1 & 0
\end{pmatrix}\begin{pmatrix}
x \\ y \\ z
\end{pmatrix} = \begin{pmatrix}
3 \\ 2 \\ -2
\end{pmatrix}. \]
\end{example}

We can view matrix multiplication as a function
\[ \F^{m\times n}\times \F^n \to \F^m: (A,\vec{v}) \mapsto \begin{pmatrix}
\sum_{i=1}^n [A]_{1,i}[\vec{v}]_i \\ \vdots \\ \sum_{i=1}^n [A]_{m,i}[\vec{v}]_i
\end{pmatrix}. \]


Let $B$ be an $(m\times n)$-matrix and $\vec{v}$ a vector in $\F^n$. Then the matrix multiplication $B\cdot \vec{v}$ gives a vector in $\F^m$. This can be used as the input for another matrix multiplication, if multiplied by a $(k\times m)$ matrix $A$. So the expression $A(B\vec{v})$makes sense.

Now we would like to define matrix multiplication between the matrices $B,A$ by the condition that
\[ (A\cdot B)\vec{v} = A(B\vec{v}). \]
In other words we are asserting the associativity of the matrix multiplication.

Consider the component equations: $[A(B\vec{v})]_i = [(B\cdot A)\vec{v}]_i$. We can then calculate:

\begin{align*}
[A(B\vec{v})]_i &= \sum_{j=1}^m [A]_{i,j}[B\vec{v}]_j = \sum_{j=1}^m [A]_{i,j}(\sum_{k=1}^n [B]_{j,k}[\vec{v}]_k) = \sum_{j=1}^m\sum_{k=1}^n [A]_{i,j}[B]_{j,k}[\vec{v}]_k \\ &= \sum_{k=1}^n\left(\sum_{j=1}^m[A]_{i,j}[B]_{j,k}\right)[\vec{v}]_k \eqdef  [(A\cdot B)\vec{v}]_i
\end{align*}
The last equation can only be satisfied for all $[v]_i$ if the matrix multiplication is defined such that
\[ [A\cdot B]_{i,k} \defeq \sum_{j=1}^m[A]_{i,j}[B]_{j,k}. \]

Of course our construction only works if the dimensions of $A,B$ are such that $A(B\vec{v})$ is well-defined.

\begin{definition}
Let $A,B$ be matrices.
\begin{itemize}
\item The matrices $A,B$ are conformal for multiplication if $A\in \F^{k\times m}$ and $B\in\F^{m\times n}$ for some $k,m,n\in\N_0$.
\item If $A$ and $B$ are conformal, we define the product $AB$ by
\[ [AB]_{i,k} = \sum_{j=1}^m[A]_{i,j}[B]_{j,k}. \]
\end{itemize}
Thus matrix multiplication can be thought of as a map $\F^{k\times m}\times \F^{m\times n} \to \F^{k\times n}$
\end{definition}
The compatibility requirement can be abbreviated by
\[ [k\times m] \cdot [m\times n] = [k\times n]. \]

Notice that the matrix multiplication $\F^{m\times n}\times \F^n \to \F^m$ we originally defined is a special case of this more general matrix multiplication if we identify $\F^n$ with $\F^{n\times 1}$ and $\F^m$ with $\F^{m\times 1}$ (that is, we view $\F^n,\F^m$ as column vectors). In this case we have the multiplication
\[ [m\times n] \cdot [n\times 1] = [m\times 1]. \]

\begin{lemma} \label{linearityMatrixMultiplication}
The matrix multiplication map $\F^{k\times m}\times \F^{m\times n} \to \F^{k\times n}$ is linear in both arguments.
\end{lemma}
\begin{proof}
For linearity in the first argument, assume $A_1,A_2\in\F^{k\times m}$ and $\lambda\in \F$. Then
\[ [(\lambda A_1+ A_2)B]_{ij} = \sum_{j=1}^m[\lambda A_1+ A_2]_{i,j}[B]_{j,k} = \lambda \left(\sum_{j=1}^m[A_1]_{i,j}[B]_{j,k}\right) + \left(\sum_{j=1}^m[A_2]_{i,j}[B]_{j,k}\right). \]
The proof of linearity in second argument is similar.
\end{proof}

\begin{definition}
The \udef{identity matrix} of dimension $n$ is the $(n\times n)$-matrix
\[ \mathbb{1}_n = \begin{pmatrix}
1 & 0 & 0 & \hdots & 0\\
0 & 1 & 0 & \hdots & 0\\
0 & 0 & 1 & \hdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \hdots & 1
\end{pmatrix} \]
The components of $\mathbb{1}_n$ are given by
\[ [\mathbb{1}_n]_{ij} = \delta_{ij}. \]
\end{definition}

\begin{lemma}
Let $A\in\F^{m\times n}$. Then
\[ A = \mathbb{1}_m\cdot A = A \cdot \mathbb{1}_n. \]
\end{lemma}
\begin{proof}
By a simple calculation in components:
\[ [\mathbb{1}_m\cdot A]_{ij} = \sum_{k=1}^m\delta_{ik}[A]_{kj} = [A]_{ij}. \]
The other equation is similar.
\end{proof}

\begin{lemma} \label{matrixOfOnesMultiplication}
Let $l,m,n\in\N_0$, then
\[ \mathbb{J}^{l\times m}\mathbb{J}^{m\times n} = m\mathbb{J}^{l\times n}. \]
\end{lemma}
\begin{proof}
\[ [\mathbb{J}^{l\times m}\mathbb{J}^{m\times n}]_{i,j} = \sum_{k=1^m}[\mathbb{J}^{l\times m}]_{i,k}[\mathbb{J}^{m\times n}]_{k,j} = \sum_{k=1^m}1 = m. \]
\end{proof}
\begin{corollary}
Let $a,b,c,d\in\R$, then
\[ (a\mathbb{1}_n + b\mathbb{J}_n)(c\mathbb{1}_n + d\mathbb{J}_n) = ac\mathbb{1}_n + (ad+bc +bdn)\mathbb{J}_n \]
\end{corollary}

\subsubsection{Left and right inverses}
\begin{definition}
Let $A\in \mathbb{F}^{m\times n}$. A matrix $B$ is a \udef{left inverse} of $A$ if $BA = \mathbb{1}_n$. A matrix $B$ is a \udef{right inverse} of $A$ if $AB = \mathbb{1}_m$.
\end{definition}
Not all matrices have a left and/or right inverses.

\subsubsection{Square matrices}
\begin{lemma}
For any $n\in\N_0$, the vector space $\F^{n\times n}$ of square matrices is a monoid with as operation matrix multiplication and as neutral element the identity matrix $\mathbb{1}_n$.
\end{lemma}

In particular we can define integer powers of matrices. We set $A^0 = \mathbb{1}_n$ by convention.

We also have that if square matrices have both a left inverse and a right inverse, they are the same. See \ref{leftRightInverseMonoid}.

In fact, we have something stronger:
\begin{lemma}
Let $A\in \mathbb{F}^{n\times n}$ be a square matrix. Then $A$ has a left inverse \textup{if and only if} $A$ has a right inverse. Both inverses are the same.
\end{lemma}
\begin{proof}
Consider the map $f:\F^{n\times n}\to \F^{n\times n}: B\mapsto AB$. (This will later be called the left regular representation of $A$). 
 
We follow a chain of implications.
\begin{itemize}
\item \textit{$A$ has left inverse $\Rightarrow$ $f$ is injective}. Assume there exist matrices $B_1,B_2$ such that $AB_1 = AB_2$, then
\[ 0 = A^{-1}0 = A^{-1}(AB_1-AB_2) = A^{-1}A(B_1-B_2) = B_1-B_2. \]
\item \textit{$f$ is injective $\Rightarrow$ $f$ is surjective}. By \ref{invertibleFiniteDim}.
\item \textit{$f$ is surjective $\Rightarrow$ $A$ has right inverse}. By surjectivity there exists a matrix $B$ such that $f(B) = AB=\mathbb{1}_n$. Then $B$ is a right inverse by definition.
\end{itemize}
We have proven that the existence of a left inverse implies the existence of a right inverse. The opposite implication is obtained by considering the right regular representation $f:\F^{n\times n}\to \F^{n\times n}: B\mapsto BA$.
\end{proof}
\begin{definition}
A square matrix is called \udef{invertible} or \udef{nonsingular} or \udef{nondegenerate} if it has a left and right inverse.

If it is not invertible, it is called \udef{singular} or \udef{degenerate}.

We denote the inverse of $A$ as $A^{-1}$.
\end{definition}
\begin{lemma}
Let $A,B\in \mathbb{F}^{n\times n}$ be invertible matrices. Then $AB$ is invertible with inverse
\[ (AB)^{-1} = B^{-1}A^{-1}. \]
\end{lemma}

\begin{lemma}
Let $a,b\in\R$. Then
\[ (a\mathbb{1}_n +b\mathbb{J}_n)^{-1} = \frac{1}{a(a+nb)}[(a+nb)\mathbb{1}_n -b\mathbb{J}_n] = \frac{1}{a}\mathbb{1}_n - \frac{b}{a(a+nb)}\mathbb{J}_n. \]
\end{lemma}
\begin{proof}
By multiplication, using \ref{matrixOfOnesMultiplication}.
\end{proof}

\begin{definition}
Let $A\in\F^{n\times n}$. We say $A$ is \udef{strictly diagonally dominant} if
\[ \sum_{\substack{j \in 1:n \\ j\neq i}}|[A]_{ij}| < |[A]_{ii}|. \]
\end{definition}
\begin{proposition} \label{invertibleDiagonallyDominant}
If $A\in\F^{n\times n}$ is strictly diagonally dominant, then it is invertible.
\end{proposition}
\begin{proof}
We prove the contraposition. Assume, then, that $A$ is not invertible, so there exists an $x\neq 0$ such that $Ax = 0$ (TODO ref). So then for all $i\in 1:n$ we have
\[ \sum_{j = 1}^n [A]_{ij}x_j. \]
Setting $|x_m| = \max_{i\in 1:n}|x_i|$, we have
\[ [A]_{mm}x_m = -\sum_{\substack{j\in 1:n \\ j\neq m}}[A]_{mj}x_j \]
and so
\[ |[A]_{mm}|\cdot |x_m| = |\sum_{\substack{j\in 1:n \\ j\neq m}}[A]_{mj}x_j| \leq \sum_{\substack{j\in 1:n \\ j\neq m}}|[A]_{mj}|\cdot |x_j| \leq \sum_{\substack{j\in 1:n \\ j\neq m}}|[A]_{mj}|\cdot |x_m|.\]
In particular this means $A$ cannot be strictly diagonally dominant.
\end{proof}

\subsection{Matrices and linear maps}
\subsubsection{Matrices as maps $\F^n\to \F^m$}
The matrix multiplication $\F^{m\times n}\times \F^n \to \F^m$ can be curried to produce a map $\ell$. The input of $\ell$, i.e.\ a matrix in $\F^{m\times n}$ is typically written as a subscript. So, for a matrix $A\in\F^{m\times n}$ we have
\[ \ell_A: \F^n \to \F^m: \vec{v}\mapsto \ell_A(\vec{v}) = A\vec{v}. \]
Because the matrix multiplication is linear in the second argument (see \ref{linearityMatrixMultiplication}), the map $\ell_A: \F^n \to \F^m$ is linear for all matrices $A\in \F^{m\times n}$. So we have
\[ \ell: \F^{m\times n}\to \Hom(\F^n, \F^m). \]

Additionally this map $\ell$ is linear due to the matrix multiplication being linear in the first argument (see again \ref{linearityMatrixMultiplication}).

\begin{proposition} \label{ellIsomorphism}
For all $n,m\in\N_0$, the map
\[ \ell: \F^{m\times n}\to \Hom(\F^n, \F^m) \]
is an isomorphism.
\end{proposition}
\begin{proof}
We will explicitly construct an inverse. Take some $L\in\Hom(\F^n, \F^m)$. Now $L$ is completely determined by the images of the elements in the standard basis $\mathcal{E}= \{\vec{e}_i\}_{i=1}^n$, so we just need to find a matrix $A$ such that $\ell_A$ maps the basis elements to the same elements as $L$.

Now $A\vec{e}_1$ is just the first column of $A$. So we set the first column of $A$ to be $L(\vec{e}_1)$. Similarly $A\vec{e}_i$ is the $i^\text{th}$ column of $A$ and we set it equal to $L(\vec{e}_i)$. This gives the required matrix.
\end{proof}
The matrix $A$ that satisfies $\ell_A = L$ is often denoted $A_L$.

The construction in the previous proof is important for practically finding matrices associated with linear maps. The construction can be recapped as follows:
\[ A_L = \begin{pmatrix}
L(\vec{e}_1) & L(\vec{e}_2) & \hdots & L(\vec{e}_n)
\end{pmatrix} = \begin{pmatrix}
L_1(\vec{e}_1) & L_1(\vec{e}_2) & \hdots & L_1(\vec{e}_n)  \\
L_2(\vec{e}_1) & L_2(\vec{e}_2) & & \\
\vdots & & \ddots & \\
L_m(\vec{e}_1) & & & L_m(\vec{e}_n)
\end{pmatrix} \qquad [A_L]_{ij} = L_i(\vec{e}_j) \]
where $\mathcal{E}= \{\vec{e}_i\}_{i=1}^n$ is the standard basis of $\F^n$ and $L_i(\vec{e}_j) = [L(\vec{e}_j)]_i$ is the $i^\text{th}$ component of $L(\vec{e}_j)$.

If $\F^n$ is equipped with the standard inner product, then this can also be written as $L_i(\vec{e}_j) = \inner{\vec{e}_i, L(\vec{e}_j)}$, so
\[ A_L = \begin{pmatrix}
\inner{\vec{e}_1, L(\vec{e}_1)} & \inner{\vec{e}_1, L(\vec{e}_2)} & \hdots & \inner{\vec{e}_1, L(\vec{e}_n)}  \\
\inner{\vec{e}_2, L(\vec{e}_1)} & \inner{\vec{e}_2, L(\vec{e}_2)} & & \\
\vdots & & \ddots & \\
\inner{\vec{e}_m, L(\vec{e}_1)} & & & \inner{\vec{e}_m, L(\vec{e}_n)}
\end{pmatrix}. \]

\begin{proposition}
The map $\ell$ translates matrix multiplication into function composition:
\[ \ell_{AB} = \ell_A \circ \ell_B \qquad\text{and}\qquad \ell^{-1}(f\circ g) = \ell^{-1}(f)\ell^{-1}(g). \]
\end{proposition}
\begin{proof}
Let $A\in \F^{k\times m}$ and $B\in \F^{m\times n}$. Let $\vec{v}\in\F^n$, then
\[ \ell_{AB}(\vec{v}) = (AB)\vec{v} = A(B\vec{v}) = A(\ell_B(\vec{v})) = \ell_A(\ell_B(\vec{v})) = (\ell_A\circ\ell_B)(\vec{v}). \]
\end{proof}

\begin{lemma}
For all $n\in\N$ we have $\id_{\F^{n\times n}} = \ell_{\mathbb{1}_n}$.
\end{lemma}

\begin{lemma} \label{invertibleMapInvertibleMatrix}
Let $A$ be a matrix over $\F$. Then $\ell_A$ is invertible \textup{if and only if} $A$ is square and invertible.
\end{lemma}
\begin{proof}
Assume $\ell_A: \F^n\to\F^m$ invertible. Then $\F^n\cong\F^m$, so $m=n$ by \ref{isomorphicDimension}. Also $\ell^{-1}((\ell_A)^{-1})$ is an inverse of A because
\[ \ell^{-1}((\ell_A)^{-1})\cdot A = \ell^{-1}((\ell_A)^{-1})\cdot\ell^{-1}(\ell_A) = \ell^{-1}[(\ell_A)^{-1}\ell_A] = \ell^{-1}(\id_{\F^n}) = \mathbb{1}_n. \]

Conversely, assume $A$ invertible with inverse $A^{-1}$. Then $\ell_{A^{-1}}$ is the inverse of $\ell_A$:
\[ \ell_{A^{-1}}\ell_A = \ell_{\mathbb{1}_n} = \id_{\F^n} \qquad \text{and}\qquad \ell_A\ell_{A^{-1}} = \ell_{\mathbb{1}_n} = \id_{\F^n}. \]
\end{proof}

\subsubsection{Linear maps as matrices}
We can associate a matrix to any linear map by passing to coordinates. Let $L: V\to W$ be a linear map from an $n$-dimensional vector space $V$ to an $m$-dimensional vector space $W$. If we fix bases $\mathcal{V}$ of $V$ and $\mathcal{W}$ of $W$, then $\ell^{-1}$ associates a unique matrix with the linear map
\[ \co_{\mathcal{W}}\circ L \circ \co^{-1}_{\mathcal{V}}: \F^n\to\F^m. \]
In other words, $A$ is the unique matrix such that
\[ \begin{tikzcd}
V\rar{L}\dar[swap]{\co_{\mathcal{V}}} & W \dar{\co_{\mathcal{W}}} \\
\F^n \rar{\ell_A} & \F^m
\end{tikzcd} \qquad \text{commutes.} \]
We call this matrix $A \defeq \ell^{-1}(\co_{\mathcal{W}}\circ L \circ \co^{-1}_{\mathcal{V}})$ the \udef{matrix of the linear map $L$} w.r.t. the bases $\mathcal{V}$ and $\mathcal{W}$. This matrix is denoted
\[ (L)_{\mathcal{V}}^{\mathcal{W}} \qquad \text{or}\qquad \prescript{\mathcal{W}}{}{(L)}^{\mathcal{V}} \qquad \text{or}\qquad (L)_{\mathcal{W}\leftarrow \mathcal{V}}. \]

The commutativity of the diagram translates to the following lemma:
\begin{lemma} \label{commutativityMatrixLinearMap}
Let $L:V\to W$ be a linear map and $\mathcal{V},\mathcal{W}$ bases of $V,W$ respectively. Then
\[ (L)^\mathcal{W}_\mathcal{V} \circ \co_\mathcal{V} = \co_{\mathcal{W}}\circ L. \]
\end{lemma}
A practical way to calculate matrices associated with linear maps is given by the following lemma.
\begin{lemma}
Let $L:V\to W$ be a linear map and $\mathcal{V}=\{\vec{v}_i\}_{i=1}^n,\mathcal{W} = \{\vec{w}_i\}_{i=1}^m$ bases of $V,W$ respectively. Then
\[ (L)^\mathcal{W}_\mathcal{V} = \begin{pmatrix}
\co_\mathcal{W}(L(\vec{v}_1)) & \co_\mathcal{W}(L(\vec{v}_2)) & \hdots & \co_\mathcal{W}(L(\vec{v}_n)).
\end{pmatrix} \]
\end{lemma}
\begin{proof}
The $i^\text{th}$ column of $(L)^\mathcal{W}_\mathcal{V}$ is equal to $(L)^\mathcal{W}_\mathcal{V}\vec{e}_i = (L)^\mathcal{W}_\mathcal{V}(\co_\mathcal{V}(\vec{v}_i))$, where $\vec{e}_i$ is the $i^\text{th}$ element of the standard basis $\mathcal{E}$. This is equal to $\co_{\mathcal{W}}(L(\vec{v}))$ by \ref{commutativityMatrixLinearMap}.
\end{proof}

\begin{proposition} \label{algebraMatricesLinearMaps}
Let $U,V,W$ be vector spaces with bases $\mathcal{U},\mathcal{V},\mathcal{W}$, resp., and $S:V\to W, T:U\to V$ linear maps. Then
\[ (S)_\mathcal{V}^\mathcal{W}(T)_\mathcal{U}^\mathcal{V} = (S\circ T)_{\mathcal{U}}^{\mathcal{W}}. \]
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
\ell^{-1}(\co_{\mathcal{W}}\circ S \circ \co^{-1}_{\mathcal{V}})\ell^{-1}(\co_{\mathcal{V}}\circ T \circ \co^{-1}_{\mathcal{U}}) &= \ell^{-1}(\co_{\mathcal{W}}\circ S \circ \co^{-1}_{\mathcal{V}}\circ\co_{\mathcal{V}}\circ T \circ \co^{-1}_{\mathcal{U}}) \\
&= \ell^{-1}(\co_{\mathcal{W}}\circ (S \circ T) \circ \co^{-1}_{\mathcal{U}}) = (S\circ T)_{\mathcal{U}}^{\mathcal{W}}.
\end{align*}
\end{proof}

\begin{proposition}
Let $V,W$ be finite-dimensional vector spaces over a field $\mathbb{F}$ with bases $\mathcal{V}$ and $\mathcal{W}$, respectively. The mapping
\[ (-)^{\mathcal{W}}_{\mathcal{V}}:\Hom_\mathbb{F}(V,W) \to \mathbb{F}^{m\times n}: L\mapsto (L)^{\mathcal{W}}_{\mathcal{V}} \]
is an isomorphism.
\end{proposition}
\begin{proof}
It is equal to
\[ \ell^{-1}\circ(\co_\mathcal{W})_*\circ(\co^{-1}_\mathcal{V})^*. \]
Now $\ell$ is an isomorphism by \ref{ellIsomorphism} and thus $\ell^{-1}$ is one by \ref{inverseLinear}. The maps $(\co_\mathcal{W})_*$ and $(\co^{-1}_\mathcal{V})^*$ are injective maps between finite-dimensional spaces by \ref{monicEpicInPrePostComposition} and thus isomorphisms by \ref{invertibleFiniteDim}.
So we have a composition of isomorphisms, which is an isomorphism.
\end{proof}
\begin{corollary}
Let $V,W$ be finite-dimensional vector spaces over a field $\mathbb{F}$, then
\[ \dim_{\mathbb{F}}\Hom_\mathbb{F}(V,W) = (\dim_{\mathbb{F}} V)\cdot (\dim_{\mathbb{F}} W). \] \label{dimHomset}
\end{corollary}

\begin{lemma}
Let $A\in\F^{m\times n}$ be a matrix and let $\mathcal{E}_m$ and $\mathcal{E}_n$ be the standard bases of $\F^m$ and $\F^n$. Then
\[ (\ell_{A})_{\mathcal{E}_n}^{\mathcal{E}_m} = A. \]
\end{lemma}

\subsubsection{Changing basis with matrices}
In particular we can apply all the theory of the previous section to the identity map $\id: V\to V$. Let $\beta, \beta'$ be two bases of $V$. Then \ref{commutativityMatrixLinearMap} gives
\[ \co_{\beta'}(v) = (\id)_{\beta}^{\beta'}\co_\beta(v) \]
which captures the effect of transforming from one basis to another.
\begin{definition}
Matrices of the form $(\id)_{\beta}^{\beta'}$ are called \udef{transition matrices} or \udef{change-of-basis matrices}.
\end{definition}
\begin{lemma}
Let $\beta, \beta'$ be two bases of a vector space $V$. Then
\[ \left((\id)_{\beta}^{\beta'}\right)^{-1} = (\id)_{\beta'}^{\beta}. \]
\end{lemma}
\begin{proof}
This follows from \ref{algebraMatricesLinearMaps} which gives
\[ (\id)_{\beta'}^{\beta}(\id)_{\beta}^{\beta'} = (\id)_\beta^\beta = \mathbb{1}_n. \]
\end{proof}

Let $L\in \Hom(V)$. If we know $(L)_\beta^\beta$, we can calculate $(L)_{\beta'}^{\beta'}$ using
\begin{align*}
(L)_{\beta'}^{\beta'} &= (\id)_{\beta}^{\beta'}(L)_\beta^\beta(\id)_{\beta'}^{\beta} \\
&= \left((\id)_{\beta'}^{\beta}\right)^{-1}(L)_\beta^\beta(\id)_{\beta'}^{\beta}
\end{align*}
\begin{definition}
Let $A,B\in \mathbb{F}^{n\times n}$, then $A$ and $B$ are called \udef{similar} if there exists an invertible matrix $P\in\mathbb{F}^{n\times n}$ such that
\[ B = P^{-1}A P\] 
\end{definition}

Any similar matrices may be seen as matrices of the same linear transformation w.r.t. different bases.



\subsection{The transpose}
\begin{definition}
Let $A\in \mathbb{F}^{m\times n}$. The \udef{transpose} of $A$, denoted $A^\transp$, is defined by
\[ [A^\transp]_{ij} = [A]_{ji}. \]
\end{definition}
\begin{lemma}
The transpose is a linear operation.
\end{lemma}

\begin{lemma}
Let $A,B$ be matrices such that $AB$ is defined, then
\[ (AB)^\transp = B^\transp A^\transp. \]
\end{lemma}
\begin{proof}
We simply calculate
\[ [(AB)^\transp]_{ij} = [AB]_{ji} = \sum_{k}[A]_{jk}[B]_{ki} = \sum_{k}[B]_{ki}[A]_{jk} = \sum_{k}[B^\transp]_{ik}[A^\transp]_{kj} = [B^\transp A^\transp]_{ij}. \]
\end{proof}

\begin{definition}
\begin{itemize}
\item A square matrix $A$ such that $A=A^\transp$ is called \udef{symmetric}.
\item A square matrix $A$ such that $A=-A^\transp$ is called \udef{skew symmetric}.
\end{itemize}
\end{definition}

\subsubsection{The standard inner product}
Let $\vec{v},\vec{w}\in\F^n$. Then the standard inner product is given by
\[ \inner{\vec{v},\vec{w}} \defeq \overline{\vec{v}}^\transp \vec{w}. \]
This reduces to $\vec{v}^\transp\vec{w}$ if $\F = \R$.

In the sequel we will always assume $\F^n$ is equipped with the standard inner product, unless otherwise specified.

The inner product also induces a norm. There are multiple norms that may be of interest. To avoid confusion we may denote the norm that arises from the inner product with a subscript 2:
\[ \norm{\vec{v}} = \norm{\vec{v}}_2 = \sqrt{\inner{\vec{v},\vec{v}}}. \]

\subsubsection{Adjoint}
From the definition of the standard inner product, it is clear that $\overline{\vec{v}}^\transp$ is an important operation. We give this operation its own symbol:
\[ \vec{v}^* \defeq  \overline{\vec{v}}^\transp \qquad\text{so}\qquad \inner{\vec{v},\vec{w}} = \vec{v}^*\vec{w}. \]
In fact this definition makes sense for all matrices:
\begin{definition}
Let $A\in\F^{m\times n}$. The conjugate transpose $A^*$ of $A$ is the matrix
\[ A^* \defeq \overline{A}^\transp = \overline{(A^\transp)}. \]
It is also called the \udef{adjoint} of $A$.
\end{definition}
\begin{lemma}
Let $A,B$ be conformal matrices. Then
\begin{enumerate}
\item $(A^*)^*$;
\item the conjugate transpose is antilinear:
\[ (cA+B)^* = \overline{c}A^* + B^* \qquad \forall c\in\F; \]
\item if $A$ is invertible, then $(A^*)^{-1} = (A^{-1})^*$.
\item if $\F=\R$, then $A^* = A^\transp$.
\end{enumerate}
\end{lemma}

\begin{proposition}
Let $A\in\F^{n\times n}$. Then for all $\vec{v},\vec{w}\in\F^n$,
\[ \inner{A\vec{v},\vec{w}} = \inner{\vec{v},A^*\vec{w}}. \]
\end{proposition}
\begin{proof}
$\inner{A\vec{v},\vec{w}} = (A\vec{v})^*\vec{w} = \vec{v}^*A^*\vec{w} = \inner{\vec{v},A^*\vec{w}}$.
\end{proof}

\begin{definition}
Let $A\in\F^{n\times n}$ be a square matrix
\begin{itemize}
\item if $A=A^*$, then $A$ is called \udef{Hermitian};
\item if $A=-A^*$, then $A$ is called \udef{skew Hermitian};
\item if $A^*A=\mathbb{1}_n$, then $A$ is called \udef{unitary}; a real unitary matrix is called \udef{orthogonal};
\item if $A^*A = AA^*$, then $A$ is called \udef{normal};
\item if $A^2 = A = A^*$, then $A$ is called an \udef{orthogonal projection}.
\end{itemize}
\end{definition}
The name orthogonal projection is due to the following: it is a projection because applying it multiple times is the same as applying it once; it is orthogonal because for all $\vec{v}\in\F^n$, $P\vec{v}$ and $\vec{v} - P\vec{v} = (\mathbb{1}-P)\vec{v}$ are orthogonal:
\[ \inner{P\vec{v},\vec{v} - P\vec{v}} = \inner{P\vec{v},\vec{v}} - \inner{P\vec{v},P\vec{v}} = \inner{P\vec{v},\vec{v}} - \inner{P^*P\vec{v},\vec{v}} = \inner{P\vec{v},\vec{v}} - \inner{P^2\vec{v},\vec{v}} = \inner{P\vec{v},\vec{v}} - \inner{P\vec{v},\vec{v}} = 0. \]

\begin{lemma}
Let $U\in\F^{n\times n}$ be a matrix. Then the following are equivalent
\begin{enumerate}
\item $U$ is unitary, i.e.\ $U^*U=\mathbb{1}_n$;
\item the columns of $U$ are orthonormal;
\item $UU^*=\mathbb{1}_n$;
\item $U^*$ is unitary;
\item the row of $U$ are orthonormal;
\item $U$ is invertible and $U^{-1} = U^*$.
\end{enumerate}
\end{lemma}
Notice that $U^*U=\mathbb{1} \iff UU^*=\mathbb{1}$ only holds for square matrices.

\begin{corollary}
If $\alpha,\beta$ are orthonormal bases of $\F^{n}$, then the transition matrix $(\id)_\alpha^\beta$ is unitary.
\end{corollary}

\subsection{Block matrices}
Any given matrix can be \textit{interpreted} as consisting of submatrices. Eg,
\[ \begin{bmatrix}
1&2&3\\4&5&6\\7&8&9
\end{bmatrix} \quad \text{can be viewed as} \quad 
\begin{bmatrix}
\begin{pmatrix}
1 & 2
\end{pmatrix} & 3 \\
\begin{pmatrix}
4 & 5 \\ 7 & 8
\end{pmatrix} & \begin{pmatrix}
6 \\ 9
\end{pmatrix}
\end{bmatrix}
\quad \text{with partitioning} \quad
\left[\begin{array}{cc|c}
1&2&3\\ \hline
4&5&6\\7&8&9
\end{array}\right]
 \]
\begin{definition}
A matrix consisting of submatrices (also known as blocks) is called a \udef{block matrix} or \udef{partitioned matrix}.

If $A\in\F^{n\times n}, \vec{x},\vec{y}\in\F^n$ and $c\in \F$, we call the block matrices
\[ \begin{bmatrix}
c & \vec{x}^\transp \\ \vec{y} & A
\end{bmatrix}, \begin{bmatrix}
\vec{x}^\transp & c \\ A & \vec{y}
\end{bmatrix}, \begin{bmatrix}
A & \vec{x} \\
\vec{y}^\transp & c
\end{bmatrix}, \quad\text{and}\quad \begin{bmatrix}
\vec{x} & A \\ c & \vec{y}^\transp
\end{bmatrix} \]
\udef{bordered matrices}. They have been obtained from $A$ by \udef{bordering}.
\end{definition}

The partition can be specified at the level of the dimensions: let $A$ be an $(m\times n)$-matrix, let $m_1,\ldots, m_k\leq m$ be the number of rows in each horizontal partition and let $n_1,\ldots, n_l\leq n$ be the number of columns in each vertical partition. Clearly
\[ m = \sum_{i=1}^k m_i \qquad \text{and} \qquad n = \sum_{i=1}^l n_i. \]

We then say the block matrix has dimensions $(m_1|\ldots|m_k) \times (n_1|\ldots|n_l)$, or $A$ is a $(m_1|\ldots|m_k) \times (n_1|\ldots|n_l)$-matrix.

\begin{example}
The block matrix considered above is a $(1|2)\times (2|1)$-matrix.
\end{example}

We write $A_{i,j}$ or $(A)_{i,j}$ for the block in the $i^\text{th}$ horizontal partition and the $j^\text{th}$ vertical partition. So if $A$ is a $(m_1|\ldots|m_k) \times (n_1|\ldots|n_l)$-matrix, we can write
\[ A = \begin{bmatrix}
A_{1,1} & \hdots & A_{1,l} \\
\vdots & \ddots & \vdots \\
A_{k,1} & \hdots & A_{k,l}
\end{bmatrix} \qquad \text{where} \qquad A_{i,j}\in\F^{m_i\times n_j}. \]

Any $m\times n$-matrix can be partitioned into columns, which means identifying it with an $m\times (1|1|\ldots|1)$-matrix.

Similarly, any $m\times n$-matrix can be partitioned into rows, which means identifying it with a $(1|1|\ldots|1)\times n$-matrix.

\begin{lemma}
Let $A$ be a partitioned matrix of dimensions $(m_1|\ldots|m_k) \times (n_1|\ldots|n_l)$, then $A^\transp$ is a partitioned matrix of dimensions $(n_1|\ldots|n_l) \times (m_1|\ldots|m_k)$ and
\[ (A)_{i,j} = (A^\transp)_{j,i}. \]
\end{lemma}

\begin{definition}
Let $A,B$ be matrices. We can then form the \udef{direct sum} matrix $A\oplus B$ as follows:
\[ A\oplus B = \begin{pmatrix}
A & \mathbb{0} \\ \mathbb{0} & B
\end{pmatrix}. \]
\end{definition}

\begin{proposition}
Let $A$ be a partitioned matrix of dimensions $(m_1|\ldots|m_k) \times (n_1|\ldots|n_l)$ and $B$ a partitioned matrix of dimensions $(n_1|\ldots|n_l)\times (p_1|\ldots|p_q)$.

The matrix product of $A$ and $B$ is an $(m_1|\ldots|m_k) \times (p_1|\ldots|p_q)$-matrix with blocks
\[ (AB)_{i,j} = \sum_{t}A_{i,t}B_{t,j}. \]
That is, multiplication of two block matrices can be carried out as if their blocks were scalars.
\end{proposition}
We can abbreviate the dimension requirements as
\[ [(m_1|\ldots|m_k) \times (n_1|\ldots|n_l)]\cdot[(n_1|\ldots|n_l)\times (p_1|\ldots|p_q)] = [(m_1|\ldots|m_k) \times (p_1|\ldots|p_q)]. \]
\begin{corollary} \label{multiplicationBlockMatrices}
Let $A,B$ be matrices of dimensions $k\times l$ and $l \times m$. Then
\[ AB = [AB]_{-,-} = \sum_k[A]_{-,k}[B]_{k, -}; \]
and
\begin{itemize}
\item we can partition $A$ into rows and $B$ into columns to get
\[ [AB]_{i,j} = [A]_{i,-}[B]_{-, j}; \]
\item we can partition $B$ into columns to get
This can also be written as
\[ [AB]_{-, j} = [A]_{-,-}[B]_{-,j} = A[B]_{-, j}; \]
\item we can partition $A$ into rows to get
This can also be written as
\[ [AB]_{i, -} = [A]_{i,-}[B]_{-,-} = [A]_{i,-}B. \]
\end{itemize}
\end{corollary}
In particular this means we can write
\[ AB = \begin{pmatrix}
A
\end{pmatrix} \begin{pmatrix}
[B]_{-,1} & \hdots & [B]_{-, m}
\end{pmatrix} = \begin{pmatrix}
A[B]_{-,1} & \hdots & A[B]_{-,m}
\end{pmatrix}. \]

\subsubsection{Identities and inverses}
\begin{lemma}
Let $X,Y,Z$ be conformal matrices and $Y,Z$ invertible, then
\[ \begin{bmatrix}
Y & X \\ \mathbb{0} & Z
\end{bmatrix}^{-1} = \begin{bmatrix}
Y^{-1} & -Y^{-1}XZ^{-1} \\ \mathbb{0} & Z^{-1}.
\end{bmatrix}. \]
In particular
\[ \begin{bmatrix}
\mathbb{1} & X \\ \mathbb{0} & \mathbb{1}
\end{bmatrix}^{-1} = \begin{bmatrix}
\mathbb{1} & -X \\ \mathbb{0} & \mathbb{1}
\end{bmatrix}. \]
\end{lemma}
\begin{corollary}
If $A$ is an upper triangular matrix with nonzero diagonal entries, the $A$ is invertible and $[A^{-1}]_{i,i} = [A]_{i,i}^{-1}$
\end{corollary}
\begin{proof}
Induction on dimension.
\end{proof}

\begin{lemma}
Let $X,Y,Z$ be conformal matrices. Then
\[ \begin{bmatrix}
Y & X \\ \mathbb{0} & Z
\end{bmatrix} = \begin{bmatrix}
\mathbb{1} & A \\ \mathbb{0} & \mathbb{1}
\end{bmatrix}\begin{bmatrix}
Y & X-AZ + YA \\ \mathbb{0} & Z
\end{bmatrix}\begin{bmatrix}
\mathbb{1} & -A \\ \mathbb{0} & \mathbb{1}
\end{bmatrix} \]
for all conformal $A$.
\end{lemma}

\begin{lemma}
Let $A,B,C,D$ be conformal matrices. Then, if $D$ is invertible 
\[ M = \begin{bmatrix}
A & B \\ C & D
\end{bmatrix} = \begin{bmatrix}
\mathbb{1} & BD^{-1} \\ \mathbb{0} & \mathbb{1}
\end{bmatrix}\begin{bmatrix}
A-BD^{-1}C & \mathbb{0} \\ \mathbb{0} & D
\end{bmatrix}\begin{bmatrix}
\mathbb{1} & \mathbb{0} \\ D^{-1}C & \mathbb{1}
\end{bmatrix}  \]
and if $A$ is invertible,
\[ M = \begin{bmatrix}
A & B \\ C & D
\end{bmatrix} = \begin{bmatrix}
\mathbb{1} & \mathbb{0} \\ CA^{-1} & \mathbb{1}
\end{bmatrix}\begin{bmatrix}
A & \mathbb{0} \\ \mathbb{0} & D - CA^{-1}B
\end{bmatrix}\begin{bmatrix}
\mathbb{1} & A^{-1}B \\ \mathbb{0} & \mathbb{1}
\end{bmatrix}.  \]
\end{lemma}
\begin{definition}
The matrix $M/D \defeq A-BD^{-1}C$ is the \udef{Schur complement} of $D$ in $M$.

Similarly $M/A \defeq D-CA^{-1}B$ is the Schur complement of $A$ in $M$.
\end{definition}
\begin{lemma}
Let $A,B,C,D$ be conformal matrices such that all requisite matrices are invertible. Then
\begin{align*}
M^{-1} &= \begin{bmatrix}
A & B \\ C & D
\end{bmatrix}^{-1} = \begin{bmatrix}
\mathbb{1} & \mathbb{0} \\ -D^{-1}C & \mathbb{1}
\end{bmatrix}\begin{bmatrix}
(M/D)^{-1} & \mathbb{0} \\ \mathbb{0} & D^{-1}
\end{bmatrix}\begin{bmatrix}
\mathbb{1} & -BD^{-1} \\ \mathbb{0} & \mathbb{1}
\end{bmatrix} \\
&= \begin{bmatrix}
(M/D)^{-1} & -(M/D)^{-1}BD^{-1} \\ -D^{-1}C(M/D)^{-1} & D^{-1}+D^{-1}C(M/D)^{-1}BD^{-1}\end{bmatrix} \\
M^{-1} &= \begin{bmatrix}
A & B \\ C & D
\end{bmatrix}^{-1} = \begin{bmatrix}
\mathbb{1} & -A^{-1}B \\ \mathbb{0} & \mathbb{1}
\end{bmatrix}\begin{bmatrix}
A^{-1} & \mathbb{0} \\ \mathbb{0} & (M/A)^{-1}
\end{bmatrix}\begin{bmatrix}
\mathbb{1} & \mathbb{0} \\ -CA^{-1} & \mathbb{1}
\end{bmatrix} \\
&= \begin{bmatrix}
A^{-1}+A^{-1}B(M/A)^{-1}CA^{-1} & -A^{-1}B(M/A)^{-1} \\-(M/A)^{-1}CA^{-1} & (M/A)^{-1}
\end{bmatrix}.
\end{align*}
\end{lemma}

\begin{lemma}
Let $A,B$ be conformal matrices. Then
\[ \begin{bmatrix}
AB & A \\ \mathbb{0} & \mathbb{0}
\end{bmatrix} \qquad\text{and}\qquad \begin{bmatrix}
\mathbb{0} & A \\ \mathbb{0} & BA
\end{bmatrix} \]
are similar
\end{lemma}
\begin{proof}
$\begin{bmatrix}
\mathbb{1} & \mathbb{0} \\ B & \mathbb{1}
\end{bmatrix}\begin{bmatrix}
AB & A \\ \mathbb{0} & \mathbb{0}
\end{bmatrix} = \begin{bmatrix}
\mathbb{0} & A \\ \mathbb{0} & BA
\end{bmatrix}\begin{bmatrix}
\mathbb{1} & \mathbb{0} \\ B & \mathbb{1}
\end{bmatrix}.$
\end{proof}

\begin{proposition}
Let $A,B,C,U,V$ be conformal matrices. Then
\begin{enumerate}
\item \textup{(Push-through identity)} $(\mathbb{1}+UV)^{-1}U = U(\mathbb{1}+VU)^{-1}$;
\item $(\mathbb{1}+A)^{-1} \begin{aligned}[t]
&= \mathbb{1}-(\mathbb{1}+A)^{-1} A\\
&= \mathbb{1}-A(\mathbb{1}+A)^{-1}
\end{aligned}$
\item $(\mathbb{1} + UV)^{-1} = \mathbb{1} - U(\mathbb{1}+VU)^{-1}V$;
\item \textup{(Woodbury identity)} $(B + UCV)^{-1} = B^{-1} - B^{-1}U(C^{-1}+VB^{-1}U)VB^{-1}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) From $U(\mathbb{1} + VU) = (\mathbb{1} + UV)U$.

(2) From $\mathbb{1} = (\mathbb{1}+A)(\mathbb{1}+A)^{-1} = (\mathbb{1}+A)^{-1} + A(\mathbb{1}+A)^{-1}$.

(3) $\begin{aligned}[t]
(\mathbb{1} + UV)^{-1} &= \mathbb{1}-(\mathbb{1} + UV)^{-1}UV & &\text{using (2)} \\
&= \mathbb{1}-U(\mathbb{1} + VU)^{-1}V & &\text{using (1)}.
\end{aligned}$

(4) $\begin{aligned}[t]
(B + UCV)^{-1} &= (B(\mathbb{1} + B^{-1}UCV))^{-1} \\
&= (\mathbb{1} + (B^{-1}U)(CV))^{-1}B^{-1} \\
&= (\mathbb{1}-(B^{-1}U)(\mathbb{1} + (CV)(B^{-1}U))^{-1}(CV))B^{-1} & &\text{using (3)}\\
&= B^{-1}-B^{-1}U(\mathbb{1} + CVB^{-1}U)^{-1}CVB^{-1} \\
&= B^{-1}-B^{-1}U(C^{-1}(\mathbb{1} + CVB^{-1}U))^{-1}VB^{-1} \\
&= B^{-1}-B^{-1}U(C^{-1} + VB^{-1}U)^{-1}VB^{-1}.
\end{aligned}$
\end{proof}
\begin{corollary}[Sherman–Morrison formula]
Let $A\in\F^{n\times n}$ and $\vec{u},\vec{v}\in\F^n$. 
Then $A+\vec{u}\vec{v}^\transp$ is invertible iff $1 + \vec{v}^\transp A^{-1}\vec{u} \neq 0$. In this case
\[ (A+\vec{u}\vec{v}^\transp)^{-1} = A^{-1} - \frac{A^{-1}\vec{u}\vec{v}^\transp A^{-1}}{1 + \vec{v}^\transp A^{-1}\vec{u}}. \]
\end{corollary}
\begin{corollary}[Hua's identity]
Let $A,B$ be conformal matrices. Then
\begin{align*}
(A+B)^{-1} &= A^{-1} - A^{-1}(B^{-1}+A^{-1})^{-1}A^{-1} \\
&= A^{-1} - A^{-1}(AB^{-1}+\mathbb{1})^{-1} \\
(A-B)^{-1} &= A^{-1} + A^{-1}B(A-B)^{-1} \\
&= \sum_{k=0}^\infty (A^{-1}B)^kA^{-1}.
\end{align*}
\end{corollary}

\subsection{Vector spaces associated with a matrix}
\subsubsection{Row and column space}
\begin{definition}
Let $A$ be an $(n\times m)$-matrix. It can be partitioned into both rows and columns. Let $R_1,\ldots, R_n$ be the rows of $A$ and $C_1,\ldots, C_m$ the columns of $A$:
\[ A = \begin{pmatrix}
R_1 \\ \vdots \\ R_n
\end{pmatrix} = \begin{pmatrix}
C_1 & \hdots & C_m
\end{pmatrix}. \]
Then
\begin{itemize}
\item $\Span\{R_1,\ldots, R_n\}$ is the \udef{row space} $\Row(A)$ of $A$; and
\item $\Span\{C_1,\ldots, C_m\}$ is the \udef{column space} $\Col(A)$ of $A$.
\end{itemize}
We call $\dim\Row(A)$ the \udef{row rank} and $\dim\Col(A)$ the \udef{column rank}.
\end{definition}
Clearly $\Col(A) = \Row(A^\transp)$.

\begin{lemma} \label{columnSpace}
Let $A\in \F^{m\times n}$ and $\vec{b}\in \F^m$. Then
\[ \exists \vec{x}\in\F^n: A\vec{x}=\vec{b} \quad\iff\quad \vec{b}\in\Col(A). \]
Moreover, if the columns of $A$ are linearly independent, then the $\vec{x}\in\F^n$ is unique. 
\end{lemma}
\begin{proof}
By \ref{multiplicationBlockMatrices}
\begin{align*}
A\vec{x} &= [A\vec{x}]_{-,-} = \sum_{j=1}^n[A]_{-,j}[\vec{x}]_{j,-} = \sum_{j=1}^n[A]_{-,j}[\vec{x}]_{j} \\
&= [A]_{-,1}[\vec{x}]_1 + \ldots [A]_{-,n}[\vec{x}]_n  \\
&= C_1[\vec{x}]_1 + \ldots C_n[\vec{x}]_n.
\end{align*}
\end{proof}

\begin{proposition} \label{rowColSubspaces}
Let $A$ and $B$ be matrices. Then
\begin{enumerate}
\item $\Col(B)\subseteq \Col(A) \iff B = AX$ for some matrix $X$;
\item $\Row(B)\subseteq \Row(A) \iff B = YA$ for some matrix $Y$.
\end{enumerate}
\end{proposition}
Note that for point (1) to hold, $A$ and $B$ must have the same number of rows. For point (2) they must have the same number of columns.
\begin{proof}
(1) $\boxed{\Rightarrow}$ Assume $\Col(B)\subseteq \Col(A)$. Then   by \ref{columnSpace}, for each column $\vec{b}_j = [B]_{-,j}$ of $B$ we can find an $\vec{x}_j\in \F^n$ such that $A\vec{x}_j = \vec{b}_j$. Then
\[ B = \begin{pmatrix}
\vec{b}_1 & \hdots & \vec{b}_k
\end{pmatrix} = \begin{pmatrix}
A\vec{x}_1 & \hdots & A\vec{x}_k
\end{pmatrix} = A \begin{pmatrix}
\vec{x}_1 & \hdots & \vec{x}_k
\end{pmatrix} = AX.\]

$\boxed{\Leftarrow}$ By \ref{multiplicationBlockMatrices} we can write
\[ AB = \begin{pmatrix}
A[B]_{-,1} & \hdots & A[B]_{-,m}
\end{pmatrix}, \]
so every column in $AB$ is of the form $A[B]_{-,i}$, which is a linear combination of the columns in $A$.

(2) We simply calculate using point (1):
\[ \Row(B)\subseteq \Row(A) \iff \Col(B^\transp)\subseteq \Col(A^\transp) \iff B^\transp = A^\transp X \iff B = X^\transp A. \]
\end{proof}
\begin{corollary} \label{RowColSpaceProduct}
Let $A,B$ be conformal matrices. Then
\begin{enumerate}
\item $\Col(AB)\subseteq \Col(A)$;
\item $\Row(AB)\subseteq \Row(B)$.
\end{enumerate}
\end{corollary}
\begin{corollary} \label{RowColSpaceInverse}
A matrix $A$ is invertible \textup{if and only if} $\Col(A) = \Row(A) = \F^n$.
\end{corollary}
\begin{proof}
$\boxed{\Rightarrow}$ From $A = \mathbb{1}_nA$ we see that $\Col(A)\subseteq\Col(\mathbb{1}_n) = \F^n$.

From $\mathbb{1}_n = AA^{-1}$ we see that $\Col(\mathbb{1}_n)\subseteq\Col(A)$, so $\Col(\mathbb{1}_n) = \Col(A)$. The calculation of the row space is similar.

$\boxed{\Leftarrow}$ From $\Col(A) = \Col(\mathbb{1}_n)$, there exists an $X$ such that $\mathbb{1}_n = AX$, so $X$ is the inverse of $A$.
\end{proof}
\begin{corollary}
Let $A\in\F^{m\times n}$. Then
\[ \dim\Row(A) = \dim\Col(A) \]
i.e.\ the row rank equals the column rank.
\end{corollary}
\begin{proof}
Take a basis for $\Col(A)$ and let $X$ have these vectors as columns. Then $\Col(X) = \Col(A)$, so $A = XY$ for some $Y\in\F^{k\times n}$.

By point 2. of the proposition, we have $\Row(A)\subseteq \Row(Y)$ and due to the dimensions, we have $\dim\Row(Y)\leq k$. So
\[ \dim\Row(A) \leq \dim\Row(Y) \leq k = \dim\Col(A). \]

Consider $A^\transp$, which can be factorised as before by taking a basis of its column space and putting the vectors in the columns of $X'$. Then $A^\transp = X'Y'$. As before, we have
\[ \dim\Col(A) = \dim\Row(A^\transp) \leq \dim\Col(A^\transp) = \dim\Row(A). \]

Combining the inequalities gives $\dim\Col(A) = \dim\Row(A)$.
\end{proof}
\begin{definition}
We can unambiguously call $\dim\Col(A)=\dim\Row(A)$ the \udef{rank} of the matrix $A$. We write $\Rank(A)$.
\end{definition}

\begin{lemma} \label{extendToInvertible}
Let $A\in\F^{m\times n}$.
\begin{enumerate}
\item If $\Rank(A) = m$, then $n\geq m$ and there exists $X\in\F^{(n-m)\times n}$ such that
\[ \begin{bmatrix}
A \\ X
\end{bmatrix} \in \F^{n\times n} \quad \text{is invertible.} \]
\item If $\Rank(A) = n$, then $m\geq n$ and there exists $Y\in\F^{m\times (m-n)}$ such that
\[ \begin{bmatrix}
A & Y
\end{bmatrix} \in \F^{m\times m} \quad \text{is invertible.} \]
\end{enumerate}
\end{lemma}
\begin{proof}
(1) In this case the rows are linearly independent and elements of $\F^n$, so they can be extended to a basis of $\F^n$. This extension is the matrix $X$.

(2) Transpose and apply (1).
\end{proof}

\begin{lemma}[Full-rank factorisation]
Let $A\in\F^{m\times n}$ and $k=\dim\Col(A)$. Then $A$ can be factorised as $A=XY$ where $X\in\F^{m\times k}$, $Y\in\F^{k\times n}$ and
\[ k = \Rank(A) = \Rank(X) = \Rank(Y). \]
Moreover, for any matrix $X$, the following are equivalent:
\begin{enumerate}
\item the columns of $X$ form a basis of $\Col(A)$;
\item there is a unique $Y\in\F^{k\times n}$ such that $A=XY$.
\end{enumerate}
Clearly considering $A^\transp$ yields dual equivalences.
\end{lemma}
\begin{proof}
By \ref{columnSpace} $Xv = [A]_{-,j}$ has a unique solution $v=y_j$ for all $j$ if and only if the columns of $X$ are linearly independent and $[A]_{-,j}$ is in their span, i.e.\ they from a basis for $\Col(A)$.
\end{proof}

\begin{proposition} \label{imageColumnSpace}
Let $L$ be a linear map. Then
\[ \im(L) = \Col(A_L). \]
\end{proposition}
This implies that the rank of $L$ is the rank of $A$.

\begin{proposition}
Let $A,B\in\F^{m\times n}$. Then
\[ \Col(A)=\Col(B) \iff \text{$\exists$ invertible $X$ such that $A = BX$}. \]
\end{proposition}
\begin{proof}
$\boxed{\Leftarrow}$ follows from \ref{rowColSubspaces}.

$\boxed{\Rightarrow}$ Let $C$ be a matrix whose columns form a basis of $\Col(A)=\Col(B)$. Then we can find matrices $S,T$ such that $CS = A$ and $CT = B$ are full-rank factorisations and these can be extended to invertible matrices by \ref{extendToInvertible}
\[ X_1 = \begin{bmatrix}
S \\ U
\end{bmatrix} \in \F^{n\times n} \qquad X_2 = \begin{bmatrix}
T \\ V
\end{bmatrix} \in \F^{n\times n}. \]
Now we claim $X = X_2^{-1}X_1$ fulfils the requirements:
\[ BX = (CT + 0V)X_2^{-1}X_1 = \begin{bmatrix}
C & \mathbb{0}
\end{bmatrix}X_2(X_2^{-1}X_1) = \begin{bmatrix}
C & \mathbb{0}
\end{bmatrix}X_1 = CS = A. \]
\end{proof}

\subsubsection{Null space}
\begin{definition}
Let $A\in \F^{m\times n}$ be a matrix. The \udef{null space} $\Null(A)$ of $A$ is the kernel of $\ell_A$. The dimension of $\Null(A)$ is called the \udef{nullity} of $A$.
\end{definition}
In other words:
\[ \Null(A) = \setbuilder{\vec{v}\in \F^n}{A\vec{v} = 0}. \]

\begin{proposition}
Let $A\in\F^{m\times n}$ be a matrix, then
\[ \Null(A) = \Col(A^*)^\perp. \]
\end{proposition}
\begin{proof}
$\vec{v}\in\Null(A) \iff A\vec{v} = 0 \iff \forall \vec{w}\in\F^n:\inner{A\vec{v},\vec{w}}=0 \iff \forall \vec{w}\in\F^n: \inner{\vec{v},A^*\vec{w}}=0 \iff \vec{v}\in\Col(A^*)^\perp$.
\end{proof}

\begin{lemma} \label{dimensionTheoremMatrices}
Let $A\in \F^{m\times n}$ be a matrix. Then
\[ \Rank(A) + \dim\Null(A) = n. \]
\end{lemma}
\begin{proof}
This is the dimension theorem applied to $\ell_A$, using $\im(\ell_A) = \Col(A)$.
\end{proof}

We can also formulate \ref{kernelCompositionLinearMaps} for matrices:
\begin{proposition} \label{nullSpaceProduct}
Let $A,B$ be conformal matrices. Then
\begin{enumerate}
\item $\Null(AB)\supseteq \Null(B)$;
\item $\dim\Null(AB) = \dim\Null(B) + \dim(\Col(B)\cap\Null(A))$.
\end{enumerate}
\end{proposition}
Note that (1) is the opposite inclusion to $\Col(AB)\subseteq \Col(A)$ and $\Row(AB)\subseteq \Row(B)$.
\begin{corollary}[Sylvester's law of nullity]
Let $A,B$ be square matrices. Then
\[ \max\{\dim\Null(A),\dim\Null(B)\} \leq \dim\Null(AB) \leq \dim\Null(A) + \dim\Null(B). \]
\end{corollary}


\subsubsection{Rank equalities and inequalities}
\begin{proposition} \label{rankMultiplication}
Let $A,B,C,D$ be conformal matrices, then
\begin{enumerate}
\item $\Rank(AB) \leq \min\{\Rank(A),\Rank(B)\}$;
\item $\max\{\Rank(A),\Rank(C)\} \leq \Rank \begin{bmatrix}
A & C
\end{bmatrix}$;
\item $\max\{\Rank(A),\Rank(D)\} \leq \Rank \begin{bmatrix}
A \\ D
\end{bmatrix}$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) This is the matrix form of \ref{rankMapComposition}. It is also an immediate consequence of \ref{RowColSpaceProduct}.
\end{proof}

\begin{lemma}
Let $A\in\F^{m\times n}$, then
\[ \Rank(A) \leq \min\{m,n\}. \]
\end{lemma}
\begin{definition}
Let $A\in\F^{m\times n}$.
\begin{itemize}
\item If $\Rank(A) = \min\{m,n\}$, we say $A$ has \udef{full rank}.
\item If $\Rank(A) = m$, we say $A$ has \udef{full row rank}.
\item If $\Rank(A) = n$, we say $A$ has \udef{full column rank}.
\end{itemize}
\end{definition}

\begin{lemma}
Let $X,A,Y$ be conformal matrices. Then
\begin{enumerate}
\item if $X$ has full column rank, then $\Rank(A) = \Rank(XA)$;
\item if $Y$ has full row rank, then $\Rank(A) = \Rank(AY)$;
\item $A$ is invertible \textup{if and only if} it has full row rank and full column rank.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) By \ref{dimensionTheoremMatrices}, $\Null(X) = \{0\}$. So $\vec{v}\in\Null(XA) \iff \vec{v}\in\Null(A)$, so $\Null(XA) = \Null(A)$. Then \ref{dimensionTheoremMatrices} implies $\Rank(XA) = \Rank(A)$.

(2) $\Rank(AY) = \Rank(Y^\transp A^\transp) = \Rank(A^\transp) = \Rank(A)$.

(3) Consequence of \ref{RowColSpaceInverse}.
\end{proof}

\begin{proposition}[Sylvester's rank inequality] \label{SylvesterRankInequality}
Let $A\in\F^{m\times k}$ and $B\in\F^{k\times n}$, then
\[ \Rank(AB) \geq \Rank(A)+\Rank(B) - k. \]
In addition, the following are equivalent:
\begin{enumerate}
\item $\Null(A)\subseteq \Col(B)$;
\item $\Rank(AB) = \Rank(A)+\Rank(B) - k$.
\end{enumerate}
In particular if $AB$ is a full-rank factorisation, i.e.\ $\Rank(AB) = k$, then (1) and (2) hold.
\end{proposition}
\begin{proof}
Let $AB = XY$ be a full-rank factorisation of $AB$ and set $r=\Rank(AB)$. Then define
\[ C = \begin{bmatrix}
A & X
\end{bmatrix}\in\F^{m\times(k+r)} \quad\text{and}\quad D=\begin{bmatrix}
B \\ -Y
\end{bmatrix}\in\F^{(k+r)\times n}\]
so $CD = \begin{bmatrix}
A & X
\end{bmatrix}\begin{bmatrix}
B \\ -Y
\end{bmatrix} = AB-XY = 0$.
This means that $\Col(D)\subseteq \Null(C)$, so $\Rank(D)\leq \dim\Null(C)$. Then
\[ \Rank(A) + \Rank(B) \leq \Rank(C) + \Rank(D) \leq \Rank(C) + \dim\Null(C) = k+\Rank(AB). \]
using \ref{dimensionTheoremMatrices} for $\Rank(C) + \dim\Null(C) = k+r$.

Now for the equivalent statements:

$\boxed{(1)\Rightarrow (2)}$ In this case \ref{nullSpaceProduct} becomes
\[ \dim\Null(AB) = \dim\Null(A) + \dim\Null(B). \]
Using \ref{dimensionTheoremMatrices}, this becomes
\[ n - \Rank(AB) = k-\Rank(A) + n-\Rank(B), \]
which can be arranged to give (2).

$\boxed{(2)\Rightarrow (1)}$ Assume, towards contraposition, $\Null(A) \nsubseteq \Col(B)$. Then we can find $\vec{v}\in\Null(A)$ such that $\vec{v}\notin \Col(B)$. Then
\[ \Rank \begin{bmatrix}
B & \vec{v}
\end{bmatrix} = \Rank(B) + 1 \qquad\text{and}\qquad \Rank(A \begin{bmatrix}
B & \vec{v}
\end{bmatrix}) = \Rank\begin{bmatrix}
AB & A\vec{v}
\end{bmatrix} = \Rank(AB). \]
And by the inequality
\[ \Rank(AB) \geq \Rank(A)+\Rank(B)+1-k, \]
so in particular $\Rank(AB) > \Rank(A)+\Rank(B)-k$ and thus $\Rank(AB) \neq \Rank(A)+\Rank(B)-k$.

Finally:

$\boxed{(\Rank(AB) = k)\Rightarrow (1)}$ From $\Rank(AB)\leq\Rank(A)\leq k$ and $\Rank(AB)\leq\Rank(B)\leq k$ we get
\[ \Rank(A) = \Rank(B) = \Rank(AB) = k \]
and so
\[ \Rank(A) + \Rank(B) - k = k =\Rank(AB). \]
\end{proof}
\begin{corollary}
Let $A\in\F^{m\times k}$ and $B\in\F^{k\times n}$ such that $\Null(A)\subseteq \Col(B)$, then
\[ AB = \mathbb{0}_{m\times n} \iff \Col(B)\subseteq \Null(A) \iff \Rank(A) + \Rank(B) = k \]
\end{corollary}

\begin{proposition}[Frobenius's rank inequality]
Let $A,B,C$ be conformal matrices, then
\[ \Rank(ABC) \geq \Rank(AB)+\Rank(BC) - \Rank(B). \]
\end{proposition}
\begin{proof}
Let $B = XY$ be a full-rank factorisation. Then
\begin{align*}
\Rank(ABC) &= \Rank(AXYC) \geq \Rank(AX) + \Rank(YC) - \Rank(B) \\
&\geq \Rank(AXY) + \Rank(XYC) - \Rank(B) = \Rank(AB)+\Rank(BC) - \Rank(B)
\end{align*}
using Sylvester's rank inequality \ref{SylvesterRankInequality}.
\end{proof}

\begin{proposition}
Let $A,B$ be conformal matrices. Then
\[ |\Rank(A)-\Rank(B)|\leq\Rank(A+B)\leq \Rank(A) + \Rank(B). \]
\end{proposition}
\begin{proof}
Let $A=X_1Y_1$ and $B=X_2Y_2$ be full-rank factorisations with $r = \Rank(A)$ and $s=\Rank(B)s$. Define
\[ C = \begin{bmatrix}
X_1 & X_2
\end{bmatrix}\qquad\text{and}\qquad \begin{bmatrix}
Y_1 \\ Y_2
\end{bmatrix}. \]
Then $CD = X_1Y_1 + X_2Y_2 = A+B$.

The second inequality follows from \ref{rankMultiplication}:
\[ \Rank(A+B) = \Rank(CD) \leq \min\{\Rank(C),\Rank(D)\} \leq r+s. \]

The first inequality follows from \ref{rankMultiplication}, which gives $\Rank(C)\geq\max\{r,s\}$ and $\Rank(D)\geq\max\{r,s\}$, Sylvester's rank inequality \ref{SylvesterRankInequality}, which gives
\[ \Rank(A+B) \geq \Rank(C) + \Rank(D) - (r+s) \geq r+r-(r+s) = r-s \]
and
\[ \Rank(A+B) \geq \Rank(C) + \Rank(D) - (r+s) \geq s+s-(r+s) = s-r. \]
These combine to give the first inequality.
\end{proof}

\begin{proposition}[Guttman rank additivity formula]
Let $M=\begin{pmatrix}
A & B \\ C & D
\end{pmatrix}$ and $A$ or $D$ invertible, then
\begin{align*}
\Rank(M) &= \Rank(D) + \Rank(A-BD^{-1}C) \\
&= \Rank(A) + \Rank(D-CA^{-1}B).
\end{align*}
\end{proposition}
\begin{proof}
This uses the Schur complement and the fact that the transformation matrices are invertible.
\end{proof}

\subsubsection{The index of matrices}
\begin{proposition}
Let $A\in\F^{n\times n}$ be a square matrix. Then
\begin{enumerate}
\item $\Rank(A^k)\geq \Rank(A^{k+1})$ for all $k\in\N$;
\item if $\Rank(A^k) = \Rank(A^{k+1})$, then for all $p\in\N$
\[ \Rank(A^k) = \Rank(A^{k+p}) \qquad\text{and}\qquad \Col(A^k) = \Col(A^{k+p}); \]
\item there is a least integer $q\in [0,n]$ such that $\Rank(A^q) = \Rank(A^{q+1})$.
\end{enumerate}
\end{proposition}
These assertions remain true for exponent zero so long as we define $A^0 = \mathbb{1}_n$.

\begin{definition}
The \udef{index} of a square matrix is the least positive integer $q$ such that $\Rank(A^q) = \Rank(A^{q+1})$.
\end{definition}
In particular, invertible matrices have index zero.

\section{Inner products and matrices}
TODO: adjoint $A^*$!
\subsection{Orthogonal matrices}
\begin{proposition}
Let $A\in\R^{n\times n}$ and $\R^n$ have the standard inner product. The following are equivalent:
\begin{enumerate}
\item The columns of $A$ form an orthonormal basis of $\R^n$;
\item The rows of $A$ form an orthonormal basis of $\R^n$;
\item $A^\transp A = \mathbb{1}_n$;
\item $A^{-1} = A^\transp$;
\end{enumerate}
\end{proposition}
\begin{definition}
A matrix $A\in\R^{n\times n}$ satisfying any of the above is called an \udef{orthogonal matrix}.
\end{definition}
We can formulate the spectral theorem as follows:
\begin{proposition}
Let $A\in\R^{n\times n}$ be a symmetric matrix. The there exists an orthogonal matrix $P\in \R^{n\times n}$ such that $P^{-1}AP = P^\transp A P$ is a diagonal matrix.
\end{proposition}

$\det(A) = \pm 1$. Orthogonal matrix means orthogonal transformation.
For orthogonal matrix $P$: $\norm{A}_F = \norm{PA}_F$
s




\section{Matrix operations}
All functions on $\F$ can of course be extended component-wise to functions on $\F^{m\times n}$. In particular, let $\overline{A}$ be the component-wise complex conjugate of $A\in\F^{m\times n}$.
\subsection{Elementary row and column operations}
\begin{definition}
Let $A$ be an $(m\times n)$-matrix. The following are \udef{elementary row operations} (EROs):
\begin{itemize}[leftmargin=3cm]
\item[$\boxed{R_i \to \lambda R_i}$] Replacing a row by a non-zero multiple (i.e.\ $\lambda \neq 0$).
\item[$\boxed{R_i \leftrightarrow R_j}$] Swapping two rows.
\item[$\boxed{R_i \to R_i+ \lambda R_j}$] Adding a multiple of a row to a different row.
\end{itemize}
The \udef{elementary column operations} (ECO) are these operations applied to the columns. The are the operations given by transposing the matrix, applying an ERO and transposing again.

Two matrices $A,B$ are called \udef{row equivalent} if it is possible to obtain $B$ from $A$ by applying EROs. We write $A\sim_R B$ or just $A\sim B$.

Two matrices $A,B$ are called \udef{column equivalent} if it is possible to obtain $B$ from $A$ by applying ECOs. We write $A\sim_C B$.

Two matrices $A,B$ are called \udef{row+column equivalent} if it is possible to obtain $B$ from $A$ by applying EROs and ECOs. We write $A\sim_{R+C} B$.
\end{definition}
In general the theory will be developed for EROs. The relevant results for ECOs are obtained by transposition.

\begin{lemma}
The relations $\sim_R, \sim_C$ and $\sim_{R+C}$ are equivalence relations.
\end{lemma}

\begin{lemma} \label{matricesEROs}
Applying the elementary row operations to an $(n\times m)$-matrix is the same as multiplying from the left by the matrices obtained by applying the ERO to the $(n\times n)$-unit matrix:
\begin{itemize}[leftmargin=3cm]
\item[$\boxed{R_i \to \lambda R_i}$]
\[ \begin{pmatrix}
1 &&&&&& \\
 & \ddots &&&&& \\ 
  & & 1 &&&& \\
 & &  & \lambda &&& \\
 & &  & & 1 && \\
 &&&&& \ddots & \\
 &&&&&& 1
\end{pmatrix} \]
\item[$\boxed{R_i \leftrightarrow R_j}$]
\[ \begin{pmatrix}1&&&&&&\\&\ddots &&&&&\\&&0&&1&&\\&&&\ddots &&&\\&&1&&0&&\\&&&&&\ddots &\\&&&&&&1\end{pmatrix} \]
\item[$\boxed{R_i \to R_i+ \lambda R_j}$]
\[ \begin{pmatrix}1&&&&&&\\&\ddots &&&&&\\&&1&&&&\\&&&\ddots &&&\\&&m&&1&&\\&&&&&\ddots &\\&&&&&&1\end{pmatrix} \]
\end{itemize}
\end{lemma}
\begin{corollary}
ECOs can be performed by multiplying by an invertible matrix from the right.
\end{corollary}
\begin{corollary}
Let $A,B\in\F^{m\times n}$ be matrices.
\begin{enumerate}
\item If $A\sim_R B$, then $\Row(A) = \Row(B)$ and $\Null(A) = \Null(B)$.
\item If $A\sim_C B$, then $\Col(A) = \Col(B)$.
\end{enumerate}
\end{corollary}
\begin{corollary}
Let $A,B\in\F^{m\times n}$ be matrices such that $A\sim_{R+C} B$. Then $\Rank(A) = \Rank(B)$ and $\dim\Null(A) = \dim\Null(B)$.
\end{corollary}
\begin{proof}
The dimension of the null space is preserved because $\dim\Null(A) = n - \Rank(A)$, by \ref{dimensionTheoremMatrices}.
\end{proof}

\subsubsection{Gauss-Jordan elimination}
\begin{definition}
Let $A$ be an $(n\times m)$-matrix.
\begin{itemize}
\item The first non-zero element from the left on each row is called the \udef{leading coefficient} of \udef{pivot} of that row.
\item The matrix $A$ is in \udef{row echelon form} (ref) if
\begin{itemize}
\item rows of all zeros are at the bottom;
\item the leading coefficients of all other rows are one;
\item the leading coefficients of each non-zero row is strictly to the right of the leading coefficient of the row above it.
\end{itemize}
\item The matrix $A$ is in \udef{reduced row echelon form} (rref) if
\begin{itemize}
\item it is in echelon form;
\item all the coefficients in the column above a leading coefficient are zero.
\end{itemize}
\end{itemize}
\end{definition}

\begin{proposition}[Gauss-Jordan elimination]
Any matrix is row equivalent to a matrix in reduced echelon form.
\end{proposition}
The proof gives an algorithm for computing the reduced echelon form. Sometimes the part of the algorithm that brings the matrix to an (unreduced) echelon form is called Gaussian elimination. We first give an algorithm for Gaussian elimination and then use this for full Gauss-Jordan elimination.
\begin{proof}[Proof (Gaussian elimination)]
Let $A\in\F^{m\times n}$ be the input-matrix. The algorithm is recursive, we call it $\operatorname{ref}$.
\begin{enumerate}
\item If $A$ is empty (i.e.\ $m=0$ or $n=0$), then return $A$.
\item Find the entry in the first column of largest non-zero absolute value. Swap the corresponding row with the first row.
\begin{itemize}
\item If all the entries in the first column are zero, return
\[ \begin{pmatrix}
0 & \operatorname{ref}\left([A]_{1:m,2:n}\right)
\end{pmatrix}. \]
\item The algorithm also works if we choose any non-zero entry, not necessarily the largest.
\end{itemize}
\item Divide the first row by $[A]_{1,1}$.
\item Perform the ERO $R_i\to R_i - [A]_{i,1}R_1$ for each row $i\in 2:m$.
\item Return
\[ \begin{pmatrix}
1 & [A]_{1,2:n} \\
0 & \operatorname{ref}\left([A]_{2:m,2:n}\right)
\end{pmatrix}. \]
\end{enumerate}
The algorithm terminates because each subsequent call involves a matrix of strictly smaller dimensions.
\end{proof}
\begin{proof}[Proof (Obtaining the reduced echelon form)]
For each leading coefficient $[A]_{ij}$, perform the EROs $R_k\to R_k-[A]_{k,j}R_i$ for all rows $k\in 1:i-1$.
\end{proof}

\begin{corollary}
Let $A$ be any matrix. Then
\[ A \;\sim_{R+C}\; \begin{pmatrix}
\mathbb{1}_k & 0^{k\times p} \\
0^{q\times k} & 0^{q\times p}
\end{pmatrix}. \]
The row and column ranks are both equal to $k$ and the nullity is equal to $p$.

This also means we can write
\[ A = P\begin{pmatrix}
\mathbb{1}_k & 0^{k\times p} \\
0^{q\times k} & 0^{q\times p}
\end{pmatrix}Q \]
for some invertible matrices $P,Q$.
\end{corollary}
This factorisation of $A$ is called the \udef{rank normal form}.


We can use Gauss-Jordan elimination to find bases for $\Row(A), \Col(A)$ and $\Null(A)$:
\begin{itemize}
\item[$\boxed{\Row(A)}$] The row space is preserved by row equivalence. So we can perform Gauss-Jordan elimination, discard the null rows and the remaining rows will form a basis of $\Row(A)$.
\item[$\boxed{\Col(A)}$] Applying an ERO to a matrix $A$ is the same as multiplying from the left by some invertible matrix $E$. Due to \ref{multiplicationBlockMatrices},
\[ EA = \begin{pmatrix}
E[A]_{-,1} & \hdots & E[A]_{-,m}
\end{pmatrix}, \]
so $\Col(EA) = \ell_E[\Col(A)]$ and because $\ell_E$ is an isomorphism, we have $\Col(A) \cong \Col(EA)$.

In the echelon form it is easy to see that the columns containing leading elements form a basis. By applying the inverse map we see that the corresponding columns in the original matrix form a basis of the original column space $\Col(A)$, see \ref{mappingOfBasisByIsomorphism}.
\item[$\boxed{\Null(A)}$] The row space is preserved by row equivalence, so we first perform Gauss-Jordan elimination $A\sim_R U$. Then solve $Ux=0$ for $x$ (see later).
\end{itemize}

\begin{proposition}
Let $L:\F^n\to\F^m$ be a linear map. Then for any bases $\beta_n,\beta_m$ of $\F^n$ and $\F^m$, we have
\[ (L)_{\beta_n}^{\beta_m} \;\sim_{R+C}\; \begin{pmatrix}
\mathbb{1}_k & 0^{k\times q} \\
0^{p\times k} & 0^{p\times q}
\end{pmatrix} \]
and
\begin{enumerate}
\item $L$ is injective \textup{if and only if} $p=0$;
\item $L$ is surjective \textup{if and only if} $q=0$.
\end{enumerate}
\end{proposition}


\subsubsection{Calculating inverse matrices}
Any invertible square matrix $A$ is row equivalent to $\mathbb{1}_n$. We want to find the matrix associated to this row reduction. One way to keep track of this matrix is to simultaneously apply the EROs to $A$ and $\mathbb{1}_n$:
\begin{lemma}
Let $A\in\F^{n\times n}$ be a square matrix. Then
\[ \left(\begin{array}{c|c} A & \mathbb{1}_n \end{array}\right) \sim_R \left(\begin{array}{c|c} \mathbb{1}_n & A^{-1} \end{array}\right). \]
\end{lemma}
\begin{proof}
$A^{-1}\begin{pmatrix}
A & \mathbb{1_n}
\end{pmatrix} = \begin{pmatrix}
\mathbb{1}_n & A^{-1}
\end{pmatrix}$.
\end{proof}


\subsection{The trace}
\begin{definition}
Let $A\in \mathbb{F}^{n\times n}$ be a square matrix. The \udef{trace} of $A$, denoted $\Tr(A)$, is the sum of the diagonal entries of $A$:
\[ \Tr(A) = \sum_{i=1}^n (A)_{i,i}. \]
\end{definition}
\begin{proposition}
Let $A\in \mathbb{F}^{n\times n}$ be a square matrix, then
\begin{enumerate}
\item $\Tr(cA+B) = c\Tr(A) + \Tr(B)$ for all $c\in\F$;
\item $\Tr(\overline{A}) = \overline{\Tr(A)}$;
\item $\Tr(A^\transp) = \Tr(A)$;
\item $\Tr(A^*) = \overline{\Tr(A)}$.
\end{enumerate}
\end{proposition}
\begin{proposition}
Let $A,B,C \in\F^{n\times n}$ be square matrices. Then
\begin{enumerate}
\item $\Tr(AB) = \sum_{i,j=1}^n [A]_{i,j}[B]_{j,i}$;
\item $\Tr(AB) = \Tr(BA)$;
\item $\Tr(ABC) = \Tr(CAB) = \Tr(BCA)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Point (1) follows by direct computation
\[ \Tr(AB) = \sum_{i=1}^n (AB)_{i,i} = \sum_{i=1}^n\sum_{j=1}^n A_{i,j}B_{j,i}. \]

(2) We use (1) and the fact that $[A]_{i,j}[B]_{j,i} = [B]_{i,j}[A]_{j,i}$.

(3) Follows straight from (2): $\Tr\big((AB)C\big) = \Tr\big(C(AB)\big)$ etc.
\end{proof}
The trace of a product of matrices is invariant under any cyclic permutation of the matrices.

The trace of a product of matrices is not invariant under noncyclic permutation of the matrices:
\[ \Tr(ABC) \neq \Tr(BAC). \]



\begin{lemma} \label{averageTraceOverDiagonal}
Let $A\in\F^{n\times n}$. Then $A$ is similar to a matrix $B$ with
\[ [B]_{i,i} = \frac{1}{n}\Tr(A) \qquad \forall i\leq n. \]
\end{lemma}
\begin{proof}
It is enough to show that this holds for matrices with zero trace: if $A$ is any matrix, then $A- \frac{\Tr(A)}{n}\mathbb{1}$ is a matrix with zero trace. If this is similar to a matrix $S(A- \frac{\Tr(A)}{n}\mathbb{1})S^{-1}$ with zeros on the diagonal, then $SAS^{-1}$ has $\frac{\Tr(A)}{n}$ on the diagonal.

So assume WLOG that $\Tr(A) = 0$ and $A\neq 0$. We can find $\vec{v}\in\F^n$ such that $\{\vec{v}, A\vec{v}\}$ is linearly independent: assume, towards contraposition, that $A\vec{v}$ is a scalar multiple of $\vec{v}$, for all $\vec{v}$. This must be the same multiple $\lambda$ for all $\vec{v}$. If not, i.e.\ there exist $\vec{v},\vec{w}$ such that $A\vec{v} = \lambda_1 \vec{v}$ and $A\vec{w} = \lambda_2 \vec{w}$ with $\lambda_1\neq \lambda_2$, then $\vec{v}+\vec{w}$ is not mapped to a multiple. In this case $A$ is $\lambda \id$, but $\Tr(A) = 0$ fixes $\lambda=0$, which we excluded.

Now we can extend $\{\vec{v}, A\vec{v}\}$ to a basis of $\F^n$ and put these as columns in a matrix $S = \begin{bmatrix}
\vec{v} & A\vec{v} & S_1
\end{bmatrix}$. We can partition $S^{-1} = \begin{bmatrix}
\vec{x}^\transp \\ S_2
\end{bmatrix}$. Then we have
\[ S^{-1}S = \begin{bmatrix}
\vec{x}^\transp \vec{v} & \vec{x}^\transp A\vec{v} & \vec{x}^\transp S_1 \\ S_2\vec{v} & S_2A\vec{v} & S_2S_1
\end{bmatrix} = \mathbb{1}. \]
In particular $\vec{x}^\transp A\vec{v} = 0$. Then
\[ S^{-1}AS = \begin{bmatrix}
\vec{x}^\transp A\vec{v} & \vec{x}^\transp A^2\vec{v} & \vec{x}^\transp A S_1 \\
S_2A\vec{v} & S_2A^2\vec{v} & S_2AS_1
\end{bmatrix}  = \begin{bmatrix}
0 & \star \\ \star & S_2AS_1
\end{bmatrix}. \]
Now $S_2AS_1$ has trace zero and we can repeat the argument. By induction we can make all elements on the diagonal zero.
\end{proof}



\begin{proposition}
The trace of a matrix of a linear map is independent of the choice of basis:
\[ \Tr(L)_{\beta}^{\beta} = \Tr(L)_{\beta'}^{\beta'}. \]
\end{proposition}
\begin{proof}
\[ \Tr(L)_{\beta'}^{\beta'} = \Tr\left[((I)_{\beta'}^{\beta})^{-1}(L)_\beta^\beta(I)_{\beta'}^{\beta}\right] = \Tr\left[(L)_\beta^\beta(I)_{\beta'}^{\beta}((I)_{\beta'}^{\beta})^{-1}\right] = \Tr(L)_\beta^\beta \]
\end{proof}
This allows us to make the following definition:
\begin{definition}
The \udef{trace} of a linear map on a finite-dimensional vector space is the trace of any matrix representation of that map.
\end{definition}
\subsection{The determinant}
\begin{definition}
A map
\[ f: \mathbb{F}^{n\times n}\to \mathbb{F} \]
with the following properties
\begin{itemize}
\item[\textbf{D-1}] $f(\mathbb{1}_n) = 1$;
\item[\textbf{D-2}] $f(A)$ changes sign if two rows in $A$ are swapped;
\item[\textbf{D-3}] $f$ is linear in the first row:
\[ f(\begin{pmatrix}
\lambda A_{1,-} + \mu A'_{1,-} \\ A_{2,-} \\ \vdots \\ A_{n,-} 
\end{pmatrix}) = \lambda f(\begin{pmatrix}
A_{1,-} \\ A_{2,-} \\ \vdots \\ A_{n,-} 
\end{pmatrix}) + \mu f(\begin{pmatrix}
A'_{1,-} \\ A_{2,-} \\ \vdots \\ A_{n,-} 
\end{pmatrix}); \]
\end{itemize}
is called a \udef{determinant map}.
\end{definition}
We will show that there exists one and only one determinant map. We are therefore justified in calling it the determinant.

The determinant of $A$ is often denoted $\det(A)$ or $|A|$.

\begin{lemma}
Let $f: \mathbb{F}^{n\times n}\to \mathbb{F}$ be a determinant map.
\begin{enumerate}
\item $f$ is linear in each row;
\item if a matrix $A$ has a row of zeros, or two identical rows, then $f(A) = 0$;
\item the ERO $R_i \to R_i+ \lambda R_j$ does not change the determinant;
\item the ERO $R_i \leftrightarrow R_j$ changes the sign of the determinant;
\item the ERO $R_i \to \lambda R_i$ multiplies the determinant by $\lambda$;
\item $f(A)$ is non-zero \textup{if and only if} $A\sim_R \mathbb{1}_n$;
\item $f(A)$ is non-zero \textup{if and only if} $A$ is invertible.
\end{enumerate}
\end{lemma}

\subsubsection{Leibniz formula}
\begin{proposition}[Liebniz formula]
There is one and only one determinant map. It is given by
\[ \det: \F^{n\times n}\to \F: A\mapsto \sum_{\sigma\in S^n}\sgn(\sigma)\prod_{i=1}^n[A]_{i,\sigma(i)}. \]
\end{proposition}
\begin{proof}
Let $f$ be a determinant map and $\mathcal{E} = \{\vec{e}_i\}_{i=1}^n$ the standard basis of $\F^n$, considered as rows and $A\in\F^{n\times n}$. Then
\[ f(A) = f(\begin{pmatrix}
\sum_{j_1=1}^n [A]_{1j_1}\vec{e}_{j_1} \\ \vdots \\ \sum_{j_n=1}^n [A]_{nj_n}\vec{e}_{j_n}
\end{pmatrix}) = \sum_{j_1,\ldots, j_n=1}[A]_{1j_1}\ldots[A]_{nj_n}f(\begin{pmatrix}
\vec{e}_{j_1} \\ \vdots \\ \vec{e}_{j_n}
\end{pmatrix}). \]
Now $f(\begin{pmatrix}
\vec{e}_{j_1} \\ \vdots \\ \vec{e}_{j_n}
\end{pmatrix})$ is only non-zero if all $j_i$ are different, i.e.\ $j_i = \sigma(i)$ for some permutation $\sigma\in S^n$. Also
\[ f(\begin{pmatrix}
\vec{e}_{\sigma(1)} \\ \vdots \\ \vec{e}_{\sigma(n)}
\end{pmatrix}) = \sgn(\sigma)f(\mathbb{1}_n) = \sgn(\sigma). \]
So the only possible candidate for a determinant map is
\[ f(A) = \sum_{\sigma\in S^n}\sgn(\sigma)\prod_{i=1}^n[A]_{i,\sigma(i)}. \]
This satisfies \textbf{D-1}, \textbf{D-2} and \textbf{D-3}.
\end{proof}
In the case of $3\times 3$ the Leibniz formula reduces to
\[ \begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{vmatrix} = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{31}a_{22}a_{13} - a_{32}a_{23}a_{11} - a_{33}a_{23}a_{12}. \]
The rule of Sarrus is the following mnemonic:
\begin{corollary}[Rule of Sarrus]
The determinant of a $3\times 3$ matrix can be seen as the sum of the following diagonals (with the correct sign):
\[ \begin{tikzpicture}
\node (A) {$\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{vmatrix}$};
\node[right of=A, node distance=5.8em] (B) {$\begin{matrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & a_{32}
\end{matrix}$};
\draw (A.south west) -- ++(7.5em,4em) ++(.2,.1) node {--};
\draw (A.south west) ++(2em,0) --  ++(7.5em,4em) ++(.2,.1) node {--};
\draw (A.south west) ++(4em,0) --  ++(7.5em,4em) ++(.2,.1) node {--};
\draw (A.north west) -- ++(7.5em,-4em) ++(.2,-.1) node {+};
\draw (A.north west) ++(2em,0) --  ++(7.5em,-4em) ++(.2,-.1) node {+};
\draw (A.north west) ++(4em,0) --  ++(7.5em,-4em) ++(.2,-.1) node {+};
\end{tikzpicture} \]
\end{corollary}

\subsubsection{Laplace expansion}
\begin{definition}
Let $A\in\F^{n\times n}$ be a square matrix. A \udef{minor determinant} or \udef{minor} is the determinant of a submatrix. In particular the \udef{$(i,j)$-minor} $M_{i,j}$ is the minor
\[ M_{i,j} \defeq \det([A]_{(1:n)\setminus\{i\},(1:n)\setminus\{j\}}). \]

The \udef{$(i,j)$-cofactor} $C_{i,j}$ is defined as
\[ C_{i,j} \defeq (-1)^{i+j}M_{i,j}. \]
\end{definition}
\begin{proposition}[Laplace expansion] \label{LaplaceExpansion}
Let $A\in\F^{n\times n}$. Then
\[ \det(A) = \sum_{i=1}^n [A]_{i,j}C_{i,j} = \sum_{i=1}^n [A]_{j,i}C_{j,i} \]
for all $j\in 1:n$.
\end{proposition}
This is also known as ``expansion along the $j^\text{th}$ column'', resp., row.
\begin{proof}
Fix some $j\in 1:n$. Then we can calculate
\begin{align*}
\det(A) &=  \sum_{\sigma\in S^n}\sgn(\sigma)\prod_{k=1}^n[A]_{k,\sigma(k)} = \sum_{i=1}^n\sum_{\substack{\sigma\in S^n \\ \sigma(i)=j}}\sgn(\sigma)\prod_{k=1}^n[A]_{k,\sigma(k)} \\
&= \sum_{i=1}^n [A]_{i,j}\sum_{\substack{\sigma\in S^n \\ \sigma(i)=j}}\sgn(\sigma)\prod_{k\in (1:n)\setminus\{j\}}^n[A]_{k,\sigma(k)} = \sum_{i=1}^n [A]_{i,j}C_{i,j}.
\end{align*}
The expression for column expansion can be obtained by transposition.
\end{proof}

\subsubsection{Volume}

\subsubsection{Properties}
\begin{lemma}
Let $A\in\F^{n\times n}$. Then
\[ \det(A^\transp) = \det(A). \]
\end{lemma}
\begin{proof}
We calculate using the Leibniz formula:
\begin{align*}
\det(A^\transp) &= \sum_{\sigma\in S^n}\sgn(\sigma)\prod_{i=1}^n[A^\transp]_{i,\sigma(i)} = \sum_{\sigma\in S^n}\sgn(\sigma)\prod_{i=1}^n[A]_{\sigma(i),i} \\
&= \sum_{\sigma\in S^n}\sgn(\sigma)\prod_{i=1}^n[A]_{i,\sigma^{-1}(i)} = \sum_{\sigma\in S^n}\sgn(\sigma^{-1})\prod_{i=1}^n[A]_{i,\sigma^{-1}(i)} = \det(A).
\end{align*}
\end{proof}

\begin{lemma}
Let $A\in\F^{n\times n}$. Then
\begin{enumerate}
\item $\det(\lambda A) = \lambda^n\det(A)$;
\item $\det(\overline{A}) = \overline{\det(A)}$;
\item $\det(A^*) = \overline{\det(A)}$;
\item if $A$ is triangular, then
\[ \det(A) = \prod_{i=1}^n [A]_{i,i}. \]
\end{enumerate}
\end{lemma}

\begin{proposition}
Let $A,B\in\F^{n\times n}$. Then
\[ \det(AB) = \det(A)\det(B). \]
\end{proposition}
\begin{proof}
First assume $\det(B) = 0$. Then $AB$ is not invertible, for if $AB$ were invertible, $B$ would have inverse $(AB)^{-1}A$. So $\det(AB) = 0$ and the formula holds.

Now assume $\det(B) \neq 0$, then $A\mapsto \det(AB)/\det(B)$ satisfies the definition of a determinant map and thus is equal to $\det$.
\end{proof}
\begin{corollary}
If $U$ is unitary, then $|\det(U)| = 1$.
\end{corollary}
\begin{proof}
$1=\det{\mathbb{1}} = \det(U^*U) = \det(U^*)\det(U) = \overline{\det(U)}\det(U) = |\det(U)|$.
\end{proof}
\begin{corollary}
Let $A,B\in\F^{n\times n}$. Then
\[ \det(AB) = \det(BA). \]
In fact we can arbitrarily commute any matrix product inside the determinant.
\end{corollary}
\begin{corollary}
The determinant of a matrix of a linear map is independent of the choice of basis:
\[ \det(L)_{\beta}^{\beta} = \det(L)_{\beta'}^{\beta'}. \]
\end{corollary}
This allows us to make the following definition:
\begin{definition}
The \udef{determinant} of a linear map on a finite-dimensional vector space is the determinant of any matrix representation of that map.
\end{definition}

\begin{lemma}
Let $A\in\F^{m\times m}$ and $n\in\N$. Then
\[ \det\begin{bmatrix}
\mathbb{1}_n & \mathbb{0} \\ \mathbb{0} & A
\end{bmatrix} = \det(A) = \det\begin{bmatrix}
A & \mathbb{0} \\ \mathbb{0} & \mathbb{1}_n
\end{bmatrix}. \]
\end{lemma}
\begin{proof}
By induction on $n$.
\end{proof}

\begin{lemma}
Let $A,B,C$ be conformal matrices and $A,D$ square. Then
\[ \det\begin{bmatrix}A & B \\ \mathbb{0} & D \end{bmatrix} = \det(A)\det(D). \]
\end{lemma}
\begin{proof}
This follows from
\[ \begin{bmatrix}A & B \\ \mathbb{0} & D \end{bmatrix} = \begin{bmatrix}\mathbb{1} & \mathbb{0} \\ \mathbb{0} & D \end{bmatrix} \begin{bmatrix}\mathbb{1} & B \\ \mathbb{0} & \mathbb{1} \end{bmatrix} \begin{bmatrix}A & \mathbb{0} \\ \mathbb{0} & \mathbb{1} \end{bmatrix}, \]
the product rule, the previous lemma and the fact that the central matrix is triangular with only ones on the diagonal.
\end{proof}

\begin{lemma} \label{blockDeterminant}
Let $M = \begin{pmatrix}
A & B \\ C & D
\end{pmatrix}$ be a partitioned matrix. Then
\begin{align*}
\det(M) &= \det(A)\det(D-CA^{-1}B) \\
&= \det(D)\det(A-BD^{-1}C)
\end{align*}
if $A$, resp. $D$, is invertible.
\end{lemma}
\begin{proof}
This follows straight from the Schur complement.
\end{proof}
\begin{corollary}[Cauchy expansion of the determinant]
Let $A\in\F^{n\times n}, \vec{x},\vec{y}\in\F^n$ and $c\in \F$. Then
\[ \det \begin{bmatrix}
c & \vec{x}^\transp \\ \vec{y} & A
\end{bmatrix} = (c-\vec{x}^\transp A^{-1}\vec{y})\det(A) = c\det(A) - \vec{x}^\transp \adj(A) \vec{y}. \]
\end{corollary}
\begin{corollary}
Let $A,B,C,D\in\F^{n\times n}$.
\begin{enumerate}
\item If $A$ or $D$ is invertible and commutes with $B$, then
\[ \det\begin{bmatrix}
A & B \\ C & D
\end{bmatrix} = \det(DA-CB). \]
\item If $A$ or $D$ is invertible and commutes with $C$, then
\[ \det\begin{bmatrix}
A & B \\ C & D
\end{bmatrix} = \det(AD-BC). \]
\end{enumerate}
\end{corollary}

\begin{lemma}
Let $a,b\in\R$. Then
\[ \det(a\mathbb{1}_n+b\mathbb{J}_n) = a^{n-1}(a+nb). \]
\end{lemma}
\begin{proof}
We can partition $a\mathbb{1}_n+b\mathbb{J}_n$ and use the ERO $R_i\to R_i-R_1$ for $1<i\leq n$ to obtain:
\[ a\mathbb{1}_n+b\mathbb{J}_n = \begin{pmatrix}
a+b & b\mathbb{J}^{1\times (n-1)} \\
b\mathbb{J}^{(n-1)\times 1} & a\mathbb{1}_{n-1}+b\mathbb{J}_{n-1}
\end{pmatrix} = \begin{pmatrix}
a+b & b\mathbb{J}^{1\times (n-1)} \\
-a\mathbb{J}^{(n-1)\times 1} & a\mathbb{1}_{n-1}
\end{pmatrix}. \]
Then by \ref{blockDeterminant}, we have
\[ \det(a\mathbb{1}_n+b\mathbb{J}_n) = a^{n-1}(a+b +b\mathbb{J}^{1\times (n-1)}\mathbb{J}^{(n-1)\times 1}) = a^{n-1}(a+nb). \]
\end{proof}

\begin{lemma}[Weinstein-Aronszajn identity]
Let $A\in\F^{m\times n}$ and $B\in\F^{n\times m}$. Then
\[ \det(\mathbb{1}_{m} + AB) = \det(\mathbb{1}_{n} + BA). \]
Also, for any $\lambda\in\R_0$,
\[ \det(AB - \lambda\mathbb{1}_{m}) = (-\lambda)^{m-n}\det(BA - \lambda\mathbb{1}_{n}). \]
\end{lemma}
This is also sometimes referred to as the Sylvester determinant identity.
\begin{proof}
Applying the two equalities in \ref{blockDeterminant} to the matrix
\[ M = \begin{bmatrix}
\mathbb{1}_m & -A \\
B & \mathbb{1}_n
\end{bmatrix} \]
give
\begin{align*}
M &= \det(\mathbb{1}_m)\det(\mathbb{1}_n - B\mathbb{1}_m^{-1}(-A)) = \det(\mathbb{1}_n+BA) \\
&= \det(\mathbb{1}_n)\det(\mathbb{1}_m - (-A)\mathbb{1}_n^{-1}B) = \det(\mathbb{1}_m+AB).
\end{align*}
\end{proof}
\begin{corollary}[Matrix determinant lemma]
Let $A\in\F^{n\times n}$ and $\vec{u},\vec{v}\in\F^n$. Then
\begin{align*}
\det(A+\vec{u}\vec{v}^\transp) &= \det(A)\det(\mathbb{1}_n + A^{-1}\vec{u}\vec{v}^\transp) \\
&= \det(A)(\mathbb{1}_n + \vec{v}^\transp A^{-1}\vec{u}) \\
&= \det(A) + \vec{v}^\transp \adj(A)\vec{u},
\end{align*}
which is interesting because $\vec{v}^\transp A^{-1}\vec{u} \in\F$  and $\vec{v}^\transp \adj(A)\vec{u}\in\F$ are scalars.
\end{corollary}

TODO
\[ \log\det M=\Tr\log M \]


\subsection{Adjugate}
\begin{definition}
Let $A\in\F^{n\times n}$ be a square matrix. The \udef{adjugate matrix} or \udef{classical adjoint} $\adj(A)$ is the transposed cofactor matrix:
\[ [\adj(A)]_{ij} = C_{ji} \]
where $C_{ij}$ is the $(i,j)-$cofactor.
\end{definition}

\begin{lemma}
Let $A\in\F^{n\times n}$, $\vec{b}\in \F^n$ and $k\in(1:n)$. Then
\[ \det(\left[\begin{cases}
[A]_{ij} & (j\neq k) \\
[\vec{b}]_i & (j=k)
\end{cases}\right]) = [\adj(A)b]_k \]
\end{lemma}
\begin{proof}
We calculate
\[ [\adj(A)\vec{b}]_k = \sum_l [\adj(A)]_{kl}[\vec{b}]_l = \sum_l C_{lk}[\vec{b}]_l. \]
Now in the definition of $C_{lk}$, the $k^\text{th}$ column is excluded. So the $(l,k)-$cofactor of $A$ is the same as the $(l,k)-$cofactor of 
\[ \left[\begin{cases}
[A]_{ij} & (j\neq k) \\
[\vec{b}]_i & (j=k)
\end{cases}\right] \]
which is the matrix where the $k^\text{th}$ column of $A$ is replaced by $\vec{b}$. Then $\sum_l C_{lk}[\vec{b}]_l$ is the determinant of this matrix by \ref{LaplaceExpansion}.
\end{proof}

\begin{proposition} \label{adjunctDeterminant}
Let $A\in\F^{n\times n}$. Then
\[ A\cdot\adj(A) = \adj(A)\cdot A = \det(A) \mathbb{1}_n. \]
\end{proposition}
\begin{proof}
\[ [\adj(A)\cdot A]_{ij} = \sum_k [A]_{ik}C_{jk} \]
Clearly if $i=j$, we have the Laplace expansion \ref{LaplaceExpansion} and the expression equals $\det(A)$. If $i\neq j$, then the $j^\text{th}$ row does not enter into the expression (it is left out of the $(i,j)-$minor) and thus may just as well be replaced by a copy of the $i^\text{th}$ row. In this case we get the expression for the determinant of a matrix with two identical rows. This must be $0$.
\end{proof}
\begin{corollary}
Let $A\in\F^{n\times n}$ be invertible. Then
\[ A^{-1} = \frac{1}{\det(A)}\adj(A). \]
\end{corollary}

\begin{proposition}
Let $A,B\in\F^{n\times n}$ be square matrices. Then
\begin{enumerate}
\item $\adj(\mathbb{0}_n) = \mathbb{0}_n$;
\item $\adj(\mathbb{1}_n) = \mathbb{1}_n$;
\item $\adj(\lambda A) = \lambda^{n-1}\adj(A)$ for any $\lambda\in \F$;
\item $\adj(A^\transp) = \adj(A)^\transp$;
\item $\det(\adj(A)) = \det(A)^{n-1}$;
\item $\adj(AB) = \adj(A)\adj(B)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Points 2. and 5. are direct consequences of \ref{adjunctDeterminant}. TODO rest.
\end{proof}
Cauchy-Binet

\subsection{Generalised inverses or pseudoinverses}
\subsubsection{Moore-Penrose pseudoinverse}

\subsection{Pfaffian}

\subsection{Vectorisation}
For calculations it is often useful to put the matrix of coordinates into a column vector. The process of fitting a matrix into a column vector is known as the \udef{vectorisation} of a matrix. Two obvious ways to do this are by going row-by-row or column by column.
\begin{itemize}
\item Column-by-column we get
\[ \vectorisation_C: \R^{m\times n}\to \R^{mn\times 1}: A \mapsto \vectorisation_C(A) = [a_{1,1},\ldots,a_{m,1}, a_{1,2},\ldots, a_{m,2}, \;\; \ldots \;\; , a_{1,n}, \ldots, a_{m,n}]^\transp \]
\begin{example}
\[ \text{If} \qquad A = \begin{pmatrix}
a & b \\ c & d
\end{pmatrix}, \qquad \text{then} \qquad \vectorisation_C(A) = \begin{pmatrix}
a \\ c \\ b \\ d
\end{pmatrix}. \]
\end{example}
\item Row-by-row we get
\[ \vectorisation_R: \R^{m\times n}\to \R^{mn\times 1}: A \mapsto \vectorisation_R(A) = [a_{1,1},\ldots,a_{1,n}, a_{2,1},\ldots, a_{2,n}, \;\; \ldots \;\; , a_{m,1}, \ldots, a_{m,n}]^\transp \]
\begin{example}
\[ \text{If} \qquad A = \begin{pmatrix}
a & b \\ c & d
\end{pmatrix}, \qquad \text{then} \qquad \vectorisation_R(A) = \begin{pmatrix}
a \\ b \\ c \\ d
\end{pmatrix}. \]
\end{example}
\end{itemize}
Obviously these are related by
\[ \vectorisation_C(A) = \vectorisation_R(A^\transp) \]

Vectorisation is a self-adjunction in the monoidal closed structure of any category of matrices. (TODO)
\subsection{The Hadamard product}
\[ \vectorisation(A\circ B) = \vectorisation(A) \circ \vectorisation(B) \]
This works for both $\vectorisation_C$ and $\vectorisation_R$.
\subsection{The outer product}
Given two column vectors $\vec{u} = [u_1 \hdots u_m]^\transp$ and $\vec{v} = [v_1 \hdots v_n]^\transp$, the \udef{outer product} is defined by
\[ \vec{u}\otimes \vec{v} = \vec{u}\vec{v}^\transp = \begin{bmatrix}
u_1 \\ \vdots \\ u_m
\end{bmatrix} \begin{bmatrix}
v_1 & \hdots & v_n
\end{bmatrix} = \begin{bmatrix}
u_1v_1 & \hdots & u_1v_n \\
\vdots & \ddots & \vdots \\
u_mv_1 & \hdots & u_m v_n
\end{bmatrix} \]
\subsection{The Kronecker product}
Given two matrices $A,B$ of dimensions $m\times n$ and $p\times q$, the \udef{Kronecker product} yields a $(pm\times qn)$-matrix:
\[ A\otimes B = \begin{bmatrix}
a_{1,1}B & \hdots & a_{1,n}B \\
\vdots & \ddots & \vdots \\
a_{m,1}B & \hdots & a_{m,n}B
\end{bmatrix} \]

The same symbol is used as for the outer product, because the Kronecker product can be seen as a generalisation of the outer product. Indeed for column vectors $\vec{u}, \vec{v}$ we have the identities
\begin{align}
\vec{u}\otimes_{\text{Kron}} \vec{v} &= \vectorisation_R(\vec{u}\otimes_\text{outer} \vec{v}) \\
&= \vectorisation_C(\vec{v}\otimes_\text{outer} \vec{u})
\end{align}
and \[ \vec{u}\otimes_\text{outer}\vec{v} = \vec{u}\otimes_{\text{Kron}} \vec{v}^\transp \]

In terms of matrix elements we can write, assuming the indices start at zero
\[ (A\otimes B)_{i,j} = a_{\lfloor{i/p}\rfloor, \lfloor{j/q}\rfloor}b_{i\%p,j\%q}. \]
where $\%$ denotes the remainder. For indices starting from 1, we have
\[ (A\otimes B)_{i,j} = a_{\lceil{i/p}\rceil, \lceil{j/q}\rceil}b_{(i-1)\%p+1,(j-1)\%q+1}. \]

\subsubsection{Properties}
The Kronecker product is bilinear and associative.
\begin{itemize}
\item[\textbf{Transpose}]
\[ (A\otimes B)^\transp = A^\transp \otimes B^\transp \]
\item[\textbf{Determinant}]
\[ \det(A\otimes B) = \det(A)^m\det(B)^n \]
if $A$ is an $n\times n$ matrix and $B$ an $m\times m$ matrix.
\item[\textbf{Trace}]
\[ \Tr(A\otimes B) = \Tr(A)\Tr(B) \]
\item[\textbf{Mixed product}]
Let $A,B,C,D$ be conformal matrices, then
\[ (A\otimes B)(C \otimes D) = (AC)\otimes (BD). \]
The proof is as follows:
\begin{align}
(A\otimes B)(C \otimes D) &= \begin{bmatrix}
a_{1,1}B & \hdots & a_{1,n}B \\
\vdots & \ddots & \vdots \\
a_{m,1}B & \hdots & a_{m,n}B
\end{bmatrix}\begin{bmatrix}
c_{1,1}D & \hdots & c_{1,n}D \\
\vdots & \ddots & \vdots \\
c_{m,1}D & \hdots & c_{m,n}D
\end{bmatrix} \\
&= \begin{bmatrix}
\sum_k a_{1,k}c_{k,1} BD & \hdots & \sum_k a_{1,k}c_{k,p} BD \\
\vdots & \ddots & \vdots \\
\sum_k a_{m,k}c_{k,1} BD & \hdots & \sum_k a_{m,k}c_{k,p} BD
\end{bmatrix}  =  \begin{bmatrix}
(AC)_{1,1} BD & \hdots & (AC)_{1,p} BD \\
\vdots & \ddots & \vdots \\
(AC)_{m,1} BD & \hdots & (AC)_{m,p} BD
\end{bmatrix}\\
&= (AC)\otimes (BD)
\end{align}
Using the fact that multiplication of two block matrices can be carried out as if their blocks were scalars.

As an immediate consequence:
\[ A\otimes B = (I_n\otimes B)(A\otimes I_k) = (A\otimes I_k)(I_n \otimes B). \]

\item[\textbf{Inverse}]
The product $A\otimes B$ is invertible iff $A$ and $B$ are invertible. In that case the inverse is given by
\[ (A\otimes B)^{-1} = A^{-1}\otimes B^{-1}. \]
This follows easily from the mixed product.
\item[\textbf{Moore-Penrose pseudoinverse}]
\[ (A\otimes B)^+ = A^+\otimes B^+. \]

\item[\textbf{A vectorisation trick}]
Let $A,B,C$ be matrices of dimensions $k\times l, l\times m$ and $m\times n$. Then
\[ \vectorisation(ABC) = (C^\transp\otimes A)\vectorisation(B). \]
From this we obtain some other formulations:
\begin{align}
\vectorisation(ABC) &= (I_n\otimes AB)\vectorisation(C) \\
&= (C^\transp B^\transp \otimes I_k)\vectorisation(A) \\
\vectorisation(AB) &= (I_m\otimes A)\vectorisation(B) = (B^\transp \otimes I_k)\vectorisation(A)
\end{align}
\end{itemize}

\subsection{The commutator}
\begin{theorem}[Shoda's theorem]
Let $A\in\F^{n\times n}$. Then there exists $X,Y$ such that $A=[X,Y] = XY-YX$ \textup{if and only if} $\Tr(A) = 0$.
\end{theorem}
\begin{proof}
Assume $A = [X,Y]$. Then $\Tr(A) = \Tr(XY) -\Tr(YX) = \Tr(XY) - \Tr(XY) = 0$.

Conversely, assume $\Tr(A) = 0$. By \ref{averageTraceOverDiagonal} $A$ is similar to a matrix $B$ that has zeros on the diagonal. Let $X = \diag(1,2,\ldots, n)$ and
\[ [Y]_{i,j} = \begin{cases}
(i-j)^{-1}[B]_{i,j} & (i\neq j) \\
1 & (i=j).
\end{cases} \]
Then
\[ [X,Y]_{i,j} = [XY-YX]_{i,j} = i[Y]_{i,j}-j[Y]_{i,j} = (i-j)[Y]_{i,j} = [B]_{i,j}. \]
So $B$ is the commutator $[X,Y]$. Then $A$ is the commutator $[SXS^{-1}, SYS^{-1}]$.
\end{proof}

\subsection{Kruskal rank and spark}
\begin{definition}
Let $A\in \F^{m\times n}$. The \udef{Kruskal rank} is the largest number $\KruskalRank(A)$ such that all $\KruskalRank(A)$-sets of columns are linearly independent.
\end{definition}
\begin{lemma}
Let $A\in \F^{m\times n}$. Then $\KruskalRank(A) \leq \Rank(A)$.
\end{lemma}
\begin{proof}
If $\Rank(A) = k$, then there exists a $k$-set of columns that spans $\Col(A)$. Then every linearly independent set is smaller than $k$ by the Steinitz exchange lemma \ref{SteinitzExchange} and $\KruskalRank(A) \leq k$.
\end{proof}

\begin{definition}
Let $A\in \F^{m\times n}$. Then the \udef{spark} of $A$ is defined as
\[ \operatorname{spark}(A) \defeq \min\setbuilder{\norm{x}_0}{x\in\Null(A)} \]
where $\norm{x}_0$ is the number of non-zero elements of $x$.
\end{definition}
TODO $\norm{\cdot}_0$ norm for finite fields.

\begin{lemma}
Let $A\in \F^{m\times n}$. Then
\[ \operatorname{spark}(A) = \KruskalRank(A) + 1. \]
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{lemma}
Let $A\in \F^{m\times n}$. If $\Rank(A) = n$, then $\KruskalRank(A) = n$.
\end{lemma}

\begin{proposition}
Let $A\in \F^{m\times n}$. Then
\[ \KruskalRank(A) \geq \frac{1}{\mu(A)} \]
where $\mu(A) = \max_{i\neq j} \frac{|\inner{[A]_{_,i},[A]_{-,j}}|}{\norm{[A]_{_,i}}\norm{[A]_{_,j}}}$.
\end{proposition}
\begin{proof}
TODO
\end{proof}


\section{Eigenvalues and eigenvectors}
\subsection{The spectrum}
In this section we study vectors that are mapped to multiples of themselves by a given matrix $A$, i.e.\ vectors $\vec{v}$ such that
\[ A\vec{v} = \lambda \vec{v} \qquad\text{for some $\lambda\in\F$.} \]
Clearly for this to be possible, $A$ needs to be square.
\begin{definition}
Suppose $A\in \F^{n\times n}$.
\begin{itemize}
\item  A scalar $\lambda\in \mathbb{F}$ is called an \udef{eigenvalue} of $A$ if there exists a $\vec{v}\in \F^n$ such that $\vec{v}\neq 0$ and $A\vec{v} = \lambda v$.
\item Such a vector $\vec{v}$ is called an \udef{eigenvector}.
\item The set of all eigenvectors associated with an eigenvalue $\lambda$ is called the \udef{eigenspace} $E_\lambda(A)$. Because
\[ E_\lambda(A) = \ker(\ell_{\lambda \mathbb{1}_{n} - A}), \]
it is indeed a vector space.

The dimension of $E_\lambda(A)$ is the \udef{geometric multiplicity} of $\lambda$.
\end{itemize}
The set of all eigenvalues is called the \udef{spectrum} of $A$.
\end{definition}
\begin{proposition}
Let $A\in \F^{n\times n}$ and $\lambda\in \mathbb{F}$, then
\[ \text{$\lambda$ is an eigenvalue of $A$} \qquad \iff \qquad \text{$\lambda \mathbb{1}_{n} - A$ is invertible.} \]
\end{proposition}
\begin{proof}
The equation $A\vec{v} = \lambda \vec{v}$ is equivalent to $(A-\lambda \mathbb{1}_n)\vec{v} = 0$. So there exist eigenvectors associated to $\lambda$ iff the kernel of $\ell_{A-\lambda \mathbb{1}_n}$ is not trivial iff $\ell_{A-\lambda \mathbb{1}_n}$ is injective (\ref{injectivityKernelTriviality}) iff $\ell_{A-\lambda \mathbb{1}_n}$ is invertible (\ref{invertibleFiniteDim}) iff $A-\lambda \mathbb{1}_n$ is invertible (\ref{invertibleMapInvertibleMatrix}).
\end{proof}

\begin{proposition}[Gerschgorin circle theorem]
Let $A\in \F^{n\times n}$. If $\lambda$ is an eigenvalue of $A$, then there is an $i\in 1:n$ such that
\[ |\lambda - [A]_{ii}| \leq \sum_{\substack{j\in 1:n \\ j \neq i}}|[A]_{ij}|. \]
\end{proposition}
\begin{proof}
If $|\lambda - [A]_{ii}| > \sum_{\substack{j\in 1:n \\ j \neq i}}|[A]_{ij}|$ for all $i$, then $(A-\lambda\mathbb{1})$ is strictly diagonally dominant and thus invertible by \ref{invertibleDiagonallyDominant}.
\end{proof}

\begin{proposition}
Let $A\in\F^{n\times n}$ be a matrix. Suppose $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $A$ and $\vec{v}_1,\ldots, \vec{v}_m$ are corresponding eigenvectors. Then $\{\vec{v}_1,\ldots, \vec{v}_m\}$ is linearly independent.
\end{proposition}
\begin{proof}
The proof goes by contradiction. Assume $\{\vec{v}_1,\ldots, \vec{v}_m\}$ is linearly dependent. Let $k$ be the smallest positive integer such that
\[ \vec{v}_k \in \Span\{\vec{v}_1,\ldots, \vec{v}_{k-1}\}. \]
So there exists a nontrivial linear combination
\[ \vec{v}_k = a_1\vec{v}_1+\ldots +a_{k-1}\vec{v}_{k-1}. \]
Multiplying by $A$ gives
\[ \lambda_k\vec{v}_k = a_1\lambda_k\vec{v}_1+\ldots +a_{k-1}\lambda_k\vec{v}_{k-1}. \]
Multiplying the previous combination by $\lambda_k$ and subtracting both equations gives
\[ 0= a_1(\lambda_k-\lambda_1)\vec{v}_1 +\ldots + a_{k-1}(\lambda_k - \lambda_{k-1})\vec{v}_{k-1}. \]
By assumption of linear independence of $\{\vec{v}_1,\ldots, \vec{v}_{k-1}\}$ this combination must be trivial, however none of the $(\lambda_k-\lambda_i)$ can be zero, so all the $a_i$ must be zero. This is a contradiction with the assumption of linear dependence.
\end{proof}
\begin{corollary}
The matrix $A\in\F^{n\times n}$ has at most $n$ linearly independent eigenvalues.
\end{corollary}
\begin{corollary}
Suppose $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $A$. Then
\[ E_{\lambda_1}(A) \oplus \ldots \oplus E_{\lambda_m}(A) \]
is a direct sum. Furthermore, the sum of geometric multiplicities is less than or equal to the dimension of $V$:
\[ \dim E_{\lambda_1}(A) + \ldots + \dim E_{\lambda_m}(A) \leq \dim V. \]
\end{corollary}

\subsubsection{The characteristic equation}
\begin{definition}
Let $A\in\F^{n\times n}$. The \udef{characteristic polynomial} $p_A(x)$ of $A$. Is the polynomial
\[ p_A(x) \defeq \det(x\mathbb{1}_n - A). \]
\end{definition}
The characteristic polynomial is also sometimes defined as $\det(A - x\mathbb{1}_n)$. This differs by a sign $(-1)^{n}$.
\begin{lemma}
The characteristic polynomial of any square matrix is a monic polynomial.
\end{lemma}
\begin{lemma}
The characteristic polynomials of similar matrices are identical.
\end{lemma}

\begin{proposition}
Let $A\in\F^{n\times n}$. Then $p_A(x)$ can be factorised as
\[ p_A(x) = \prod_{i=1}^m (x - \lambda_i)^{m_i}  \]
where $\lambda_i$ are the eigenvalues of $A$ and the multiplicities $m_i$ are positive integers such that $\sum_{i=1}^m m_i = n$.
\end{proposition}
\begin{corollary}
The eigenvalues of $A$ are the solutions of the equation
\[ p_A(x) = 0. \]
\end{corollary}
\begin{corollary}
The determinant of a matrix is the product of its eigenvalues, counting algebraic multiplicity: for $A\in\F^{n\times n}$
\[ \det(A) = \prod_{i=1}^m\lambda_i^{m_i}. \]
\end{corollary}
\begin{proof}
We have
\[ \det(A) = (-1)^n\det(0-A) = (-1)^np_A(0) = (-1)^n\prod_{i=1}^m(0 - \lambda_i)^{m_i} = \prod_{i=1}^m\lambda_i^{m_i}. \]
\end{proof}

\begin{definition}
Let $A\in\F^{n\times n}$. The equation
\[ p_A(x) = \det(x\mathbb{1}_n - A) = 0 \]
is called the \udef{characteristic equation} of $A$.

Let $\lambda$ be a solution of the characteristic equation. The multiplicity of $\lambda$ as a root of $p_A(x)$ is the \udef{algebraic multiplicity} of $\lambda$.
\end{definition}
\begin{lemma}
Let $A\in\F^{n\times n}$ and $\lambda$ be an eigenvalue of $A$.

The geometric multiplicity of $\lambda$ is less than or equal to the algebraic multiplicity of $\lambda$.
\end{lemma}
\begin{proof}
Set $k=\dim E_\lambda$. Take a basis of $E_\lambda(A)$ and extend it to a basis $\beta$ of $\F^{n}$. With respect to this basis the matrix of $\ell_A$ is of the form
\[ (\ell_A)_\beta^\beta =  \begin{pmatrix}
\lambda \mathbb{1}_k & B \\ 0 & C
\end{pmatrix} = P^{-1}(\ell_A)_\mathcal{E}^\mathcal{E}P = P^{-1}AP \]
for some matrices $B,C$ and some invertible matrix $P$ where $\mathcal{E}$ is the standard basis of $\F^n$. Then 
\[ p_A(x)= p_{P^{-1}AP} = p_{\lambda \mathbb{1}_{k}}(x)p_C(x) = (\lambda - x)^kp_C(x), \]
so the algebraic multiplicity of $\lambda$ is at least the geometric multiplicity $k$. It may be greater if $\lambda$ is also an eigenvector of $C$, but in this case the eigenvector is a linear combination of the eigenvectors already chosen for $\beta$.
\end{proof}

\subsubsection{Diagonalisable matrices}
\begin{definition}
A matrix $A\in\F^{n\times n}$ is called \udef{diagonalisable} if $\F^n$ has a basis of eigenvectors of $A$.
\end{definition}
\begin{proposition}
Let $A\in\F^{n\times n}$ and let $\lambda_1,\ldots, \lambda_m$ denote the distinct eigenvalues of $A$. The following are equivalent:
\begin{enumerate}
\item $A$ is diagonalisable;
\item there exist $1$-dimensional subspaces $U_1,\ldots, U_n$ of $A$, each invariant under $\ell_A$, such that
\[ \F^n = U_1\oplus \ldots \oplus U_n; \]
\item $\F^n = E_{\lambda_1}(A) \oplus \ldots \oplus E_{\lambda_m}(A);$
\item $n = \dim E_{\lambda_1}(A) + \ldots + \dim E_{\lambda_m}(A);$
\item for each $\lambda_i$ the geometric multiplicity is equal to the algebraic multiplicity and the sum of algebraic multiplicities is $n$.
\end{enumerate}
\end{proposition}
In the case of complex vector spaces, the sum of algebraic multiplicities is always $n$ by the fundamental theorem of algebra.
\begin{corollary}
If $A\in\F^{n\times n}$ has $n$ distinct eigenvalues, then $A$ is diagonalisable.
\end{corollary}
So a matrix may fail to be diagonalisible for two reasons: not enough geometric multiplicity or not enough geometric and algebraic multiplicity

\subsection{Spectral theorem}
\begin{theorem}
Let $V$ be a complex finite-dimensional inner product space. Let $L$ be an operator on $V$. Then
\begin{enumerate}
\item there exists an orthonormal basis of $V$ consisting of eigenvectors of $L$ \textup{if and only if} $L$ is normal;
\item if $L$ is self-adjoint, then the eigenvalues of $L$ are real.
\end{enumerate}
\end{theorem}

TODO: replace following: + in real case we need self-adjoint!
\begin{theorem}[Spectral theorem for matrices]
Let $V$ be a finite-dimensional inner product space over $\R$ or $\C$. Let $L=L^*$ be self-adjoint. Then
\begin{enumerate}
\item there exists an orthonormal basis of $V$ consisting of eigenvectors of $L$;
\item the eigenvalues of $L$ are real.
\end{enumerate}
\end{theorem}
\begin{proof}
We first prove the theorem for complex vector spaces. The proof is by finite induction:

By the fundamental theorem of algebra the characteristic polynomial has at least 1 root $\lambda_1$. Choose a corresponding eigenvector $\vec{v}_1$. Then by
\[ \lambda_1\inner{\vec{v}_1,\vec{v}_1} = \inner{\vec{v}_1, L\vec{v}_1} = \inner{L\vec{v}_1, \vec{v}_1} = \overline{\lambda_1}\inner{\vec{v}_1, \vec{v}_1}, \]
$\lambda_1$ is real.

Now $\Span\{\vec{v}_1\}^\perp$ is invariant under $L$:
\[ \vec{x}\in \Span\{\vec{v}_1\}^\perp \quad\iff\quad \inner{\vec{x},\vec{v}_1} = 0 \quad\implies\quad \inner{L\vec{x},\vec{v}_1} = \inner{\vec{x},L\vec{v}_1} = \lambda_1\inner{\vec{x},\vec{v}_1} = 0. \]

We can now apply the same argument to $L|_{\Span\{\vec{v}_1\}^\perp}:\Span\{\vec{v}_1\}^\perp\to \Span\{\vec{v}_1\}^\perp$, whose eigenvector are orthogonal to $v_1$. Finite induction then finishes the proof in the complex case. 

In the real case: we can linearly extend $L$ to be an operator $L_\C$ on the complexification $V_\C$. Then $L_\C$ has formally the same characteristic polynomial as $L$, except now interpreted as a function $\C\to\C$, not $\R\to\R$. Now we know the roots of $p_{L_\C}(x)$ are real, so they are also roots of $p_L(x)$. The rest of the proof can be completed in the same way. 
\end{proof}

The spectral decomposition is a special case of both the Schur decomposition and the singular value decomposition.

TODO: Kronecker product: multiply eigenvalues: all there (by multiplicity)

\subsection{Computing eigenvalues and vectors}
\subsubsection{Power method}
+ inverse

\subsubsection{Deflation}
\url{https://quickfem.com/wp-content/uploads/IFEM.AppE_.pdf}

\subsubsection{QR}

\section{Matrix classes and decompositions}
\subsection{Matrix classes}
\subsubsection{Rank-1 projections}
\begin{proposition}
Let $P\in\F^{n\times n}$. Then $P$ is a rank-1 (orthogonal) projection \textup{if and only if} there is a unit vector $\vec{u}\in\F^n$ such that $P= \vec{u}\vec{u}^*$.
\end{proposition}
\begin{proof}
Due to $P$ being rank-1, we can find a unit vector $\vec{u}$ such that $\Col(P) = \Span\{\vec{u}\}$. So the columns of $P$ are all multiples of $\vec{u}$, meaning we can write $P$ as $\vec{u}\vec{v}^*$ for some $\vec{v}\in\F^n$.

Now $P = P^*= (\vec{u}\vec{v}^*)^* = \vec{v}\vec{u}^*$, so $\vec{u}\vec{v}^* = \vec{v}\vec{u}^*$ and thus $\Col(P) = \Span\{\vec{v}\}$, meaning $\vec{v} = \lambda \vec{u}$.

Also $P^2 = \vec{u}\vec{v}^*\vec{u}\vec{v}^* = \vec{u}\inner{v,u}\vec{v}^* = \inner{v,u}\vec{u}\vec{v}^*$, so $1 = \inner{\vec{v},\vec{u}} = \overline{\lambda}\inner{\vec{u},\vec{u}} = \overline{\lambda}$.

So $\vec{v}=\vec{u}$ and $P= \vec{u}\vec{u}^*$.
\end{proof}
We write $P_{\vec{u}}$ to denote $\vec{u}\vec{u}^*$. Then in particular $P_{\vec{u}}\vec{v} = \inner{\vec{u},\vec{v}}\vec{u}$.

\subsubsection{Householder matrices}
\begin{definition}
Let $\vec{w}\in\F^n$ be a non-zero vector. Set $\vec{u} = \vec{w}/\norm{\vec{w}}$. Then the corresponding \udef{Householder matrix} is
\[ U_{\vec{w}} \defeq \mathbb{1}_n - 2P_{\vec{u}} = \mathbb{1}_n - 2 \frac{\vec{w}\vec{w}^*}{\inner{\vec{w},\vec{w}}} = \mathbb{1}_n - 2 \frac{\vec{w}\vec{w}^*}{\vec{w}^*\vec{w}}. \]
The corresponding transformation $\ell_{U_{\vec{w}}}$ is called a \udef{Householder transformation}.
\end{definition}
The Householder transformation reflects vectors across a hyperplane orthogonal to $\vec{w}$.

\begin{lemma}
Householder matrices are unitary, Hermitian and involutive.
\end{lemma}

For any two vectors of the same length, we can construct a unitary matrix that maps one to the other, using Householder matrices.

\begin{proposition}
Let $\vec{x},\vec{y}\in\F^n$ such that $\norm{\vec{x}} = \norm{\vec{y}} \neq 0$. Let
\[ \sigma = \begin{cases}
1& (\inner{\vec{x}\vec{y}} = 0) \\ -\overline{\inner{\vec{x},\vec{y}}}/|\inner{\vec{x},\vec{y}}| & (\inner{\vec{x}\vec{y}} \neq 0),
\end{cases} \]
and let $\vec{w} = \vec{y}-\sigma \vec{x}$. Then $\sigma U_{\vec{w}}$ is unitary and $\sigma U_{\vec{w}}\vec{x} = \vec{y}$.
\end{proposition}
The use of $\sigma$ is purely to improve numerical stability.

\subsubsection{Upper Hessenberg matrices}
\begin{definition}
A matrix $A$ is called an \udef{upper Hessenberg matrix} if $i>j+1\implies [A]_{i,j}=0$.
\end{definition}
This is a matrix of the form
\[ \begin{bmatrix}
\star & \star & \star & \star & \star \\
\star & \star & \star & \star & \star \\
0 & \star & \star & \star & \star \\
0 & 0 & \star & \star & \star \\
0 & 0 & 0 & \star & \star \\
\end{bmatrix} \]
Every square matrix is unitarily similar to an upper Hessenberg matrix, and the unitary
similarity can be constructed from a sequence of Householder matrices and complex rotations.

\subsection{Matrix decompositions}
\subsubsection{LU and LDU factorisation}
\subsubsection{QR factorisation}
The QR factorization of an $m \times n$ matrix $A$ is a factorisation
$A=QR$
where $Q\in\F^{m\times n}$ has orthonormal columns and $R\in\F^{n\times n}$ is square upper triangular.
This requires that $m\geq n$.

\begin{proposition}[QR factorisation]
Let $A\in\F^{m\times n}$ and $m\geq n$. Then
\begin{enumerate}
\item there exists a unitary $U\in\F^{m\times m}$ and an upper triangular matrix $R\in\F^{n\times n}$ such that
\[ A = U \begin{bmatrix}
R \\ \mathbb{0}
\end{bmatrix} \]
we can take $R$ to have real, non-negative values on the diagonal;
\item writing $U = \begin{bmatrix}
Q & Q'
\end{bmatrix}$, we get the decomposition
\[ A = QR \]
where $Q$ has orthonormal columns;
\item if $\Rank(A)= n$, then fixing the values on the diagonal of $R$ to be positive makes the factorisation unique; all values on the diagonal are non-zero.
\end{enumerate}
\end{proposition}
\begin{proof}
We can find a (unitary) Householder transformation $U_1$ that maps the first columns $[A]_{-,1}$ to $\norm{[A]_{-,1}}\vec{e}_1$. So
\[ U_1 A = \begin{bmatrix}
\norm{[A]_{-,1}} & \star \\ \mathbb{0} & A_1
\end{bmatrix} \]
Then we can do the same for $A_1$, meaning $U_2 = \mathbb{1}\oplus U'$ transforms $A$ as
\[ U_2U_1A = \begin{bmatrix}
\norm{[A]_{-,1}} & \star & \star \\
0 & \norm{[A_1]_{-,1}} & \star \\
\mathbb{0} & \mathbb{0} & A_2
\end{bmatrix}. \]
Repreating this gives the required factorisation.
\end{proof}
The factorisation $A = U \begin{bmatrix}
R \\ \mathbb{0}
\end{bmatrix}$ is called the wide QR factorisation and $A=QR$ the (narrow) QR factorisation.

\subsection{Polar decomposition}
\subsubsection{Singular value decomposition}
spectral decomposition is a special case
\subsubsection{Schur decomposition}
spectral decomposition is a special case




\section{Systems of linear equations}
\url{https://encyclopediaofmath.org/wiki/Motzkin_transposition_theorem}
TODO
A homogeneous system of linear equations with more variables than equations has non-zero solutions.

An inhomogeneous system of linear equations with more equations thanvariables has no solution for some choice of the constant terms.

Calculation of inverse via row reduction.

free and bounded variables.

\begin{lemma}
If $Ax=b$ is consistent for all $b\in \F^n$, then $A$ has a right inverse $B$, i.e.\ $AB = \mathbb{1}$.
\end{lemma}
\begin{proof}
For each $e_i$ in the standard basis we can find a $c_i$ such that $Ac_i = e_i$. Then
\[ A \begin{pmatrix}
c_1 & c_2 & \hdots & c_n
\end{pmatrix} = \begin{pmatrix}
Ac_1 & Ac_2 & \hdots & Ac_n
\end{pmatrix} = \begin{pmatrix}
e_1 & e_2 & \hdots & e_n
\end{pmatrix} = \mathbb{1}_n. \]
\end{proof}

\subsection{Cramer's rule}
\begin{proposition}
\[ x_i = \frac{\det(A_i)}{\det(A)} \]
\end{proposition}
\begin{proof}
$x = \begin{pmatrix}
x_1 \hdots x_n
\end{pmatrix}^\transp$
\begin{align*}
x_i &= \det \begin{pmatrix}
e_1 & \hdots & e_{i-1} & x & e_{i+1} & \hdots & e_n
\end{pmatrix} \\
&= \det \begin{pmatrix}
A^{-1}a_1 & \hdots & A^{-1}a_{i-1} & A^{-1}b & A^{-1}a_{i+1} & \hdots & A^{-1}a_n
\end{pmatrix} \\
&= \det (A^{-1}\begin{pmatrix}
a_1 & \hdots & a_{i-1} & b & a_{i+1} & \hdots & a_n
\end{pmatrix}) = \det(A^{-1}A_i) \\
&= \frac{\det(A_i)}{\det(A)}.
\end{align*}
\end{proof}


\section{Polynomials applied to endomorphisms}
\section{The spectra of matrices}
What are eigenvectors of rotation? -> complex eigenvalues.

Real matrix: complex conjugate eigenvalues have complex conjugate eigenvectors

Finite order endomorphisms are diagonalisable over $\C$ (or any algebraically closed field where the characteristic of the field does not divide the order of the endomorphism) with roots of unity on the diagonal. This follows since the minimal polynomial is separable, because the roots of unity are distinct.

See \url{https://en.wikipedia.org/wiki/Minimal_polynomial_(linear_algebra)}


\section{Euclidean geometry}
\begin{definition}
The \udef{$n$-dimensional Euclidean space} is $\R^n$ equipped with the inner product
\[ \inner{\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix}, \begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}} = x_1y_1 + x_2y_2 \ldots x_ny_n. \]
\end{definition}


\subsection{Affine subspaces}

Line as intersection of planes with normals $n_1,n_2$. Then direction of line is $n_1\times n_2$.

\subsection{Distances}

\subsection{Distance point to plane}
\begin{lemma}
\[ d(P,\pi) = \frac{|p_x\alpha + p_y\beta +p_z\gamma -d|}{\sqrt{\alpha^2 + \beta^2 + \gamma^2}} \]
\end{lemma}

Thus in the Cartesian expression for a plane, $\alpha x + \beta y + \gamma z = d$, the $d$ is the distance to the origin. (Also $(\alpha, \beta, \gamma)$ is the normal vector).

\subsection{Angles}

\subsection{Rotations}

\subsection{Spheres}