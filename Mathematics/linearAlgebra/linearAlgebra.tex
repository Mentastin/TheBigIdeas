\chapter{Vector spaces}

Gauss-Jordan reduction

TODO projective transformations

orientation
\url{https://en.wikipedia.org/wiki/Orientation_(vector_space)}
also for fixed set of $n$ vectors

\url{http://www.physics.rutgers.edu/~gmoore/618Spring2018/GTLect2-LinearAlgebra-2018.pdf}

\section{Formal definition}
A vector space is a collection of vectors, which are objects that have a natural addition and scalar multiplication.
\begin{definition}
A \udef{vector space} over a field $\mathbb{F}$ is a set $V$ together with an \udef{addition}
\[ +: V\times V \to V \]
and a \udef{scalar multiplication}
\[ \cdot: \mathbb{F}\times V \to V \]
such that $(V,+)$ is a commutative group and the following properties hold:
\begin{itemize}[leftmargin=4cm]
\item[\textbf{Distributivity 1}] $\lambda\cdot(v+w) = \lambda v + \lambda w$ for all $\lambda \in \mathbb{F}$ and all $v,w \in V$.
\item[\textbf{Distributivity 2}] $(\lambda_1+\lambda_2)\cdot v = \lambda_1 v + \lambda_2 v$ for all $\lambda_1, \lambda_2 \in \mathbb{F}$ and all $v \in V$.
\item[\textbf{Mixed associativity}] $\lambda_1\cdot(\lambda_2\cdot v) = (\lambda_1 \lambda_2) \cdot v$ for all $\lambda_1, \lambda_2 \in \mathbb{F}$ and all $v \in V$.
\item[\textbf{Multiplicative identity}] $1\cdot v = v$ for all $v \in V$.
\end{itemize}
This vector space can be denoted $\sSet{\mathbb{F}, V, +}$.
\end{definition}
In the definition we have used the following convention: for all $v,w\in V$ and $\lambda\in \mathbb{F}$, we denote $+(v,w)$ as $v+w$ and $\cdot(\lambda, v)$ as $\lambda \cdot v$ or $\lambda v$.

We call the elements of the field \udef{scalars} and the elements of the set $V$ \udef{vectors}. The zero of the group is known as the \udef{zero vector}.

Almost always we will actually be interested in $\mathbb{F} = \R$ or $\mathbb{F} = \C$.
\subsection{Examples}
\begin{enumerate}
\item The $n$-tuples in $\mathbb{F}^n$ with pointwise addition and multiplication. If the entries of the $n$-tuples are written one above the other in a column, it is called a \udef{column vector}.
\item The polynomials in $\mathbb{F}[X]$.
\item The polynomials in $\mathbb{F}[X]_{\leq n}$ of maximally degree $n$.
\item For any set $S$, the functions $(S\to \mathbb{F})$, denoted $\mathbb{F}^S$, with pointwise addition and multiplication.
\item For any topological space $X$, the continuous functions in $(X\to \C)$, denoted $\cont(X)$.
\item The trivial vector space $\{ 0\}$. A vector space can never be empty, because a commutative group always has a neutral element.
\item The set of all possible \textit{displacements} in (Euclidean) space forms a vector space. Once we have chosen an origin, we can view space as a vector space.
\end{enumerate}
\subsection{Some elementary manipulations}
\begin{lemma}
Given the vector space $(\mathbb{F}, V, +)$  and arbitrary $u,v,w\in V$ and $\lambda \in \mathbb{F}$, we have
\begin{enumerate}
\item $0v = 0 = \lambda \cdot 0$;
\item $(-1)v = -v = 1(-v)$;
\item $(-\lambda)v = -(\lambda v) = \lambda(-v)$;
\item $u+v = w+v \implies u = w$.
\end{enumerate}
By $-v$ we mean the additive inverse of $v$.
\end{lemma}
\begin{proof}
\begin{enumerate}
\item First, use distributivity to get
\[ 0v = (0+0)v = 0v + 0v. \]
The apply the previous lemma to $0+0v = 0v = 0v+0v$ to get $0=0v$. The equality $\lambda\cdot 0 = 0$ is proved analogously.
\item To show that $(-1)\cdot v$ is the additive inverse of $v$, i.e.\ $-v$, we simply add $(-1)\cdot v + v$ and observe the result is $0$.
\[ (-1)\cdot v + v = (-1)\cdot v + 1\cdot v = (1+(-1))\cdot v = 0\cdot v = 0. \]
\item Similar to the previous point.
\item The additive inverse $-v$ exists, so we can just add it left and right.
\end{enumerate}
\end{proof}

\begin{lemma} \label{scalarMultiplicationBijection}
Let $V$ be a vector space over a field $\F$. Suppose $x,y\in V\setminus\{0\}$ and $\lambda,\mu\in \F$. Then
\begin{enumerate}
\item if $\lambda x = \mu x$, then $\lambda = \mu$;
\item if $x$ is not a scalar multiple of $y$ and $\lambda x = \mu y$, then $\lambda = 0 = \mu$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We calculate that $(\lambda - \mu)x = 0$. If $(\lambda - \mu) \neq 0$, then we have $x = \frac{0}{\lambda - \mu} = 0$, which is a contadiction. So $\lambda = \mu$.

(2) Suppose, towards a contradiction, that $\mu \neq 0$. Then $y = \frac{\lambda}{\mu}x$, which is a contradiction. So $\mu = 0$.

Now $x \neq 0$, since $0$ is a scalar multiple of any vector $y$. We conclude that $\lambda = \mu = 0$ by (1).
\end{proof}

\subsection{Modifying the field}
\begin{proposition} \label{subfieldVectorSpace}
Let $V$ be a vector space over a field $K$ and $F\subseteq K$ a subfield. Then $V$ is also a vector space over the field $F$. 
\end{proposition}
The $F$-scalar multiplication is given by the restriction of the $K$-scalar multiplication to $F$.

We denote the vector space $V$ viewed as a vector space over $F$ by $V_F$.
\begin{proof}
The addition is unchanged and thus still makes $\sSet{V,+}$ a commutative group. The other properties are easily verified.
\end{proof}

\subsubsection{Realification of a complex vector space}
\begin{definition}
Let $V$ be a complex vector space. The real vector space $V_\R$ is called the \udef{realification} of $V$.
\end{definition}

\subsection{Subspaces}
\begin{definition}
A \textit{subset} $U$ of a vector space $V$ is called a \udef{subspace} of $V$ if $U$ is also a vector space.
\end{definition}
The subset $U$ automatically inherits a lot of the structure of $V$. We only need to verify a couple of conditions.
\begin{proposition}[Subspace criterion] \label{subspaceCriterion}
A subset $U$ of a vector space $V$ is a subspace of $V$ \textup{if and only if} $U$ satisfies the following conditions:
\begin{enumerate}
\item \textbf{Additive identity}: $0 \in U$. Alternatively it is enough to show that $U$ is not empty.
\item \textbf{Closed under addition}: $v,w \in U$ implies $v+w\in U$;
\item \textbf{Closed under scalar multiplication}: $\lambda \in \mathbb{F}$ and $u\in U$ implies $\lambda u \in U$.
\end{enumerate}
This is equivalent to saying $U$ is a sub-$\{+,0, (\lambda\cdot -)\}_{\lambda\in\F}$-algebra.
\end{proposition}
Alternatively the last two criteria are equivalent to:
\[ v,w\in U; \lambda \in \mathbb{F} \qquad \text{implies} \qquad v+\lambda w \in U. \]

If the question is whether a set is a subspace, this criterion is almost always the answer. An elementary application:
\begin{proposition}
Any arbitrary intersection of subspaces is a subspace.
\end{proposition}
\begin{corollary}
Let $V$ be a vector space. Then the subspaces of $V$ form a complete sublattice of $\sSet{\powerset(V),\subseteq}$.
\end{corollary}

\begin{definition}
The closure operator into the complete lattice of subspaces of $V$ is called the \udef{span}.

If $D$ is a subset of $V$ such that $V = \Span(D)$, then $D$ \udef{spans} $V$.

\begin{itemize}
\item A vector space is called \udef{finite-dimensional} if it is spanned by a finite set of vectors.
\item A vector space is \udef{infinite-dimensional} if it is not finite-dimensional.
\end{itemize}
\end{definition}

\begin{lemma} \label{realSubspaceComplexVectorSpace}
Let $V$ be a complex vector space and $A\subseteq V$ a subset. Then $A$ is a subspace \textup{if and only if} $A$ is a subspace of the realification $V_\R$ and $iA \subseteq A$.
\end{lemma}
We call a subspace of $V_\R$ a \udef{real subspace} of $V$. In order to highlight the distinction we may call an ordinary subspace a \udef{complex subspace} of $V$.
\begin{proof}
$\boxed{\Rightarrow}$ Suppose $A$ is a subspace. Clearly $iA \subseteq A$ as $A$ is closed under scalar multiplication by \ref{subspaceCriterion}. The subspace criterion also easily gives that $A$ is a subspace in $V_\R$.

$\boxed{\Leftarrow}$ The first two points of the subspace criterion \ref{subspaceCriterion} follow because $A$ is a real subspace. For point (3), take arbitrary $\lambda \in \C$ and $u\in A$. Then $\lambda = a+bi$ for some $a,b\in \R$ and $au,bu\in A$ because it is a real subspace. By assumption we also have $ibu\in A$ and finally by additive closure we have $\lambda u = au+biu \in A$.
\end{proof}

\begin{definition}
Let $V$ be a vector space. A \udef{hyperplane} in $V$ is a coatom in the lattice of subspaces of $V$.
\end{definition}

\begin{lemma} \label{realComplexHyperplane}
Let $V$ be a complex vector space and $K \subseteq V$ a subset that is a hyperplane in the realification $V_\R$. Then $K\cap iK$ is a hyperplane in $V$.
\end{lemma}
\begin{proof}
The real subspace $K\cap iK$ is also a complex subspace by \ref{realSubspaceComplexVectorSpace}.

TODO Robertson p28
\end{proof}

\section{Basis and dimension}
\subsection{Linear combinations and span}
\begin{definition}
A \udef{(finite) linear combination} of vectors $v_1, \ldots, v_n$ is a vector of the form
\[ a_1v_1 + \ldots + a_nv_n \]
where $a_1, \ldots, a_n \in \mathbb{F}$.
\end{definition}

\begin{proposition}
Let $V$ be a vector space over a field $\F$ and $D\subseteq V$ a subset. Then $\Span(D)$ is the set of all finite linear combinations of vectors in $D$ if $D \neq \emptyset$. If $D = \emptyset$, then $\Span(D) = \{0\}$.
\end{proposition}

\subsection{Linear independence}
\begin{definition}
A set of vectors $D$ is \udef{linearly independent} if the only linear combinations in $D$ that equal $0$ are the trivial ones with all scalars zero. i.e.\,
\[ \sum_{i=1}^n a_iv_i = 0 \qquad\implies\qquad a_1=\ldots=a_n = 0 , \]
assuming the $v_i$ are vectors in $D$ and the $a_i$ are scalars.

\udef{Linear dependence} is the opposite of linear independence.
\end{definition}
The empty set $D=\emptyset$ is taken as linearly independent. No non-trivial combinations of vectors in $\emptyset$ are equal to zero, because there are no non-trivial combinations of vectors in $\emptyset$.

\begin{lemma}
Let $D$ be a linearly dependent set of vectors. Then there exists a vector $v\in D$ such that
\begin{enumerate}
\item $v$ is a linear combination of other vectors in $D$;
\item $v\in \Span(D\setminus\{v\})$;
\item $\Span(D) = \Span(D\setminus\{v\})$.
\end{enumerate}
\label{linearDependence}
\end{lemma}
\begin{proof}
Take a linear combination of vectors in $D$ equalling zero,
\[ \sum_i a_iv_i = 0. \]
By linear dependence such a combination can be found such that not all $a_i$ are zero. In particular at least two must be non-zero. Take $a_j\neq 0$. Then
\[ v_j = \sum_{i\neq j}\frac{a_iv_i}{a_j}. \]

To prove the last point, take a $u\in \Span(D)$. Then
\[ u = \sum_i b_iv_i = b_j v_j + \sum_{i\neq j} b_iv_i = b_j\sum_{i\neq j}\frac{a_iv_i}{a_j} + \sum_{i\neq j} b_iv_i = \sum_{i\neq j}\left(\frac{b_ja_i}{a_j}+b_i\right)v_i.  \]
So $u\in \Span(D\setminus\{v\})$. The opposite inclusion is obvious. 
\end{proof}

\subsection{Bases}
\begin{definition}
A \udef{basis} of a vector space $V$ is a set of vectors in $V$ that spans $V$ and is linearly independent.
\end{definition}
\begin{example}
The \udef{standard basis} or \udef{natural basis} of $\mathbb{F}^n$ is given by
\begin{align*}
(1,0,0,&\ldots,0), \\
(0,1,0,&\ldots,0), \\
(0,0,1,&\ldots,0), \\
&\ldots \\
(0,0,0,&\ldots,1).
\end{align*}
We will denote it $\mathcal{E}$ or $\mathcal{E}_n$.
\end{example}
\subsubsection{In finite-dimensional spaces}
\begin{proposition} \label{finiteBasisUniqueDecomposition}
A finite set $\{v_1, \ldots, v_n\}$ of vectors in $V$ is a basis of $V$ \textup{if and only if} every $v\in V$ can be written uniquely in the form
\[ v = a_1v_1 + \ldots + a_nv_n, \]
where $a_1, \ldots, a_n \in \mathbb{F}$.
\end{proposition}
\begin{proof}
We prove both directions.
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Suppose $\{v_1, \ldots, v_n\}$ is a basis of $V$. Then any vector $v$ can be written as $a_1v_1 + \ldots + a_nv_n$, because the basis spans the space. We just need to show the decomposition is unique. To that end, assume there was another decomposition $v = b_1v_1 + \ldots + b_nv_n$. Subtracting both decompositions gives
\[ 0 = (a_1-b_1)v_1 + \ldots + (a_n-b_n)v_n. \]
Because $\{v_1, \ldots, v_n\}$ is linearly independent, $a_i = b_i$ for all $i$.
\item[$\boxed{\Leftarrow}$] Now suppose every vector has such a decomposition. Clearly $\{v_1, \ldots, v_n\}$ spans $V$. The unique decomposition of $0$ gives linear independence.
\end{itemize}
\end{proof}

\begin{theorem}[Steinitz exchange lemma] \label{SteinitzExchange}
Let $V$ be a vector space.
If $U = \{u_1, \ldots, u_m\}$ is a linearly independent set of $m$ vectors in $V$, and $W = \{ w_1, \ldots, w_n \}$ spans $V$, then:
\begin{enumerate}
\item $m\leq n$;
\item There is a set $\{u_1, \ldots, u_m, w'_{m+1}, \ldots, w'_n\} \supset U$ that spans $V$ where $w'_{m+1},\ldots, w'_n \in W$.
\end{enumerate}
\end{theorem}
\begin{proof}
We obtain the set $\{u_1, \ldots, u_m, w'_{m+1}, \ldots, w'_n\}$ by starting with the list $B_0 = (w_1, \ldots, w_n)$ and applying the following steps for each element $u_i \in U$, in the process defining sets $B_1, \ldots, B_m$. Each of these sets spans $V$.
\begin{enumerate}
\item Add $u_i$ to $B_{i-1}$. The set is now linearly dependent, because $B_{i-1}$ spans $V$.
\item By lemma \ref{linearDependence}, we can find a vector $v$ that is a linear combination of $B_{i-1}\setminus \{v\}$. Because $u_1,\ldots, u_i$ are linearly independent, we can choose this vector to be an element of $W$. Define $B_i = B_{i-1}\setminus\{v\}$. By lemma \ref{linearDependence}, $B_i$ still spans $V$, as required.
\end{enumerate}
This process only stops when we have had all elements of $U$.
\end{proof}
\begin{corollary}
If a vector space $V$ has a basis with $n$ vectors, then any basis of $V$ has $n$ vectors. \label{nBasis}
\end{corollary}

\begin{theorem} \label{extensionReductionBasisFiniteDimensions}
Suppose $V$ is a finite-dimensional vector space spanned by $D = \{v_1, \ldots, v_n\}$.
\begin{enumerate}
\item We can find a subset of $D$ that is a basis of $V$, i.e.\ $D$ can be reduced to a basis;
\item Each linearly independent set of vectors can be expanded to a basis.
\end{enumerate}
\label{basis}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item Remove $0$ from $D$, if it is an element. If $D$ is not linearly independent, find a vector in $D$ that is a linear combination of other vectors in $D$. Repeat until the set is linearly independent. This process stops due to the finite number of vectors. The set spans $V$ at every step.
\item Follows easily from the Steinitz exchange lemma, taking $W$ to be a basis.
\end{enumerate}
\end{proof}
\begin{corollary}
Every finite-dimensional vector space has a basis. \label{existenceBasis}
\end{corollary}

Thanks to corollaries \ref{nBasis} and \ref{existenceBasis}, the following definition makes sense:
\begin{definition}
The \udef{dimension} of a finite-dimensional vector space is the length of any basis of the vector space.
The dimension of $V$ (if $V$ is finite-dimensional) is denoted by $\dim V$ or $\dim_\mathbb{F}V$.\footnote{The latter notation is particularly useful if when distinguishing between real and complex vector spaces, because every complex vector space can be seen as a real vector space. In this case $\dim_\R V = 2\dim_\C V$, because $v$ and $iv$ are linearly independent over $\R$.}

If $V = \{0\}$, we take $\dim V = 0$.
\end{definition}

\begin{corollary}
Every linearly independent set of vectors in $V$ with length $\dim V$ is a basis of $V$. \label{maxLinearlyIndependent}
\end{corollary}
\begin{corollary}
Every spanning set of vectors in $V$ with length $\dim V$ is a basis of $V$.
\end{corollary}

\begin{proposition} \label{vectorSpaceEquality}
Let $V$ be a finite-dimensional vector space and $U$ a subspace of $V$. Then
\begin{enumerate}
\item $U$ is finite-dimensional and $\dim U \leq \dim V$;
\item $\dim U = \dim V \iff U=V$.
\end{enumerate}
\end{proposition}
\begin{proof}
We construct a basis for $U$ using the following process:
\begin{enumerate}
\item If U=\{0\}, then we can take the basis $\emptyset$ and we are done. If $U\neq \{0\}$, we choose a nonzero vector $v_1 \in U$.
\item If $U$ is the span of all the vectors we have chosen, we are done. If not choose a vector in $U$, not in the span of the other vectors.
\item Repeat step (2).
\end{enumerate}
By construction, the chosen set of vectors is linearly independent. By the Steinitz exchange lemma this process must stop. In particular it must stop before reaching $\dim V$ vectors.

If the process reaches this upper bound, then by corollary \ref{maxLinearlyIndependent}, the set of vectors in $U$ is also a basis for $V$.
\end{proof}
We now have two tools for proving equalities of finite-dimensional vector spaces: either by proving both inclusions, or by leveraging point (2) of the previous proposition.

\subsubsection{In infinite-dimensional spaces}
Our definition of a basis of a vector spaces still makes sense for infinite-dimensional vector spaces, and many results of the previous section still make sense for infinite-dimensional vector spaces.

For infinite-dimensional vector spaces, there are, however, other notions of basis we might be interested in. In particular, our definition of basis requires all vectors to be constructible as \emph{finite} linear combinations of basis elements. In some contexts we might want to relax this to allow infinite combinations as well. For that, of course, we need some notion of infinite sum. Often we construct infinite sums as the limit of a sequence of finite sums, in which case we need a topology on our vector space that allows us to take limits.\footnote{Although other options exist, such as taking sums over hyperintegers.}  

In order to distinguish our purely algebraic definition of basis from these other notions of basis, a basis in the sense defined above is sometimes known as an \udef{algebraic basis} of \udef{Hamel basis}.

We will be discussing Hamel bases in this section.

\begin{theorem} \label{extensionReductionBasis}
Let $V$ be a vector space.
\begin{enumerate}
\item Any spanning set contains a basis.
\item Any linearly independent subset can be expanded to a basis.
\end{enumerate}
\label{infBasis}
\end{theorem}
\begin{proof}
Requires the axiom of choice. We will use Zorn's lemma twice.
\begin{enumerate}
\item Let $S$ be a spanning subset of $V$. Define
\[ \mathcal{A} = \{ D\subset S \;|\; \text{$D$ is linearly independent}\} \]
ordered by inclusion. It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union which is still linearly independent. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. 
We show that $\Span(R) \supset S$ by contradiction. If $\Span(R) \not\supset S$, we can consider $R\cup \{v\}$ for some $v \in S$ that is not in $\Span(R)$ and we obtain an element of $\mathcal{A}$ which is greater than a maximal element. This is a contradiction. Then from $\Span(R) \supset S$ we conclude
\[ \Span(R) = \Span(\Span(R)) \supset \Span(S) = V \]
from which it follows that $\Span(R) = V$.
\item Let $S$ be a linearly independent subset of $V$. Define
\[ \mathcal{A} = \{ D\subset V \;|\; S \subset D \; \text{and $D$ is linearly independent}\} \]
ordered by inclusion. 
It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. We show that $\Span(R) = V$ by contradiction. If $\Span(R) \neq V$, we can consider $R\cup \{v\}$ for some $v\notin \Span(R)$ and we obtain an element of $\mathcal{A}$ which is greater than a maximal element. This is a contradiction.
\end{enumerate}
\end{proof}
\begin{corollary} \label{existenceHamelBasis}
Every vector space has a Hamel basis
\end{corollary}

\begin{theorem}[Dimension theorem for vector spaces]
Given a vector space $V$, any two bases have the same cardinality.
\end{theorem}
\begin{proof}
The finite-dimensional case has already been proved. Suppose $A$ is a basis of $V$ with $|A| \geq \aleph_0$. Let $B$ be another basis of $V$. Each element $a\in A$ can be written as a finite combination of elements in $B$. Collect all the elements that go into the finite linear combination in a finite set $B_a \subset B$. We claim
\[ B = \bigcup_{a\in A} B_a. \]
Indeed, assume $b\in B \setminus (\cup_{a\in A} B_a)$. Since $A$ spans $V$, so does $\cup_{a\in A} B_a$. Thus $b$ can be written as a non-trivial combination of vectors in $\cup_{a\in A} B_a\subset B$, contradicting the linear independence of $B$. Then we have
\[ |B| = \left| \bigcup_{a\in A}B_a \right| \leq \aleph_0 \cdot |A| = |A| \]
A similar argument gives
\[ |A| \leq \aleph_0 \cdot |B| = |B|. \]
By the Schröder–Bernstein theorem \ref{SchroederBernstein}, we conclude $|A| = |B|$.
\end{proof}
TODO: does this proof work with only the ultrafilter lemma?

Thus the notion of dimension (also known as \udef{Hamel-dimension}) also makes sense for infinite-dimensional vector spaces, except it is a cardinality, not a number.

TODO: do we need a strong cardinality assignment? (Assumed for now)

Many textbooks state results using dimensions only for the finite-dimensional case. As we will see, these results almost always generalise directly to the infinite-dimensional case as well, if we assume the axiom of choice.

\begin{note}
The inverse of this theorem (i.e.\ the infinite-dimensional analogue of proposition \ref{vectorSpaceEquality}) does not hold: infinite-dimensional vector spaces always have proper subspaces with a basis of the same cardinality. This is obvious because dropping one vector in the Hamel basis of an infinite-dimensional vector space will not change the cardinality, but will make it a proper subspace.

What does hold is that any two vector spaces with bases of the same cardinality are isomorphic, see \ref{isomorphicDimension}.
\end{note}

 \begin{corollary}
 Let $V$ and $W$ be vector spaces.
 \begin{enumerate}
 \item If $\dim V > \dim W$, then no linear map from $V$ to $W$ is injective.
 \item If $\dim V < \dim W$, then no linear map from $V$ to $W$ is surjective.
 \end{enumerate}
 \end{corollary}

\begin{lemma}
Let $V$ be an infinite-dimensional vector space over a field $\mathbb{F}$. Assume $|\mathbb{F}|\leq \dim_{\mathbb{F}} V$, then $\dim_{\mathbb{F}} V = |V|$. \label{vsCardinality}
\end{lemma}
\begin{proof}
Let $B$ be a basis of $V$. It is supposed infinite. There is a surjection
\[\bigcup_{n\in\N}(\mathbb{F}\times B)^{n} \to V: (a_i,v_i)^{i<n} \mapsto \sum_{i<n}a_iv_i. \]
So we have
\[ |V| \leq \left|\bigcup_{n\in\N}(F\times B)^{n}\right| = \sum_{n\in \N}|F\times B|^n \leq \aleph_0\cdot |\mathbb{F}| \cdot |B| = \max\{\aleph_0, |\mathbb{F}|, |B|\} = |B|. \]
Thus $|V|\leq \dim_{\mathbb{F}} V$. The other inequality is obvious. By the Schröder–Bernstein theorem \ref{SchroederBernstein}, we conclude $\dim_{\mathbb{F}} V = |V|$.
\end{proof}

\section{Sums of subspaces}
\begin{definition}
Suppose $\{U_i\}_{i\in I}$ a set of subspaces of a vector space $V$. The \udef{sum} of these subspaces, denoted $\sum_{i\in I}U_i$, is the set of all finite linear combinations of elements in $\bigcup_{i\in I}U_i$:
\[ \sum_{i\in I}U_i = \Span\left(\bigcup_{i\in I} U_i\right) = \setbuilder{\sum_{i\in J} u_i}{\text{$J\subset I$ finite}, u_i\in \bigcup_{i\in I}U_i}. \]
\end{definition}
For finite sums this reduces to
\[ U_1+\ldots + U_m = \setbuilder{\sum_{i=1}^m u_i}{u_1\in U_1, \ldots, u_m\in U_m}. \]

\begin{proposition} \label{basisSum}
Let $\{U_i\}_{i\in I}$ be a set of subspaces of a vector space $V$ and $\beta_i$ a basis of $U_i$ for all $i\in I$. Then
\[ \sum_{i\in I}U_i = \Span\left(\bigcup_{i\in I}\beta_i\right). \]
\end{proposition}
\begin{proof}
From $\bigcup_{i\in I}\beta_i \subseteq \bigcup_{i\in I} U_i$, we get $\Span\left(\bigcup_{i\in I}\beta_i\right) \subseteq \Span\left(\bigcup_{i\in I} U_i\right) = \sum_{i\in I}U_i$.

Conversely, take $u\in \sum_{i\in I}U_i$. Then $u = \sum_{j\in J}u_j$ where $J$ is finite subset of $I$ and $u_i\in U_i$. Now each $u_j$ can be written as $\sum_k a_{j,k}v_{j,k}$, where $a_{j,k}$ are scalars and $v_{j,k}$ are vectors in $\beta_j$. So
\[ u = \sum_{j,k}a_{j,k}v_{j,k}, \]
which is a finite linear combination of vectors in $\bigcup_{i\in I}\beta_i$. So $u\in \Span\left(\bigcup_{i\in I}\beta_i\right)$.
\end{proof}

\begin{proposition}
Let $V$ be a vector space and $A,B,C$ subspaces. Then
\begin{enumerate}
\item $A+(B\cap C) \subseteq (A+B)\cap (A+C)$;
\item $(A+B)\cap C \supseteq (A\cap C) + (B\cap C)$. 
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Take $v = v_1+v_2 \in A+(B\cap C)$ where $v_2 \in B$ and $v_2 \in C$, so $v_1+v_2\in A+B$ and $v_1+v_2\in A+C$.

(2) Take $v = v_1+v_2\in (A\cap C) + (B\cap C)$. Then $v_1,v_2\in C$ and thus $v\in (A+B)\cap C$.
\end{proof}

\begin{theorem}[Dimension of a sum]
Let $U_1$ and $U_2$ be subspaces of a finite-dimensional vector space, then
\[ \dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim(U_1\cap U_2). \]
\label{dimOfASum}
\end{theorem}
\begin{proof}
Let $\dim U_1 = r, \dim U_2 = s$ and $\dim(U_1\cap U_2) = t$. Then $t\leq r$ and $t\leq s$.  Take a basis $\{v_1,\ldots, v_t\}$ of $U_1\cap U_2$. This can be expanded to a basis $\beta_{U_1} = \{ v_1, \ldots, v_t, u_{t+1}, \ldots u_{r} \}$ of $U_1$ and also to a basis $\beta_{U_2} = \{ v_1, \ldots, v_t, u'_{t+1}, \ldots u'_{s} \}$ of $U_2$. We will show that $\{ v_1, \ldots, v_t, u_{t+1}, \ldots u_{r}, u'_{t+1}, \ldots, u'_{s} \}$ is a basis of $U_1\cap U_2$. This completes the proof because
\begin{align*}
\dim(U_1 + U_2) &= t + (s-t) + (r-t) = s + r -t\\
&= \dim U_1 + \dim U_2 - \dim(U_1\cap U_2).
\end{align*}
The spanning property is easy. Linear independence is slightly more difficult: Take a linear combination
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j + \sum^s_{k=t+1}\beta'_ku_k' =0. \]
We must show this combination is trivial. Indeed observe that
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j  =-\sum^s_{k=t+1}\beta'_ku_k'. \]
The left-hand side is a vector in $U_1$, the right-hand side is a vector in $U_2$, so it must lie in $U_1\cap U_2$, so we rewrite the left-hand side as
\[ \sum_{i=1}^t\lambda_i v_i =  -\sum^s_{k=t+1}\beta'_ku_k'.\]
Due to $\beta_{U_2}$ being a basis, this linear combination must be trivial and all $\beta'_k$ are zero. This leaves us 
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j =0 \]
from our original linear combination. Due to $\beta_{U_2}$ being a basis this combination must also be trivial. 
\end{proof}
\begin{note}
If $\dim(U_1\cap U_2)<\dim U_1$ and $\dim(U_1\cap U_2)< \dim U_2$, this proof generalises to infinite-dimensional vector spaces.
\end{note}

\subsection{(Internal) direct sum}
\begin{definition}
Suppose $\{U_i\}_{i\in I}$ is a set of subspaces of $V$. The sum $\sum_{i\in I}U_i$ is called a \udef{direct sum} if each element $u$ of the sum can be \emph{uniquely} written as
\[ u = \sum_{i\in I}u_i \qquad (u_i\in U_i) \]
where only finitely many of the $u_i$ are nonzero.

In this case we write $\bigoplus_{i\in I} U_i$, or $U_1 \oplus \ldots \oplus U_m$ if $I = \{1,\ldots, m\}$. 
\end{definition}

\begin{proposition}[Conditions for a direct sum] \label{directSumCriterion}
Let $\{U_i\}_{i\in I}$ be a set of subspaces of a vector space $V$ and $\beta_i$ a basis of $U_i$ for all $i\in I$. Let $U,W\subseteq V$ also be subspaces of $V$.
\begin{enumerate}
\item The sum $\sum_{i\in I}U_i$ is direct \textup{if and only if} $0$ has the unique decomposition as in the definition.
\item The sum $\sum_{i\in I}U_i$ is direct \textup{if and only if} the union $\bigcup_{i\in I}\beta_i$ is disjoint and linearly independent.
\item The sum $U+W$ is direct \textup{if and only if} $U\cap W = \{0\}$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Let $\{U_i\}_{i\in I}$ be a set of subspaces of a vector space $V$ and $\beta_i$ a basis of $U_i$ for all $i\in I$. Then
\[ \dim\left(\bigoplus_{i \in I}U_i\right) = \sum_{i\in I} \dim U_i \]
\end{corollary}

\begin{definition}
In a vector space $V$, a subspace $W$ is a \udef{complementary subspace} (or a \udef{complement}) of the subspace $U$ if $V = U \oplus W$.
\end{definition}

\begin{proposition}
Let $V$ be a vector space, then each subspace $U$ of $V$ has a complement.
\end{proposition}
A subspace may in general have many different complements.
\begin{proof}
By \ref{existenceHamelBasis}, we can find a basis $B$ of $U$ and, by \ref{extensionReductionBasis}, we can extend it to a basis $D$ of $V$. Now $V = U \oplus \Span(D\setminus B)$ by \ref{basisSum} and \ref{directSumCriterion}.
\end{proof}
Note this requires the axiom of choice, and is in fact equivalent with it.
\begin{corollary}
Suppose $V$ is finite-dimensional and $U_1,\ldots, U_m$ are subspaces of $V$. Then $U_1+\ldots+ U_m$ is a direct sum \textup{if and only if}
\[ \dim(U_1+\ldots+U_m) = \dim U_1 + \ldots \dim U_m. \]
\end{corollary}


\section{Maps on vector spaces}
\subsection{Homogeneity}
\begin{definition}
Let $V, W$ be vector spaces over the same field $\F$. Let $f: V\to W$ be a function. We say
\begin{itemize}
\item $f$ is \udef{homogeneous} if $\forall x\in V,\lambda\in \F: f(\lambda x) = \lambda f(x)$;
\item $f$ is \udef{positively homogeneous} if $\forall x\in V,\lambda\geq 0: f(\lambda x) = \lambda f(x)$;
\item $f$ is \udef{strictly positively homogeneous} if $\forall x\in V,\lambda > 0: f(\lambda x) = \lambda f(x)$;
\item $f$ is \udef{absolutely homogeneous} if $\forall x\in V,\lambda\in\F: f(\lambda x) = |\lambda| f(x)$.
\end{itemize}
\end{definition}

\begin{lemma} \label{homogeneousFunctionLemma}
Let $f: V\to W$ be a strictly positively homogeneous map. Then
\begin{enumerate}
\item $f(0) = 0$;
\item $f$ is positively homogeneous.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Take $x\in V$. Then $f(0) = f(2\cdot 0) = 2f(0)$. Subtracting $f(0)$ from both sides gives $f(0) = 0$.

(1) We have $f(0\cdot v) = f(0) = 0 = 0f(v)$.
\end{proof}

\subsection{Linear maps}
\begin{definition}
Let $(\F, V, +)$ and $(\F, W, +)$ be vector spaces over the same field. A \udef{linear map} or \udef{linear transformation} is a function $L:V\to W$ with the following properties:
\begin{itemize}[leftmargin=3cm]
\item[\textbf{Additivity}] $L(u+v) = L(u)+L(v)$ for all $u,v \in V$;
\item[\textbf{Homogeneity}] $L(\lambda v) = \lambda L(v)$ for all $\lambda \in \mathbb{R}$ and all $v\in V$.
\end{itemize}
These conditions are equivalent to the condition that
\[ L(\lambda_1 v_1 + \lambda_2v_2) = \lambda_1L(v_1) + \lambda_2 L(v_2) \qquad \text{for all $\lambda_1,\lambda_2\in \mathbb{F}$ and all $v_1,v_2\in V$.} \]
We denote the set of all linear maps from $V$ to $W$ as $\Lin_\mathbb{F}(V,W)$, or $\Lin(V,W)$. The set of endomorphisms on $V$ is denoted $\Lin(V) \defeq \End(V) = \Lin(V,V)$.
\end{definition}

\begin{lemma} \label{linearMaps}
Let $L\in \Lin(V,W)$.
\begin{enumerate}
\item $L(0) = 0$ and $L(-v) = -L(v)$
\item $L\left(\sum^n_{i=1}\lambda_i v_i\right) = \sum_{i=1}^n\lambda_i L(v_i)$.
\item Let $D$ be a set of vectors. Then $L^\imf[D]$ is linearly independent \textup{if and only if} $D$ is linearly independent.
\end{enumerate}
\end{lemma}

\begin{proposition} \label{linearMapsDeterminedByBasis}
Let $V,W$ be vector spaces and $\beta$ a basis of $V$. Then
\begin{enumerate}
\item A linear map $L$ is completely determined by $L^\imf(\beta)$.
\item Let $\beta$ be a basis of $V$. Any function $f: \beta \to W$ can be extended by linearity to a linear map in $\Lin(V,W)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Let $v\in V$. Then there exists a unique $\{x_0, \ldots, x_{n-1}\}\subseteq \beta$ such that $v = \sum_{i=0}^{n-1} a_ix_i$. Then
\[ L(v) = L\Big(\sum_{i=0}^{n-1} a_ix_i\Big) = \sum_{i=0}^{n-1}a_i L(x_i). \]
\end{proof}

\begin{example}
\begin{enumerate}
\item The zero map that maps everything to zero.
\item Identity maps.
\item Differentiation of polynomials.
\item Integration of polynomials.
\item Shifting elements in a list.
\item Projections.
\end{enumerate}

\end{example}

\begin{definition}
A (linear) \udef{operator} between two vector spaces $V$ and $W$ is a linear partial function $T: V \not\to W$ such that the domain $\dom(T)$ is a vector space.

We also say an operator is a function $T: \dom(T)\subseteq V\to W$.
\end{definition}
The requirement that $\dom(T)$ be a subspace of $V$ is necessary for linearity to make sense!

Some authors (e.g.\ Axler) use the word ``operator'' to mean a linear endomorphism.

\subsubsection{Image and kernel}
\begin{definition}
Let $L \in \Lin(V,W)$. The \udef{kernel} or \udef{null space} of $L$ is the set of vectors that $L$ maps to zero:
\[ \ker(L) = \{ v\in V \;|\; L(v) = 0 \}. \]
\end{definition}
\begin{proposition} \label{kernelSubspace}
The kernel of $L\in \Lin(V,W)$ is a subspace of $V$.
\end{proposition}
\begin{definition}
The dimension of the kernel of a linear map is its \udef{nullity}.
\end{definition}
\begin{proposition} \label{injectivityKernelTriviality}
Let $L\in\Lin(V,W)$. Then $L$ is injective if and only if $\ker(L) = 0$.
\end{proposition}
TODO: generalise to groups
\begin{proof}
We show both implications.
\begin{itemize}
\item[\boxed{\Rightarrow}] We know $\{0\}\subset \ker(L)$ by lemma \ref{linearMaps}. Suppose $v\in \ker(L)$, then $L(v) = 0 = L(0)$. So $v=0$ by injectivity and $\{0\}\supset \ker(L)$.
\item[\boxed{\Leftarrow}] Suppose $u,v \in V$ such that $L(u)=L(v)$. Then
\[ 0 = L(u) - L(v) = L(u-v). \]
Thus $u-v\in \ker(L)$, meaning $u-v = 0$ and $u=v$.
\end{itemize}
\end{proof}

\begin{definition}
Let $L \in \Lin(V,W)$. The \udef{image} or \udef{range} of $L$ is the set of vectors that are of the form $L(v)$ for some $v\in V$:
\[ \im(L) = \setbuilder{L(v)}{v\in V}. \]
\end{definition}

\begin{lemma}
The range of $L\in \Lin(V,W)$ is a subspace of $W$.
\end{lemma}
\begin{proof}
Immediate from \ref{imageSubalgebra}.
\end{proof}
\begin{definition}
The dimension of the image of a linear map is its \udef{rank}.
\end{definition}

\begin{theorem}
Every short exact sequence of vector spaces splits.
\end{theorem}
\begin{proof}
Let
\[ \begin{tikzcd}
0 \rar & U \rar{S} & V \rar{T} & W \rar & 0
\end{tikzcd} \]
be a short exact sequence of vector spaces.
By the splitting lemma TODO ref, it is enough to find a left inverse of $S$. Pick a basis $\beta$ of $U$. Because $S$ is injective, $S[\beta]$ is linearly independent and we can extend it to a basis $\beta'$. We can now define the left inverse by specifying how the basis elements are mapped, by \ref{linearMapsDeterminedByBasis}. To wit: $\beta'\setminus S[\beta]$ is mapped to $0$ and each element $S[\beta]$ has exactly one origin be injectivity and it is to this origin that it is now mapped.
\end{proof}
\begin{corollary} \label{directSumKernelImage}
Let $L \in \Lin(V,W)$. Then
\[ V \cong \ker L \oplus \im L. \]
\end{corollary}
TODO coordinate with section on external direct sum.
\begin{proof}
Given $L$ we have the short exact sequence
\[ \begin{tikzcd}
0 \rar & \ker L \ar[r, hook] & V \rar{L} & \im L \rar & 0.
\end{tikzcd} \]
The isomorphism then follows from the splitting lemma TODO ref.
\end{proof}
\begin{corollary}[Dimension theorem for linear maps] \label{dimensionLinearMaps}
Let $L \in \Lin(V,W)$. Then
\[ \dim(V) = \dim(\ker L) + \dim(\im L). \]
\end{corollary}
This corollary is also known as the rank-nullity theorem or the fundamental theorem of linear maps.
\begin{proof}
By $\dim(V) = \dim(\ker L \oplus \im L) = \dim(\ker L) + \dim(\im L)$.

Alternatively this can be proven directly as follows:

Take a basis $\beta_0$ of $\ker(L)$. We can expand this to a basis $\beta$ of $V$, by theorem \ref{infBasis}. It is easy to show that $L[\beta\setminus \beta_0]$ is a basis of $\im(L)$. Now $L[\beta\setminus \beta_0] =_c \beta\setminus \beta_0$ and $(\beta\setminus \beta_0) \cap \beta_0 = \emptyset$. Thus $|\beta| = |(\beta\setminus \beta_0) \cup \beta_0| = |\beta\setminus \beta_0| + |\beta_0|$. This proves the assertion.
\end{proof}
\begin{corollary}
Let
\[ \begin{tikzcd}
0 \rar & V_1 \rar & V_2 \rar & \ldots \rar & V_n \rar & 0
\end{tikzcd} \]
be an exact sequence of vector spaces, then
\[ \sum_{i=1}^n (-1)^i\dim(V_i) = 0. \]
\end{corollary}
\begin{proof}
Let $f_i$ be the map $V_i\to V_{i+1}$. By exactness $\im f_i=\ker f_{i+1}$ and $\dim(\im f_i)=\dim(\ker f_{i+1})$. By the previous corollary $\dim(V_i) = \dim(\ker f_i) + \dim(\im f_i)$. Then
\[ \sum_{i=1}^n (-1)^i\dim(V_i) = \sum_{i=1}^n (-1)^i\dim(\ker f_i) + \sum_{i=1}^n (-1)^i\dim(\ker f_{i+1}) = \sum_{i=2}^{n} (-1)^i\dim(\ker f_i) - \sum_{i=2}^{n} (-1)^i\dim(\ker f_{i}) = 0. \]
\end{proof}
\begin{corollary} \label{dimensionImageSmaller}
Let $L \in \Lin(V,W)$. Then
\[ \dim(\im L) \leq \dim(V). \]
\end{corollary}
\begin{proof}
TODO ref cardinal arithmetic.
\end{proof}

\begin{lemma} \label{rankMapComposition}
Let $S,T$ be compatible linear maps. Then
\[ \text{rank of $ST$}\;\leq\;\min\{\text{rank of $S$, rank of $T$}\}. \]
If $T$ is invertible, then the rank of $ST$ equals the rank of $S$. Similarly if $S$ is invertible, then the rank of $ST$ equals the rank of $T$.
\end{lemma}
\begin{proof}
Clearly $\im(ST) \subset \im(S)$, so $\dim\im(ST)\leq \dim\im(S)$.
We also have $ST = S|_{\im T}T$, where $S|_{\im T}$ is $S$ restricted to $\im T$.  Then corollary \ref{dimensionImageSmaller} applied to $S|_{\im T}$ gives $\dim\im(ST)\leq \dim\im T$. Together these inequalities give the result.

To show equality in the invertible case, first assume $T$ invertible:
\[ \dim\im ST \leq \dim\im STT^{-1} = \dim\im S. \]
Together with the first inequality this gives an equality. The case for $S$ invertible is similar.
\end{proof}

\begin{proposition} \label{kernelCompositionLinearMaps}
Let $S,T$ be compatible linear maps. Then
\begin{enumerate}
\item $\ker(ST)\supseteq \ker(T)$;
\item $\dim\ker(ST) = \dim\ker(T) + \dim(\im(T)\cap\ker(S))$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) $x\in\ker(T) \implies (ST)x = S(Tx) = S(0) = 0 \implies x\in\ker(ST)$.
(2) Consider the restriction $T|_{\ker(ST)}$. Applying the dimension theorem gives
\[ \dim\ker(ST) = \dim\ker(T|_{\ker(ST)}) + \dim\im(T|_{\ker(ST)}) = \dim\ker(ST) = \dim\ker(T) + \dim\im(T|_{\ker(ST)}) , \]
so it is enough to show $\im(T|_{\ker(ST)}) = \im(T)\cap\ker(S)$. First take $v\in\im(T|_{\ker(ST)}$, then there exists some $w\in\ker(ST)$ such that $v=Tw$, meaning $v\in\im(T)$. Also $Sv = STw = 0$, meaning $v\in\ker(S)$.

Then take $v\in\im(T)\cap\ker(S)$, so we can find a $w$ such that $v = Tw$. Also $Sv = STw = 0$, so $w\in\ker(ST)$ and $v\in\im(T|_{\ker{ST}})$.
\end{proof}

\begin{lemma} \label{complementMapIdentityLemma}
Let $V$ be a vector space, $L\in \Lin(V)$ and $W\subseteq V$ a subspace. Then $W \subseteq \ker(L)$ \textup{if and only if} $(\id_V - L)|_W = \id_W$.
\end{lemma}
\begin{proof}
For all $w\in W$, we have $(\id_V - L)(w) = w - L(w)$, so $L(w) = w - (\id_V - L)(w)$. We have $L(w) = 0$ iff $w - (\id_V - L)(w) = 0$ iff $w = (\id_V - L)(w)$.
\end{proof}

\subsubsection{Algebraic operations on linear maps}
\begin{definition}
Suppose $K,L \in \Lin_{\mathbb{F}}(V,W)$ and $\lambda \in \mathbb{F}$.
\begin{itemize}
\item The \udef{sum} $K+L$ is defined by $(K+L)(v) = Kv+Lv$ for all $v\in V$;
\item The \udef{scalar product} is defined by $(\lambda K)(v) = \lambda K(v)$ for all $v\in V$.
\end{itemize}
\end{definition}
\begin{proposition} \label{linearMapsVectorSpace}
\begin{itemize}
\item The sum of linear maps is again a linear maps. Scalar multiples of linear maps are linear maps.
\item With addition and scalar multiplication defined as above, $\Lin_\mathbb{F}(V,W)$ is a vector space.
\end{itemize}
\end{proposition}

\begin{definition}
Let $K\in \Lin_\mathbb{F}(U,V)$ and $L\in \Lin_\mathbb{F}(V,W)$. The \udef{product} $LK$ is defined as the composition
\[ (LK)(u) = L(K(u)) \qquad \text{for all $u\in U$.} \]
If the product of two linear maps $K,L$ makes sense, we call the linear maps \udef{compatible}.
\end{definition}
\begin{proposition}
The product of two (compatible) linear maps is a linear map.
\end{proposition}
\begin{proposition}[Algebraic properties of linear maps] \label{linearMapsAlgebra}
The product of linear maps has the following properties. 
\begin{itemize}[leftmargin=4.2cm]
\item[\textbf{Associativity}] Let $L_1, L_2, L_3$ be compatible linear maps, then
\[ (L_1L_2)L_3 = L_1(L_2L_3) \]
\item[\textbf{Identity}] Let $L\in \Lin(V,W)$. The identity maps $I_V:V\to V$ and $I_W:W\to W$ are linear and have the property that
\[ LI_V = I_W L = L. \]
\item[\textbf{Distributive properties}]
$ (S_1+S_2)T = S_1T + S_2T \qquad \text{and} \qquad S(T_1 + T_2) = ST_1 + ST_2 $
whenever $T,T_1, T_2 \in \Lin(U,V)$ and $S,S_1, S_2\in \Lin(V,W)$.
\end{itemize}
These properties mean that for any vector space $V$, $\Lin(V)$ forms a unital algebra.
\end{proposition}
Note that multiplication of linear maps is not commutative, not even for maps that are compatible both ways.

\subsubsection{Invertibility and isomorphisms}
\begin{lemma} \label{inverseLinear}
Let $L$ be a linear map. If $L$ is invertible as a function (i.e.\ bijective), its inverse $L^{-1}$ is linear.
\end{lemma}
\begin{proof}
We calculate for $x,y$ vectors and $a\in\mathbb{F}$
\[ L^{-1}(ax + y) = L^{-1}(aLL^{-1}x + LL^{-1}y) = L^{-1}L(aL^{-1}x + L^{-1}y) = aL^{-1}x + L^{-1}y. \]
\end{proof}

\begin{definition}
\begin{itemize}
\item An invertible linear map is called an \udef{isomorphism}.
\item Two vector spaces $V,W$  are \udef{isomorphic} if there is an isomorphism between them. This is denoted $V\cong W$.
\end{itemize}
\end{definition}

\begin{proposition} \label{isomorphicDimension}
Let $V,W$ be vector spaces over the same field $\mathbb{F}$ and $n\in \N$. Then
\begin{enumerate}
\item $V\cong W \iff \dim V = \dim W$;
\item $V \cong \mathbb{F}^n \iff \dim V = n$;
\item $\F^n \cong \F^m \iff n=m$.
\end{enumerate}
\end{proposition}
\begin{proof}
We prove the first statement. The second and third follow easily, using $\dim_\mathbb{F} \mathbb{F}^n = n$.
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Let $T:V\to W$ be an isomorphism. Then $\ker T = \{0\}$ and $\im T = W$. Thus
\[ \dim V = \dim \ker T + \dim \im T = 0 + \dim W = \dim W. \]
\item[$\boxed{\Leftarrow}$] Assume $\dim V = \dim W$. Thus there exists an invertible function from a basis of $V$ to a basis of $W$. This can be extended by linearity to a function on $V$, because it is defined on a Hamel basis. It is easy to see this function is linear and bijective.
\end{itemize}
\end{proof}

\begin{proposition} \label{mappingOfBasisByIsomorphism}
Let $L\in\Lin(V,W)$ be an isomorphism. Let $\beta$ be a basis of $V$, then $L[\beta]$ is a basis of $W$.
\end{proposition}

\begin{proposition} \label{invertibleFiniteDim}
Suppose $V$ is a finite-dimensional vector space and $L\in \Lin(V)$ is a linear map on $V$, then
\[ L \;\text{is invertible} \iff L \;\text{is injective} \iff L \;\text{is surjective} \]
\end{proposition}
\begin{proof}
All we need to prove is
\[ L \;\text{is injective} \iff L \;\text{is surjective} \]
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Assume $L$ injective. Then $\ker L = \{0\}$. By the dimension theorem for linear maps, theorem \ref{dimensionLinearMaps}
\[ \dim \im L = \dim V - \dim \ker L = \dim V. \]
Because $\im L \subset V$ and using proposition \ref{vectorSpaceEquality}, we conclude that $\im L = V$ and thus $L$ is surjective.
\item[$\boxed{\Leftarrow}$] Assume $L$ surjective. Then, by the dimension theorem for linear maps,
\[ \dim \ker L = \dim V - \dim \im L = 0, \]
which means $L$ is injective.
\end{itemize}
\end{proof}
Remark that the proof of the first implication uses proposition \ref{vectorSpaceEquality}, and thus cannot be generalised to infinite-dimensional vector spaces. In the proof of the second implication the subtraction of infinite cardinals is only uniquely defined if  $\dim V > \dim \im L$, which is clearly not the case.

\begin{example}
Counterexamples to the previous theorem in the infinite-dimensional case are given by the left shift map on $\mathbb{F}^\N$ (which is injective, but not surjective) and the right shift map on $\mathbb{F}^\N$ (which is surjective, but not injective).
\end{example}

\subsection{Types of linear maps}
\subsubsection{Finite-rank operators}
\begin{definition}
A linear map $T: V\to V$ is said to be a \udef{finite-rank operator} if it has finite rank.
\end{definition}

\subsubsection{Projectors}
\begin{definition}
Let $V$ be a vector space. A function $P: V\to V$ is called a \udef{projector} if it is
\begin{itemize}
\item linear; and
\item idempotent (i.e.\ $P^2 = P$).
\end{itemize}
\end{definition}

\begin{lemma} \label{projectorKernelImageLemma}
Let $V$ be a vector space and $P:V\to V$ a projector and $v\in V$. Then
\begin{enumerate}
\item $v\in \im P$ \textup{if and only if} $v = Pv$;
\item $\id_V - P$ is a projector;
\item $\ker(P) = \im(\id_V - P)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have that $v\in \im P$ iff $\exists u\in V: Pu = v$. Then we have $v = Pu = P^2u = Pv$.

(2) The operator $\id_V - P$ is clearly linear. Idempotency can be calculated as follows:
\[ (\id_V - P)^2 = \id_V - P -P + P^2 = \id_V - P -P + P = \id_V - P. \]

(3) First take $u\in \ker(P)$. Then $Pu = 0$, so $u = u - Pu = (\id_V - P)u \in \im(\id_V - P)$.

Now take $u\in \im(\id_V - P)$. Then there exists $w\in V$ such that $u = w - Pw$. Applying $P$ gives $Pu = Pw - P^2w = Pw-Pw = 0$, so $u\in \ker(P)$.
\end{proof}

\begin{proposition} \label{projectorComplementarySubspaces}
Let $V$ be a vector space and $P: V\to V$ a function. Then the following are equivalent:
\begin{enumerate}
\item $P$ is a projector;
\item there exist complementary subspaces $U,W$ such that $P: U\oplus W = V \to V: u+w \mapsto u$.
\end{enumerate}
In this case $\id_V - P: U\oplus W = V \to V: u+w \mapsto w$.
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ Set $U = \im P$ and $W = \im(\id_V - P)$. Also $W = \ker(P)$ by \ref{projectorKernelImageLemma}. Then
\[ V = \im(\id_V) = \im(P + \id_V - P) \subseteq \im P + \im(\id_V - P) = U + W \subseteq V, \]
so $U+W = V$. Next we show that the sum is direct. Take $v\in U \cap W = \im P \cap \ker P$. Since $v\in \im P$, we have $v= Pv$ by \ref{projectorKernelImageLemma}. Since $v\in \ker P$, we have $v= Pv = 0$. The sum is direct by \ref{directSumCriterion}.

Finally take $u+w\in U\oplus W$. Then $P(u+w) = Pu + Pw = u+0 = u$ and $(\id_V-P)(u+w) = u+w - u = w$.

$(2) \Rightarrow (1)$ To show linearity, take $v_1 = u_1 +w_1 \in U\oplus W$, $v_2 = u_2 +w_2 \in U\oplus W$ and $\lambda \in \F$. Then $v_1 + \lambda v_2 = (u_1 + \lambda u_2) + (w_1 + \lambda w_2)$. Since $(u_1 + \lambda u_2) \in U$ and $(w_1 + \lambda w_2) \in W$, this is the unique decomposition and thus $P(v_1 + \lambda v_2) = u_1 + \lambda u_2 = Pv_1 + \lambda Pv_2$.

Next, for idempotency, note that $0\in W$, so if $u\in U$, then $u+0\in U\oplus W$ is the unique decomposition. Thus $P^2(u+w) = Pu = P(u + 0) = u = P(u+w)$ for all $u\in U,w\in W$.
\end{proof}
\begin{corollary} \label{directSumKernelImageIdempotent}
Let $V$ be a vector space and $P: V\to V$ a projector. Then $V= \im P \oplus \ker P$.
\end{corollary}
\begin{proof}
Let $U,W$ be the complementary subspaces of the proposition, so $V = U\oplus W$. It is clear that $U = \im P$ and $W = \im(\id_V - P) = \ker(P)$, by \ref{projectorKernelImageLemma}.
\end{proof}

\begin{definition}
Let $V$ be a vector space and $U,W$ subspaces such that $U\oplus W = V$. Then the function $P_U: U\oplus W \to U: u+w \mapsto u$ is called the \udef{projector on $U$ along $W$}.
\end{definition}
Note that a projector $P$ is associated to a \emph{pair} of subspaces $(U,W)$. For any one subspace $U$ there exist many possible projectors $P$ with $\im(P) = U$.

\begin{lemma} \label{projectorEquivalentsLemma}
Let $V$ be a vector space, $U, W\subseteq V$  subspaces such that $U\cap W = \{0\}$ and $P: V\to V$ a linear function. Then the following are equivalent:
\begin{enumerate}
\item $V = U\oplus W$ and $P$ is the projector on $U$ along $W$;
\item $P|_U = \id_U$, $\im(P) \subseteq U$ and $\im(\id_V - P) \subseteq W$;
\item $V = U\oplus W$, $P|_U = \id_U$ and $\ker(P) = W$.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Rightarrow (2)$ Immediate.

$(2) \Rightarrow (3)$ For all $v\in V$, we have $v = v -P(v) + P(v) \in \im(\id_V - P) + \im(P)$, so $V = U+W$. Since $U\cap W = \{0\}$, we have that the sum is direct.

First take $v\in \ker(P)$. Then $v = v-P(v) \in \im(\id_V - P) \subseteq W$.

Now take $w\in W$. Then $(\id_V - P)w = w - P(w) \eqdef w' \in W$, so $P(w) = w - w'$. We have $P(w) \in \im(P) \subseteq U$ and $w-w'\in W$. Since the sum is direct, we have $U\cap W = \{0\}$ and thus $P(w) = 0$.

$(3) \Rightarrow (1)$ Take $v = u+w \in U\oplus W = V$. Then $P(v) = P(u)+ P(w) = u+0 = u$.
\end{proof}

Cfr. \ref{complementMapIdentityLemma}.

\begin{lemma} \label{commutingProjectorLemma}
Let $V$ be a vector space and $P,Q: V\to V$ projectors. Then each of the following implies the next:
\begin{enumerate}
\item $PQ = QP$;
\item $\im(PQ) = \im(P)\cap \im(Q)$;
\item $\im(PQ) \subseteq \im(Q)$;
\item $PQ$ is a projector.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Rightarrow (2)$ We have $\im(PQ) \subseteq \im(P)$ and $\im(PQ) = \im(QP) \subseteq \im(Q)$, so $\im(PQ) \subseteq \im(P)\cap \im(Q)$. Conversely, take $x\in \im(P)\cap\im(Q)$. Then $PQx = Px = x$ by \ref{projectorKernelImageLemma}, so $x\in \im(PQ)$.

$(2) \Rightarrow (3)$ Immediate.

$(3) \Rightarrow (4)$ Take arbitrary $v\in V$. Then we can write $v = v_1 + v_2 \in \im(Q)\oplus \ker(Q)$ and $v_1 = v_1' + v_2'' \in \im(P)\oplus \ker(P)$. Now $PQv = Pv_1 = v_1'$. Since $\im(PQ) \subseteq \im(Q)$, we have $v_1'\in \im(Q)$. By construction $v_1'\in \im(P)$. Thus $PQv_1' = Pv_1' = v_1'$, which implies $PQPQv = PQv_1' = v_1' = PQv$. Thus $PQ$ is idempotent.
\end{proof}

\begin{example}
In \ref{commutingProjectorLemma}, it is not true that $PQ$ being a projector implies $PQ = QP$. Indeed, set
\[ A = \begin{pmatrix}
1 & 0 \\ 0 & 0
\end{pmatrix}\qquad \text{and}\qquad B = \begin{pmatrix}
1 & 1 \\ 0 & 0
\end{pmatrix}. \]
Then $A^2 = A$ and $B^2 = B$, but $AB = B \neq A = BA$.
\end{example}


TODO trace:
\begin{lemma}
Let $V$ be a vector space and $P$ an idempotent linear map. Then
\[ \Tr(P) = \dim(\im(P)). \]
\end{lemma}


\subsubsection{Invariant, reducing and irreducible subspaces}
\begin{definition}
Let $V$ be a vector space, $T$ a linear operator on $V$ and $U\subseteq V$ a subspace. Then $U$ is called
\begin{itemize}
    \item \udef{invariant} under $T$ is $T[U]\subseteq U$;
    \item \udef{reducing} for $T$ if $V = U\oplus W$ and both $U$ and $W$ are invariant under $T$;
    \item \udef{irreducible} w.r.t. $T$ if for all $W\subseteq U$ such that $W$ is reducing for $T$, we have $W = U$ or $W = \emptyset$.
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a vector space, $T$ a linear operator on $V$ and $P$ an idempotent operator on $V$ with image $U = P[V]$. Then 
\begin{enumerate}
\item $U$ is invariant under $T$ \textup{if and only if} $PTP = TP$;
\item the following are equivalent:
\begin{enumerate}
\item $U$ is reducing for $T$;
\item $P[V]$ and $(\id_V - P)[V]$ are invariant under $T$;
\item $PT = PTP = TP$;
\item $PT = TP$.
\end{enumerate} \textup{if and only if} .
\end{enumerate}
\end{lemma}
\begin{proof}
(1) The invariance of $U$ under $T$ can be stated as $TP[V] \subseteq \im P$. By \ref{projectorKernelImageLemma} this can be restated as $TPv = PTPv$ for all $v\in V$.

(2) Points (a) and (b) are equivalent by \ref{projectorKernelImageLemma}.

Points (b) and (c) are equivalent by point (1) and $(\id_V - P)T(\id_V - P) = T - PT - TP + PTP = T(\id_V - P) + PTP - PT$.

Point (d) follows immediately from (c). The converse follows from $P(TP) = P(PT) = PT$ and $(PT)P = (TP)P = TP$.
\end{proof}

\subsubsection{Irreducible operators}
\begin{definition}
Let $V$ be a vector space and $T$ an operator on $V$. Then $T$ is called \udef{irreducible} if $V$ is irreducible w.r.t. $T$.
\end{definition}

\subsection{Affine maps}
\begin{definition}
Let $V,W$ be real vector spaces and $f: V\to W$ a function. Then $f$ is called \udef{affine} if
\[ f\big(\lambda x + (1-\lambda)y\big) = \lambda f(x) + (1-\lambda)f(y) \]
for all $x,y\in V$ and $\lambda\in \interval{0,1}$.
\end{definition}

\begin{lemma} \label{midpointPreservingMapLemma}
Let $V,W$ be real vector spaces and $f: V\to W$ a function. If $f\Big(\frac{x}{2} + \frac{y}{2}\Big) = \frac{1}{2}\big(f(x)+f(y)\big)$ for all $x,y\in V$, then
\begin{enumerate}
\item $f(2y-x) = 2f(y)-f(x)$ for all $x,y\in V$;
\item $f(-x) = 2f(0)-f(x)$ for all $x\in V$;
\item $f(x+y) = f(x)+f(y)-f(0)$ for all $x,y\in V$;
\item $f(x-y) = f(x)-f(y)+f(0)$ for all $x,y\in V$;
\item $f(kx) = kf(x) - (k-1)f(0)$ for all $x\in V$ and $k\in \N$;
\item $f\Big(\frac{x}{2^m} + \sum_{i=0}^{m-1} \frac{y_i}{2^{i+1}}\Big) = \frac{f(x)}{2^m} + \sum_{i=0}^{m-1} \frac{f(y_i)}{2^{i+1}}$ for all $m\in \N$, $x\in V$ and $\seq{y_n}\in V^m$;
\item $f\Big(\frac{k}{2^m}x + (1- \frac{k}{2^m})y\Big) = \frac{k}{2^m}f(x) + (1- \frac{k}{2^m})f(y)$ for all $m,k\in\N$, $x,y\in V$.
\end{enumerate}
\end{lemma}
In particular, all these properties hold for affine maps.
\begin{proof}
(1) This follows from
\[ f(y) = f\Big(y - \frac{x}{2} + \frac{x}{2}\Big) = f\Big(\frac{1}{2}(2y - x) + \frac{1}{2}x\Big) = \frac{1}{2}f(2y-x) + \frac{1}{2}f(x). \]

(2) Follows from (1) by setting $y=0$.

(3) We calculate
\begin{align*}
f(x+y) &= \frac{1}{2}\Big(f(2x)+f(2y)\Big) \\
&= \frac{1}{2}\Big(f(2x)+f(2y)\Big) \\
&= \frac{1}{2}\Big(f(2x-0)+f(2y-0)\Big) \\
&= \frac{1}{2}\Big(2f(x)-f(0)+2f(y)-f(0)\Big) \\
&= f(x) + f(y) - f(0).
\end{align*}

(4) We calculate
\begin{align*}
f(x-y) &= f(x) + f(-y) - f(0) \\
&= f(x) - f(y) +2f(0) - f(0) \\
&= f(x) - f(y) + f(0).
\end{align*}

(5) This is just a $k$-fold iteration of (3).

(6) This follows by induction on $m$. The base case is immediate. The induction step is given by the following calculation (where we use the induction hypothesis):
\begin{align*}
f\Big(\frac{x}{2^m} + \sum_{i=0}^{m-1} \frac{y_i}{2^{i+1}}\Big) &= f\bigg(\frac{1}{2}\Big(\frac{x}{2^{m-1}} + \sum_{i=1}^{m-1} \frac{y_i}{2^i} + y_0\Big)\bigg) \\
&= \frac{1}{2}f\Big(\frac{x}{2^{m-1}} + \sum_{i=1}^{m-1} \frac{y_i}{2^i}\Big) + \frac{1}{2}f(y_0) \\
&= \frac{1}{2}f\Big(\frac{x}{2^{m-1}} + \sum_{i=0}^{m-2} \frac{y_{i+1}}{2^{i+1}}\Big) + \frac{1}{2}f(y_0) \\
&= \frac{1}{2}\Big(\frac{f(x)}{2^{m-1}} + \sum_{i=0}^{m-2} \frac{f(y_{i+1})}{2^{i+1}}\Big) + \frac{1}{2}f(y_0) \\
&= \frac{f(x)}{2^{m}} + \sum_{i=0}^{m-2} \frac{f(y_{i+1})}{2^{i+2}} + \frac{1}{2}f(y_0) \\
&= \frac{f(x)}{2^{m}} + \sum_{i=1}^{m-1} \frac{f(y_{i})}{2^{i+1}} + \frac{1}{2}f(y_0) \\
&= \frac{f(x)}{2^{m}} + \sum_{i=0}^{m-1} \frac{f(y_{i})}{2^{i+1}}.
\end{align*}

(7) We prove this by induction on $m$. For the base case $m=0$, we calculate, using (3), (4) and (5),
\begin{align*}
f\big(kx + (1-k)y\big) &= f\big(k(x-y) + y\big) \\
&= f\Big(k(x-y)\Big) + f(y) - f(0) \\
&= kf(x-y) - (k-1)f(0) + f(y) - f(0) \\
&= k\Big(f(x)-f(y)+f(0)\Big) +f(y) - kf(0) \\
&= kf(x) - kf(y) + f(y) \\
&= kf(x) + (1-k)f(y).
\end{align*}
Now for the induction step, we calculate, using the induction hypothesis,
\begin{align*}
f\Big(\frac{k}{2^m}x + (1- \frac{k}{2^m})y\Big) &= f\bigg(\frac{1}{2}\Big(\frac{k}{2^{m-1}}x + (2- \frac{k}{2^{m-1}})y\Big)\bigg) \\
&= f\bigg(\frac{1}{2}\Big(\frac{k}{2^{m-1}}x + (1- \frac{k}{2^{m-1}})y + y\Big)\bigg) \\
&= \frac{1}{2}f\Big(\frac{k}{2^{m-1}}x + (1- \frac{k}{2^{m-1}})y\Big) + \frac{1}{2}f(y) \\
&= \frac{1}{2}\Big(\frac{k}{2^{m-1}}f(x) + (1- \frac{k}{2^{m-1}})f(y)\Big) + \frac{1}{2}f(y) \\
&= \frac{k}{2^{m}}f(x) + (\frac{1}{2}- \frac{k}{2^{m}})f(y) + \frac{1}{2}f(y) \\
&= \frac{k}{2^{m}}f(x) + (1- \frac{k}{2^{m}})f(y).
\end{align*}
\end{proof}

\begin{lemma}
Let $V,W$ be real vector spaces and $f: V\to W$ a function. Then the following are equivalent:
\begin{enumerate}
\item $f$ is affine;
\item $f\big(\lambda x + (1-\lambda)y\big) = \lambda f(x) + (1-\lambda)f(y)$ for all $x,y\in V$ and $\lambda \in \R$;
\item $f-\constant{f(0)}$ is linear.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1)\Rightarrow (2)$ We first show that $f\big(\lambda x + (1-\lambda)y\big) = \lambda f(x) + (1-\lambda)f(y)$ for all $x,y\in V$ and all $\lambda \in \interval{-1,0}$. Then $\lambda'\defeq -\lambda \in \interval{0,1}$, so, using \ref{midpointPreservingMapLemma},
\begin{align*}
f\big(\lambda x + (1-\lambda)y\big) &= f\big(\lambda' (-x) + (1+\lambda')y\big) \\
&= f\big(\lambda' (2y-x) + (1-\lambda')y\big) \\
&= \lambda'f(2y-x) + (1-\lambda')f(y) \\
&= \lambda'\big(2f(y)-f(x)\big) + (1-\lambda')f(y) \\
&= -\lambda'f(x) + (1+\lambda')f(y) \\
&= \lambda f(x) + (1-\lambda)f(y).
\end{align*}
Next we show that if
\[ \forall x,y\in V: \quad f\big(\lambda x + (1-\lambda)y\big) = \lambda f(x) + (1-\lambda)f(y) \]
holds for all $\lambda \in \interval{-k,k}$ ($k\in\N$), then it holds for $2\lambda\in \interval{-2k,2k}$. Indeed, using \ref{midpointPreservingMapLemma},
\begin{align*}
f\big(2\lambda x + (1-2\lambda)y\big) &= f\big(\lambda (2x-y) + (1-\lambda)y\big) \\
&= \lambda f(2x-y) + (1-\lambda)f(y) \\
&= \lambda \big(2f(x) - f(y)\big) + (1-\lambda)f(y) \\
&= 2\lambda f(x) + (1-2\lambda)f(y).
\end{align*}
By induction it holds for all $\lambda\in \R$.

$(2) \Rightarrow (3)$ Take arbitrary $x,y\in V$ and $\lambda \in \R$. We calculate
\begin{align*}
\big(f - \constant{f(0)}\big)(x+\lambda y) &= f(x+\lambda y)- f(0) \\
&= f\big(\lambda (x+ y) + (1-\lambda)x\big)- f(0) \\
&= \lambda f(x+y) + (1-\lambda)f(x) - f(0) \\
&= \lambda \big(f(x)+f(y)-f(0)\big) + (1-\lambda)f(x) - f(0) \\
&= f(x) - f(0) + \lambda \big(f(y)-f(0)\big) \\
&= \big(f - \constant{f(0)}\big)(x)+ \lambda\big(f - \constant{f(0)}\big)(y).
\end{align*}

$(3) \Rightarrow (1)$ Take arbitrary $x,y\in V$ and $\lambda\in\interval{0,1}$. Then
\begin{align*}
f\big(\lambda x + (1-\lambda)y\big) &= \big(f - \constant{f(0)}\big)\big(\lambda x + (1-\lambda)y\big) + f(0) \\
&= \lambda\big(f - \constant{f(0)}\big)(x) + (1-\lambda)\big(f - \constant{f(0)}\big)(y) + f(0) \\
&= \lambda f(x) - \lambda f(0) + (1-\lambda)f(y) - (1-\lambda)f(0) + f(0) \\
&= \lambda f(x) + (1-\lambda)f(y).
\end{align*}
\end{proof}

\section{Constructions of vector spaces}
\subsection{Free vector space}
Given any set, we can construct a vector space by viewing each element in the set as a (linearly) independent (basis) vector. The vector space then consists of formal linear combinations of these vectors.

To be more precise:
\begin{definition}
Let $S$ be a set and $K$ a field. Then define
\[ F_K(S) \defeq \setbuilder{f\in(S\to K)}{f^{\preimf}\big(K\setminus\{0\}\big)\;\text{is finite}}, \]
as a subspace of $(S\to K)$ with pointwise operations.

Then $F_K(S)$ is called the \udef{free vector space} over $S$.
\end{definition}

\begin{proposition} \label{basisFreeVectorSpace}
Let $S$ be a set and $K$ a field. Then we can identify $S$ with a subset of $F_K(S)$ by
\[ S\hookrightarrow F(S): x\mapsto \charFunc{\{x\}}. \]
With this identification $S$ forms a basis for $F_K(S)$.
\end{proposition}
\begin{proof}
For all $f\in F_K(S)$, we can write
\[ f = \sum_{s\in f^{\preimf}\big(K\setminus\{0\}\big)}f(s)\charFunc{\{s\}}, \]
so $S$, under this identification, spans $F_K(S)$.

In order to show linear independence, take some linear combination
\[ f = \sum_{i=0}^n a_i \charFunc{\{s_i\}}, \]
where all $s_i$ are distinct.
Now, if $f = 0$, then $f(s_i) = 0$ for all $s_i$ and $f(s_i) = a_i$, so all $a_i$ are zero.
\end{proof}

\begin{lemma}
Let $V$ be a vector space over $K$ and $\beta$ a basis for $V$. Then $V\cong  F_K(\beta)$.
\end{lemma}
\begin{proof}
We have that $\beta$ is a basis of $V$. By \ref{basisFreeVectorSpace}, there is also a bijection between $\beta$ and a basis of $F_K(\beta)$. Thus the isomorphism follows from \ref{isomorphicDimension}.
\end{proof}

\subsubsection{The free functor}
\begin{proposition}
The operation $F$ of finding the free vector space over a set can be extended to an contravariant functor
\[ F: \cat{Set} \to \cat{Vect}. \]
\end{proposition}
\begin{proof}
Let $f:X\to Y$ be a function between sets.
\end{proof}
TODO: just specific instance up to isomorphism?? covariant?? isomorphism class of all vector spaces with basis $S$?? Is this why tensor product only up to isomorphism??

\subsubsection{Universal property}
\begin{proposition} \label{universalPropertyFreeVectorSpace}
Let $\phi$ be an arbitrary function from $S$ to a vector space $W$ over a field $K$, then there exists a unique linear map $\overline{\phi}: F(S)\to W$ such that the diagram
\[ \begin{tikzcd}
S \ar[r,"\charFunc{\{-\}}"] \ar[dr,"\phi"] & F(S) \ar[d,dashed,"\overline{\phi}"] \\
& W
\end{tikzcd} \qquad \text{commutes.} \]
Furthermore, $F(S)$ is the unique $K$-vector space with this property.
\end{proposition}
\begin{proof}
Immediate from \ref{linearMapsDeterminedByBasis}.
\end{proof}

\subsection{External direct sum}
\begin{definition}
Let $U,W$ be vector spaces over the same field $\mathbb{F}$. We define the vector space  $U\oplus W$, called the \udef{(external) direct sum}, as the set $U\times W$ with the operations
\[ \begin{cases}
(u_1,w_1) + (u_2, w_2) = (u_1 +_U u_2, w_1 +_W w_2) & (u_1,u_2 \in U; w_1, w_2 \in W) \\
r\cdot (u,w) = (ru, rw) & (r\in \mathbb{F}; u\in U; w \in W)
\end{cases} \]
In general we can define a direct sum of an arbitrary collection of vector spaces $\{U_i\}_{i\in I}$, denoted
\[ \bigoplus_{i\in I}U_i \]
as the vector space with as field the subset of the Cartesian product $\prod_{i\in I}U_i$ where all but finitely many of the terms are zero. The operations are defined point-wise.
\end{definition}

TODO categorical stuff. Isomorphic to direct product for finite dimensions.

\begin{proposition}
Direct sum is span of set-theoretic coproduct inside the product.
\end{proposition}


\begin{proposition}
Suppose $V_1, \ldots V_m$ are vector spaces over $\mathbb{F}$. Then
\[ \dim(V_1\oplus\ldots \oplus V_m) = \dim V_1 + \ldots + \dim V_m \]
\label{dimDirectSum}
\end{proposition}
\begin{proof}
We construct a basis $\beta$ of $V_1\oplus\ldots \oplus V_m$ from bases $\beta_{V_i}$ of $V_i$:
\[ \beta = (\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \}) \cup (\{0 \} \times \beta_{V_2}\times \{0 \} \times \ldots \times \{0 \}) \cup \ldots \cup (\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m}). \]
All these unions are disjunct, so
\begin{align*}
|\beta| &= |(\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \}) \cup  \ldots \cup (\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m})| \\
&= |(\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \})| + \ldots + |(\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m})| \\
&= | \beta_{V_1}| + \ldots + |\beta_{V_m}| \\
&= \dim V_1 + \ldots + \dim V_m.
\end{align*}
\end{proof}

\begin{proposition}
Let $U,W$ be subspaces of $V$. Then the external direct sum of $U$ and $W$ is isomorphic to the internal direct sum of $U$ and $W$.
\end{proposition}
\begin{proof}
The map $f: U\times W\to V: (u,w) \mapsto u+w$ is an isomorphism.
\end{proof}
For this reason we use the same symbol for both.

\begin{definition}
Let $V,W, X,Y$ be vector spaces over $\mathbb{F}$. Let $S: V\to X$ and $T: W\to Y$ be linear maps. Then the \udef{direct sum} of $S$ and $T$ is a linear map
\[ S\oplus T: V \oplus W \to X\oplus Y: (v,w) \mapsto (S(v), T(v)). \]
\end{definition}

\begin{lemma}
Let $V,W$ be vector spaces over a field $\F$ and $A,C\in\Lin(V)$ and $B,D\in\Lin(W)$. Then
\begin{enumerate}
\item $a(A\oplus B) + b(C\oplus D) = (aA+bC)\oplus (aB + bD)$;
\item $(A\oplus B)(C\oplus D) = AC\oplus BD$;
\item $(A\oplus B)^k = A^k\oplus B^k$.
\end{enumerate}
\end{lemma}

\subsubsection{Matrix representation}
TODO: move
Assume $V$ and $W$ are finite-dimensional vector spaces with resp. bases $\{\vec{e}_i\}_{i=1}^m$ and $\{\vec{f}_j\}_{j=1}^n$.
As in the proof of proposition \ref{dimDirectSum}, we can take the basis $\{\vec{e}_i\}_i\times\{0\} \cup \{0\}\times\{\vec{f}_j\}_j$ of $V\oplus W$.

We can naturally fit the basis into a list of $m+n$ elements:
\[ (\vec{e}_1,0),\ldots (\vec{e}_m, 0), (0, \vec{f}_1), \ldots, (0,\vec{f}_n)  \]
\subsubsection{Linear maps}
TODO: also move
Let $S: V\to X$ and $T:W\to Y$ be linear maps, with matrix representations $A$ and $B$, respectively. The matrix representation of $S\oplus T$ is given by
\[ A\oplus B = \begin{bmatrix}
A & 0 \\
0 & B
\end{bmatrix} \]
with respect to the basis $\{\vec{e}_i\}_i\times\{0\} \cup \{0\}\times\{\vec{f}_j\}_j$.

\subsubsection{Linear maps as relations}
By identifying a function with its graph, we may consider a function $f: V\to W$ between vector spaces as a subset $\graph(f)$ of $V\oplus W$.

\begin{proposition}
Let $V,W$ be vector spaces.
\begin{enumerate}
\item A function $f: V\to W$ is linear \textup{if and only if} $\graph(f)$ is a subspace of $V\oplus W$.
\item A subset $A\subseteq V\oplus W$ is the graph of a linear functions \textup{if and only if}
\begin{enumerate}
\item $A$ is a subspace;
\item $(0,y)\in A \implies y=0$.
\end{enumerate}
\end{enumerate}
\end{proposition}

\subsection{Projective and inductive systems}
\subsubsection{Projective systems}
\begin{proposition} \label{vectorSpaceProjectiveLimit}
Let $\sSet{I, \{X_i\}_{i\in I}, \{p_{j,i}\}_{i\preceq j}}$ be an inductive system of convergence vector spaces with linear linking morphisms $p_{j,i}$. Then the projective limit $\varprojlim_{i\in I} V_i$ in $\cat{Set}$ has a unique vector space structure that makes each $\pi_i$ linear.
\end{proposition}
\begin{proof}
By the construction in \ref{projectiveLimitSet}, it is clear that this vector space structure must consist of pointwise addition and scalar multiplication.

By linearity of the linking morphisms, $p_{j,i}$, the projective limir $\varprojlim_{i\in I}V_i$ is closed under these operations.
\end{proof}


\subsubsection{Inductive systems}
\begin{proposition} \label{vectorSpaceInductiveLimit}
Let $\sSet{I, \{\sSet{V_i, \xi_i}\}_{i\in I}, \{e_{i,j}\}_{i\preceq j}}$ be an inductive system of convergence vector spaces with linear linking morphisms $e_{i,j}$. Then the inductive colimit $\varinjlim_{i\in I} V_i$ in $\cat{Set}$ has a unique vector space structure that makes each $e_i$ linear.
\end{proposition}
\begin{proof}
To define the addition, take two elements $v,w\in \varinjlim_{i\in I} V_i$. By \ref{elementsOfInductiveLimitImages} there exist $v'\in V_i$ and $w'\in V_j$ such that $e_i(v') = v$ and $e_j(w') = w$. Now take $k \succeq i,j$ and define $v+w \defeq e_k\big(e_{i,k}(v') + e_{j,k}(w')\big)$. Similarly define $\lambda v = e_i(v')$.

If this is well-defined, then it clearly makes each $e_i$ linear. To show this, assume there exist $v^{\prime\prime}\in V_{i'}$, $w^{\prime\prime}\in V_{j'}$ and $k' \succeq i',j'$ such that $e_{i'}(v^{\prime\prime}) = v$ and $e_{j'}(w^{\prime\prime}) = w$. By construction, \ref{inductiveLimitSet}, we have
\[ e_k\big(e_{i,k}(v') + e_{j,k}(w')\big) = e_{k'}\big(e_{i',k'}(v^{\prime\prime}) + e_{j',k'}(w^{\prime\prime})\big) \]
iff $e_{i,k}(v') + e_{j,k}(w') \sim e_{i',k'}(v^{\prime\prime}) + e_{j',k'}(w^{\prime\prime})$ iff 
\[ e_{k,k^{\prime\prime}}\big(e_{i,k}(v') + e_{j,k}(w')\big) = e_{k',k^{\prime\prime}}\big(e_{i',k'}(v^{\prime\prime}) + e_{j',k'}(w^{\prime\prime})\big), \]
which is immediate by linearity of the linking morphisms and the equalities
\[ e_{k,k^{\prime\prime}} \circ e_{i,k} = e_{i,k^{\prime\prime}} = e_{k',k^{\prime\prime}} \circ e_{i,k'} \qquad\text{and}\qquad e_{k,k^{\prime\prime}} \circ e_{j,k} = e_{j,k^{\prime\prime}} = e_{k',k^{\prime\prime}} \circ e_{j,k'}. \]
\end{proof}

\section{Sets of vectors}
\begin{proposition} \label{latticesOfVectorSubsets}
Let $V$ be vector space and consider a function $f: \powerset(V) \to \powerset(V)$ and define
\[ \mathcal{X} = \setbuilder{X\subseteq V}{A \subseteq X \implies f(A)\subseteq X}. \]
Then $\mathcal{X}$ is closed under arbitrary intersections and thus a complete sublattice of $\powerset(V)$.

The closure operator into $\mathcal{X}$ is given by the intersection of all supersets in $\mathcal{X}$.
\end{proposition}

Most of the types of sets of vectors in this section are of this form.



\subsection{Set-valued functions}
\begin{definition}
Let $V,W$ be vector spaces over a field $\F$ and $F: \powerset(V)\to \powerset(W)$. Then $F$ is called \udef{linear} if
\[ F(A+\lambda B) = F(A) + \lambda F(B) \]
for all $\lambda \in \F$ and $A,B\subseteq V$.
\end{definition}

\begin{lemma} \label{linearImageFunction}
Let $V,W$ be vector spaces over a field $\F$ and $f: V\to W$ a function. Then the following are equivalent:
\begin{enumerate}
\item $f$ is linear;
\item $f^\imf$ is linear.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Rightarrow (2)$ We calculate, for arbitrary $A,B\subseteq V$ and $\lambda\in \F$:
\begin{align*}
f^\imf[A+\lambda B] &= f^\imf\Big\{\setbuilder{a+\lambda b}{a\in A, b\in B}\Big\} \\
&= \setbuilder{f(a+\lambda b)}{a\in A, b\in B} \\
&= \setbuilder{f(a)+\lambda f(b)}{a\in A, b\in B} \\
&= \setbuilder{f(a)}{a\in A, b\in B} + \setbuilder{\lambda f(b)}{a\in A, b\in B} \\
&= f^\imf[A]+ \lambda f^\imf[B].
\end{align*}

$(2) \Rightarrow (1)$ We calculate, for arbitrary $u,v\in V$ and $\lambda\in \F$:
\[ f(u+\lambda v) \in f^\imf[\{u+\lambda v\}] = f^\imf[\{u\}] + \lambda f^\imf[\{v\}] = \{f(u)\} +\{\lambda f(v)\} = \{f(u+\lambda v)\}. \]
\end{proof}

\subsection{Star-shaped sets}
\begin{definition}
A subset $S$ of a real or complex vector space $V$ is called
\begin{itemize}
\item \udef{star-shaped at} $a\in V$ if for all $x\in S$ and $0\leq r \leq 1$, $rx + (1-r)a\in C$;
\item \udef{absolutely star-shaped at} $a\in V$ if for all $x\in S$ and $|r| \leq 1$, $rx + (1-r)a\in C$;
\item \udef{(absolutely) star-shaped} if there exists $a\in V$ such that $S$ is (absolutely) star-shaped at $a$.
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a vector space and $S\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $S$ is star-shaped at $a$;
\item $r S + (1-r)a \subseteq S$ for all $0\leq r \leq 1$.
\end{enumerate}
\end{lemma}

\subsection{Affine sets}
\begin{definition}
A subset $A$ of a real or complex vector space $V$ is called \udef{affine} if for all $x,y\in A$ and $\lambda\in\F$, $\lambda x + (1-\lambda)y\in A$.

The closure of a set $X$ into the lattice of affine sets is called the \udef{affine hull} of $X$ and is denoted $\affine(X)$.
\end{definition}

\begin{proposition}
Let $V$ be a vector space and $A\subseteq V$ a subset. Then following are equivalent:
\begin{enumerate}
\item $A$ is affine;
\item for all $x\in A$: the set $A-x$ is a linear subspace;
\item for some $x\in A$: the set $A-x$ is a linear subspace.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ Assume $A$ affine and take arbitrary $x\in A$. We verify the subspace criterion \ref{subspaceCriterion}: clearly $0\in A-x$ because $x\in A$.

Take $y-x, z-x \in A-x$. Then $y-x + z-x = 2\Big(\frac{1}{2}y + \frac{1}{2}z\Big) - x -x \in A-x$.

Take $y-x\in A-x$ and $\lambda\in \R$. Then $\lambda(y-x) = \lambda y + (1-\lambda)x - x \in A-x$.

$(2) \Rightarrow (3)$ Immediate.

$(3) \Rightarrow (1)$ Assume $A-x$ is a linear subspace for some $x\in A$. Take $y,z\in A$ and $\lambda\in \F$. Then $A-x \ni \lambda(y-x)+(1-\lambda)(z-x) = \lambda y + (1-\lambda)z - x$, so $\lambda y + (1-\lambda)z \in A$. 
\end{proof}


\subsection{Balanced and semibalanced sets}
\subsubsection{Semibalanced sets}
\begin{definition}
A subset $B$ of a vector space $V$ is called \udef{semibalanced} if $rB \subseteq B$ for all $0\leq r \leq 1$.

\begin{itemize}
\item The closure of a set $X\subseteq V$ into the lattice of semibalanced sets is called the \udef{semibalanced hull} of $X$ and is denoted $\semibalanced(X)$.
\item The dual closure of a set $X\subseteq V$ into the lattice of semibalanced sets is called the \udef{semibalanced core} of $X$ and is denoted $\semibalancedCore(X)$.
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a vector space and $B\subseteq V$ a subset. Then
\begin{enumerate}
\item $\semibalanced(B) = \bigcup_{0\leq r\leq 1}rB = \interval{0,1}\cdot B$;
\item $\semibalancedCore(B) = \begin{cases}
\bigcap_{r\geq 1}rB & 0\in B\\
\emptyset & 0\notin B
\end{cases}$.
\end{enumerate}
\end{lemma}

\subsubsection{Balanced sets}
\begin{definition}
A subset $B$ of a vector space $V$ over a field $\F$ with valuation $|\cdot|$ is called \udef{balanced} or \udef{circled} if for all $|r|\leq 1$, $rB \subseteq B$.

\begin{itemize}
\item The closure of a set $X\subseteq V$ into the lattice of balanced sets is called the \udef{balanced hull} of $X$ and is denoted $\balanced(X)$.
\item The dual closure of a set $X\subseteq V$ into the lattice of balanced sets is called the \udef{balanced core} of $X$ and is denoted $\balancedCore(X)$.
\end{itemize}
\end{definition}
Note that $0$ is an element of any balanced set. The lattice of balanced sets is closed under unions and thus a complete sublattice of $\powerset(X)$.

\begin{proposition}
The lattice of balanced sets is closed under arbitrary unions and intersections.
\end{proposition}

\begin{lemma} \label{balancedClosures}
Let $V$ be a vector space and $B\subseteq V$ a subset. Then
\begin{enumerate}
\item $\balanced(B) = \bigcup_{|r|\leq 1}rB = \cball(0,1)\cdot B$;
\item $\balancedCore(B) = \begin{cases}
\bigcap_{|r|\geq 1}rB & 0\in B\\
\emptyset & 0\notin B
\end{cases}$.
\end{enumerate}
\end{lemma}
\begin{corollary} \label{linearFunctionsPreserveBalancedHull}
Let $V,W$ be vector spaces, $f\in \Lin(V,W)$ and $X\subseteq V, Y\subseteq W$ subsets. Then $f^\imf\big(\balanced(X)\big) = \balanced\big(f^\imf(X)\big)$.
\end{corollary}
\begin{proof}
We have, using linearity and \ref{imagePreimageGaloisConnection},
\[ \balanced\big(f^\imf(X)\big) = \bigcup_{|r|\leq 1}rf^{\imf}(X) = \bigcup_{|r|\leq 1}f^{\imf}(rX) = f^{\imf}\Big(\bigcup_{|r|\leq 1}rX\Big) = f^{\imf}\big(\balanced(X)\big). \]
\end{proof}
\begin{corollary} \label{balancedHullHomogeneous}
Let $V$ be a vector space, $B\subseteq V$ a subset and $\lambda\in \F$. Then $\balanced(\lambda X) = \lambda \balanced(X)$.
\end{corollary}
\begin{proof}
The function $v\mapsto \lambda v$ is linear.
\end{proof}

\begin{lemma}
Let $V$ be a vector space and $B\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $B$ is balanced;
\item $B\cap U$ is balanced for all subspaces $U\subseteq V$;
\item $\balanced(B) = \cball(0,1)\cdot B \subseteq B$;
\item $\balanced(B) = \cball(0,1)\cdot B = B$;
\item $B\subseteq \balancedCore(B)$;
\item for all $|r|\geq 1$, $C\subseteq rC$;
\item $B$ is absolutely star-shaped at $0$.
\end{enumerate}
\end{lemma}

\begin{lemma} \label{balancedLemma}
Let $V$ be a vector space and $B,C\subseteq V$ balanced subsets. Then
\begin{enumerate}
\item for all $\lambda\in \F$: $\lambda B = |\lambda| B$;
\item $B\cap C$ is balanced;
\item the set of balanced sets is closed under arbitrary unions.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $\lambda = e^{i\theta}|\lambda|$, so 
\[ \lambda B \subseteq \lambda (e^{-i\theta}B) = |\lambda|B \subseteq |\lambda| (e^{i\theta}B) = \lambda B. \]

(2) We have, using \ref{orderPreservingFunctionLatticeOperations},
\[ \cball(0,1)\cdot (B\cap C) \subseteq \cball(0,1)\cdot B \cap \cball(0,1)\cdot C = B\cap C. \]

(3) Let $\{B_i\}_{i\in I}$ be a set of balanced sets. Take $v\in \bigcup_{i\in I}B_i$ and $r\in \cball_\F(0,1)$. Then there exists some $j\in I$ such that $v\in B_j$. Because $B_j$ is balanced, we have $rv\in B_j$, so $rv\in \bigcup_{i\in I}B_i$.
\end{proof}



\subsection{Convexity}
\begin{definition}
Let $V$ be a vector space and $N\in \N$. A \udef{convex combination} of $\seq{x_n}_{n=0}^N \in V^N$ is a vector of the form
\[ \sum_{n=0}^N \lambda_n x_n \qquad \text{where $\seq{\lambda_n}_{n=0}^N \in \R^N$ is a set of reals such that} \sum_{n=0}^N \lambda_n = 1. \]
\end{definition}

\subsubsection{Convex sets}
\begin{definition}
A subset $C$ of a real or complex vector space $V$ is called \udef{convex} if for all $x,y\in C$, all convex combinations of $x$ and $y$ are in $C$. In other words, $rx + (1-r)y\in C$ for all $0\leq r \leq 1$.

The closure of a set $X\subseteq V$ into the lattice of convex sets is called the \udef{convex hull} of $X$ and is denoted $\convex(X)$.
\end{definition}
Note that this is a stronger property than metric convexity!
\begin{example}
Let $C$ be the set of all vectors with norm in $\Q\cap [0,1]$. The is metrically convex, but not a convex set of vectors.
\end{example}

\begin{lemma} \label{convexHullLemma}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then 
\begin{align*}
\convex(X) &= \setbuilder{rx + (1-r)y}{0\leq r \leq 1, x,y\in B} \\
&= \bigcup_{0\leq r\leq 1} rX + (1-r)X.
\end{align*}
\end{lemma}
\begin{corollary} \label{linearFunctionsPreserveConvexHull}
Let $V,W$ be vector spaces, $f\in \Lin(V,W)$ and $X\subseteq V, Y\subseteq W$ subsets. Then $f^\imf\big(\convex(X)\big) = \convex\big(f^\imf(X)\big)$.
\end{corollary}
\begin{proof}
We have, using linearity (\ref{linearImageFunction}) and \ref{imagePreimageGaloisConnection},
\begin{align*}
\convex\big(f^\imf(X)\big) &= \bigcup_{0\leq r\leq 1}rf^{\imf}(X) + (1-r)f^\imf(X) \\
&= \bigcup_{0\leq r\leq 1}f^{\imf}\big(rX + (1-r)X\big) \\
&= f^{\imf}\Big(\bigcup_{0\leq r\leq 1}rX + (1-r)X\Big) = f^{\imf}\big(\convex(X)\big) = f^{\imf}(X).
\end{align*}
\end{proof}
\begin{corollary} \label{convexHullHomogeneous}
Let $V$ be a vector space, $X\subseteq V$ a subset and $\lambda\in \F$. Then $\convex(\lambda X) = \lambda \convex(X)$.
\end{corollary}
\begin{proof}
The function $v\mapsto \lambda v$ is linear.
\end{proof}

\begin{lemma} \label{convexCriteria}
Let $V$ be a vector space an $X\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $X$ is convex;
\item $U\cap X$ is convex for all subspaces $U\subseteq V$;
\item $rX + (1-r)X \subseteq X$ for all $0< r < 1$;
\item any convex combination of any finite set of vectors in $X$ is in $X$;
\item $X$ is star-shaped at each $x\in X$.
\end{enumerate}
\end{lemma}

\begin{lemma} \label{translationScalingConvexSet}
Let $V$ be a vector space over $\F$, $v\in V$, $\lambda\in \F$ and $X\subseteq V$ a convex subset subset. Then $v+\lambda X$ is convex.
\end{lemma}
\begin{proof}
Take $v+\lambda x_1, v+\lambda x_2 \in v+\lambda X$ and $r\in [0,1]$. Then
\[ r(v+\lambda x_1) + (1-r)(v+\lambda x_2) = v + \lambda(rx_1 + (1-r)x_2) \in v+\lambda X. \]
\end{proof}

\begin{lemma} \label{imagePreimageConvexSet}
Let $V,W$ be vector spaces, $f\in \Lin(V,W)$ and $A\subseteq V, B\subseteq W$ convex sets. Then
\begin{enumerate}
\item $f^\imf[A]$ is convex;
\item $f^\preimf[B]$ is convex.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Take $0\leq r \leq 1$. We have that $f^\imf$ is linear by \ref{linearImageFunction}. So $rf^\imf[A] + (1-r)f^\imf[A] = f^\imf[rA + (1-r)A] \subseteq f^\imf[A]$ and $A$ is convex by \ref{convexCriteria}.

(2) Take $0\leq r \leq 1$ and $x_1,x_2\in f^\preimf[B]$. Then $f(x_1),f(x_2)\in B$ and $rf(x_1) + (1-r)f(x_2) = f(rx_1 + (1-r)x_2)\in B$ by convexity. Thus $rx_1 + (1-r)x_2\in f^\preimf[B]$.
\end{proof}


\begin{lemma} \label{convexSemibalanced}
Let $V$ be a vector space and $A\subseteq V$ convex. Then $A$ is semibalanced \textup{if and only if} $0\in A$.
\end{lemma}

\begin{lemma} \label{convexSubspace}
Let $V$ be a vector space and $A \subseteq V$ a convex subset. Then $A$ is a subspace \textup{if and only if} $A$ is closed under multiplication by $2$.
\end{lemma}
In fact multip
\begin{proof}
Any subspace is closed under multiplication by $2$.

Assume $A$ closed under multiplication by $2$. We need to show that $A$ is closed under addition. Take $x,y\in A$. By convexity $\frac{1}{2}x + \frac{1}{2}y\in A$, so $x+ y = 2\left(\frac{1}{2}x + \frac{1}{2}y\right)\in A$.
\end{proof}

\begin{lemma} \label{convexSetsChainComplete}
Let $V$ be a real or complex vector space. Then the set of convex subsets of $V$ is chain-complete.
\end{lemma}
\begin{proof}
Let $\mathcal{C}$ be a chain of convex subsets of $V$. Take $x,y\in \bigcup \mathcal{C}$ and $r\in \interval{0,1}$. Then there exist $C, C'\in \mathcal{C}$ such that $x\in C$ and $y\in C'$. If $C\subseteq C'$, then $x\in C'$ and $rx+(1-r)y\in C'$ by convexity of $C'$, so $rx+(1-r)y\in \bigcup\mathcal{C}$. If $C'\subseteq C$ we have a similar argument.
\end{proof}

\subsubsection{Extreme sets}
\begin{definition}
Let $A$ be a subset of a vector space $V$. A set $E\subseteq A$ is called an \udef{extreme set} in $A$ if
\[ \forall x,y\in A, \forall r\in\interval[o]{0,1}: \; rx+(1-r)y\in E \implies x,y\in E. \]
A point $a\in A$ is called an \udef{extreme point} of $A$ if $\{a\}$ is an extreme set in $A$.

The set of extreme points of $A$ is denoted $\ext(A)$.

A \udef{face} of a convex set is a convex extreme subset.
\end{definition}

\begin{lemma} \label{extremePointCosetConvex}
Let $K$ be a convex subset of a linear space $V$ and $a\in K$. Then $a\in \ext(K)$ \textup{if and only if} $K\setminus\{a\}$ is convex.
\end{lemma}
\begin{proof}
Suppose $a\in\ext(K)$ and take $x,y\in K\setminus\{a\}$. Take $r\in \interval{0,1}$. If $r=0$ or $r=1$, then $rx+(1-r)y\in K\setminus\{a\}$. Now set $r\notin\{0,1\}$. Then $rx+(1-r)y \neq a$, indeed if this were the case, then $rx+(1-r)y \in \{a\}$ and since $\{a\}$ is an extreme set, we would have $x,y\in \{a\}$, which is a contradiction. Since $rx+(1-r)y\in K$ by convexity, we have $rx+(1-r)y\in K\setminus\{a\}$.

Now suppose $K\setminus\{a\}$ is convex. Take $x,y\in K$ and $r\in \interval[o]{0,1}$. Suppose $rx+(1-r)y = a$. We need to show that $x=a=y$, which we achieve by showing that $x\neq a \neq y$, $x\neq a=y$ and $x=a\neq y$ are all imposible.

First take $x\neq a \neq y$. Then $a = rx+(1-r)y \in K\setminus \{a\}$ by convexity, clearly a contradiction.

Next take $x\neq a=y$. Then $rx+(1-r)a = a$, so $r(x-a) = 0$, so $x=a$ which is a contradiction.

Finally take $x=a \neq y$. Then $ra+(1-r)y = a$, so $(1-r)y = (1-r)a$ and $y=a$ which is also a contradiction.
\end{proof}

\begin{lemma} \label{setExtremeInItself}
Let $A$ be a subset of a vector space $V$. Then $A$ is extreme in $A$.
\end{lemma}

\begin{lemma} \label{extremeSetOfExtremeSetIsExtreme}
Let $A$ be a subset of a vector space $V$. If $E\subseteq A$ is extreme in $A$ and $E'\subseteq E$ is extreme in $E$, then $E'$ is extreme in $A$.
\end{lemma}
\begin{proof}
Take arbitrary $x,y\in A$ and $r\in \interval[o]{0,1}$. Suppose $rx+(1-r)y\in E'$. Then $rx+(1-r)y\in E$, since $E'\subseteq E$, so $x,y\in E$, since $E$ is extreme in $A$. Since $rx+(1-r)y\in E'$ and $E'$ is extreme in $E$, we have $x,y\in E'$. Thus $E'$ is extreme in $A$.
\end{proof}

\begin{lemma} \label{extremeSetOfSubset}
Let $A$ be a subset of a vector space $V$. Let $B\subseteq A$ be a subset and $E\subseteq A$ an extreme subset. Then $E\cap B$ is an extreme subset of $B$.
\end{lemma}
\begin{proof}
Take arbitrary $x,y\in B$ and $r\in \interval[o]{0,1}$. Suppose $rx+(1-r)y\in E\cap B$. In particular $rx+(1-r)y\in E$, so $x,y\in E$ by the extremity of $E$. Thus $x,y\in E\cap B$.
\end{proof}

\begin{lemma} \label{extremeSetsClosedUnderUnionIntersection}
Let $A$ be a subset of a vector space $V$ and $\mathcal{E}$ a set of sets extreme in $A$. Then $\bigcap\mathcal{E}$ and $\bigcup\mathcal{E}$ are both extreme in $A$.
\end{lemma}
TODO: intersection because collection of finite character.
\begin{proof}
Take arbitrary $x,y\in A$ and $r\in \interval[o]{0,1}$. We have
\begin{align*}
rx+ (1-r)y\in \bigcap\mathcal{E} &\implies \forall E\in \mathcal{E}: rx+(1-r)y\in E \\
&\implies \forall E\in \mathcal{E}: x,y\in E \\
&\implies x,y\in \bigcap\mathcal{E}.
\end{align*}
The same holds for $\bigcup \mathcal{E}$ by replacing $\forall$ by $\exists$.
\end{proof}

\begin{lemma} \label{notExtremePointLemma}
Let $K$ be a convex subset of a linear space $V$ and $a\in K$. Then the following are equivalent:
\begin{enumerate}
\item $a\notin \ext(K)$;
\item $K\setminus \{a\}$ is not convex;
\item $a\in \convex\big(K\setminus \{a\}\big)$
\item there exist $x,y\in K$ and $t\in \interval{0,1}$ such that $x\neq a\neq y$ and $a = tx+ (1-t)y$;
\item there exist $x,y\in K$ and $t\in \interval[o]{0,1}$ such that $x\neq a$ or $a\neq y$ and $a = tx+ (1-t)y$;
\item there exist $x\neq y\in K$ and $t\in \interval[o]{0,1}$ such that $a = tx+ (1-t)y$;
\item there exist $x\neq y\in K$ such that $a = \frac{1}{2}(x+ y)$;
\item there exist $x,y\in K$ such that $x\neq a$ or $a\neq y$ and $a = \frac{1}{2}(x+ y)$;
\item there exist $x,y\in K$ such that $x\neq a\neq y$ and $a = \frac{1}{2}(x+ y)$.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Leftrightarrow (2)$ By \ref{extremePointCosetConvex}.

$(2) \Rightarrow (3)$ We have that $K\setminus\{a\}$ is not convex, so $K\setminus \{a\} \subsetneq \convex\big(K\setminus \{a\}\big)$ and $\convex\big(K\setminus \{a\}\big) \setminus \big(K\setminus\{a\}\big)$ is not empty. Since $K$ is convex, we have $\convex\big(K\setminus \{a\}\big) \subseteq \convex(K) = K$. Thus $\convex\big(K\setminus \{a\}\big) \setminus \big(K\setminus\{a\}\big) \subseteq K \setminus\big(K\setminus \{a\}\big) = \{a\}$. This implies $a \in \convex\big(K\setminus \{a\}\big) \setminus \big(K\setminus\{a\}\big) \subseteq \convex\big(K\setminus \{a\}\big)$.

$(3) \Rightarrow (2)$ Since $a\notin K\setminus \{a\}$, this implies $K\setminus\{a\} \neq \convex\big(K\setminus \{a\}\big)$, so $K\setminus\{a\}$ is not convex and $a\notin \ext(K)$.

$(3) \Rightarrow (4)$ Immediate by \ref{convexHullLemma}.

$(4) \Rightarrow (5)$ The $t$ in (4) cannot be either $0$ or $1$, as in the first case $a = y$ and in the second $a = x$. Both of these cases are disallowed.

$(5) \Rightarrow (6)$ Take $x,y$ and $t$ as in (5). We need to show that $x\neq y$. Suppose, towards a contradiction, that $x=y$. Then $a = tx + (1-t)x = x$, so $x=a=y$, which is a contradiction.

$(6) \Rightarrow (7)$ Take $x,y,t$ as in (6). 

If $t = \frac{1}{2}$, we are done. Next suppose $t<\frac{1}{2}$ and set $x' \defeq 2tx + (1-2t)y$, which is an element of $K$, since $0<2t<1$ and $K$ is convex. Then
\[ a = tx + (1-t)y = \frac{1}{2}\big(2tx + (2-2t)y\big) = \frac{1}{2}\big(2tx + (1-2t)y + y\big) = \frac{1}{2}(x' + y). \]
We just need to show that $x' \neq y$. Indeed,
\[ x' - y = 2tx + (1-2t)y - y = 2t(x-y) \neq 0. \]

Finally, suppose $t>\frac{1}{2}$. Since $t\in \interval[o]{\frac{1}{2}, 1}$, we have $2t-1 \in \interval[o]{0, 1}$. Set $y'\defeq (2t-1)x + \big(1-(2t-1)\big)y= (2t-1)x + (2-2t)y$, which is an element of $K$ since $K$ is convex. Then
\[ a = tx + (1-t)y = \frac{1}{2}\big(2tx + (2-2t)y\big) = \frac{1}{2}\big(x + (2t-1)x + (2-2t)y\big) = \frac{1}{2}(x+y'). \]
We just need to show that $x \neq y'$. Indeed,
\[ y' - x = (2t-1)x + (2-2t)y - x = (2t-1)x - (2t-2)y = (2t-1)(x-y) \neq 0. \]

$(7) \Rightarrow (8)$ Take $x \neq y\in K$ as is (7).  If $x\neq a$, we are done. If $x=a$, then $y \neq x=a$. 

$(8) \Rightarrow (9)$ We prove this by contradiction. Suppose $x = a$. Then $y \neq a$ by assumption. We also have $2a = a+y$, so $y=a$, which is a contradiction. The argument for $y$ is similar.


$(9) \Rightarrow (3)$ Immediate by \ref{convexHullLemma}.
\end{proof}

\begin{lemma} \label{extremePointsSubset}
Let $K_1 \subseteq K_2$ be convex subsets of a vector space $V$. Then
\begin{enumerate}
\item $K_2\setminus \ext(K_1) \subseteq \big(K_2\setminus K_1\big) \cup \big(K_2 \setminus \ext(K_2)\big)$;
\item $K_2\setminus \ext(K_1) \subseteq K_2\setminus \big(\ext(K_2)\cap K_1\big)$;
\item $\ext(K_2)\cap K_1 \subseteq \ext(K_1)$;
\item $\ext(K_2) \subseteq \ext(K_1) \cup \big(K_2\setminus K_1\big)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Take $a\in K_2\setminus \ext(K_1)$. First suppose $a\notin K_1$. Then $a\in K_2\setminus K_1$.

Now suppose $a\in K_1$. Then $a\in K_1\setminus \ext(K_1)$, so $a = \frac{1}{2}(x+y)$ for some $x\neq y\in K_1$, by \ref{notExtremePointLemma}, and thus $a\notin \ext(K_2)$, since $x\neq y \in K_2$. 

(2) Follows from (1) by \ref{differenceProperties}.

(3) Starting from (2) and taking complements gives
\[ K_2^c \cup \big(K_1 \cap \ext(K_2)\big) \subseteq K_2^c \cup \ext(K_1). \]
Taking the intersection with $K_2$ gives the result.

(4) Using \ref{setDifferenceComplement} and (3), we calculate
\begin{align*}
\ext(K_2) &= \big(\ext(K_2) \cap K_1\big) \cup \big(\ext(K_2) \cap K_1^c\big) \\
&\subseteq \ext(K_1) \cup \big(K_2 \cap K_1^c\big) \\
&= \ext(K_1) \cup \big(K_2\setminus K_1\big).
\end{align*}
\end{proof}

\begin{lemma}
Let $K\subseteq V$ be a convex subset of a vector space $V$ and let $f: V\to W$ be a linear function. Then $f^\preimf\big(\ext(f^\imf(K))\big) \cap K \subseteq \ext(K)$.
\end{lemma}
\begin{proof}
Take arbitrary $a\in K$.

Suppose $a\notin \ext(K)$, then $a = \frac{x+y}{2}$ for some $x\neq y \in K$, by \ref{notExtremePointLemma}, and $f(a) = \frac{f(x)+f(y)}{2}$, so $f(a)\notin \ext\big(f^\imf(K)\big)$ and thus $a\notin f^\preimf\big(\ext(f^\imf(K))\big)$.

The result follows by contraposition.
\end{proof}

\begin{example}
Consider $K = \setbuilder{f\in L^1\interval{0,1}}{\norm{f}_1 \leq 1} \subseteq L^1\interval{0,1}$. Then $\ext(K) = \emptyset$.

Indeed, take $f\in K$. If $\norm{f}_1 < 1$, then $f = \big(1-\norm{f}_1\big)\cdot 0 + \norm{f}_1\Big(\frac{f}{\norm{f}_1}\Big)$ and $f\notin \ext(K)$ by \ref{notExtremePointLemma}, since $\frac{f}{\norm{f}_1}\in K$.

If $\norm{f}_1 = 1$, we can find $a\in \interval[o]{0,1}$ such that $\int_0^a |f(s)| \diff{s} = \frac{1}{2}$ by the intermediate value theorem \ref{intermediateValueTheorem} (using \ref{integralPositiveFunctionContinuous}). Now set $g(x) = \begin{cases}
2f(x) & (x\leq a) \\ 0 & (x>a)
\end{cases}$ and $h(x) = \begin{cases}
0 & (x\leq a) \\ 2f(x) & (x>a)
\end{cases}$. Clearly $f = \frac{1}{2}(g+h)$ and $g\neq f\neq h$. Also
\[ \norm{g}_1 = \int_0^1 |g| \diff{s} = \int_0^a 2|f|\diff{s} = 1 \qquad\text{and}\qquad \norm{h}_1 = \int_0^1 |h| \diff{s} = \int_a^1 2|f|\diff{s} = 1, \]
so $g,h\in K$.
\end{example}


\subsubsection{Absolutely convex sets}
\begin{definition}
A subset $B$ of a vector space $V$ over a field $\F$ with valuation $|\cdot|$ is called \udef{absolutely convex} or \udef{disked} if for all $x,y\in B$ and $r,r'\in \R$ such that $|r| + |r'| \leq 1$, $rx + r'y\in B$.

The closure of a set $X\subseteq V$ into the lattice of absolutely convex sets is called the \udef{absolute convex hull} or \udef{disked hull} of $X$ and is denoted $\disked(X)$.
\end{definition}

\begin{lemma} \label{balancedCoreConvexSet}
The balanced core of a convex set is convex.
\end{lemma}
\begin{proof}
Let $B\subseteq V$ be a convex subset of a vector space $V$. Then
$\balancedCore(B) = \begin{cases}
\bigcap_{|r|\geq 1}rB & 0\in B\\
\emptyset & 0\notin B
\end{cases}$. The empty set is convex. For all $r\in \F$, $rB$ is convex by \ref{translationScalingConvexSet} and arbitrary intersections of convex sets are convex.
\end{proof}

\begin{lemma} \label{absolutelyConvexCriteria}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $X$ is absolutely convex;
\item for all $|a|+|b| \leq 1$, $aX +bX \subseteq X$;
\item $X$ is convex and balanced.
\end{enumerate}
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{lemma} \label{diskedIsCoBal}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then
\begin{enumerate}
\item $\disked(X) \supseteq \balanced(\convex(X))$;
\item $\disked(X) = \convex(\balanced(X))$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Clearly $\convex(X) \subseteq \disked(X)$, so $\balanced\big(\convex(X)\big) \subseteq \balanced\big(\disked(X)\big) = \disked(X)$.

(2) As in (1), we immediately have $\disked(X) \supseteq \convex\big(\balanced(X)\big)$. For the other inclusion, it is enough to show that $\convex\big(\balanced(X)\big)$ is balanced.

Take arbitrary vector in $\convex\big(\balanced(X)\big)$, which can be written as $rv + (1-r)w$ for some $v,w\in \balanced(X)$ and $0\leq r\leq 1$ by \ref{convexHullLemma}. Take arbitrary $r'\in \R$ such that $|r'|\leq 1$. Then
\[ r'\big(rv + (1-r)w\big) = r(r'v) + (1-r)(r'w) \in \convex\big(\balanced(X)\big), \]
since $r'v$ and $r'w$ are elements of $\balanced(X)$.
\end{proof}

\begin{example}
In general $\disked(X) \neq \balanced(\convex(X))$.

Consider $X= \{(1,0), (0,1)\}\subseteq \R^2$. Then
\begin{itemize}
\item $\balanced(X) = \{(a,0)\}_{-1\leq a\leq 1} \cup \{(0,b)\}_{-1\leq b\leq 1}$;
\item $\convex(X) = \{\big(r,(1-r)\big)\}_{0\leq r\leq 1}$;
\item $\convex\big(\balanced(X)\big) = \setbuilder{(a,b)}{-1\leq a\leq 1, |a|-1 \leq b\leq 1-|a|}$;
\item $\balanced\big(\convex(X)\big) = \setbuilder{(a,b)}{-1\leq a\leq 1, |a|-1 \leq b\leq 1-|a|, a\cdot b\geq 0}$.
\end{itemize}
It is clear that $\balanced\big(\convex(X)\big) \subsetneq \convex\big(\balanced(X)\big)$.

TODO picture.
\end{example}

\begin{lemma} \label{linearFunctionsPreserveDiskedHull}
Let $V$ be a vector space, $X\subseteq V$ a subset and $f: V\to W$ a linear function to another vector space. Then $f^\imf\big(\disked(X)\big) = \disked\big(f^\imf(X)\big)$. 
\end{lemma}
\begin{proof}
By \ref{linearFunctionsPreserveBalancedHull}, \ref{linearFunctionsPreserveConvexHull} and \ref{diskedIsCoBal}.
\end{proof}
\begin{corollary} \label{diskedHullHomogeneous}
Let $V$ be a vector space, $X\subseteq V$ a subset and $\lambda\in \F$. Then $\convex(\lambda X) = \lambda \convex(X)$.
\end{corollary}
\begin{proof}
The function $v\mapsto \lambda v$ is linear.
\end{proof}

\subsection{Cones}
\begin{definition}
A subset $C$ of a real or complex vector space $V$ is called a \udef{cone} if for all real $r>0$, $rC \subseteq C$. A cone is called
\begin{itemize}
\item \udef{pointed} if it contains the origin and \udef{blunt} if not;
\item \udef{flat} if $\exists x\neq 0: x\in C \land -x\in C$, and \udef{salient} if not.
\end{itemize}
The closure of a set $X$ into the lattice of cones is called the \udef{conic hull} of $X$ and is denoted $\cone(X)$.

The closure of a set $X$ into the lattice of convex cones is called the \udef{convex conic hull} of $X$ and is denoted $\convCone(X)$.
\end{definition}

\begin{lemma}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then $\cone(X) = \R^{> 0}\cdot X = \interval[o]{0,+\infty}$.
\end{lemma}

The closure of a set $X\subseteq V$ into the lattice of cones is given by $\R\cdot X$.

\begin{lemma} \label{coneEqualityLemma}
A subset $C$ of a vector space $V$ is a cone \textup{if and only if} $rC = C$ for all $r> 0$.
\end{lemma}

\begin{lemma} \label{convexityAdditiveClosure}
A cone $C$ is convex if and only if $C + C \subseteq C$. 
\end{lemma}
\begin{proof}
Assume $C$ convex. Take $v,w\in C$, then $v/2 + w/2\in C$ by convexity and so $v+w = 2(v/2+w/2)\in C$.

Assume $C$ closed under addition. Take $v,w\in C$ and $\lambda\in[0,1]$. Then $(1-\lambda)v$ and $\lambda w$ are elements of $C$ and so the convex combination $(1-\lambda)v + \lambda w$ is too.
\end{proof}

\begin{lemma}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then
\begin{enumerate}
\item $\convCone(X) = \convex\big(\cone(X)\big)$;
\item $\convCone(X) = \cone\big(\convex(X)\big)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Clearly we have $\cone(X) \subseteq \convCone(X)$, so $\convex\big(\cone(X)\big) \subseteq \convex\big(\convCone(X)\big) = \convCone(X)$.

It is then enough to show that $\convex\big(\cone(X)\big)$ is a cone. To that end, take an arbitrary vector in $\convex\big(\cone(X)\big)$, which can be written as $r v + (1-r)w$ for some $v,w\in \cone(X)$ by \ref{convexHullLemma}, and arbitrary $\lambda \in \R^+$. Then
\[ \lambda \big(r v + (1-r)w\big) = r (\lambda v) + (1-r)(\lambda w) \in \convex\big(\cone(X)\big)
\]
by \ref{convexHullLemma} and the fact that $\cone(X)$ is a cone.

(2) The argument for $\cone\big(\convex(X)\big) \subseteq \convCone(X)$ is similar to the one in (1).

It is then enough to show that $\cone\big(\convex(X)\big)$ is convex. To that end, take arbitrary $0\leq r\leq 1$ and two arbitrary vectors in $\cone\big(\convex(X)\big)$, which can be written as $\lambda v$ and $\mu w$ for some $\lambda,\mu\in \R^+$ and $v,w\in \convex{X}$. Then
\begin{align*}
r\lambda v + (1-r)\mu w &= \big(r\lambda + (1-r)\mu\big)\Big(\frac{r\lambda}{r\lambda + (1-r)\mu}v + \frac{(1-r)\mu}{r\lambda + (1-r)\mu}w\Big) \\
&= \big(r\lambda + (1-r)\mu\big)\bigg(\frac{r\lambda}{r\lambda + (1-r)\mu}v + \Big(1 - \frac{r\lambda}{r\lambda + (1-r)\mu}\Big)w\bigg).
\end{align*}
This is an element of $\cone\big(\convex(X)\big)$ since $\frac{r\lambda}{r\lambda + (1-r)\mu}v + \Big(1 - \frac{r\lambda}{r\lambda + (1-r)\mu}\Big)w\in \convex(X)$. Thus $\cone\big(\convex(X)\big)$ is closed under arbitrary convex combinations, which makes it convex.
\end{proof}

\begin{lemma} \label{mappingConeToCone}
Let $V,W$ be vector spaces, $A\subseteq V$, $B\subseteq W$ subsets and $f:V\to W$ a positively homogeneous function. Then
\begin{enumerate}
\item if $A$ is a cone, then $f^\imf(A)$ is a cone;
\item if $B$ is a cone, then $f^\preimf(B)$ is a cone;
\item if $A$ is a pointed cone, then $f^\imf(A)$ is a pointed cone;
\item if $B$ is a pointed cone, then $f^\preimf(B)$ is a pointed cone.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We calculate
\[ \interval[o]{0,\infty}\cdot f^\imf(A) = \bigcup_{r>0}r\cdot f^\imf(A) = \bigcup_{r>0}f^\imf(r\cdot A) \subseteq \bigcup_{r>0}f^\imf(A) = f^\imf(A). \]

(2) We calculate
\[ \interval[o]{0,\infty}\cdot f^\preimf(B) = \bigcup_{r>0}r\cdot f^\preimf(B) = \bigcup_{r>0}f^\preimf(r\cdot B) \subseteq \bigcup_{r>0}f^\preimf(B) = f^\preimf(B). \]

(3) Assume $0\in A$, then $0 = f(0)\in f^\imf(A)$ by \ref{homogeneousFunctionLemma}.

(4) Assume $0\in B$, then $f(0) = 0\in B$ by \ref{homogeneousFunctionLemma}, so $0\in \preimf(B)$.
\end{proof}

\subsection{Absorbing and absorbent sets}
\begin{definition}
Let $V$ be a vector space and $A,B\subseteq V$. The $A$ \udef{absorbs} $B$ if there exists a real $r>0$ such that for all $|c| \geq r$: $B\subseteq cA$.

The set $A$ is called \udef{absorbent} if it absorbs $\{v\}$ for all $v\in V$.
\end{definition}

\begin{lemma} \label{absorbingSetLemma}
Let $V$ be a vector space and $A, B\subseteq V$ subsets. Then the following are equivalent:
\begin{enumerate}
\item $A$ absorbs $B$;
\item there exists an $\epsilon>0$ such that $\cball(0,\epsilon)\cdot B\subseteq A$;
\item there exists an $\epsilon>0$ such that $\ball(0,\epsilon)\cdot B\subseteq A$.
\end{enumerate}
In particular every absorbent set contains $0$.
\end{lemma}
\begin{corollary} \label{convexAbsorbentImpliesSemibalanced}
Every convex absorbent set is semibalanced.
\end{corollary}


\begin{proposition} \label{absorbingSetProperties}
Let $V$ be a vector space and $A_1, A_2, B\subseteq V$ subsets. Then
\begin{enumerate}
\item if $A_1$ absorbs $B$ and $A_1\subseteq A_2$, then $A_2$ absorbs $B$;
\item if both $A_1$ and $A_2$ absorb $B$, then $A_1\cap A_2$ absorbs $B$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) If $A_1$ absorbs $B$, then there exists $\epsilon >0$ such that $\ball(0,\epsilon)\cdot B\subseteq A_1 \subseteq A_2$, so $A_2$ absorbs $B$.

(2) If $A_1$ and $A_2$ absorb $B$, then there exist $\epsilon_1, \epsilon_2 >0$ such that $\ball(0,\epsilon_1)\cdot B\subseteq A_1$ and $\ball(0,\epsilon_2)\cdot B\subseteq A_2$, so $A_2$ absorbs $B$. Then, by \ref{orderPreservingFunctionLatticeOperations},
\[ \ball(0,\min\{\epsilon_1, \epsilon_2\})\cdot B = \big(\ball(0,\epsilon_1) \cap \ball(0,\epsilon_2)\big) \subseteq \ball(0,\epsilon_1)\cdot B \cap \ball(0,\epsilon_2)\cdot B \subseteq A_1 \cap A_2. \]
\end{proof}

\subsection{Relative interior}
\begin{definition}
Let $V$ be a vector space and $A\subseteq V$ a subset. We define the \udef{relative interior} of $A$ as
\[ \relint(A) \defeq \setbuilder{x\in A}{\forall y\in \affine(A): \exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; \lambda y + (1-\lambda) x \in A}. \]
We call $A$ \udef{relatively open} if $A = \relint(A)$. 
\end{definition}

\begin{example}
If we define the relative interior of $A$ as
\[\setbuilder{x\in A}{\forall y\in A: \exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; \lambda y + (1-\lambda) x \in A}, \]
then the cross $\setbuilder{(x,y)\in \R^2}{(x=0)\lor (y=0)}$ is relatively open.
\end{example}

\begin{lemma} \label{relativeInteriorEquivalent}
Let $V$ be a vector space and $A\subseteq V$ a subset. Then
\[ \relint(A) = \setbuilder{x\in A}{\forall y,z\in A: \exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; x + \lambda (y-z) \in A}. \]
\end{lemma}
\begin{proof}
For the inclusion $\subseteq$, we just need to note that, for all $x,y,z\in A$ we have $x + \lambda (y-z) = \lambda (y-z+x) + (1-\lambda)x$ and $y-z+x\in \affine(A)$.

For the other inclusion, let $x$ be an element of the right-hand set. We need to show $\exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; \lambda y + (1-\lambda) x \in A$ for arbitrary $y\in \affine(A)$.

We can write $y = \mu z + (1-\mu) z'$ for some $\mu\in \F$. By assumption, there exist $\delta, \delta'>0$ such that $\forall \nu\in \cball_\F(0,\delta): x+ \nu(x-z)\in A$ and $\forall \nu'\in \cball_\F(0,\delta'): x+ \nu'(x-z')\in A$. In particular set $\eta \defeq \min\{\frac{\delta}{|\mu|}, \frac{\delta'}{|1-\mu|}\}$, so $x+ \eta \mu(x-z)\in A$, since $|\eta\mu| \leq \frac{\delta}{|\mu|}|\mu| = \delta$ and $x+ \eta (1-\mu)(x-z')\in A$, since $|\eta(1-\mu)| \leq \frac{\delta'}{|1-\mu|}|1-\mu| = \delta'$.
By assumption, there exists $\epsilon'> 0$ such that
\[ \forall \lambda'\in\cball_F(0,\epsilon'):\; x + \lambda'\Big(\big(x + \eta\mu(z-x)\big) - \big(x + \eta(1-\mu)(z'- x)\big)\Big)\in A. \]

Now set $\epsilon = \epsilon'\cdot \eta$ and take arbitrary $\lambda\in \cball_\F(0,\epsilon)$. Then
\begin{align*}
\lambda y + (1-\lambda) x &= \lambda\big(\mu z + (1-\mu) z'\big) + (1-\lambda)x \\
&= x + \lambda\big(\mu z + (1-\mu) z' - x\big) \\
&= x + \lambda\big(\mu z + (1-\mu) z' - \mu x - (1-\mu)x\big) \\
&= x + \lambda\big(\mu(z-x) - (1-\mu)(z'- x)\big) \\
&= x + \frac{\lambda}{\eta}\big(\eta\mu(z-x) - \eta(1-\mu)(z'- x)\big) \\
&= x + \frac{\lambda}{\eta}\Big(\big(x + \eta\mu(z-x)\big) - \big(x + \eta(1-\mu)(z'- x)\big)\Big),
\end{align*}
which is an element of $A$ because $\big|\frac{\lambda}{\eta}\big| \leq \frac{\epsilon}{\eta} = \epsilon'$.
\end{proof}

\begin{lemma}
Let $V$ be a vector space and $A\subseteq V$ a convex subset. Then
\begin{enumerate}
\item $\relint(A) = \setbuilder{x\in A}{\forall y\in A: \exists r > 1: \; rx + (1-r)y \in A}$;
\item $\relint(A) = \setbuilder{x\in A}{\forall y\in A: \exists z\in A, 0 < r < 1:\; x = ry + (1-r)z}$.
\end{enumerate}
\end{lemma}
\begin{proof}

\end{proof}

\begin{lemma}
Let $V$ be a vector space and $A, B\subseteq V$ subsets.
\begin{enumerate}
\item $\relint(A)+\relint(B) \subseteq \relint(A+B)$;
\item $\relint(A)+\relint(B) = \relint(A+B)$ if $A$ and $B$ are both convex;
\item $\cone\big(\relint(A)\big) \subseteq \relint\big(\cone(A)\big)$;
\item $\cone\big(\relint(A)\big) = \relint\big(\cone(A)\big)$ if $A$ is convex;
\item if $A$ and $B$ are relatively open, then $A+B$ is relatively open.
\end{enumerate}
\end{lemma}
\begin{proof}

\end{proof}

\begin{example}
Let $V$ be a vector space and $A\subseteq V$ a subset. At one point I thought that $A$ is relatively open \textup{if and only if}
\[ A = \setbuilder{x\in A}{\forall y\in A: \exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; \lambda y + (1-\lambda)x \in A}. \]
This is not true. Consider the following subset of $\R^2$:
\[ A = \{0\} \cup \setbuilder{(x,y)\in \R^2}{x<0, y<0} \cup \setbuilder{(x,y)\in \R^2}{x>0, y>0}. \]
It is not relatively open (consider the origin), but it does satisfy the equality.
\end{example}

\begin{lemma}
Let $V$ be a vector space and $A\subseteq V$ a subset. Then $A$ is relatively open \textup{if and only if}
\[ A = \setbuilder{x\in A}{\forall y\in A: \exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; \lambda y + (1-\lambda)x \in A}. \]
\end{lemma}
\begin{proof}
First assume $A$ is relatively open. Then
\begin{align*}
A &= \relint(A) \\
&= \setbuilder{x\in A}{\forall y\in \affine(A): \exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; \lambda y + (1-\lambda) x \in A} \\
&\subseteq \setbuilder{x\in A}{\forall y\in A: \exists \epsilon >0: \forall \lambda \in \cball_\F(0,\epsilon):\; \lambda y + (1-\lambda) x \in A} \\
&\subseteq A,
\end{align*}
since $A \subseteq \affine(A)$. Thus all the inclusions are equalities and the first implication is proved.

Now assume the relation holds. We need to show that $A\subseteq \relint(A)$. For this we use \ref{relativeInteriorEquivalent}.

Take arbitrary $y,z\in A$. By assumption, there exists $\epsilon > 0$ such that $x+\lambda(y-x)\in A$ for all $\lambda \in \cball_\F(0,\epsilon)$. 

Since $x+\lambda(y-x)\in A$ for all such $\lambda$, we can find $\epsilon' > 0$ such that
\[ \lambda' z + (1-\lambda')\big(x+\lambda(y-x)\big) \in A \]
for all $\lambda'\in \cball_F(0,\epsilon')$.







WLOG we may take $\epsilon < 1$. Set $\epsilon' \defeq \frac{\epsilon}{1+\epsilon}$. We claim that for all $\lambda'\in \cball_\F(0,\epsilon')$, we have $x + \lambda' (y-z) \in A$.

To see this, we first note that $\frac{\lambda'}{\lambda'-1} \in \cball_\F(0,\epsilon)$. Indeed, we have
\begin{align*}
|\lambda'|\leq \frac{\epsilon}{\epsilon +1} \iff& (1+\epsilon)|\lambda'| \leq \epsilon \\
\implies& |\lambda'|\leq (1-|\lambda'|)\epsilon \leq \big|1-\lambda'|\epsilon \\
\implies& \left|\frac{\lambda'}{\lambda' - 1}\right| \leq \epsilon, 
\end{align*}
where we have used the reverse triangle inequality (\ref{metricReverseTriangleInequality}, with $a = 0$) to get $|\lambda' - 1|\geq \big||\lambda'| - 1 \big|$. Thus $x+ \frac{\lambda'}{\lambda' - 1}(y-x)\in A$. 
\end{proof}

\begin{lemma}
Let $V$ be a vector space and $A\subseteq V$. Then
\begin{enumerate}
\item $\relint(A)\subseteq A$;
\item $\relint^2(A) = \relint(A)$.
\end{enumerate}
\end{lemma}
In particular, $\relint(A)$ is relatively open.
\begin{proof}
(1) Clear.

(2) TODO.
\end{proof}

\begin{example}
The function $\relint: \powerset(V)\to \powerset(V)$ is not a dual Moore closure because it is not monotone.

Consider $A = \{(x,0)\}_{0<x<1}$. Then $A$ is relatively open. We also have $\relint\big(A\cup \{(0,1)\}\big) = \emptyset$, so $\relint(A) \nsubseteq \relint\big(A\cup \{(0,1)\}\big)$ even though $A\subseteq A\cup \{(0,1)\}$.
\end{example}

\subsection{Translation invariance}
In this section we will frequently use the function $\Delta_V: V\oplus V\to V: (x,y) \mapsto y-x$, for some vector space $V$. We will also write simply $\Delta$ if the space $V$ is clear.

Cfr. sections \ref{sec:translationInvariance} and \ref{sec:groupUniformStructure}.

\begin{lemma} \label{vectorspaceTranslationInvariantSubset}
Let $V$ be a vector space and $A\subseteq V\oplus V$. Then the following are equivalent:
\begin{enumerate}
\item $A$ is translation invariant;
\item $A+\id \subseteq A$;
\item $x_2 - x_1\in \Delta^\imf(A) \implies (x_1, x_2)\in A$;
\item $x_2 - x_1\in \Delta^\imf(A) \iff (x_1, x_2)\in A$;
\item $A = \Delta^{\preimf}\big(\Delta^\imf(A)\big)$
\end{enumerate}
\end{lemma}
\begin{proof}
$\boxed{(1) \Rightarrow (2)}$ Restatement of definition.

$\boxed{(2) \Rightarrow (3)}$ Take arbitrary $x_2 - x_1\in \Delta^\imf(A)$. Then there exists $(y_1,y_2)\in A$ such that $x_2 - x_1 = y_2 - y_1$. Then we can set $a \defeq x_1 - y_1 = x_2 - y_2$ and we have $(x_1, x_2) = (y_1 + a, y_2 + a) \subseteq A+\id \subseteq A$.

$\boxed{(3) \Rightarrow (4)}$ Then implication $(x_1, x_2)\in A \implies x_1 - x_2\in \Delta^\imf(A)$ is immediate by the definition of $\Delta^\imf(A)$.

$\boxed{(4) \Rightarrow (5)}$ We have
\[ A = \setbuilder{(x_1, x_2)}{x_2-x_1 \in \delta^{\preimf}(A)} = \Delta^\preimf\big(\Delta^\imf(A)\big). \]

$\boxed{(5) \Rightarrow (1)}$ The set $\Delta^{\preimf}\big(\Delta^\imf(A)\big)$ is translation invariant because $\Delta(x,y) = \Delta(x+a,y+a)$ for all $x,y,a\in V$.
\end{proof}

\begin{lemma}
Let $V$ be a vector space. Then $\Delta$ is homogeneous.
\end{lemma}
\begin{proof}
Immediate by scalar distributivity:
\[ \lambda\Delta(x,y) = \lambda(y-x) = \lambda y - \lambda x = \Delta(\lambda x, \lambda y). \]
\end{proof}

\begin{lemma}
Let $V$ be a vector space and $A\subseteq V\oplus V$. Then
\begin{enumerate}
\item if $A$ is convex, then $\Delta^{\imf}(A)$ is convex;
\item if $A$ is (semi)balanced, then $\Delta^\imf(A)$ is (semi)balanced.
\end{enumerate}
If $A$ is translation invariant, then the converse implications also hold.
\end{lemma}
\begin{proof}
(1) Take $x_2-x_1, y_2-y_1\in \Delta^{\imf}(A)$. Then for all $r\in\interval{0,1}$,
\[ r(x_2-x_1) + (1-r)(y_2- y_1) = \big(rx_2 + (1-r)y_2\big) - \big(rx_1 + (1-r)y_1\big), \]
and the right-hand side is an element of $\Delta^{\imf}(A)$ by convexity of $A$.

(2) Take $(x_1,x_2)\in A$ and $\lambda\in\F$. Then we have
\[ \lambda(x_1,x_2)\in A \iff (\lambda x_1, \lambda x_2)\in A \implies \lambda x_2 - \lambda x_1\in \Delta^\imf(A) \iff \lambda(x_2 - x_1)\in \Delta^\imf(A). \]

(Assuming translation invariance) Now we can run the arguments in reverse, using \ref{vectorspaceTranslationInvariantSubset}.
\end{proof}

\begin{lemma} \label{vectorDeltaLemma}
Let $\sSet{V}$ be a convergence vector spaces, $U\subseteq V$, $A\subseteq V\oplus V$ and $m\in \N$. Then
\begin{enumerate}
\item $\Delta^\preimf(mU) \subseteq \Delta^\preimf(U)^m$;
\item if $A$ is translation invariant, then $m\Delta^{\imf}(A) \subseteq \Delta^{\imf}(A^m)$;
\item if $\Delta^{\imf}(A)$ is convex, then $m\Delta^{\imf}(A) \supseteq \Delta^{\imf}(A^m)$.
\end{enumerate}
Here $A^m$ means $\underbrace{A;\ldots;A}_{\text{$m$ times}}$.
\end{lemma}
\begin{proof}
(1) Take $(x,y)\in \Delta^\preimf(mU)$. Then there exists $z\in U$ such that $y-x = mz$. We have that $(x,x+z), (x+z, x+2z), \ldots \big(x+(m-1)z, x+mz\big)\in \Delta^\preimf(U)$, so $(x,x+mz) = (x,y) \in \Delta^\preimf(U)^m$.


(2) If $A$ is translation invariant, then $A = \Delta^\preimf\circ\Delta^\imf(A)$, so (using the homogeneity of $\Delta$)
\[ m\Delta^{\imf}(A) = m\Delta^\imf\circ \Delta^\preimf\circ\Delta^\imf(A) = \Delta^\imf\circ \Delta^\preimf\big(m\Delta^\imf(A)\big) \subseteq \Delta^{\imf}\Big(\big(\Delta^preimf\circ\Delta^\imf(A)\big)^m\Big) = \Delta^\imf(A^m).  \]

(3) Take $x\in \Delta^{\imf}(A^m)$. Then we can write $x= y-z$ such that $(y,z_1), (z_1,z_2), \ldots (z_m, z)\in A$. Now
\begin{align*}
x &= y - z_1 + z_1 - z_2 + \ldots + z_m - z \\
&\in \Delta^{\imf}(A) + \Delta^{\imf}(A) + \ldots + \Delta^{\imf}(A) \\
&\subseteq m\Delta^{\imf}(A),
\end{align*}
by convexity.
\end{proof}

Note also $Ax = x + \Delta^\imf(A)$ for all $A\subseteq V\oplus V$ and $x\in V$ by \ref{deltaPreimageLemma}.

\subsubsection{Quotient spaces}

\begin{proposition} \label{congruenceSubspace}
Let $V$ be a vector space.
A translation invariant binary relation $\mathfrak{q}$ on $V$ is a $\{+,0, (\lambda\cdot -)\}_{\lambda\in\F}$-congruence \textup{if and only if} $\widetilde{\mathfrak{q}}$ is a subspace.
\end{proposition}
Note that all congruences are translation invariant by \ref{congruenceTranslationInvariant}. The hypothesis is necessary for $\widetilde{\mathfrak{q}}$ to be well-defined.
\begin{proof}
From \ref{congruenceNormalSubgroup}, we know that $\mathfrak{q}$ is a $\{+,0, \big((-1)\cdot -\big)\}_{\lambda\in\F}$-congruence if and only if $\widetilde{\mathfrak{q}}$ is a subgroup (i.e.\ a sub-$\{+,0, \big((-1)\cdot -\big)\}_{\lambda\in\F}$-algebra).

We just need to show that $\mathfrak{q}$ is closed under scalar multiplication iff $\widetilde{\mathfrak{q}}$ is. This follows straight from
\[ \lambda\cdot(x,y) = (\lambda\cdot x, \lambda\cdot y)\in \mathfrak{q} \qquad\iff\qquad \lambda\cdot(x-y) = \lambda\cdot x - \lambda\cdot y\in \widetilde{\mathfrak{q}}. \]
\end{proof}


\begin{definition}
Let $V$ be a vector space and $U\subseteq V$ a subspace.
We define the \udef{quotient vector space} $V/U$ as the quotient algebra
\[ V/U \defeq V/\setbuilder{(x,y)}{x-y\in U}. \]
We call the dimension of $V/U$ the \udef{codimension} of $U$ in $V$:
\[ \codim(U) = \dim(V/U). \]
\end{definition}


Set $\mathfrak{q} = \setbuilder{(x,y)}{x-y\in U}$. Then
\[ [v]_\mathfrak{q}=[w]_\mathfrak{q} \iff (v,w)\in\mathfrak{q} \iff w-v\in U \iff w+U = v+U. \]
So the equivalence classes in $V/U$ are of the form $x+U$.

\begin{definition}
Let $V$ be a vector space and $U\subseteq V$ a subspace.
\begin{itemize}
\item An \udef{affine subset} of $V$ is a subset of $V$ of the form $v+U$ for some $v\in V$ and some subspace $U$ of $V$.
\item An affine subset $v+U$ is \udef{parallel} to $U$.
\end{itemize}
\end{definition}

\begin{proposition} \label{dimensionTheoremQuotientSpace}
Let $U$ be a subspace of a vector space $V$. Then
\[ \dim V = \dim U + \dim V/U = \dim U + \codim U.  \]
\end{proposition}
\begin{proof}
Apply the dimension theorem for linear maps to the quotient map.
\end{proof}

\begin{definition}
Let $f:V\to W$ be a linear map of vector spaces. The \udef{cokernel} of $f$ is the quotient space
\[ \coker(f) = W/\im(f). \]
The dimension of the cokernel is called the \udef{corank}.
\end{definition}
\begin{lemma}
Let $U$ be a subspace of a vector space $V$. The codimension of $U$ is the corank of the inclusion $U\hookrightarrow V$:
\[ \codim(U) = \dim\coker(U\hookrightarrow V). \]
\end{lemma}

\begin{proposition} \label{splittingMap}
Let $T\in \Lin(V,W)$. Then $T$ induces a linear map
\[ \tilde{T}: V/\ker(T) \to W: v +\ker(T) \mapsto Tv \]
with the following properties:
\begin{enumerate}
\item $\tilde{T}$ is injective;
\item $\im\tilde{T} = \im T$;
\item $\tilde{T}$ is an isomorphism from $V/\ker(T)$ to $\im T$.
\end{enumerate}
\end{proposition}

 TODO each short exact sequence of vector spaces splits \url{https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem}




\input{linearAlgebra/representation}

\input{linearAlgebra/multilinear}

\input{linearAlgebra/matrices}











\chapter{Indices and symbols}
\section{Contravariant and covariant vectors and tensors}
When working in finite-dimensional spaces with specified bases, we often get expressions of the form
\[ v = \sum_{i=1}^n a_i \vec{e}_i. \]
We may replace this expression with
\[ v = a^i \vec{e}_i \]
if we take the convention that if an index is repeated once up and once down, then there is a sum over all values of that index. This is the \udef{Einstein summation convention}.

Note that coordinates have their indices up, and basis vectors have their indices down.

Now in the dual space, we have the dual basis $\{\varphi^j\}_j$. In the dual space we take the opposite convention: coordinates have their indices down, and dual basis vectors have their indices up, so
\[ \varphi = b_j \varphi^j. \]
This allows us to write
\[ \varphi(v) = \varphi(a_i \vec{e}_i) = a_i b^j \varphi^j(\vec{e}_i) = a_i b^i \]
where for the last equality we have used that $\varphi^j(\vec{e}_i)$ only does not vanish if $i=j$ and is $1$ in this case.

We call vectors in $V$ \udef{contravariant} vectors and vectors in $V^*$ \udef{covariant} vectors, or covectors.

Per convention we put the coordinates of contravariant vectors in column vectors. This means, by proposition \ref{transpDual}, we must put the coordinates of covariant vectors in row vectors. Indeed, let $v=a^i \vec{e}_i\in V$ and $\varphi = b_j\varphi^j \in V^*$, then
\[ \varphi(v) = a^ib_i = \begin{bmatrix}
a^1 & \hdots & a^n
\end{bmatrix}\begin{bmatrix}
b_1 \\ \vdots  \\ b_n
\end{bmatrix}. \]

We can view covariant vectors as functions that take a contravariant vector and produce a number, and we can view contravariant vectors as functions that take a covariant vector and produce a number. In general we may have linear functions that accept several co- and contravariant vectors and produce a number. By (TODO), such functions are tensor products of various co- and contravariant vectors. They would have multiple up- and down-indices. e.g\
\[ \vec{T} = \tensor{T}{^i_j_k^l^m}(\vec{e}_i\otimes \vec{e}^j\otimes \vec{e}^k\otimes \vec{e}_l \otimes \vec{e}_m) \]

Where $\tensor{T}{^i_j_k^l^m}$ are the coordinates w.r.t. the basis vectors $\vec{e}_i\otimes \vec{e}^j\otimes \vec{e}^k \otimes\vec{e}_l \otimes \vec{e}_m$.

For example, once the basis has been chosen, matrices map contravariant vectors to contravariant vectors. And contravariant vectors map covariant vectors to numbers, so by reverse currying a matrix is maps a contravariant and a covariant vector to a number.

For the indices of matrices we have taken the convention that the first index is for rows and the second for the columns. For a constant row index, the column index spells out a covariant vector, so the column index is down. Conversely, the row index is up. A matrix $A$ with components $(A)_{i,j}$ becomes
\[ \tensor{A}{^i_j}(\vec{e}_i\otimes \vec{e}^j). \]
This is consistent with the observation that the matrix sends a vector to a function on covectors, in other words is a function which accepts vectors in first place and covectors in second place.

In the expressions so far only repeated indices were present. Such repeated indices are called  \udef{bound indices} or \udef{dummy indices}. They may be replaced in the expression by other letters, so long as there is no clash. If an index is not repeated, it is a \udef{free index} and may not just be changed.

\subsection{``Tensors are objects that transform as tensors''}
Variants:
\begin{itemize}
\item ``$N$ arbitrary numbers are not the components of a vector'' (Peres p.65)
\end{itemize}


\section{Covectors}

\subsection{Multi-index notation}
Let $e_1,\ldots, e_n$ be a basis for a real vector space $V$. Let $\alpha^1,\ldots, \alpha^n$ be the dual basis for $V^*$. A \udef{multi-index}
\[ I = (i_1,\ldots,i_k)\]
is a $k$-tuple of numbers $\in (1,\ldots,n)$. We write
\[ \begin{cases}
e_I \defeq e_{i_1}\otimes\ldots\otimes e_{i_k}\\
\alpha^I \defeq \alpha^{i_1}\wedge\ldots \wedge\alpha^{i_k}
\end{cases}. \]
The covector $\alpha^I$ is completely determined by the values in $I$, the order only changes the sign. A multi-index $I = (i_1,\ldots,i_k)$ is \udef{ascending} if
\[ 1\leq i_1<\ldots<i_k\leq n. \]
\begin{proposition}
Let $I,J$ be ascending multi-indices of length $k$, then
\[ \alpha^I(e_J) = \begin{cases}
1 & I=J \\ 0& I\neq J
\end{cases}. \]
\end{proposition}
\begin{proposition}
The covectors $\alpha^I$, with $I$ an ascending multi-index of length $k$, form a basis of $A_k(V)$.
\end{proposition}
\begin{corollary}
If $\dim V=n$, then
\[ \dim A_k(V) = \begin{pmatrix}
n\\k
\end{pmatrix}. \]
\end{corollary}

\section{Symmetrisation and anti-symmetrisation of indices}

\[ T_{\{a_1\dots a_n\}} = \frac{1}{n!} \sum_{\sigma\in S_n} T_{a_{\sigma(1)} \dots a_{\sigma(n)}} \]

\[ T_{[a_1\dots a_n]} = \frac{1}{n!} \sum_{\sigma\in S_n} (\sgn \sigma)T_{a_{\sigma(1)} \dots a_{\sigma(n)}} \]
\section{Symbols}
\subsection{Kronecker delta}
\begin{definition}
The \udef{Kronecker delta} is defined by
\[ \delta_{ij} = \delta^i_j = \begin{cases}
1 & (i=j) \\
0 & (i \neq j)
\end{cases}.\]
\end{definition}
\subsection{Levi-Civita symbol}
\begin{definition}
The \udef{Levi-Civita symbol} is defined by
\[ \varepsilon_{a_{1}\ldots a_{n}} = \begin{cases}
+1 & \text{$(a_{1},\ldots, a_{n})$ is an even permutation of $(1,\ldots, n)$} \\
-1 & \text{$(a_{1},\ldots, a_{n})$ is an odd permutation of $(1,\ldots, n)$} \\
0 & \text{otherwise}
\end{cases}.\]
The indices may be placed up or down.
\end{definition}

\begin{lemma} \label{LeviCivitaProduct}
The Levi-Civita symbol is given by the explicit expression
\[ \varepsilon_{a_{1}\ldots a_{n}} = \prod_{1\leq i<j\leq n}\sgn(a_j-a_i).\]
\end{lemma}

\begin{proposition}
Working in $n$ dimensions, when all $i_1,\ldots i_n;j_1,\ldots, j_n$ take values in $\{ 1,\ldots, n \}$:
\begin{enumerate}
\item $\displaystyle \varepsilon_{i_1\ldots i_n}\varepsilon^{j_1\ldots j_n} = n!\delta^{j_1}_{[i_1}\ldots \delta^{j_n}_{i_n]} = \sum_{\sigma\in S_n} (-1)^{{\sgn}(\sigma)} \delta^{j_1}_{i_{\sigma(1)}} \dots \delta^{j_n}_{i_{\sigma(n)}}$
\item $\displaystyle \varepsilon _{i_{1}\dots i_{n}}\varepsilon ^{i_{1}\dots i_{n}}=n!$
\item $\displaystyle \varepsilon _{i_{1}\dots i_{k}~i_{k+1}\dots i_{n}}\varepsilon ^{i_{1}\dots i_{k}~j_{k+1}\dots j_{n}}=k!(n-k)!~\delta _{[i_{k+1}}^{j_{k+1}}\dots \delta _{i_{n}]}^{j_{n}}$.
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item Both sides of the equation are a sum over the same indices. We consider each term in the sum separately and show that the sums are equal term-by-term. We split the terms into two categories.
\begin{enumerate}
\item First consider the case that $j_1\ldots j_n$ is not a permutation of $(1,\ldots, n)$, i.e.\ a number is repeated. Then $\varepsilon_{i_1\ldots i_n}\varepsilon_{j_1\ldots j_n}$ is automatically zero. The right-hand side is definitely zero if the $i$s do not take the same values as the $j$s. If they do take the same values, there is a number that is repeated at least twice. For every term in the sum over permutations, there is another term with the repeated $i$s swapped, which also adds a minus due to the change of sign of the permutation. Hence the sum over permutations is zero.
\item Now assume that $j_1\ldots j_n$ is a permutation of $(1,\ldots, n)$. Then either the $i$s are also a permutation, or $\delta^{j_1}_{i_{\sigma(1)}} \dots \delta^{j_n}_{i_{\sigma(n)}}$ is always zero. The only possible non-zero term is with a $\sigma\in S_n$ such that $j_k = i_{\sigma(k)}$ for all $k$. If $\sgn(i_1,\ldots, i_n) = \sgn(j_1,\ldots, j_n)$, then $\sgn(\sigma)=1$ and both sides match. If $\sgn(i_1,\ldots, i_n) = -\sgn(j_1,\ldots, j_n)$, then $\sgn(\sigma)=-1$ and both sides again match. 
\end{enumerate}
So, in fact, we have shown something slightly stronger, namely 
\[ \varepsilon_{i_1\ldots i_n}\varepsilon_{j_1\ldots j_n} = n!\delta^{j_1}_{[i_1}\ldots \delta^{j_n}_{i_n]} \]
where there is no sum over indices.
\item The number of permutations of any $n$-element set number is exactly $n!$. Every permutation is either even or odd and $(+1)^2 = (-1)^2 = 1$. Non-permutations do not contribute to the sum.
\item The sum on the left only has terms where the $i$s and $j$s are permutations of $(1,\ldots, n)$. In each such term we can bring the indices with values $1-k$ to the first $k$ spots, each by a transposition. Because both Levi-Civita symbols have the same first $k$ indices, each will need the same number of transpositions and thus the sign does not change. Then by considering lemma \ref{LeviCivitaProduct} we see that we have obtained a product of cases 1. and 2. This yields the answer.

\end{enumerate}
\end{proof}
\begin{corollary}
In two dimensions, where all $i,j,m,n$ each take values in $\{1,2\}$,
\begin{enumerate}
\item $\varepsilon _{ij}\varepsilon ^{mn}={\delta _{i}}^{m}{\delta _{j}}^{n}-{\delta _{i}}^{n}{\delta _{j}}^{m}$
\item $\varepsilon _{ij}\varepsilon ^{in}={\delta _{j}}^{n}$
\item $\varepsilon _{ij}\varepsilon ^{ij}=2.$
\end{enumerate}
\end{corollary}
\begin{corollary}
In three dimensions, where all $i,j,k,m,n$ each take values in $\{1,2,3\}$,
\begin{enumerate}
\item $\varepsilon _{ijk}\varepsilon ^{imn}={\delta _{j}}^{m}{\delta _{k}}^{n}-{\delta _{j}}^{n}{\delta _{k}}^{m}$
\item $\varepsilon _{jmn}\varepsilon ^{imn}={\delta _{j}}^{i}$
\item $\varepsilon _{ijk}\varepsilon ^{ijk}=6.$
\end{enumerate}
\end{corollary}
\begin{proposition}
Working in 3 dimensions,
\begin{align*}
\varepsilon _{ijk}\varepsilon _{lmn}&={\begin{vmatrix}\delta _{il}&\delta _{im}&\delta _{in}\\\delta _{jl}&\delta _{jm}&\delta _{jn}\\\delta _{kl}&\delta _{km}&\delta _{kn}\\\end{vmatrix}}\\[6pt]&=\delta _{il}\left(\delta _{jm}\delta _{kn}-\delta _{jn}\delta _{km}\right)-\delta _{im}\left(\delta _{jl}\delta _{kn}-\delta _{jn}\delta _{kl}\right)+\delta _{in}\left(\delta _{jl}\delta _{km}-\delta _{jm}\delta _{kl}\right).
\end{align*}
This can directly be generalised to $n$ dimensions.
\end{proposition}

\section{Writing matrix operations using using tensor notation}
A matrix $A$ with components $(A)_{i,j}$ becomes
\[ \tensor{A}{^i_j}(\vec{e}_i\otimes \vec{e}^j). \]
\subsection{Trace}
The trace of $\tensor{A}{^i_j}$ is $\tensor{A}{^i_i}$.
\subsection{Matrix multiplication}
\[ \tensor{(AB)}{^{i}_{k}}=\tensor{A}{^{i}_{j}}\tensor{B}{^{j}_{k}} \]
which in particular for matrix-vector multiplication becomes
\[ (Av)^i = \tensor{A}{^i_j} v^j. \]
\subsection{Transpose}
The transpose of $\tensor{A}{^i_j}$ is $\tensor{(A^\transp)}{^j_i}$.

Or: $(A^\transp)_{ab} = A_{ba}$ and $\tensor{(A^\transp)}{_i^j} = \tensor{A}{^j_i}$?
\subsection{Determinant}
\begin{align*}
\det(A) &= \varepsilon^{j_1\ldots j_n}\tensor{A}{^{1}_{j_1}}\ldots \tensor{A}{^{n}_{j_n}} \\
&= \frac{1}{n!}\varepsilon_{i_1\ldots i_n}\varepsilon^{j_1\ldots j_n}\tensor{A}{^{i_1}_{j_1}}\ldots \tensor{A}{^{i_n}_{j_n}}
\end{align*}

\chapter{Ordered vector spaces}
TODO link ordered groups.
\section{Preordered vector spaces}
\begin{definition}
Let $\sSet{\R, V, +}$ be a real vector space and $\precsim$ a preorder on the set $V$. Then $\precsim$ is a \udef{vector preorder} if it is compatible with the vector space structure as follows: $\forall x,y,z\in V, \lambda\in\R$
\begin{enumerate}
\item $x\precsim y$ implies $x+z \precsim y+z$;
\item if $\lambda\geq 0$, then $x \precsim y$ implies $\lambda x \precsim \lambda y$.
\end{enumerate}
We call $(\R, V, +, \precsim)$ a \udef{preordered vector space}.

\begin{itemize}
\item If $\precsim$ is a partial order, we call $(\R, V, +, \precsim)$ a \udef{partially ordered vector space} or simply a \udef{ordered vector space}.
\item If $\sSet{V, \precsim}$ is a lattice, we call $(\R, V, +, \precsim)$ a \udef{vector lattice} or a \udef{Riesz space}.
\end{itemize}
\end{definition}

Note that $\mu \leq \nu \in \R$ does not in general implies $\mu v \precsim \nu v$ for $v\in V$. For this to hold, we need $0\precsim v$.

\begin{example}
\begin{itemize}
\item The finite-dimensional vector spaces $\R^n$ with coordinate-wise addition, scalar multiplication and order are Riesz spaces.
\item The finite-dimensional vector spaces $\R^n$ with coordinate-wise addition, scalar multiplication and lexicographical order are Riesz spaces.
\item Let $X$ be a set. The set $(X\to \R)$ is a real vector space with point-wise addition and scalar multiplication. If the order is also defined point-wise, i.e.\ $f \leq g$ iff $\forall x\in X: f(x) \leq g(x)$, then $(X\to \R)$ is a Riesz space.
\end{itemize}
\end{example}

\begin{lemma} \label{positiveConeOrderCharacterisation}
Let $\sSet{\R, V, +}$ be a real vector space and $\precsim$ a preorder on the set $V$. The compatibility of the order can equivalently be expressed by:
$\forall x,y,z\in V, \lambda\in\R$
\[ \begin{cases}
\text{$x \precsim y$ implies $x+z \precsim y+z$;} \\
\text{if $\lambda\geq 0$ and $0 \precsim x$, then $0 \precsim \lambda x$.}
\end{cases} \]
\end{lemma}

\begin{lemma} \label{elementaryVectorPreorderManipulations}
Let $V$ be a preordered vector space. For all $v,w \in V$ we have
\[ v \precsim w \;\iff\; 0 \precsim  w - v \;\iff\; -w \precsim -v.  \]
\end{lemma}
\begin{proof}
We get the implications
\[ v \precsim w \implies 0 \precsim  w - v \implies -w \precsim -v \implies v-w \precsim 0 \implies v\precsim w \]
by subsequently adding $-v, -w, v,w$ to both sides by compatibility of the order.
\end{proof}
\begin{corollary} \label{orderPreservationReversalScalarMultiplication}
Let $V$ be a preordered vector space and $\alpha \in \R\setminus\{0\}$. Then for all $v,w\in V$
\[ v \precsim w \quad \iff \quad \begin{cases}
\alpha v \precsim \alpha w & (0 < \alpha) \\
\alpha v \succsim \alpha w & (\alpha > 0)
\end{cases}. \] 
\end{corollary}

\begin{lemma} \label{additionVectorInequalities}
Let $V$ be a preordered vector space. For all $v,w, x, y \in V$ we have
\[ \big(v \precsim w\big) \land \big(x \precsim y\big) \;\implies\; v+ x \precsim w+y \;\implies\; \Big( w \precsim v \implies x\precsim y\Big). \]
If $\precsim$ is connex, then
\[ v+ x \precsim w+y \;\implies\; \big(v\precsim w\big)\lor\big( x\precsim y\big). \]
\end{lemma}
\begin{proof}
We calculate $v + x \precsim w + x \precsim w+y$ for the first implication.

For the second implication, assume $w\precsim v$, so $w-v \precsim 0$. We can then calculate
\[ v+ x \precsim w+y \implies x\precsim y + w - v \precsim y + 0 = y. \]

Now let $\precsim$ be connex and assume $v+ x \precsim w+y$. By connexity, we either have $v\precsim w$ or $w\precsim v$. By the previous part, $w\precsim v$ implies $x\precsim y$. So either $v\precsim w$ or $x\precsim y$.
\end{proof}
\begin{corollary} \label{intervalSumInclusion}
Let $V$ be a preordered vector space, $u_1\precsim v_1$ and $u_2 \precsim v_2$. Then $\interval{u_1, v_1} + \interval{u_2, v_2} \subseteq \interval{u_1 + u_2, v_1+ v_2}$.
\end{corollary}
\begin{proof}
Take $a+ b \in \interval{u_1, v_1} + \interval{u_2, v_2}$. Since $u_1 \precsim a$ and $u_2 \precsim b$, we have $u_1 + u_2 \precsim a+b$. Similarly $a+b \precsim v_1 + v_2$, so $a+b \in \interval{u_1 + u_2, v_1+ v_2}$.
\end{proof}

\begin{lemma}
Let $X$ be a topological space. The spaces
\begin{enumerate}
\item $\cont(X,\R)$;
\item $\cont_0(X,\R)$;
\item $\cont_c(X,\R)$; and
\item $\cont_b(X,\R)$
\end{enumerate}
with point-wise operations are Riesz spaces.
\end{lemma}
\begin{proof}
In all these cases the join and meet of $f,g$ are given by
\begin{align*}
f \vee g &= \frac{1}{2}(f+g)+ \frac{1}{2}|f-g| \\
f \wedge g &= \frac{1}{2}(f+g) - \frac{1}{2}|f-g|.
\end{align*}
So the join and meet are still continuous and have the same properties as $f,g$.
\end{proof}

\begin{lemma} \label{vectorPreorderDual}
Let $\sSet{V,\precsim}$ be a preordered vector space. Then $\sSet{V,\precsim^\transp}$ is also a preordered vector space.
\end{lemma}
\begin{proof}
We show the two conditions for $\precsim^\transp$ to be a vector preorder. Take $x,y,z\in V$ such that $x\precsim^\transp y$ and $\lambda > 0$. Then $y \precsim x$ and thus $y + z \precsim x+z$, so $x+z \precsim^\transp y+z$.

Similarly $y\precsim x$ implies $\lambda y \precsim \lambda x$, so $\lambda x \precsim^\transp \lambda y$.
\end{proof}

\subsection{The positive cone}
\begin{definition}
Let $V$ be a preordered vector space. The subset
\[ V^+ \defeq \setbuilder{v\in V}{0 \precsim v} \]
is called the \udef{positive cone} of $V$. The elements of the positive cone $V^+$ are called the \udef{positive elements} of $V$.

Let $A\subseteq V$ be a subset. We define $A^+ \defeq A\cap V^+$.
\end{definition}
That the positive cone is in fact a cone follows from \ref{positiveConeOrderCharacterisation}
\begin{proposition} \label{positiveCone}
Let $V$ be a vector space.
\begin{enumerate}
\item A vector preorder on $V$ is uniquely determined by its positive cone:
\[ x \precsim y \quad\iff\quad y-x \in V^+. \]
\item The positive cone of a vector preorder is pointed and convex.
\item Any pointed convex cone in $V$ determines a (unique) vector preorder.
\item A vector preorder is a partial order \textup{if and only if} the positive cone is salient.
\end{enumerate}
\end{proposition}
Convexity is equivalent to closure under addition (see \ref{convexityAdditiveClosure})
\begin{proof}
(1) This is just \ref{elementaryVectorPreorderManipulations}.

(2) $V^+$  is pointed by reflexivity: $0\precsim 0$. It is closed under addition by \ref{additionVectorInequalities}.

(3) Compatibility with addition is immediate from the definition of the order. Compatibility with scalar multiplication is due to it being a cone (see \ref{positiveConeOrderCharacterisation}). Reflexivity is equivalent with pointedness. Finally transitivity follows from closure under addition:
\begin{align*}
 \begin{cases}
x\precsim y \\ y\precsim x
\end{cases} &\iff \quad \begin{cases}
0 \precsim y -x \\ 0 \precsim z-y
\end{cases} \iff\quad \begin{cases}
y-x \in V^+ \\ z-y \in V^+
\end{cases} \\
&\implies (z-y)+(y-x) = z-x \in V^+ \iff x \precsim z. 
\end{align*}

(4) We have $x\precsim y$ and $y\precsim x$ iff $(y-x) \in V^+$ and $-(y-x) \in V^+$. Thus both salience and anti-symmetry are equivalent to this situation implying $x-y = 0$.
\end{proof}

\begin{lemma} \label{scalarInequalityImpliesVectorInequality}
Let $V$ be a preordered vector space, $v\in V^+$ and $\lambda \leq \mu\in \R$. Then $\lambda v \precsim \mu v$.
\end{lemma}
\begin{proof}
We have $\mu-\lambda \geq 0$, so $0\precsim v$ implies $0 = (\mu-\lambda)\cdot 0 \precsim (\mu-\lambda)v$ and so $\lambda v \precsim \mu v$.
\end{proof}
\begin{corollary} \label{scalarVectorInequalityMultiplication}
Let $V$ be a preordered vector space, $v,w\in V$ and $\alpha, \beta\in \F$ such that $0\precsim v\precsim w$ and $0\leq \alpha \leq \beta$. Then $\alpha v \precsim \beta w$.
\end{corollary}
\begin{proof}
We have $\alpha v \precsim \alpha w \precsim \beta w$.
\end{proof}
\begin{corollary} \label{intervalScalarMultiplicationInclusion}
Let $V$ be a preordered vector space, $0\precsim v \precsim w\in V$ and $0\leq \alpha \leq \beta \in \R$. Then $\interval{\alpha, \beta} \cdot \interval{v, w} \subseteq \interval{\alpha v, \beta w}$.
\end{corollary}
\begin{proof}
Take $\mu x\in \interval{\alpha, \beta} \cdot \interval{v, w}$. Then $\alpha \leq \mu$ and $v \precsim x$, so $\alpha v \precsim \mu x$. Similarly $\mu x \precsim \beta w$. Thus $\mu x \in \interval{\alpha v, \beta w}$.
\end{proof}

TODO following corollary of previous.
\begin{lemma} \label{scalarMultiplicationInequalities}
Let $V$ be a preordered vector space, $v\in V^+$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha \geq 1$, then $\alpha v \succsim v$;
\item if $\alpha \leq 1$, then $\alpha v \precsim v$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $v\succsim 0$ and $(\alpha-1) \geq 0$, so $(\alpha-1)v \succsim 0$ and $\alpha v \succsim v$.

(2) We have $v\succsim 0$ and $(\alpha-1) \leq 0$, so $(\alpha-1)v \precsim 0$ and $\alpha v \precsim v$.
\end{proof}

\begin{lemma} \label{positiveVectorAddition}
Let $V$ be a preordered vector space, $v\in V^+$ and $x\in V$. Then
\begin{enumerate}
\item $x \precsim x+v$;
\item $x - v \precsim x$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $x\precsim x$ and $0\precsim v$, so $x = (x+0)\precsim x+v$ by \ref{additionVectorInequalities}.

(2) Immediate from (1).
\end{proof}

\begin{lemma} \label{positiveElementsSumToZeroImpliesZero}
Let $V$ be a partially ordered vector space and $v,w\in V^+$. If $v+w = 0$, then $v = 0$ and $w = 0$.
\end{lemma}
\begin{proof}
We have $v \leq v$ and $0\leq w$, so $0\leq v\leq v+w = 0$. Thus $v = 0$. Similarly $w = 0$.
\end{proof}

\subsection{Archimedean spaces}
\begin{definition}
A partially ordered vector space $V$ is called \udef{Archimedean} if $\bigwedge_{\epsilon > 0}\epsilon v = 0$ for all $v\in V^+$.
\end{definition}

\begin{lemma} \label{ArchimedeanEquivalents}
Let $\sSet{V,\leq}$ be a partially ordered vector space. Then the following are equivalent:
\begin{enumerate}
\item $V$ is Archimedean;
\item $\bigwedge S\cdot v = 0$ for all $v\in V^+$ and $S\subseteq \R^+$ such that $\bigwedge S = 0$.
\end{enumerate}
\end{lemma}
\begin{proof}
$(1) \Rightarrow (2)$ Clearly $(S\cdot v)^\geq \supseteq \big( \interval[o]{0,+\infty}\cdot v\big)^\geq$. For the other inclusion, take $w\in (S\cdot v)^\geq$ and an arbitrary $\epsilon > 0$. Then there exists $\mu \in S$ such that $\mu \leq \epsilon$, so $w \leq \mu v \leq \epsilon v$ by \ref{scalarInequalityImpliesVectorInequality}. Since $\epsilon$ was taken arbitrarily, we have $w\in \big( \interval[o]{0,+\infty}\cdot v\big)^\geq$.

$(2) \Rightarrow (1)$ Immediate, by setting $S = \interval[o]{0,+\infty}$.
\end{proof}

\begin{example}
The space $\R^2$ with the lexicographic ordering $\leq_l$ is not Archimedean. Indeed $(0,1)$ is a lower bound of $n^{-1}(1,1) = (n^{-1}, n^{-1})$, but $(0,1) \neq \vec{0}$.
\end{example}

\subsection{Upsets, downsets and bounds}
\begin{lemma} \label{sumUpperLowerBounds}
Let $V$ be a preordered vector space, $A, B\subseteq V$ subsets and $v\in V$. Then
\begin{enumerate}
\item $A^\precsim + B^\precsim \subseteq (A + B)^\precsim$ and $A^\succsim + B^\succsim \subseteq (A + B)^\succsim$;
\item $(A+v)^\precsim = A^\precsim + \{v\}^\precsim = A^\precsim + v$ and $(A+v)^\succsim = A^\succsim + \{v\}^\succsim = A^\succsim + v$.
\end{enumerate}
\end{lemma}
TODO: point (2) also follows immediately from the observation that $(-)+v$ is an order similarity.
\begin{proof}
(1) Take $u\in A^\precsim$ and $w\in B^\precsim$. Take arbitrary $a+b\in A+B$. Then $a\precsim u$ and $b\precsim w$, so $a+b \precsim u+w$ by \ref{additionVectorInequalities}. Since $a+b$ was taken arbitrarily, we have $u+w \in (A+B)^\precsim$. The other inclusion is dual.

(2) First we show that $A^\precsim + \{v\}^\precsim = A^\precsim + v$. Indeed $A^\precsim + \{v\}^\precsim \supseteq A^\precsim + v$ is clear. Conversely, take $u + w \in A^\precsim + \{v\}^\precsim$. Then $u+ w = u + (w-v) + v$. Since $v\precsim w$, we have $0\precsim w-v$ and thus $u + (w-v)\in A^\precsim$ by upwards closure. This shows $A^\precsim + \{v\}^\precsim \subseteq A^\precsim + v$.

From (1), we have $A^\precsim+ \{v\}^\precsim \subseteq (A+ v)^\precsim$. Similarly,
\[ (A+v)^\precsim - v = (A+v)^\precsim + \{-v\}^\precsim \subseteq (A+v - v)^\precsim = A^\precsim, \]
so $(A+v)^\precsim \subseteq A^\precsim + v = A^\precsim+ \{v\}^\precsim$.
\end{proof}
\begin{corollary} \label{vectorSumOrderHomomorphism}
Let $V$ be a preordered vector space, $S\subseteq V$ a subset, $v\in V$ and $\alpha\in \R$. Then $\sup(S+v) = \sup(S)+v$ and $\inf(S+v) = \inf(S)+v$.
\end{corollary}

\begin{lemma} \label{sumUpDownsets}
Let $V$ be a preordered vector space, $A, B\subseteq V$ subsets and $v\in V$. Then
\begin{enumerate}
\item $\upset A + \upset B \subseteq \upset (A + B)$ and $\downset A + \downset B \subseteq \downset(A + B)$;
\item $\upset (A+v) = \upset A + \upset\{v\} = \upset A + v$ and $\downset (A+v) = \downset A + \downset \{v\} = \downset A + v$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Take $u\in \upset A$ and $w\in \upset B$. Then there exist $a\in A$ and $b\in B$ such that $a\precsim u$ and $b\precsim v$.

So $a+b \precsim u+w$ by \ref{additionVectorInequalities}. Since $a+b \in A+B$, we have $u+w\in \upset(A+B)$.

The other point is dual.

(2) First we show that $\upset A + \upset\{v\} = \upset A + v$. Indeed $\upset A + \upset \{v\} \supseteq \upset A + v$ is clear. Conversely, take $u + w \in \upset A + \upset\{v\}$. Then $u+ w = u + (w-v) + v$. Since $v\precsim w$, we have $0\precsim w-v$ and thus $u + (w-v)\in \upset A$ by upwards closure. This shows that $\upset A + \upset \{v\} \subseteq \upset A + v$.

From (1), we have $\upset A + \upset\{v\} \subseteq \upset(A+ v)$. Similarly,
\[ \upset (A+v) - v = \upset (A+v) + \upset\{-v\} \subseteq \upset(A+v - v) = \upset A, \]
so $\upset (A+v) \subseteq \upset A + v = \upset A + \upset\{v\}$.
\end{proof}



\begin{lemma} \label{scalarMultipleUpperLowerBounds}
Let $V$ be a preordered vector space, $A\subseteq V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha > 0$, then $(\alpha A)^\precsim = \alpha A^\precsim$ and $(\alpha A)^\succsim = \alpha A^\succsim$;
\item if $\alpha < 0$, then $(\alpha A)^\precsim = \alpha A^\succsim$ and $(\alpha A)^\succsim = \alpha A^\precsim$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) For all $u\in V$ and $x\in A$, we have $\alpha x \precsim u$ iff $x\precsim \alpha^{-1}u$. Since this is true for arbitrary $x\in A$, we have $u\in (\alpha A)^\precsim$ iff $\alpha^{-1}u\in A^\precsim$. This is equivalent to $u\in \alpha A^\precsim$. The other point is dual.

(2) For all $u\in V$ and $x\in A$, we have $\alpha x \precsim u$ iff $x\succsim \alpha^{-1}u$ by \ref{orderPreservationReversalScalarMultiplication}. Since this is true for arbitrary $x\in A$, we have $u\in (\alpha A)^\precsim$ iff $\alpha^{-1}u\in A^\succsim$. This is equivalent to $u\in \alpha A^\succsim$. The other point is dual.
\end{proof}
\begin{corollary} \label{scalarMultipleOrderHomomorphism}
Let $V$ be a preordered vector space, $S\subseteq V$ a subset, $v\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha > 0$, then $\sup(\alpha S) = \alpha \sup(S)$ and $\inf(\alpha S) = \alpha \inf(S)$;
\item if $\alpha < 0$, then $\sup(\alpha S) = \alpha \inf(S)$ and $\inf(\alpha S) = \alpha \sup(S)$.
\end{enumerate}
\end{corollary}

\begin{lemma} \label{scalarMultipleSetsUpperLowerBounds}
Let $V$ be a preordered vector space, $A\subseteq V^+$, $S\subseteq \R$ and $v\in V^+$. Then
\begin{enumerate}
\item if $S \subseteq \R^+$, then $S^\leq \cdot A^\precsim \subseteq (S\cdot A)^\precsim$ and $S^\geq\cdot A^\succsim \subseteq (S\cdot A)^\succsim$;
\item if $S \subseteq -\R^+$, then $S^\leq \cdot A^\succsim \subseteq (S\cdot A)^\precsim$ and $S^\geq \cdot A^\precsim \subseteq (S\cdot A)^\succsim$.
\end{enumerate}
Note that, even for $S\subseteq \R^+$, we do not have $(Sv)^\precsim = S^\leq\cdot \{v\}^\precsim = S^\leq \cdot v$ or similar equations.
\end{lemma}
Note that $A$ needs to lie in the positive cone and $v$ needs to be positive.
\begin{proof}
(1) Take $\mu \cdot u \in S^\precsim \cdot A^\precsim$. Take arbitrary $\nu\in S$ and $a\in A$. Then $0\leq \nu \leq \mu$ and $0 \precsim a\precsim u$, so $\nu a \precsim \mu u$ by \ref{scalarVectorInequalityMultiplication}. Since $\nu$ and $a$ were taken arbitrarily, we have $\mu u \in (S\cdot A)^\precsim$.

The other point is dual.

(2) We calculate, using point (1) and \ref{scalarMultipleUpperLowerBounds}, setting $\alpha = -1$,
\begin{align*}
S^\leq \cdot A^\succsim &= \big((-1)\cdot -S\big)^\leq \cdot A^\succsim \\
&= -(-S)^\geq \cdot A^\succsim \\
&\subseteq - \big((-S)\cdot A\big)^\succsim \\
&= \big((-1)(-S)\cdot A\big)^\precsim = (S\cdot A)^\precsim.
\end{align*}
The other point is dual.
\end{proof}
\begin{corollary}
Let $V$ be a preordered vector space, $A\subseteq V^+$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha > 0$, then $(\alpha A)^\precsim = \{\alpha\}^\leq\cdot A^\precsim = \alpha A^\precsim$ and $(\alpha A)^\succsim = \{\alpha\}^\geq\cdot A^\succsim = \alpha A^\succsim$;
\item if $\alpha < 0$, then $(\alpha A)^\precsim = \{\alpha\}^\leq\cdot A^\succsim = \alpha A^\succsim$ and $(\alpha A)^\succsim = \{\alpha\}^\geq \cdot A^\precsim = \alpha A^\precsim$.
\end{enumerate}
\end{corollary}
Compare with \ref{scalarMultipleUpperLowerBounds}, which still holds if $A \nsubseteq V^+$.
\begin{proof}
(1) By the lemma (and reflexivity), we have $(\alpha A)^\precsim \supseteq \{\alpha\}^\leq\cdot A^\precsim \supseteq \alpha A^\precsim$. The equality of the first and last sets follows from \ref{scalarMultipleUpperLowerBounds}.

The other point is dual.

(2) By the lemma (and reflexivity), we have $(\alpha A)^\precsim \subseteq \{\alpha\}^\leq\cdot A^\succsim \subseteq \alpha A^\succsim$. The equality of the first and last sets follows from \ref{scalarMultipleUpperLowerBounds}.

The other point is dual.
\end{proof}

\begin{lemma} \label{scalarMultipleUpDownSets}
Let $V$ be a preordered vector space, $A, B\subseteq V$ subsets, $v\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha > 0$, then $\upset (\alpha A) = \alpha \upset A$ and $\downset(\alpha A) = \alpha \downset A$;
\item if $\alpha < 0$, then $\upset(\alpha A) = \alpha \downset A$ and $\downset(\alpha A) = \alpha \upset A$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) First take $u\in \upset (\alpha A)$. Then there exists $a\in A$ such that $\alpha a \precsim u$. Then $a\precsim \alpha^{-1}u$, so $u = \alpha (\alpha^{-1}u) \in \alpha \upset A$.

Conversely, take $u\in \alpha \upset A$. Then there exists $a\in A$ and $u'\in V$ such that $u = \alpha u'$ and $a\precsim u'$. Now $\alpha a \precsim \alpha u' = u$, so $u \in \upset (\alpha A)$.

The other point is dual.

(2) First take $u\in \upset (\alpha A)$. Then there exists $a\in A$ such that $\alpha a \precsim u$. Then $a\succsim \alpha^{-1}u$, so $u = \alpha (\alpha^{-1}u) \in \alpha \downset A$.

Conversely, take $u\in \alpha \downset A$. Then there exists $a\in A$ and $u'\in V$ such that $u = \alpha u'$ and $a\succsim u'$. Now $\alpha a \precsim \alpha u' = u$, so $u \in \upset (\alpha A)$.

The other point is dual.
\end{proof}

\begin{lemma}
Let $V$ be a preordered vector space, $A\subseteq V^+$, $S\subseteq \R$ and $v\in V^+$. Then
\begin{enumerate}
\item if $S \subseteq \R^+$, then $\upset S \cdot \upset A \subseteq \upset (S\cdot A)$ and $\downset S\cdot \downset A \subseteq \downset(S\cdot A)$;
\item if $S \subseteq -\R^+$, then $\upset S \cdot \downset A \subseteq \upset(S\cdot A)$ and $\downset S \cdot \upset A \subseteq \downset(S\cdot A)^\succsim$.
\end{enumerate}
Note that, even for $S\subseteq \R^+$, we do not have $\upset (Sv) = \upset S\cdot \upset \{v\} = S^\leq \cdot v$ or similar equations.
\end{lemma}
Note that $A$ needs to lie in the positive cone and $v$ needs to be positive.
\begin{proof}
(1) Take $\mu \cdot u \in \upset S \cdot \upset A$. Then there exist $\nu\in S$ and $a\in A$ such that $\nu \leq \mu$ and $a \precsim u$. Since $0\precsim a$ and $0\leq \nu$, we have $\nu a \precsim \mu u$  by \ref{scalarVectorInequalityMultiplication}.
Thus $\mu u \in \upset (S\cdot A)$.

The other point is dual.

(2) We calculate, using point (1) and \ref{scalarMultipleUpDownSets}, setting $\alpha = -1$,
\begin{align*}
\upset S \cdot \downset A &= \upset\big((-1)\cdot -S\big) \cdot \downset A \\
&= -\downset(-S) \cdot \downset A \\
&\subseteq - \downset\big((-S)\cdot A\big) \\
&= \upset\big((-1)(-S)\cdot A\big) = \upset (S\cdot A).
\end{align*}
The other point is dual.
\end{proof}
\begin{corollary} \label{scalarMultipleUpDownSetPositiveSet}
Let $V$ be a preordered vector space, $A\subseteq V^+$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha > 0$, then $\upset (\alpha A) = \upset\{\alpha\}\cdot \upset A = \alpha \upset A$ and $\downset(\alpha A) = \downset\{\alpha\}\cdot \downset A = \alpha \downset A$;
\item if $\alpha < 0$, then $\upset (\alpha A) = \upset\{\alpha\}\cdot \downset A = \alpha \downset A$ and $\downset(\alpha A) = \downset\{\alpha\} \cdot \upset A = \alpha \upset A$.
\end{enumerate}
\end{corollary}
Compare with \ref{scalarMultipleUpDownSets}, which still holds if $A \nsubseteq V^+$.
\begin{proof}
(1) By the lemma (and reflexivity), we have $\upset(\alpha A) \supseteq \upset\{\alpha\}\cdot \upset A \supseteq \alpha \upset A$. The equality of the first and last sets follows from \ref{scalarMultipleUpDownSets}.

The other point is dual.

(2) By the lemma (and reflexivity), we have $\upset(\alpha A) \subseteq \upset\{\alpha\}\cdot \downset A \subseteq \alpha \downset A$. The equality of the first and last sets follows from \ref{scalarMultipleUpDownSets}.

The other point is dual.
\end{proof}


\subsubsection{Extrema in vector posets}
\begin{lemma} \label{extremaVectorSum}
Let $V$ be a vector poset and $A,B\subseteq V$. Then
\begin{enumerate}
\item if $\bigvee A$ and $\bigvee B$ exist, then $\bigvee(A+B)$ exists and $\bigvee(A+B) = \bigvee A + \bigvee B$;
\item if $\bigwedge A$ and $\bigwedge B$ exist, then $\bigwedge(A+B)$ exists and $\bigwedge(A+B) = \bigwedge A + \bigwedge B$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Set $x \defeq \bigvee A$ and $y \defeq \bigvee B$. We have that $x+y$ is an upper bound of $A+B$ by \ref{sumUpperLowerBounds}. To show that $x+y$ is the least upper bound, let $v$ be some other upper bound of $A+B$. Take arbitrary $a\in A$ and $b\in B$. Then $a+b \leq v$, so $b\leq v-a$. Thus $v-a$ is an upper bound of $B$ and so $y \leq v-a$. Now $a\leq v-y$, so $v-y$ is an upper bound of $A$, meaning $x \leq v-y$. This implies $x+y \leq v$ and thus that $x+y$ is the least upper bound.

(2) Follows from (1) by duality, see \ref{vectorPreorderDual}.
\end{proof}

\begin{lemma} \label{extremaScalarProduct}
Let $V$ be an Archimedean vector poset, $A\subseteq V^+$ and $S\subseteq \R^+$. Then
\begin{enumerate}
\item if $\bigvee A$ and $\bigvee S$ exist, then $\bigvee(S\cdot A)$ exists and $\bigvee(S\cdot A) = \bigvee S\cdot \bigvee A$;
\item if $\bigwedge A$ and $\bigwedge S$ exist, then $\bigwedge(S\cdot A)$ exists and $\bigwedge(S\cdot A) = \bigwedge S\cdot \bigwedge A$;
\end{enumerate}
\end{lemma}
In fact any of these points is equivalent to $V$ being Archimedean.
\begin{proof}
(1) Set $\lambda \defeq \bigvee S$ and $x \defeq \bigvee A$. First suppose $\lambda = 0$. Then $S = \{0\}$, so $S\cdot A = \{0\}$ and $0 = \bigvee(S\cdot A) = 0\cdot \bigvee A = \bigvee S \cdot \bigvee A$.

Now suppose $\lambda \neq 0$. Then, WLOG, we may assume $0\notin S$. We have that $\lambda x$ is an upper bound of $S\cdot A$ by \ref{scalarMultipleSetsUpperLowerBounds}.

To show that $\lambda x$ is the least upper bound, let $v$ be some other upper bound of $S\cdot A$. Take arbitrary $\mu\in S$ and $a\in A$, then $\mu a \leq v$, so $a \leq \mu^{-1}v$. Since $a\in A$ was taken arbitrarily, we have that $\mu^{-1}v$ is an upper bound of $A$ and thus $x \leq \mu^{-1}v$, so $\mu x\leq v$.

Now we have $\lambda x - v \leq \lambda x - \mu x = (\lambda - \mu) x$. Since $\mu$ was taken arbitrarily, we have that $\lambda x - v$ is a lower bound of $(\lambda - S)x$. Since $V$ is Archimedean and $\bigwedge (\lambda - S) = \lambda + \bigwedge (-S) = \lambda - \bigvee S = \lambda - \lambda = 0$, we have $\bigwedge (\lambda - S)x = 0$ by \ref{ArchimedeanEquivalents}. Thus $\lambda x - v \leq 0$ and so $\lambda x \leq v$. This implies that $\lambda x$ is the least upper bound.

(2) First suppose $0\in S$. Then $\bigvee S = 0$, so $\bigvee S \cdot \bigvee A = 0$. Also $0\in S\cdot A$, which implies that $0 = \bigvee S\cdot A$ (since $S\cdot A \subseteq V^+$).

Now suppose $0\notin S$. Set $\lambda \defeq \bigvee S$ and $x\defeq \bigvee A$.

We have that $\lambda x$ is a lower bound of $S\cdot A$ by \ref{scalarMultipleSetsUpperLowerBounds}.

To show that $\lambda x$ is the greatest lower bound, let $v$ be some other lower bound of $S\cdot A$. Take arbitrary $\mu\in S$ and $a\in A$, then $v \leq \mu a$, so $\mu^{-1}v \leq a$. Since $a\in A$ was taken arbitrarily, we have that $\mu^{-1}v$ is a lower bound of $A$ and thus $\mu^{-1}v \leq x$, so $v \leq \mu x$.

Now we have $v - \lambda x \leq \mu x - \lambda x = (\mu - \lambda) x$. Since $\mu$ was taken arbitrarily, we have that $v - \lambda x$ is a lower bound of $(S - \lambda)x$. Since $V$ is Archimedean and $\bigwedge (S - \lambda) = \lambda - \lambda = 0$, we have $\bigwedge (S - \lambda)x = 0$ by \ref{ArchimedeanEquivalents}. Thus $v - \lambda x \leq 0$ and so $v \leq \lambda x$. This implies that $\lambda x$ is the greatest lower bound.
\end{proof}


\subsection{Riesz decomposition property}
\begin{definition}
Let $\sSet{V, \precsim}$ be an ordered vector space. We say that $V$ has the \udef{Riesz decomposition property} if for all $u_1,v_1,u_2,v_2\in V$ such that $u_1\precsim v_1$ and $u_2\precsim v_2$, we have $\interval{u_1,v_1} + \interval{u_2,v_2} = \interval{u_1+u_2,v_1+v_2}$.
\end{definition}

For unbounded intervals, the analogous decomposition property is $\upset\{u\} + \upset\{v\} = \upset\{u+v\}$ and $\downset\{u\} + \downset\{v\} = \downset\{u+v\}$. This is always true, by \ref{sumUpDownsets}.

\begin{lemma} \label{RieszDecompositionLemma}
Let $\sSet{V, \precsim}$ be an ordered vector space. Then the following are equivalent:
\begin{enumerate}
\item $V$ has the Riesz decomposition property;
\item $\interval{u_1,v_1} + \interval{u_2,v_2} \supseteq \interval{u_1+u_2,v_1+v_2}$ for all $u_1,v_1,u_2,v_2\in V$ such that $u_1\precsim v_1$ and $u_2\precsim v_2$;
\item $\interval{0,v_1} + \interval{0,v_2} \supseteq \interval{0,v_1+v_2}$ for all $v_1,v_2\in V^+$.
\end{enumerate}
\end{lemma}
\begin{proof}
The direction $(1) \Rightarrow (3)$ is obvious.

$(3) \Rightarrow (2)$ Suppose $z\in \interval{u_1+u_2,v_1+v_2}$. Then $u_1+u_2 \precsim z$ and $z\precsim v_1+v_2$, so $0\precsim z - (u_1+u_2) \precsim (v_1+v_2) - (u_1+u_2)$. By assumption, this means that $z - (u_1+u_2)\in \interval{0, v_1 - u_1} + \interval{v_2 - u_2}$, so
\[ z \in u_1 + \interval{0, v_1 - u_1} + u_2 + \interval{v_2 - u_2} = \interval{u_1,v_1} + \interval{u_2,v_2}. \]

$(2) \Rightarrow (1)$ It is enough to prove that the opposite inclusion always holds. Indeed, take $z_1 + z_2 \in \interval{u_1,v_1} + \interval{u_2,v_2}$, so $u_1 \precsim z_1 \precsim v_1$ and $u_2 \precsim z_2 \precsim v_2$. Then $u_1 + u_2 \precsim z_1 + z_2 \precsim v_1 + v_2$ by \ref{additionVectorInequalities}. This implies $z_1+z_2 \in \interval{u_1+u_2,v_1+v_2}$.
\end{proof}

\subsection{Functions on ordered vector spaces}
\subsubsection{Positive functions}
\begin{definition}
Let $V,W$ be preordered vector spaces and $f: V\to W$ a function. Then $f$ is called \udef{positive} if $f^\imf(V^+) \subseteq W^+$.
\end{definition}

\begin{lemma} \label{positiveLinearFunctionIsotone}
Let $V,W$ be preordered vector spaces and $f: V\to W$ a linear function. Then $f$ is positive \textup{if and only if} $f$ is order-preserving.
\end{lemma}
\begin{proof}
First assume $f$ is order-preserving and take $v\in V^+$. Then $0\precsim v$, so $0=f(0) \precsim f(v)$ and thus $f(v)\in W^+$. Since $v\in V^+$ was taken arbitrarily, we have $f^\imf(V^+) \subseteq W^+$.

Now assume $f$ is positive and take $v\precsim w$ in $V$. Then $w-v \in V^+$ and so $f(w) - f(v) = f(w-v) \in W^+$. This implies $f(v)\precsim f(w)$.
\end{proof}


\section{Riesz spaces}

We take the convention that lattice operations bind tighter than addition, so $u\vee v + w = (u\vee v) + w$.

\begin{lemma} \label{lemmaRieszSpaces}
Let $V$ be a Riesz space, $u,v,w\in V$ and $\alpha\in \R$, then
\begin{enumerate}
\item $-(v \wedge w) = (-v)\vee (-w)$ and $-(v \vee w) = (-v)\wedge (-w)$;
\item if $\alpha \geq 0$, then $\alpha(v \wedge w) = (\alpha v)\wedge (\alpha w)$ and $\alpha(v \vee w) = (\alpha v)\vee (\alpha w)$;
\item if $\alpha \leq 0$, then $\alpha(v \wedge w) = (\alpha v)\vee (\alpha w)$ and $\alpha(v \vee w) = (\alpha v)\wedge (\alpha w)$;
\item $u+(v \wedge w) = (u+v)\wedge (u+w)$ and $u+(v \vee w) = (u+v)\vee (u+w)$.
\end{enumerate}
\end{lemma}
Cfr. also \ref{vectorSumOrderHomomorphism} and \ref{scalarMultipleOrderHomomorphism}.
\begin{proof}
We apply \ref{imagePolars} to

(1) the reverse order-embedding $v\mapsto -v$;

(2) the order-embedding $v\mapsto \alpha v$ for $\alpha > 0$; (if $\alpha = 0$ the result is trivial);

(3) the reverse order-embedding $v\mapsto \alpha v$ for $\alpha > 0$; (if $\alpha = 0$ the result is trivial);

(4) the order-embedding $v\mapsto u+v$.
\end{proof}

\begin{example}
Let space $\R^2$ with lexicographic ordering $\leq_l$ is a Riesz space. We have
\[ (x,y)\vee (u,v) = \begin{cases}
(x,y) & (x>u) \\ (u,v) & (x<u) \\ \big(x, \max\{v,y\}\big) & (x = u)
\end{cases} \qquad\text{and}\qquad (x,y)\wedge (u,v) = \begin{cases}
(x,y) & (x<u) \\ (u,v) & (x>u) \\ \big(x, \min\{v,y\}\big) & (x = u).
\end{cases} \]
\end{example}

\begin{theorem}[Riesz decomposition] \label{RieszDecomposition}
Every Riesz space has the Riesz decomposition property.
\end{theorem}
\begin{proof}
We use \ref{RieszDecompositionLemma}. Take $v_1,v_2\in V^+$ and $0 \leq w \leq v_1+v_2$. Set $w_1 \defeq w\wedge v_1$ and $w_2 \defeq w-w_1$, so $w = w_1+w_2$. We want to show that $0\leq w_1 \leq v_1$ and $0 \leq w_2 \leq v_2$.

Since $0\leq w$ and $0\leq v_1$, we have $w_1 = w\wedge v_1 \geq 0$. Clearly we also have $w_1 = w\wedge v_1 \leq v_1$.

Since $w_1 = w\wedge v_1 \leq w$, we have $0 \leq w - w_1 = w_2$. Since $w\leq v_1 + v_2$, we have $v_2 \geq w-v_1$. Now
\[ v_2 = 0\vee v_2 \geq 0\vee (w - v_1) = w + (-w)\vee(-v_1) = w - w\wedge v_1 = w-w_1 = w_2. \]
\end{proof}


\begin{proposition} \label{sumAsMeetJoin}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ (v \vee w) + (v \wedge w) = v+w. \]
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
(v \vee w) + (v \wedge w) &= v + 0 \vee (w-v) + w + (v-w)\wedge 0 \\
&= (v + w) + 0 \vee (w-v) + (-0)\wedge \big(-(w-v)\big) \\
&= (v + w) + 0 \vee (w-v) - 0 \vee (w-v) \\
&= v + w.
\end{align*}
\end{proof}

\begin{proposition}[Infinite distributivity in Riesz spaces] \label{distributivityRieszSpaces}
Let $V$ be a Riesz space, $v\in V$ and $S\subseteq V$ a subset. Then
\begin{enumerate}
\item if $\bigvee S$ exists, then $\big(\bigvee S\big) \wedge v = \bigvee (S\wedge v)$;
\item if $\bigwedge S$ exists, then $\big(\bigwedge S\big) \vee v = \bigwedge (S\vee v)$;
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We already have that $\big(\bigvee S\big) \wedge v$ is an upper bound of $S\wedge v$ by \ref{infiniteDistributiveInequalities}. Now let $m\in V$ be another upper bound. For all $w\in S$, we then have
\[ m \geq w\wedge v = w+v - w\vee v \geq w+v - \big(\bigvee S\big)\vee v, \]
using \ref{sumAsMeetJoin}. So $m - v + \big(\bigvee S\big)\vee v \geq w$ and thus $m - v + \big(\bigvee S\big)\vee v \geq \big(\bigvee S\big)$, which means that
\[ m \geq \big(\bigvee S\big) + v - \big(\bigvee S\big)\vee v = \big(\bigvee S\big)\wedge v. \]
This shows that $\big(\bigvee S\big)\wedge v$ is the least upper bound.

(2) Dual.
\end{proof}
\begin{corollary}
Riesz spaces are distributive lattices.
\end{corollary}

\begin{lemma} \label{additionLatticeOperationsInequalities}
Let $V$ be a Riesz space, $u,v,w\in V$ and $x,y,z\in V^+$. Then
\begin{enumerate}
\item $(u+v)\vee (2w) \leq u\vee w + v\vee w$;
\item $u\wedge w + v\wedge w \leq (u+v)\wedge (2w)$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) From $u\leq u\vee w$ and $v\leq v\vee w$, we get $u+v \leq u\vee w + v\vee w$. Similarly from $w\leq u\vee w$ and $w\leq v\vee w$, we get $2w \leq u\vee w + v\vee w$. Together this gives (1).

(2) Dual.
\end{proof}


\begin{lemma} \label{additionLatticeOperationsPositiveElementsInequalities}
Let $V$ be a Riesz space, $u,v,w\in V$ and $x,y,z\in V^+$. Then
\begin{enumerate}
\item $(u+v)\vee z \leq u\vee z + v\vee z$;
\item $(x+y)\wedge z \leq x\wedge z + y\wedge z$;
\item if $x\wedge y = 0$, then $(x+y)\wedge z = x\wedge z + y\wedge z$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $2z \geq z$ by \ref{scalarMultiplicationInequalities}, so we can apply \ref{additionLatticeOperationsInequalities}.

(2) We have $z \leq z+y$ by \ref{positiveVectorAddition}. By \ref{lemmaRieszSpaces},
\[ (x+y)\wedge z \leq (x+y)\wedge (z+y) = y + x\wedge z \]
and thus $(x+y)\wedge z - x\wedge z \leq y$. 

Now $x\wedge z \geq 0$ from $x\geq 0$ and $z\geq 0$, so $(x+y)\wedge z - x\wedge z \leq (x+y)\wedge z \leq z$ by \ref{positiveVectorAddition}.

Putting the two inequalities together gives $(x+y)\wedge z - x\wedge z \leq y\wedge z$. This implies the result.

(3) By \ref{sumAsMeetJoin}, we have $x+y = x\vee y$. Then $(x\wedge z)\wedge (y\wedge z) = x\wedge y \wedge z \leq 0$, but also $x\wedge y \wedge z \geq 0$, so it equals $0$. Then $(x\wedge z) + (y\wedge z) = (x\wedge z) \vee (y\wedge z)$, similarly by \ref{sumAsMeetJoin}. We can then calculate
\[ (x+y)\wedge z = (x\vee y)\wedge z = (x\wedge z)\vee (y\wedge z) = (x\wedge z) + (y\wedge z), \]
where we have used distributivity \ref{distributivityRieszSpaces}.
\end{proof}

\subsection{Positive elements}
\subsubsection{Positive and negative parts}
\begin{definition}
Let $V$ be a Riesz space and $v\in V$. Then we define
\begin{align*}
v^+ &\defeq v \vee 0 \\
v^- &\defeq (-v) \vee 0 = - (v \wedge 0).
\end{align*}
We call $v^+$ the \udef{positive part} of $v$ and $v^-$ the \udef{negative part} of $v$.
\end{definition}

\begin{lemma} \label{MeetJoinAsPositiveNegative}
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $v\vee w = (v-w)^+ + w = (v-w)^- + v$;
\item $v\wedge w = v - (v-w)^+ = w - (v-w)^-$.
\end{enumerate}
\end{lemma}
\begin{proof}
We calculate $v\vee w = (v-w)\vee 0 + w = (v-w)^+ + w$, using point (4) of \ref{lemmaRieszSpaces}. The other equalities are similar.
\end{proof}

\begin{proposition} \label{PositiveNegativeElements} \label{minimalPositiveDecomposition} 
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $v^+, v^- \in V^+$;
\item $v= v^+ - v^-$;
\item $v^+ \wedge v^- = 0$.
\end{enumerate}
Furthermore,
\begin{enumerate} \setcounter{enumi}{3}
\item if $p,q\in V$ satisfy 1 and 2, i.e.\ $p,q\in V^+$ and $v = p-q$, then $p \geq v^+$ and $q \geq v^-$; we may say $v=v^+-v^-$ is the minimal such decomposition; 
\item the elements $v^+, v^-$ are uniquely determined by properties 2 and 3. 
\end{enumerate}
Also
\begin{enumerate} \setcounter{enumi}{5}
\item $(-v)^- = v^+$ and $(-v)^+ = v^-$;
\item if $\alpha \geq 0$, then $(\alpha v)^+ = \alpha v^+$ and $(\alpha v)^- = \alpha v^-$;
\item $-v^- \leq v \leq v^+$;
\item $v\leq w$ \textup{if and only if} $v^+ \leq w^+$ and $v^- \geq w^-$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Evident from definitions.

(2) We calculate $v^+ - v = (v \vee 0) - v = (v-v) \vee (0-v) = 0\vee (-v) = v^-$.

(3) We calculate $0 = v^- - v^- = v^-  + (v\wedge 0) = (v^- + v)\wedge (0 + v^-) = v^+ \wedge v^-$.

(4) From $v\leq p$ and $0\leq p$, we get $v^+ = v \vee 0 \leq p$. Then we also have $v^- = v^+ - v \leq p - v = q$.

(5) Assume $p,q\in V$ satisfy (2) and (3), then (1) automatically follows from (3). Using \ref{MeetJoinAsPositiveNegative}, we calculate
\[ 0 = p\wedge q = p - (p-q)^+ = p - v^+. \]
So $p = v^+$ and $q = p - v = v^+ - v = v^-$.

(6) It is evident that $(-v)^- = (--v)\vee 0 = v\vee 0$.

(7) We calculate $\alpha v^+ = \alpha (v \vee 0) = (\alpha v) \vee 0 = (\alpha v)^+$; the calculation for $\alpha v^-$ is similar.

(8) This is clear from $-v^- = v\wedge 0 \;\leq\; v \;\leq\; v \vee 0 = v^+$.

(9) $v\leq w$ implies $v^+ = v\vee 0 \leq w\vee 0 = w^+$ and $-v^- = v\wedge 0 \leq w\wedge 0 = - w^-$.

Conversely, we have $v = v^+ - v^- \leq w^+ - w^- = w$.
\end{proof}


\begin{proposition} \label{triangleInequalityPositiveNegativeElements}
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $(v+w)^+ \leq v^+ + w^+$;
\item $(v+w)^- \leq v^- + w^-$.
\end{enumerate}
\end{proposition}
\begin{proof}
From \ref{PositiveNegativeElements} we get $v \leq v^+$ and $w\leq w^+$, so $v+w \leq v^+ + w^+$. Also $0 \leq v^+ + w^+$. So
\[ (v+w)^+ = (v+w)\vee 0 \leq v^+ + w^+. \]
Then we also have
\[ (v+w)^- = (-v-w)^+ \leq (-v)^+ + (-w)^+ = v^- + w^-. \]
\end{proof}

\subsubsection{Absolute value}
\begin{definition}
Let $V$ be a Riesz space and $v\in V$. Then the \udef{absolute value} of $v$ is
\[ |v| \defeq v\vee (-v) = -(v\wedge (-v)). \]
\end{definition}

If the Riesz space is a real function space with pointwise order, then $|f| = |\cdot|\circ f$ as usual, where $|\cdot|: \R\to \R$ is the usual absolute value function.

\begin{lemma} \label{absoluteValue}
Let $V$ be a Riesz space, $v,w\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item $|v| = v^+ + v^-$;
\item $|v| \in V^+$;
\item if $v\in V^+$, then $|v| = v$;
\item $|v| = |-v|$;
\item $|\alpha v| = |\alpha|\cdot |v|$;
\item $\big||v|\big| = |v|$;
\item $0 \leq v^+ \leq |v|$ and $0 \leq v^- \leq |v|$;
\item $|v| = 0$ \textup{if and only if} $v = 0$.
\end{enumerate}
\end{lemma}
\begin{proof}
We prove (1):
\[ |v| = v\vee (-v) = (2v)\vee 0 - v = 2v^+ - v = 2v^+ - (v^+ - v^-) = v^+ + v^- \]
and (6), using the absorption law:
\[ \big||v|\big| = |v|\vee (-|v|) = v \vee (-v) \vee \big( v\wedge (-v) \big) = v \vee (-v) = |v|. \]
The rest are immediate consequences, using the results of \ref{PositiveNegativeElements}.

In particualar, for (8) we have that $|v|=0$ implies $v^+ + v^- = 0$, which implies $v^+ = -v^-$. Thus $0\leq v^-$ and $0\leq -v^-$, so $v^- = 0$. Then $v^+ = 0$ and thus so is $v$.
\end{proof}

\begin{lemma} \label{solidLemma}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |w|\leq |v| \iff -|v| \leq w \leq |v|. \]
\end{lemma}
\begin{proof}
First sssume $|w| \leq |v|$. Since we have $w \leq |w|$, we also have $w\leq |v|$. Also $-w \leq |-w| = |w|$, so $-|v| \leq -|w| \leq w$.

Conversely, we have $w\leq |v|$ and $-|v| \leq w$ implies $-w\leq |v|$. So $|w| = w\vee (-w) \leq |v|$.
\end{proof}

\begin{lemma} \label{positiveNegativePartAverage}
Let $V$ be a Riesz space and $v\in V$. Then
\begin{enumerate}
\item $\displaystyle v^+ = \frac{|v|+v}{2}$;
\item $\displaystyle v^- = \frac{|v|-v}{2}$.
\end{enumerate}
\end{lemma}
\begin{proof}
We calculate
\begin{align*}
\frac{|v|+v}{2} &= \frac{(v^+ + v^-) + (v^+ - v^-)}{2} = \frac{2v^+}{2} = v^+ \\
\frac{|v|-v}{2} &= \frac{(v^+ + v^-) - (v^+ - v^-)}{2} = \frac{2v^-}{2} = v^-.
\end{align*}
\end{proof}

\begin{lemma} \label{absoluteValueMeetJoin}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $(v+w)\vee (v-w) = v + |w|$;
\item $(v+w)\wedge (v-w) = v - |w|$;
\end{enumerate}
or, equivalently,
\begin{enumerate} \setcounter{enumi}{2}
\item $v \vee w = \frac{1}{2}\big(v+w + |v - w|\big)$;
\item $v \wedge w = \frac{1}{2}\big(v+w - |v - w|\big)$.
\end{enumerate}
\end{lemma}
\begin{proof}
We calculate, using \ref{lemmaRieszSpaces}
\[ (v+w)\vee (v-w) = v+ w\vee(-w) = v+ |w| \quad\text{and}\quad (v+w)\wedge (v-w) = v + w\wedge(-w) = v-|w|. \]
The next two equalities follow by the substitutions $v+w \leftrightarrow v$ and $v-w \leftrightarrow w$.
\end{proof}
\begin{corollary} \label{absoluteValueDifference}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |v - w| = (v \vee w) - (v \wedge w). \]
\end{corollary}
\begin{proof}
We have
\begin{align*}
|v-w| &= 2(v\vee w) - (v+w) \\
&= 2(v\vee w) - v\vee w - v \wedge w = v\vee w - v\wedge w.
\end{align*}
using \ref{sumAsMeetJoin}.
\end{proof}
\begin{corollary} \label{meetJoinAbsoluteValues}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $|v| \vee |w| = \frac{1}{2}\Big(|v|+|w| + \big||v| - |w|\big|\Big)$;
\item $|v| \wedge |w| = \frac{1}{2}\Big(|v|+|w| - \big||v| - |w|\big|\Big)$.
\end{enumerate}
\end{corollary}
\begin{proof}
Substitute $v\to |v|$ and $w\to |w|$.
\end{proof}

\begin{proposition} \label{meetJoinAbsoluteValues2}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $|v|\vee |w| = \frac{1}{2}\Big( |v+w| + |v - w| \Big)$;
\item $|v|\wedge |w| = \frac{1}{2}\Big| |v+w| - |v - w| \Big|$;
\end{enumerate}
or, equivalently,
\begin{enumerate} \setcounter{enumi}{2}
\item $|v+w|\vee |v-w| = |v| + |w|$;
\item $|v+w|\wedge |v-w| = \big| |v| - |w| \big|$.
\end{enumerate}
Also
\begin{enumerate} \setcounter{enumi}{4}
\item $|v|+|w| = |v+w| + |v-w| - \big||v|-|w|\big|$;
\item $|v+w|+|v-w| = 2|v| + 2|w| - \big||v+w|-|v-w|\big|$;
\end{enumerate}
and
\begin{enumerate} \setcounter{enumi}{6}
\item $|v|+|w| = \big||v|-|w|\big| + \big||v+w|-|v-w|\big|$.
\end{enumerate}
\end{proposition}
The only real trick is in the proof of (1). All the other results follow from elementary substitutions.
\begin{proof}
(3,4,6) Are equivalent to (1,2,5) by the replacements $v \leftrightarrow v+w$ and $w \leftrightarrow v-w$.

(1) We calculate
\begin{align*}
|v|\vee |w| &= v\vee (-v)\vee w \vee (-w) = \big(v\vee(-w)\big)\vee \big((-v)\vee w\big) \\
&= \frac{1}{2}\Big((v-w) + |v + w|\Big)\vee \frac{1}{2}\Big( (-v+w) + |- v - w| \Big) \\
&= \frac{1}{2}|v+ w| + \frac{1}{2}\big((v-w)\vee (-v+w)\big) = \frac{1}{2}\Big( |v+w| + |v - w| \Big).
\end{align*}

(5) Using \ref{sumAsMeetJoin}, (1) and \ref{meetJoinAbsoluteValues} we get
\begin{align*}
|v|+|w| &= |v|\vee|w| + |v|\wedge |w| \\
&= \frac{1}{2}\Big( |v+w| + |v - w| \Big) + \frac{1}{2}\Big(|v|+|w| - \big||v| - |w|\big|\Big).
\end{align*}
This simplifies to the required equation.

(7) Follows from substituting (6) into (5).

(2) Follows from (7) and \ref{meetJoinAbsoluteValues}.
\end{proof}

The absolute value also satisfies the triangle inequality.
\begin{proposition}[Triangle and reverse triangle inequality in Riesz spaces] \label{triangleInequalityRieszSpaces}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |v| + |w| \geq \big|v+w\big| \geq \big||v|-|w|\big|. \]
\end{proposition}
\begin{proof}
The first inequality is the triangle inequality. It follows straight from \ref{triangleInequalityPositiveNegativeElements}.

The second inequality is the reverse triangle inequality and follows from the triangle inequality as in \ref{reverseTriangleInequality}.
\end{proof}

\begin{proposition}[Birkhoff-Nakano identity] \label{birkhoffIdentity}
Let $V$ be a Riesz space and $u,v,w\in V$, then
\[ |u\vee v - u\vee w| + |u\wedge v - u\wedge w| = |v-w|. \]
\end{proposition}
\begin{proof}
Using \ref{absoluteValueDifference} and \ref{sumAsMeetJoin}, we get
\begin{align*}
|u\vee v - u\vee w| + |u\wedge v - u\wedge w| &= (u\vee v)\vee(u\vee w) - (u\vee v)\wedge (u\vee w) + (u\wedge v)\vee(u\wedge w) - (u\wedge v)\wedge(u\wedge w) \\
&= u\vee (v \vee w) - u \vee (v\wedge w) + u\wedge (v\vee w) - u\wedge (v \wedge w) \\
&= \big(u\vee (v \vee w) + u\wedge (v\vee w)\big) - \big(u \vee (v\wedge w) + u\wedge (v \wedge w)\big) \\
&= \big(u + (v \vee w)\big) - \big(u + (v \wedge w)\big) \\
&= (v \vee w) - (v \wedge w) = |v-w|.
\end{align*}
\end{proof}
\begin{corollary}[Birkhoff inequalities] \label{BirkhoffInequalities}
Let $V$ be a Riesz space and $t,u,v,w\in V$, then
\begin{enumerate}
\item $|u\vee v - u\vee w| \leq |v-w|$ and $|u\wedge v - u\wedge w| \leq |v-w|$;
\item $|t\vee u - v\vee w| \leq |t-v| + |u-w|$ and $|t\wedge u - v\wedge w| \leq |t-v| + |u-w|$;
\item $|v^+-w^+|\leq |v-w|$ and $|v^- - w^-|\leq |v-w|$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) These follow from the lemma, \ref{positiveVectorAddition} and the fact that the absolute value is positive.

(2) We calculate, using the triangle inequality \ref{triangleInequalityRieszSpaces} and point (1),
\begin{align*}
|t\vee u - v\vee w| &= |t\vee u - v \vee u + v\vee u - v\vee w| \\
&\leq |t\vee u - v \vee u| + |v\vee u - v\vee w| \\
&\leq |t - v| + |u - w|.
\end{align*}
The argument for the second part is similar.

(3) Follows from (1) because
\[ |0\vee v - 0\vee w| = |v^+ -w^+| \qquad\text{and}\qquad |0\wedge v - 0\wedge w| = |v^- - w^-|. \]
\end{proof}

\begin{proposition} \label{offPerpendicularTerms}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $|v| + |w| - |v+w| = 2(v^+\wedge w^- + v^-\wedge w^+)$;
\item $|v| + |w| - |v-w| = 2(v^+\wedge w^+ + v^-\wedge w^-)$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) As $v^+, v^-, w^+, w^- \geq 0$, they are all equal to their absolute values. Then \ref{meetJoinAbsoluteValues} gives
\begin{align*}
v^+\wedge w^- &= \frac{1}{2}\big(v^++w^- - |v^+-w^-|\big) \\
v^-\wedge w^+ &= \frac{1}{2}\big(v^-+w^+ - |v^--w^+|\big). 
\end{align*}
Adding these gives
\begin{align*}
2(v^+\wedge w^- + v^-\wedge w^+) &= |v| + |w| - \big(|v^+-w^-|+ |v^--w^+|\big) \\
&= |v| + |w| - \big(|0\vee v - 0\vee (-w)|+ |0\wedge v- 0\wedge (-w)|\big) \\
&= |v| + |w| - |v- (-w)| = |v| + |w| - |v + w|.
\end{align*}
where we have used using \ref{birkhoffIdentity}.

(2) Now we use \ref{meetJoinAbsoluteValues} to get
\begin{align*}
v^+\wedge w^+ &= \frac{1}{2}\big(v^++w^+ - |v^+-w^+|\big) \\
v^-\wedge w^- &= \frac{1}{2}\big(v^-+w^- - |v^--w^-|\big). 
\end{align*}
Adding these gives
\begin{align*}
2(v^+\wedge w^+ + v^-\wedge w^-) &= |v| + |w| - \big(|v^+-w^+|+ |v^--w^-|\big) \\
&= |v| + |w| - \big(|0\vee v - 0\vee w|+ |0\wedge v- 0\wedge w|\big) \\
&= |v| + |w| - |v- w|,
\end{align*}
where we have used using \ref{birkhoffIdentity}.
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space and $v,v',w,w'\in V$, then
\[ |(v\vee v')- (w\vee w')| \leq |v-w| \vee |v'- w'|. \]
\end{proposition}
\begin{proof}
We calculate
\begin{align*}
|(v\vee v')- (w\vee w')| &= (v\vee v')\vee(w\vee w') - (v\vee w)\wedge (v'\vee w') \\
&\leq (v\vee w)\vee(v\vee w') - (v\wedge w)\vee (v'\wedge w') \\
&= \big((v\vee w) - (v\wedge w)\vee (v'\wedge w')\big)\vee\big((v'\vee w') - (v\wedge w)\vee (v'\wedge w')\big) \\
&\leq \big((v\vee w) - (v\wedge w)\big)\vee\big((v'\vee w') - (v'\wedge w')\big) \\
&= |v-w|\vee |v'-w'|,
\end{align*}
where we have used \ref{absoluteValueDifference}, \ref{firstMiniMaxCorollary} and \ref{lemmaRieszSpaces}.
\end{proof}

\subsection{Disjointness}
\begin{definition}
Let $V$ be a Riesz space and $v,w\in V$. We call $v$ and $w$ \udef{(Riesz) disjoint} if $|v|\wedge |w| = 0$. We write $v\perp_r w$.

Let $D\subseteq V$ be a subset. Then $D^{\perp_r}$ is called the \udef{(Riesz) disjoint complement} of $D$.

Two subsets $D_1,D_2\subseteq V$ are called \udef{(Riesz) disjoint} if $D_1 \perp_r D_2$.
\end{definition}
We have $v^+\perp_r v^-$ for all $v\in V$ by \ref{PositiveNegativeElements}.

\begin{lemma} \label{vectorDisjointWithItself}
Let $V$ be a Riesz space, $v\in V$ and $D_1, D_2 \subseteq V$ subsets. Then
\begin{enumerate}
\item if $v\perp_r v$, then $v = 0$;
\item if $D_1\perp_r D_2$, then $D_1\cap D_2 \subseteq \{0\}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Suppose $v\perp_r v$, then $0 = |v|\wedge |v| = |v|$, so $v = 0$ by \ref{absoluteValue}.

(2) Suppose $v\in D_1\cap D_2$ and $D_1\perp_r D_2$. Then $v\perp_r v$, so $v=0$ by (1).
\end{proof}

\begin{proposition} \label{RieszDisjointEquivalents}
Let $V$ be a Riesz space and $v,w\in V$. Then the following are equivalent:
\begin{enumerate}
\item $v\perp_r w$;
\item $|v-w| = |v+w|$;
\item $|v+w| = |v|+|w|$;
\item $|v-w| = \big||v|-|w|\big|$;
\item $|v|\vee |w| = |v|+|w|$;
\item $|v-w| = |v|+|w|$;
\end{enumerate}
which are all also equivalent to any of
\begin{enumerate} \setcounter{enumi}{6}
\item $v^+\wedge w^- = 0 = v^- \wedge w^+$;
\item $v^+\perp_r w^+$ and $v^-\perp_r w^-$ (i.e.\ $v^+\wedge w^+ = 0 = v^- \wedge w^-$);
\item $(v+w)^+ = v^++w^+$ and $(v+w)^- = v^- + w^-$;
\item $(v+w)^+ = v^+\vee w^+$ and $(v+w)^- = v^- \vee w^-$.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ By \ref{meetJoinAbsoluteValues2}, we have
\[ \big||v+w| - |v-w|\big| = 2\big(|v|\wedge |w|\big) = 0, \]
so (2) follows from \ref{absoluteValue}.

$(2) \Leftrightarrow (3)$ By the sixth point of \ref{meetJoinAbsoluteValues2}.

$(3) \Leftrightarrow (4)$ By the fifth point of \ref{meetJoinAbsoluteValues2}.

$(2,3,4) \Rightarrow (5)$ From the first point of \ref{meetJoinAbsoluteValues}.

$(5) \Rightarrow (1)$ From \ref{sumAsMeetJoin}:
\[ |v|\vee |w| = |v|+ |w| = |v|\vee |w| + |v|\wedge |w|. \]

$(2,3) \Rightarrow (6)$ Immediate.

$(6) \Rightarrow (1)$ We have $|v+(-w)| = |v|+|w| = |v|+|-w|$, so $v\perp_r (-w)$, which is clearly equivalent to $v\perp_r w$.

$(3) \Leftrightarrow (7)$ From \ref{offPerpendicularTerms}, we have that (3) is equivalent to $v^+\wedge w^- + v^- \wedge w^+ = 0$. Since both terms are positive, this is equivalent to (6).

$(6) \Leftrightarrow (8)$ Similar to the previous point, again using \ref{offPerpendicularTerms} and the fact that all positive and negative parts are positive.

$(3) \Leftrightarrow (9)$ We first calculate, using \ref{positiveNegativePartAverage},
\[ (v+w)^+ = \frac{|v+w|+v+w}{2} = \frac{|v|+|w|+v+w}{2} = \frac{|v|+v}{2} + \frac{|w|+w}{2} = v^+ + w^+, \]
and similarly for $(v+w)^- = v^- + w^-$.

Conversely, we have, using \ref{absoluteValue},
\[ |v+w| = (v+w)^+ + (v+w)^- = (v^+ + w^+) + (v^- + w^-) = (v^+ + v^-) + (w^+ + w^-) = |v| + |w|. \]

(10) Clearly follows from (8), (9) and (5). For the converse, we use \ref{absoluteValue}, \ref{sumAsMeetJoin} and \ref{offPerpendicularTerms} to calculate
\begin{align*}
2|v+w| &= 2\big((v+w)^+ + (v+w)^-\big) \\
&= 2\big(v^+\vee w^+ + v^-\vee w^-\big) \\
&= 2\big(v^+ + w^+ - v^+\wedge w^+ + v^- + w^-  - v^-\wedge w^-\big) \\
&= 2\big(|v| + |w| - \frac{1}{2}(|v|+ |w| - |v-w|)\big) \\
&= |v|+|w| + |v-w|.
\end{align*}
Thus $|v|+ |w| - |v+w| = |v+w| - |v-w|$. Since the left-hand side is positive, by the triangle inequality \ref{triangleInequalityRieszSpaces}, we have $|v+w| - |v-w| = \big||v+w| - |v-w|\big|$. Then the sixth point of \ref{meetJoinAbsoluteValues2} simplifies to $|v+w| = |v| + |w|$.
\end{proof}
TODO: more elegant proof for (10).

\begin{lemma} \label{disjointComplementBandLemma}
Let $V$ be a Riesz space, $u,v,w\in V$ and $\alpha \in \R$. Then
\begin{enumerate}
\item $u\perp_r v$ and $u\perp_r w$ imply $u\perp_r v+w$;
\item $u\perp_r v$ implies $u\perp_r \alpha v$;
\item $u\perp_r v$ and $|w|\leq |v|$ imply $u\perp_r w$;
\item for all $S\subseteq (u^{\perp_r})^+$, we have $\big(\bigvee S\big) \perp_r u$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have, by the triangle inequality \ref{triangleInequalityRieszSpaces} and \ref{additionLatticeOperationsPositiveElementsInequalities},
\[ 0\leq |v+w|\wedge |u| \leq \big(|v| + |w|\big)\wedge |u| \leq |v|\wedge |u| + |w|\wedge |u| = 0.  \]
Thus $|v+w|\wedge |u| = 0$.

(2) If $|\alpha|\leq 1$, then $|\alpha v|\leq |v|$, so $0\leq |\alpha v|\wedge |u| \leq |v|\wedge |u| = 0$.

If $|\alpha| > 1$, then
\[ 0\leq |\alpha v|\wedge |u| \leq |\alpha v|\wedge |\alpha u| = |\alpha|\big(|v|\wedge |u|\big) = 0. \]

(3) We have
\[ 0\leq |u|\wedge |w| \leq |u|\wedge |v| = 0. \]

(4) For all $x\in S$, we have $x\wedge |u| = |x|\wedge |u| = 0$, so $S\wedge |u| = \{0\}$. Thus, since Riesz spaces are infinitely distributive, by \ref{distributivityRieszSpaces}, we have $\big(\bigvee S\big) \wedge |u| = \bigvee S\wedge |u| = \bigvee\{0\} = 0$. 
\end{proof}

\subsection{Subsets}
\begin{definition}
Let $V$ be a Riesz space. A subset $E$ is called
\begin{itemize}
\item a \udef{Riesz subspace} if it is both a subspace and a sublattice;
\item \udef{solid} if for all $v\in E$ the interval $[-|v|,|v|]$ is a subset of $E$;
\item a \udef{Riesz ideal} if it is a solid (vector) subspace;
\item a \udef{band} if it is a Riesz ideal such that for all subsets $S\subseteq E$, we have $\sup(S) \subset E$. 
\end{itemize}
\end{definition}
For the definition of a Riesz ideal we only assume it is a linear subspace, not a Riesz subspace. However it turns out that a Riesz ideal is automatically a Riesz subspace, see \ref{RieszIdealRieszSubspace}. 

\begin{lemma} \label{RieszSubspaceJoinClosure}
Let $V$ be a Riesz space and $E\subseteq V$ a subset. Then $E$ is a Riesz subspace if $E$ is a subspace that is closed under taking binary joins.
\end{lemma}
\begin{proof}
For all $v,w\in E$, we have $v\wedge w = -\big((-v)\vee (-w)\big)\in E$.
\end{proof}

\begin{example}
Consider the Riesz space $\cont(\interval{0,1})$.
\begin{itemize}
\item The subset of polynomials is a subspace, but not a Riesz subspace;
\item The subset of constant functions is a Riesz subspace, but not a Riesz ideal.
\item The subset $\setbuilder{f\in \cont(\interval{0,1})}{f(0) = 0}$ is a Riesz ideal, but not a band. Consider the set $\{f_n\}_{n\in\N}$ where $f_n(x) = 1 \wedge nx$. Then $\sup f_n$ exists in $\cont(\interval{0,1})$ and equals $\underline{1}$, which is not an element of the set (note that it is not $[x\neq 0]$, because we are working in $\cont(\interval{0,1})$).
\item The subset $\setbuilder{f\in \cont(\interval{0,1})}{\forall x\in \interval{0,\frac{1}{2}}: f(x) = 0}$ is a band. 
\end{itemize}
Consider the Riesz space $\R^\N$ with pointwise ordering.
\begin{itemize}
\item the set of bounded sequences $\ell_\infty$ is an ideal;
\item the set of convergent sequences $c$ is a Riesz subspace of $\ell_\infty$;
\item the set $c_0$ of sequences that converge to $0$ is an ideal in $c$.
\end{itemize}
\end{example}

\begin{proposition} \label{RieszSpaceIntersectionSystems}
Let $V$ be a Riesz space. Then the following are complete $\wedge$-subsemilattices of $\powerset(V)$:
\begin{enumerate}
\item the set of Riesz subspaces;
\item the set of solid subsets;
\item the set of Riesz ideals;
\item the set of bands.
\end{enumerate}
\end{proposition}

\begin{proposition} \label{disjointDoublePolarConstruction}
Let $V$ be a Riesz space, $A\subseteq V$ a solid subset and $v\in V\setminus\{0\}$. Then $v\in A^{\perp_r\perp_r}$ \textup{if and only if} there exists $u\in A$ such that $0 < |u| \leq |v|$.
\end{proposition}
\begin{proof}
First suppose $v\in A^{\perp_r\perp_r}$. Then $v\notin A^{\perp_r}$, because $v$ was assumed nonzero and $A^{\perp_r}\cap A^{\perp_r\perp_r} = \{0\}$. Thus there exists $u'\in A$ such that $|v|\wedge |u'| \neq 0$. Since $A$ is solid, we have $|v|\wedge |u'| \in A$, so we can set $u = |v|\wedge |u'|$.

Now suppose there exists $u\in A$ such that $0 < |u| \leq |v|$. Assume, towards a contradiction, that $v\notin A^{\perp_r\perp_r}$. Then there exists $w\in A^{\perp_r}$ such that $|v|\wedge |w| \neq 0$. Since both $A$ and $A^{\perp_r}$ are solid, we have $|v|\wedge |w| \in A\cap A^{\perp_r}$, which by \ref{vectorDisjointWithItself} implies $|v|\wedge |w| = 0$. A contradiction.
\end{proof}

\subsubsection{Riesz ideals}
\begin{lemma} \label{RieszIdealRieszSubspace}
Every Riesz ideal is a Riesz subspace.
\end{lemma}
\begin{proof}
The Riesz ideal is a Riesz subspace if it is closed under lattice operations, i.e.\ under meets and joins. By \ref{RieszSubspaceJoinClosure}, we just need to check closure under joins. This follows from
\[ v \vee w = \frac{1}{2}\big(v+ w+|v-w|\big) \]
(\ref{absoluteValueMeetJoin}) and the fact that the Riesz ideal is a linear subspace.
\end{proof}

\begin{proposition} \label{RieszSolidEquivalents}
Let $V$ be a Riesz space and $E\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $E$ is solid;
\item $|w|\leq |v| \implies w\in E$ for all $v\in E$ and $w\in V$;
\item we have
\begin{enumerate}
\item $w \in E \iff |w|\in E$ for all $w\in V$;
\item $0\leq w \leq v \implies w\in E$ for all $v\in E$ and $w\in V$;
\end{enumerate}
\item we have
\begin{enumerate}
\item $w \in E \iff |w|\in E$ for all $w\in V$;
\item $v\wedge w\in E$ for all $v\in E^+$ and $w\in V^+$.
\end{enumerate}
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Leftrightarrow (2)$ Immediate from \ref{solidLemma}.

$(2) \Rightarrow (3a)$ First suppose $w\in E$. Since $\big||w|\big| = |w|\leq |w|$, we have $|w|\in E$. Now suppose $|w|\in E$. Since $|w|\leq |w| = \big||w|\big|$, we have $w\in E$.

$(2) \Rightarrow (3b)$ Assume $0 \leq w \leq v$. Since $v = |v|$ and $w = |w|$, we have $|w|\leq |v|$, so $w\in E$.

$(3) \Rightarrow (2)$ Assume $|w| \leq |v|$ for some $v\in E$ and $w\in V$. As $|v|\in E$ by (3a), we have $|w|\in E$ by (3b) and so $w\in E$ by (3a).

$(3b) \Rightarrow (4b)$ We have $0\leq v\wedge w\leq v$, so $v\wedge w\in E$.

$(4b) \Rightarrow (3b)$ 
\end{proof}
\begin{corollary} \label{RieszIdealEquivalents}
Let $V$ be a Riesz space and $E\subseteq V$ a subspace. Then the following are equivalent:
\begin{enumerate}
\item $E$ is a Riesz ideal;
\item we have
\begin{enumerate}
\item $w \in E \implies |w|\in E$ for all $w\in V$;
\item $0\leq w \leq v \implies w\in E$ for all $v\in E$ and $w\in V$.
\end{enumerate}
\end{enumerate}
\end{corollary}
\begin{proof}
The only point we need to prove is that $(2b)$ implies that $|w|\in E \implies w\in E$. Take $w\in V$ such that $|w|\in E$. Since $w^+ \leq w^+ + w^- = |w|$, we have $w^+\in E$. Similarly $w^-\in E$. Since $E$ is a subspace, $w = w^+ - w^- \in E$.
\end{proof}

\begin{proposition} \label{sumRieszIdeals}
Let $V$ be a Riesz space and $E_1, E_2$ Riesz ideals in $V$. Then
\begin{enumerate}
\item $E_1 + E_2$ is a Riesz ideal in $V$;
\item $(E_1 + E_2)^+ = E_1^+ + E_2^+$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Since $E_1+E_2$ is automatically a linear subspace, we just need to prove that it is solid, for which we use \ref{RieszSolidEquivalents}. To that end, take $v_1+v_2\in E_1+E_2$ and $w\in V$ such that $|w| \leq |v_1 + v_2| \leq |v_1| + |v_2|$.

Since $w^+ \leq |w| \leq |v_1| + |v_2|$, there exist $w_1, w_2\in V^+$ such that $w^+= w_1+w_2$ and $w_1\leq |v_1|, w_2\leq |v_2|$ by the Riesz decomposition theorem \ref{RieszDecomposition}. By \ref{RieszSolidEquivalents}, $w_1\in E_1$ and $w_2\in E_2$, so $w^+\in E_1+E_2$. Similarly $w^-\in E_1+E_2$ and thus $w = w^+-w^-\in E_1+E_2$.

(2) It is clear that $(E_1 + E_2)^+ \supseteq E_1^+ + E_2^+$, since $0\leq v$ and $0\leq w$ implies $0 = 0+0\leq v+w$.

For the converse, take $v\in (E_1+E_2)^+$. Then we can write $v = v_1+v_2$, where $v_1\in E_1$ and $v_2\in E_2$. Now $v = |v| = |v_1 + v_2| \leq |v_1| + |v_2|$. By the Riesz decomposition theorem \ref{RieszDecomposition}, we can find $v_1',v_2'\in V^+$ such that $v = v_1'+v_2'$ and $v_1'\leq |v_1|, v_2'\leq |v_2|$. Then $v_1'\in E_1^+$ and $v_2'\in E_2^+$, so $v\in E_1^+ + E_2^+$.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $E_1,E_2\in V$ be Riesz ideals. Then the join of $E_1$ and $E_2$ in the complete $\wedge$-subsemilattice of Riesz ideals is $E_1+E_2$.
\end{corollary}
\begin{corollary} \label{RieszIdealDirectSumPreservesOrder}
Let $V$ be a Riesz space and $E_1, E_2$ Riesz ideals in $V$ such that the sum $E_1\oplus E_2$ is direct. Then
\begin{enumerate}
\item For all $0\leq v = v_1 + v_2 \in E_1\oplus E_2$, we have $0\leq v_1$ and $0\leq v_2$.
\item For all $v_1 +v_2 \leq w_1 + w_2 \in E_1\oplus E_2$, we have $v_1 \leq w_1$ and $v_2 \leq w_2$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Restatement of point (2) of the proposition.

(2) We have that $0\leq (w_1 - v_1) + (w_2 - v_2) \in E_1\oplus E_2$, so $0\leq w_1 - v_1$ and $0\leq w_2 - v_2$, by the first point. Thus $v_1\leq w_1$ and $v_2\leq w_2$.
\end{proof}

\begin{example}
The algebraic sum of two bands is not always a band. Take $V = \cont(\interval{-1,1})$ and
\begin{align*}
B_1 &= \setbuilder{f\in\cont(\interval{-1,1})}{\forall x\in \interval{-1,0}: f(x) = 0} \\
B_2 &= \setbuilder{f\in\cont(\interval{-1,1})}{\forall x\in \interval{0,1}: f(x) = 0}.
\end{align*}
Then $B_1+B_2 = \setbuilder{f\in\cont(\interval{-1,1})}{f(0) = 0}$, which is not a band. 
\end{example}

\begin{lemma} \label{generatedRieszIdeals}
Let $V$ be a Riesz space and $A\subseteq V$ a subset. The Riesz ideal generated by $A$ is given by
\[ R = \setbuilder{w\in V}{\exists N\in \N: \exists \seq{\alpha_n}_{n=0}^N \subseteq \R: \exists \seq{v_n}_{n=0}^N \subseteq A: \; |w| \leq \sum_{n=0}^N |\alpha_nv_n|}. \]
In particular, the principal Riesz ideal generated by $v\in V$ is given by
\[ R_v = \setbuilder{w\in V}{\exists \alpha\in \R: \; |w|\leq |\alpha v|}. \]
\end{lemma}
\begin{proof}
We first show that any $w\in R$ is an element of each ideal that contains $A$. Take $N\in \N, \seq{\alpha_n}_{n=0}^N \subseteq \R$ and $\seq{v_n}_{n=0}^N \subseteq A$ such that $|w| \leq \sum_{n=0}^N |\alpha_nv_n|$. By the Riesz decomposition theorem \ref{RieszDecomposition}, we can write $|w| = \sum_{n=0}^N v_{n}'$, where $0\leq v_n' \leq |\alpha_nv_n|$.

Let $I$ be a Riesz ideal that contains $A$. Then $\alpha_nv_n \in I$ for all $n\leq N$ since $I$ is a linear subspace and $v_n'\in I$ for all $n\leq N$ by solidity. So $|w|\in I$ because it is a linear subspace and $w\in I$ by solidity. Thus $R$ is contained in any Riesz ideal that contains $A$.

We now straightforward to see that $R$ is indeed a Riesz ideal.
\end{proof}

\begin{proposition} \label{latticePropertiesPrincipalRieszIdeals}
Let $V$ be a Riesz space and $v,w\in V^+$. Then
\begin{enumerate}
\item $R_{v\wedge w} = R_v \cap R_w$;
\item $R_{v\vee w} = R_{v+w} = R_v + R_w$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) It is clear, by \ref{generatedRieszIdeals}, that
\[ R_{v\wedge w} = \setbuilder{x\in V}{\exists \alpha\in \R: \; |x|\leq |\alpha| v\wedge w} \subseteq \setbuilder{x\in V}{\exists \alpha\in \R: \; |x|\leq |\alpha| v} = R_v. \]
Similarly $R_{v\wedge w} \subseteq R_w$, so $R_{v\wedge w} \subseteq R_v \cap R_w$.

Conversely, take $x\in R_v \cap R_w$. Then there exist $\alpha, \beta\in\R$ such that $|x| \leq |\alpha| v$ and $|x|\leq |\beta| w$. Set $\gamma = |\alpha|\vee|\beta|$, so $|x| \leq \gamma v$ and $|x|\leq \gamma w$ by \ref{scalarInequalityImpliesVectorInequality}. We have $\gamma^{-1}|x| \leq v$ and $\gamma^{-1}|x| \leq w$, so $\gamma^{-1}|x| \leq v\wedge w$ and $|x| \leq \gamma v\wedge w$. This implies $x\in R_{v\wedge w}$.

(2) Since $v\vee w \leq v\vee w + v\wedge w = v+w$ by \ref{sumAsMeetJoin}, it is clear, by \ref{generatedRieszIdeals}, that
\[ R_{v\vee w} = \setbuilder{x\in V}{\exists \alpha\in \R: \; |x|\leq |\alpha| v\vee w} \subseteq \setbuilder{x\in V}{\exists \alpha\in \R: \; |x|\leq |\alpha| (v+w)} = R_{v+w}. \]
Conversely, take $x\in R_{v+w}$. Then there exist $\alpha\in\R$ such that $|x| \leq |\alpha| (v+w) \leq |2\alpha|(v\vee w)$, so $x\in R_{v\vee w}$.

Now take $x\in R_{v+w}$. Then, by \ref{generatedRieszIdeals}, there exists $\alpha \in \R$ such that $|x| \leq |\alpha|(v+w)$. Since $|\alpha|(v+w) \in R_v + R_w$ and $R_v + R_w$ is a Riesz ideal, by \ref{sumRieszIdeals}, we have $x\in R_v+R_w$ and so $R_{v+w} \subseteq R_v+R_w$.

Finally we have, by \ref{generatedRieszIdeals}, and the fact that $v \leq v+w$, that
\[ R_{v} = \setbuilder{x\in V}{\exists \alpha\in \R: \; |x|\leq |\alpha| v} \subseteq \setbuilder{x\in V}{\exists \alpha\in \R: \; |x|\leq |\alpha| (v+w)} = R_{v+w}. \]
Similarly $R_w \subseteq R_{v+w}$. Thus $R_v + R_w \subseteq R_{v+w} + R_{v+w} = R_{v+w}$.
\end{proof}

\begin{definition}
Let $V$ be a Riesz space and $v\in V$. If the principal Riesz ideal generated by $v$ is equal to $V$, then $u$ is called a \udef{strong Riesz unit}.
\end{definition}

\begin{example}
The function $\underline{1}$ is a strong Riesz unit in $L^\infty(\R)$.
\end{example}

\subsubsection{Bands}

\begin{proposition} \label{bandFromIdealByPositiveCone}
Let $V$ be a Riesz space and $E\subseteq V$ a Riesz ideal. Then $E$ is a band \textup{if and only if} $\sup(S) \subseteq E$ for all $S\subseteq E^+$.
\end{proposition}
\begin{proof}
The direction $\Rightarrow$ is immediate.

For the converse, take $D\subseteq E$ such that the supremum exists in $V$. Call it $v$. We need to show $v\in E$.

Pick $w\in D$. And consider the set $D' \defeq \setbuilder{w\wedge u - w}{u\in D}$. Then $D'\subseteq E^+$ and $\bigvee D' = w\wedge v - w$ by \ref{vectorSumOrderHomomorphism} and $\bigvee D'\in E$. Since $w\in D$, we have $w\leq v$ and thus $w\wedge v = v$.

Since $v-w,w\in E$, we have $v\in E$.
\end{proof}

\begin{lemma} \label{disjointPolarBand}
Let $V$ be a Riesz space and $D\subseteq V$ a subset. Then
\begin{enumerate}
\item $D^{\perp_r}$ is a band;
\item $D^{\perp_r} \cap D^{\perp_r\perp_r} = \{0\}$;
\item $D \cap D^{\perp_r} \subseteq \{0\}$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have that $D^{\perp_r}$ is a linear subspace from the first two points of \ref{disjointComplementBandLemma}. It is solid by the third point and a band by the fourth point and \ref{bandFromIdealByPositiveCone}.

(2) By \ref{vectorDisjointWithItself}, we have $D^{\perp_r} \cap D^{\perp_r\perp_r} \subseteq \{0\}$. The other inclusion follows because both $D^{\perp_r}$ and $D^{\perp_r\perp_r}$ are linear subspaces and thus contain $0$.

(3) Follows from $D\subseteq D^{\perp_r\perp_r}$.
\end{proof}
For any subset $D$, the set $D^{\perp_r\perp_r}$ is a band that contains $D$ and thus it contains the band generated by $D$, but in general it may be larger.

\begin{proposition} \label{bandGeneratedByRieszIdeal}
Let $V$ be a Riesz space and $E\subseteq V$ a Riesz ideal. Then the band generated by $E$ is
\[ B = \setbuilder{v\in V}{\exists D\subseteq E^+: \; |v| = \bigvee D}. \]
\end{proposition}
\begin{proof}
It is enough to prove that $B$ is a band.

First we show that $B$ is solid, for which we use \ref{RieszSolidEquivalents}. By construction $w\in B \iff |w| \in B$, so it is enough to prove that $0\leq w \leq v$ for some $v\in B$ implies $w\in B$. We can write $v = \bigvee D$ for some $D\subseteq E^+$. Then there exists $d\in D$ such that $w \leq d$, which implies that $w\in E$ and thus in $B$.

Now we show that $B$ is a linear subspace. Take $v,w\in B$. Then $|v| = \bigvee D_1$ and $|w| = \bigvee D_2$ for some $D_1, D_2\subseteq E^+$. By \ref{vectorSumOrderHomomorphism}, we have $|v|+|w| = \bigvee (D_1+D_2)$, so $|v|+|w|\in B$. Since $|v+w|\leq |v|+|w|$ and $B$ is solid, we have $v+w\in B$.

Take $\alpha\in \R$ and $v\in B$. Then $|v| = \bigvee D$ for some $D\subseteq E^+$. We have, by \ref{scalarMultipleOrderHomomorphism},
\[ |\alpha v| = |\alpha|\,|v| = |\alpha|\bigvee D = \bigvee |\alpha|D. \]
Since $|\alpha|D\subseteq E^+$, we have $\alpha v\in B$.

Finally, we use \ref{bandFromIdealByPositiveCone} to see that $B$ is a band.
\end{proof}

\begin{proposition} \label{DedekindCompleteDirectSumBands}
Let $V$ be a Dedekind complete Riesz space and $B_1, B_2\subseteq V$ bands such that the sum $B_1\oplus B_2$ is direct. Then $B_1 \oplus B_2$ is a band.
\end{proposition}
\begin{proof}
We have that $B_1\oplus B_2$ is a Riesz ideal by \ref{sumRieszIdeals}, so we just need to show that $\sup(D)\subseteq B_1\oplus B_2$ for all $D\subseteq B_1\oplus B_2$. By \ref{bandFromIdealByPositiveCone}, it is enough to consider $D\subseteq (B_1\oplus B_2)^+$. Take $u\in \sup(D)$. Set
\begin{align*}
D_1 &= \setbuilder{v_1\in B_1}{\exists v_2\in B_2, v\in D: \; v_1 + v_2 = v} \\
D_2 &= \setbuilder{v_2\in B_2}{\exists v_1\in B_1, v\in D: \; v_1 + v_2 = v}.
\end{align*}
All elements of $D_1$ and $D_2$ are positive by \ref{RieszIdealDirectSumPreservesOrder}. Since each $v\in D$ is less than $u$, each element of $D_1$ and $D_2$ is less than $u$. Thus $D_1$ and $D_2$ are bounded above. By Dedekind completeness, $u_1\defeq \bigvee D_1$ and $u_2\defeq \bigvee D_2$ exist. By definition of band, $u_1\in B_1$ and $u_2\in B_2$.

Now $D_1 + D_2 \supseteq D$, so $u_1 + u_2 = \bigvee D_1 + \bigvee D_2 = \bigvee(D_1\oplus D_2) \geq u$. Conversely, let $u = u'_1 + u'_2\in B_1\oplus B_2$. By \ref{RieszIdealDirectSumPreservesOrder}, each $v_1\in D_1$ is less than $u_1'$ and each $v_2\in D_2$ is less than $u_2'$. Thus $u_1 +u_2 \leq u_1'+u_2' = u$ and so $u_1 + u_2 = u$, which means that $u\in B_1\oplus B_2$.
\end{proof}


\begin{lemma} \label{approximationInPrincipalBand}
Let $V$ be a Riesz space and $v\in V^+$. Let $B_v$ be the principal band generated by $v$. Then for all $u \in B_v \cap V^+$, we have $u = \bigvee_{n\in \N}u\wedge (nv)$.
\end{lemma}
\begin{proof}
By \ref{generatedRieszIdeals} and \ref{bandGeneratedByRieszIdeal}, there exists $D\subseteq V^+$ such that $u = \bigvee D$ and for all $d\in D$ there exists $\alpha_d\in \R$ such that $d \leq |\alpha_d| v$.

Take arbitrary $d\in D$. Then $\bigvee_{n\in \N}d\wedge (nv) = d$. In fact $d$ the maximum, since $d$ is clearly an upper bound and for all $n \geq |\alpha_d|$, we have $d\wedge (nv) = d$. By the infinite distributivity of the Riesz space, \ref{distributivityRieszSpaces}, we have
\[ u = \bigvee_{d\in D}d = \bigvee_{d\in D}\bigvee_{n\in \N}d\wedge (nv) = \bigvee_{n\in \N}\bigvee_{d\in D}d\wedge (nv) = \bigvee_{n\in \N}\big(\bigvee D\big)\wedge (nv) = \bigvee_{n\in \N}u\wedge (nv). \]
\end{proof}
\begin{proof}[Alternate proof]
We can also base a proof on \ref{principalProjectionBandCriterion}, see later.

The principal band $B_v$ is itself a Riesz subspace by \ref{RieszIdealRieszSubspace} and is a projection band in itself. Thus $u = \bigvee_{n\in \N}u\wedge (nv)$ by \ref{principalProjectionBandCriterion}, where the join is taken in $B_v$. We need to show that this is also the join in $V$. Take arbitrary $w\in \setbuilder{u\wedge (nv)}{n\in \N}^\leq \subseteq V$. Since $u\in \setbuilder{u\wedge (nv)}{n\in \N}^\leq$, we have $w\wedge u \in \setbuilder{u\wedge (nv)}{n\in \N}^\leq$. Also $w\wedge u \in B_v$ by solidity, which implies $w\wedge u \geq u$ and thus $u = w\wedge u$, so $u \leq w$. Thus $u$ is also the least of all the upper bounds in $V$.
\end{proof}

\begin{proposition} \label{latticePropertiesPrincipalBands}
Let $V$ be a Riesz space and $v,w\in V^+$. Then
\begin{enumerate}
\item $B_{v\wedge w} = B_v \cap B_w$;
\item $B_{v\vee w} = B_{v+w}$.
\end{enumerate}
\end{proposition}
Here $B_v$ is the principal band generated by $v$. We have $B_{v+w} = B_v + B_w$ if $V$ is Archimedean and $B_v, B_w$ are projection bands, see TODO.
\begin{proof}
(1) By \ref{latticePropertiesPrincipalRieszIdeals}, we have $R_{v\wedge w} \subseteq R_v$. Taking the closure into the lattice of bands gives $B_{v\wedge w} \subseteq B_v$. Similarly $B_{v\wedge w} \subseteq B_w$ and thus $B_{v\wedge w} \subseteq B_v \cap B_w$.

(2) This follows straight from $\frac{v+w}{2} \leq v\vee w \leq v\vee w + v\wedge w = v + w$ (see \ref{sumAsMeetJoin}).
\end{proof}



\subsection{Archimedean spaces}

\begin{proposition} \label{ArchimedeanBandEquivalents}
Let $V$ be a Riesz space. Then the following are equivalent:
\begin{enumerate}
\item $V$ is Archimedean;
\item if $v,w\in V$ are such that $0\leq nv\leq w$ for all $n\in \N$, then $v = 0$;
\item $B \supseteq B^{\perp_r\perp_r}$ for every band $B\subseteq V$;
\item $B = B^{\perp_r\perp_r}$ for every band $B\subseteq V$.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Leftrightarrow (2)$ Suppose $V$ is Archimedean and $v,w\in V$ are such that $0\leq nv\leq w$ for all $n\in \N$. Then $0\leq v\leq n^{-1}w$ for all $n\in \N$, so $\bigwedge_{n\in \N}n^{-1}w \geq v$. Since $V$ was assumed Archimedean, we have $0\leq v \leq \bigwedge_{n\in \N}n^{-1}w = 0$ and thus $v = 0$.

For the converse, take $v\in V$. Then $0$ is a lower bound of $\R^+\cdot v$. For any other lower bound $u\in V$, $u' = 0\vee u$ is also a lower bound. Now $nu'$ is a lower bound of $n\cdot \R^+\cdot v = \R^+\cdot v$ for all $n\in \N$. In particular $nu'\leq v$ for all $n\in \N$. Thus $u'=0$ by assumption. Since $u\vee 0 = u' = 0$, we have $u\leq 0$ and thus $0$ is the greatest lower bound.

$(2) \Rightarrow (3)$ Take arbitrary $u\in B^{\perp_r\perp_r}$ and consider the set $M_u \defeq \setbuilder{v\in B}{0\leq v \leq |u|}$. Clearly $|u|$ is an upper bound of $M_u$. If we can show that it is the supremum, then $|u|$ (and thus also $u$) is an element of $B$, by definition.

Take some other upper bound $w$ of $M_u$. We need to show that $|u|\leq w$. Set $w'\defeq w\wedge |u|$, which is an element of $B^{\perp_r\perp_r}$, since $0\leq w' \leq |u|$, $B^{\perp_r\perp_r}$ is solid and $|u|\in B^{\perp_r\perp_r}$. Also $0\leq |u| - w'$ and $|u|-w'\in B^{\perp_r\perp_r}$. If $|u|- w' = 0$, then $|u| = |u|\wedge w$, so $|u|\leq w$ and we are done.

If $0 < |u|- w'$, then there exists $z\in B$ such that $0< z < |u|-w'$ by \ref{disjointDoublePolarConstruction}. Now, for all $v\in M_u$, we have $0 < z+v \leq z+w' < \big(|u|-w'\big)+w' = |u|$. Thus $z+v\in M_u$.

In particular, $0\in M_u$, so $z+0 = z\in M_u$. Since $z\in M_u$, $z+z = 2z\in M_u$. By induction, we have $nz\in M_u$ for all $n\in\N$. Thus $nz\leq |u|$ for all $n\in \N$, which, by assumption, means that $z=0$. This is a contradiction.

$(3) \Rightarrow (4)$ Immediate.

$(4) \Rightarrow (2)$ Take $v,w\in V$ such that $0\leq nv\leq w$ for all $n\in \N$.

Let $R_v$ be the principal Riesz ideal generated by $v$ and set $A = R_v \oplus R_v^{\perp_r}$. Now
\[ A^{\perp_r} = (R_v \oplus R_v^{\perp_r})^{\perp_r} \subseteq (R_v \cup R_v^{\perp_r})^{\perp_r} = R_v^{\perp_r} \cap R_v^{\perp_r\perp_r} = \{0\}, \]
so $V = A^{\perp_r\perp_r}$. Let $B$ be the band generated by $A$. Then $V = A^{\perp_r\perp_r} \subseteq B^{\perp_r\perp_r} = B \subseteq V$, using the assumption. Thus the band generated by $A$ is $V$. 

Since $A$ is a Riesz ideal by \ref{sumRieszIdeals} and \ref{disjointPolarBand}, we have that $w = \bigvee D$ for some $D\subseteq A^+$ by \ref{bandGeneratedByRieszIdeal}.

Take arbitrary $u\in D$. Then we can write $u = u_1 + u_2$, where $u_1\in R_v$ and $u_2\in R_v^{\perp_r}$. By \ref{generatedRieszIdeals}, we have $u_1 \leq |u_1| \leq nv$ for some $n\in \N$. Thus $u_1 + v \leq (n+1)v \leq w$ and $u_1+v\in R_v$. Since $u_2\in R_v^{\perp_r}$, we have $(u_1+v)\perp_r u_2$, so
\[ u+v = (u_1+v)+u_2 = (u_1+v)\vee u_2 \leq (u_1+v)\vee u \leq w, \]
by \ref{RieszDisjointEquivalents} and the fact that $u\leq w$. Since the choice of $u\in D$ was arbitrary, we have $\bigvee (D+v) \leq w$. Thus, by \ref{vectorSumOrderHomomorphism},
\[ v+w = v+\bigvee D = \bigvee(v+D) \leq w, \]
so $v\leq 0$. Since, by assumption, $v\geq 0$, we have $v = 0$.
\end{proof}
\begin{corollary} \label{bandClosureArchimedeanSpaces}
Let $V$ be an Archimedean Riesz space. Then $(-)^{\perp_r\perp_r}$ is the Moore closure of the lattice of bands in $V$.
\end{corollary}

\begin{example}
In the non-Archimedean space $\sSet{\R^2, \leq_l}$, the band $B = \setbuilder{(0,y)}{y\in \R}$ is such that $B \subsetneq B^{\perp_r\perp_r}$.

Indeed, $B^{\perp_r} = \{\vec{0}\}$, so $B^{\perp_r\perp_r} = \R^2$.
\end{example}

\subsection{Projection bands}
\begin{definition}
Let $V$ be a Riesz space and $B\subseteq V$ a band. Then $B$ is called a \udef{projection band} if $B\oplus B^{\perp_r} = V$.
\end{definition}

\begin{lemma}
Let $V$ be a Riesz space and $B\subseteq V$ a band. Then
\begin{enumerate}
\item if $B$ is a projection band, then $B^{\perp_r}$ is a projection band;
\item if $V$ is Archimedean, then the converse also holds.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Assume $B$ is a projection band. We have $V = B\oplus B^{\perp_r} \subseteq B^{\perp_r\perp_r} + B^{\perp_r} \subseteq V$, so $B^{\perp_r} + B^{\perp_r\perp_r} = V$. The sum is direct by \ref{disjointPolarBand}.

(2) If $B^{\perp_r}$ is a projection band, then $B^{\perp_r\perp_r}$ is a projection band by (1). By \ref{ArchimedeanBandEquivalents}, $B = B^{\perp_r\perp_r}$, so $B$ is a projection band.
\end{proof}

\begin{proposition} \label{complementaryRieszIdealsProjectionBands}
Let $V$ be a Riesz space and $A,B\subseteq V$ Riesz ideals. If $A \oplus B = V$, then
\begin{enumerate}
\item $A = B^{\perp_r}$ and $B = A^{\perp_r}$;
\item both $A$ and $B$ are projection bands.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) It is enough to prove $A = B^{\perp_r}$. The other equation follows by symmetry of the statement. We prove both inclusions. Take arbitrary $a\in A$ and $b\in B$. Let $c = |a|\wedge |b|$, so $c\leq |a|, c\leq |b|$. By solidity of the ideals, $c\in A$ and $c\in B$. Since the sum is direct, we have $c = 0$. Thus $a\perp_r b$. Since $b$ was taken arbitrarily, $a\in B^{\perp_r}$ and thus $A\subseteq B^{\perp_r}$.

Now take $u\in B^{\perp_r}$. Since $V = A\oplus B$, we can write $|u| = u_1 + u_2$ for some $u_1\in A$ and $u_2\in B$. By \ref{RieszIdealDirectSumPreservesOrder}, $0\leq u_1$ and $0\leq u_2$, so $u_2 \leq |u|$. By solidity, $u_2\in B^{\perp_r}$, so $u_2\in B\cap B^{\perp_r} = \{0\}$. Thus $|u| = u_1\in A$ and $u\in A$ by solidity, so $B^{\perp_r} \subseteq A$.

(2) Immediate from (1).
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space and $A\subseteq V$ a linear subspace such that $A\oplus A^{\perp_r} = V$. Then $A = A^{\perp_r\perp_r}$.
\end{proposition}
\begin{proof}
We just need to prove that $A^{\perp_r\perp_r} \subseteq A$. Take arbitrary $v\in A^{\perp_r\perp_r}$. We can find a unique decomposition $v= v_1+v_2 \in A\oplus A^{\perp_r}$. Now $A\oplus A^{\perp_r} \subseteq A^{\perp_r\perp_r}+ A^{\perp_r}$ and the sum is direct, by \ref{disjointPolarBand}. Since $v= v_1+v_2$ is a decomposition in $A^{\perp_r\perp_r}\oplus A^{\perp_r}$, it is the unique decomposition, which is also given by $v= v+0$. Thus $v_2 = 0$ and $v_1 = v$, which means that $v\in A$.
\end{proof}

\begin{proposition} \label{projectionBandCriterion}
Let $V$ be a Riesz space and $B\subseteq V$ a band. Then $B$ is a projection band \textup{if and only if} $u_1 = \bigvee \setbuilder{v\in B}{0\leq v\leq u}$ exists for all $u\in V^+$.

Then $u_1$ is the component of $u$ in $B$, w.r.t. the decomposition $B\oplus B^{\perp_r}$.
\end{proposition}
\begin{proof}
First assume $B$ is a projection band and take $u\in V^+$. Since $B$ is a projection band, there exists a unique decomposition $u = u_1 + u_2$, where $u_1 \in B$ and $u_2 \in B^{\perp_r}$. We claim that $u_1 = \bigvee \setbuilder{v\in B}{0\leq v\leq u}$. Since $u_1 \in \setbuilder{v\in B}{0\leq v\leq u}$ it is less than any upper bound. Thus we just need to prove that it is an upper bound.

To that end, take $v\in B$ such that $0\leq v\leq u$. Then $u-v = (u_1 - v) + u_2 \in B\oplus B^{\perp_r}$. By \ref{RieszIdealDirectSumPreservesOrder}, $0\leq u_1 - v$, so $v\leq u_1$. This shows that $u_1$ is an upper bound.

Now assume $u_1 = \bigvee \setbuilder{v\in B}{0\leq v\leq u}$ exists for all $u\in V^+$. In this case $u_1\in B$ by definition of band. We need to show that $V = B\oplus B^{\perp_r}$. Since $B\oplus B^{\perp_r}$ is solid by \ref{sumRieszIdeals}, it is enough to show that $V^+ = (B\oplus B^{\perp_r})^+$.  Fix arbitrary $u\in V^+$. It is enough to show that $u_2 \defeq u - u_1 \in B^{\perp_r}$. We have $0\leq u - u_1 \leq u$.

To that end, take $b\in B$ and set $c = |b|\wedge |u_2|$. Then $c\leq |b|$, so $c\in B$ by solidity and $0\leq c \leq |u_2| = u-u_1$, so $u_1 + c \leq u_1 + u_2 = u$. This means that $u_1 + c\in \setbuilder{v\in B}{0\leq v\leq u}$ and thus
\[ u_1 + c \leq \bigvee \setbuilder{v\in B}{0\leq v\leq u} = u_1, \]
so $c\leq 0$. By construction, $0\leq c$, so $c = 0$. This means that $u_2 \perp_r b$ and thus $u_2\in B^{\perp_r}$.
\end{proof}
\begin{corollary} \label{principalProjectionBandCriterion}
Let $V$ be a Riesz space and $v\in V^+$. Let $B_v$ be the principal band generated by $v$. Then $B_v$ is a projection band \textup{if and only if} $u_1 = \bigvee_{n\in \N} u\wedge (nv)$ exists for all $u\in V^+$.

Then $u_1$ is the component of $u$ in $B_v$, w.r.t.\ the decomposition $B_v \oplus B_v^{\perp_r}$.
\end{corollary}
\begin{proof}
It is enough to show $\setbuilder{w\in B_v}{0\leq w\leq u}^\leq = \setbuilder{u\wedge (nv)}{n\in \N}^\leq$. First take an upper bound $b$ of $\setbuilder{w\in B_v}{0\leq w\leq u}$ and an arbitrary $n\in \N$. Then $u\wedge (nv)$ is an element of the princial ideal generated by $v$ by \ref{generatedRieszIdeals}, since $u\wedge (nv) \leq nv = |nv|$. Thus it is an element of $B_v$ and $u\wedge (nv)\leq b$.

Now take an upper bound $c$ of $\setbuilder{u\wedge (nv)}{n\in \N}$. Take an arbitrary $x\in \setbuilder{w\in B_v}{0\leq w\leq u}$, so in particular $x\leq u$. By \ref{bandGeneratedByRieszIdeal}, there exists a subset $D$ of the principal ideal generated by $v$ such that $x = \bigvee D$. Take an arbitrary $y\in D$. By \ref{generatedRieszIdeals}, there exists $\alpha \in \R$ such that $y\leq |y|\leq |\alpha v|$. Take $n\geq |\alpha|$, so $y \leq |nv| = nv$. Now we also have that $y\leq x \leq u$, so $y \leq u\wedge (nv)$, which means that $y \leq c$.

Since $y\in D$ was taken arbitrarily, we have $x\leq c$ and thus $c$ is an upper bound of $\setbuilder{w\in B_v}{0\leq w\leq u}$.
\end{proof}

\subsubsection{Band projectors}
\begin{definition}
Let $V$ be a Riesz space and $B \subseteq V$ a projection band. Then the projector on $B$ along $B^{\perp_r}$ is called the \udef{band projector} on $B$.
\end{definition}

\begin{proposition} \label{projectorOnProjectionBand}
Let $V$ be a Riesz space and $P: V\to V$ a function. Then the following are equivalent:
\begin{enumerate}
\item $P$ is the band projector for some projection band $B$;
\item $P$ is a projector and $0\leq Pu \leq u$ holds for all $u\in V^+$.
\end{enumerate}
\end{proposition}
Compare with \ref{projectorComplementarySubspaces}.
\begin{proof}
$(1) \Rightarrow (2)$ Linearity and idempotency are immediate from \ref{projectorComplementarySubspaces}. Take $u\in V^+$, then, by \ref{projectionBandCriterion}, $Pu = \bigvee \setbuilder{v\in B}{0\leq v\leq u} \leq u$.

$(2) \Rightarrow (1)$ Set $B = \im(P)$. By \ref{projectorComplementarySubspaces} we have $V = \im(P)\oplus \im(\id_V-P)$. By \ref{complementaryRieszIdealsProjectionBands}, it is enough to show that $B$ and $\im(\id_V-P)$ are Riesz ideals. They are clearly linear subspaces from \ref{imageSubalgebra}, so we just need to show solidity.

If $P$ satisfies the requirements of (2), then so does $\id_V-P$. The fact it is a projector was given by \ref{projectorKernelImageLemma}. For the inequalities, take $u\in V^+$. Then $0\leq Pu$ implies $u\leq Pu + u$, or $(\id_V-P)(u) = u-Pu \leq u$. Also $Pu \leq u$ implies $0\leq u - Pu = (\id_V - P)(u)$. Thus it is enough to show that $\im(P)$ is solid.

Suppose $0\leq w \leq Pv$, where $Pv\in \im(P)$. Then $x\defeq Pv - w \geq 0$, so $Pv = x+w$ and $Pv = P^2v = Px + Pw$. So $x+w = Px+Pw$, or $x-Px + w-Pw = 0$. Since $x-Px\geq 0$ and $w-Pw \geq 0$ by assumption, we have $w-Pw = 0$ by \ref{positiveElementsSumToZeroImpliesZero}. Thus $Pw = w$, or $w\in \im(P)$ by \ref{projectorKernelImageLemma}.

In order to conclude with \ref{RieszIdealEquivalents}, we need to show that for all $w\in V$, $w\in \im(P)$ implies $|w|\in \im(P)$. We have $w^+ - w^- = w =Pw = Pw^+ - Pw^-$, so $w^+ - Pw^+ + w^- - Pw^- = 0$. Since, by assumption, $w^+ - Pw^+ \geq 0$ and $w^- - Pw^- \geq 0$, we have $w^+  = Pw^+$ and $w^- = Pw^-$ by \ref{positiveElementsSumToZeroImpliesZero}. Thus $w^+, w^-\in\im(P)$, so $|w| = w^++w^- \in \im(P)$.
\end{proof}

\begin{proposition} \label{intersectionProjectionBands}
Let $V$ be an Archimedean Riesz space and $B_1,B_2\subseteq V$ projection bands with band projectors $P_1$ and $P_2$. Then
\begin{enumerate}
\item $B_1 \cap B_2$ is a projection band and $P_1P_2 = P_2P_1$ is the band projector on $B_1 \cap B_2$;
\item $P_1P_2(u) = P_1(u)\wedge P_2(u)$ for all $u\in V^+$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) We show that $P_1P_2$ is the band projector on $B_1 \cap B_2$. Since we may swap $B_1$ and $B_2$, the same argument holds for $P_2P_1$, making them equal.

Since $(B_1 \cap B_2) \cap (B_1 \cap B_2)^{\perp_r}$ by \ref{disjointPolarBand} and $B_1\cap B_2$ is a band by \ref{RieszSpaceIntersectionSystems}, the results follow from \ref{projectorEquivalentsLemma}, if we can show $P_1P_2|_{B_1 \cap B_2} = \id_{B_1 \cap B_2}$ (which is immediate), $\im(P_1P_2)\subseteq B_1 \cap B_2$ and $\im(\id_V - P_1P_2) \subseteq (B_1 \cap B_2)^{\perp_r}$.

Take $v\in V$ and write $v = v^+ - v^-$. By \ref{projectorOnProjectionBand}, we have $P_2v^+ \in V^+$ and $0 \leq P_1P_2v^+ \leq P_2v^+$. Since $P_2v^+\in B_2$, we have $P_1P_2v^+ \in B_2$ by solidity. Clearly $P_1P_2v^+ \in B_1$, so $P_1P_2v^+\in B_1\cap B_2$. Similarly $P_1P_2v^- \in B_1\cap B_2$ and thus $P_1P_2v\in B_1\cap B_2$, meaning that $\im(P_1P_2)\subseteq B_1 \cap B_2$.

Take $v\in V$. Then
\begin{align*}
v - P_1P_2v &= (v-P_2v) + (P_2v - P_1P_2v) \\
&= (\id_V - P_2)(v) + (\id_V - P_1)(P_2 v) \in B_2^{\perp_r} + B_1^{\perp} \subseteq (B_1 \cap B_2)^{\perp_r} + (B_1 \cap B_2)^{\perp_r} = (B_1 \cap B_2)^{\perp_r}.
\end{align*}

(2) Take $u\in V^+$. By \ref{projectorOnProjectionBand} we have $P_1(u), P_2(u) \in V^+$, $P_1P_2(u) \leq P_2(u)$ and $P_2P_1(u)\leq P_1(u)$. Thus $P_1P_2(u) \leq P_1(u) \wedge P_2(u)$.

For the other inequality, we note that $0 \leq P_1(u) \wedge P_2(u) \leq P_1(u)$, so $P_1(u) \wedge P_2(u) \in B_1$ by solidity. Similarly $P_1(u) \wedge P_2(u)\in B_2$, so $P_1(u) \wedge P_2(u)\in B_1\cap B_2$. By \ref{projectorOnProjectionBand}, we have $P_1(u) \wedge P_2(u) \leq u$. Thus $P_1(u) \wedge P_2(u) \in \setbuilder{v\in B_1\cap B_2}{0\leq v\leq u}$. By \ref{projectionBandCriterion}, we have $P_1P_2(u) = \bigvee\setbuilder{v\in B_1\cap B_2}{0\leq v\leq u} \geq P_1(u) \wedge P_2(u)$.
\end{proof}
\begin{corollary}
Let $V$ be an Archimedean Riesz space and $B_1,B_2\subseteq V$ projection bands with band projectors $P_1$ and $P_2$. Then the following are equivalent:
\begin{enumerate}
\item $B_1 \subseteq B_2$;
\item $P_1P_2 = P_2P_1 = P_1$;
\item $P_1(u) \leq P_2(u)$ for all $u\in V^+$.
\end{enumerate}
\end{corollary}
\begin{proof}
$(1) \Rightarrow (2)$ Immediate from the proposition.

$(2) \Rightarrow (3)$ By \ref{projectorOnProjectionBand}, we have $P_2(u) \in V^+$ and $P_1 = P_1P_2(u) \leq P_2(u)$.

$(3) \Rightarrow (1)$ Take $v\in B_1$. Then $|v| \in B_1$ by solidity. Now $|v| = P_1(|v|) \leq P_2(|v|) \in B_2$, so $|v| \in B_2$ and $v\in B_2$.
\end{proof}

\begin{proposition} \label{sumProjectionBands}
Let $V$ be an Archimedean Riesz space and $B_1,B_2\subseteq V$ projection bands with projectors $P_1$ and $P_2$. Then
\begin{enumerate}
\item $(B_1 \cup B_2)^{\perp_r\perp_r}$ is a projection band with band projector $P_1 + P_2 - P_1P_2$;
\item $B_1 + B_2 = (B_1 \cup B_2)^{\perp_r\perp_r}$;
\item $P_1 + P_2 - P_1P_2 = P_1(u)\vee P_2(u)$ for all $u\in V^+$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) By \ref{intersectionProjectionBands}, we have that $B_1^{\perp_r}\cap B_2^{\perp_r}$ is a projection band with band projector
\[ (\id_V - P_1)(\id_V - P_2) = \id_V - P_1 - P_2 + P_1P_2. \]
Therefore, $(B_1^{\perp_r}\cap B_2^{\perp_r})^{\perp_r}$ is a projection band with band projector $P_1 + P_2 - P_1P_2$. By \ref{polarOfUnion} we have $(B_1^{\perp_r}\cap B_2^{\perp_r})^{\perp_r} = (B_1 \cup B_2)^{\perp_r\perp_r}$.

(2) We have
\[ B_1 + B_2 = \Span(B_1 \cup B_2) \subseteq (B_1 \cup B_2)^{\perp_r\perp_r}, \]
since $(B_1 \cup B_2)^{\perp_r\perp_r}$ is a band (and thus a subspace) by \ref{disjointPolarBand}. Conversely, for all $v\in (B_1 \cup B_2)^{\perp_r\perp_r}$, we have $v = P_1(v) + P_2(v) - P_1P_2(v) \in B_1 + B_2$, so $(B_1^{\perp_r}\cap B_2^{\perp_r})^{\perp_r} \subseteq B_1 + B_2$.

(3) We calculate, using \ref{intersectionProjectionBands} and \ref{sumAsMeetJoin},
\begin{align*}
(P_1 + P_2 - P_1P_2)(u) &= P_1(u) + P_2(u) - P_1(u)\wedge P_2(u) \\
&= P_1(u)\vee P_2(u).
\end{align*}
\end{proof}

\begin{definition}
Let $V$ be a Riesz space and $v\in V$. Then $v$ is called a \udef{projection element} if $B_v$ is a projection band.
\end{definition}

\begin{proposition}
Let $V$ be an Archimedean Riesz space and $v,w\in V^+$ are projection elements. Then
\begin{enumerate}
\item $B_{v\wedge w} = B_v \cap B_w$ and $P_{v\wedge w} = P_vP_w = P_wP_v$;
\item $B_{v\vee w} = B_{v+w} = B_v + B_w$ and $P_{v\vee w} = P_{v+w} = P_u+P_v - P_uP_v$;
\item $v\wedge w$, $v\vee w$ and $v+w$ are projection elements;
\item the following are equivalent:
\begin{enumerate}
\item $v$ and $w$ are disjoint;
\item $P_vP_w = 0$;
\item $P_wP_v = 0$;
\item $P_{v+w} = P_v + P_w$.
\end{enumerate}
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Immediate from \ref{latticePropertiesPrincipalBands} and \ref{intersectionProjectionBands}.

(2) We have $B_{v\vee w} = B_{v+w}$ by \ref{latticePropertiesPrincipalBands}. From \ref{latticePropertiesPrincipalRieszIdeals}, we have $R_{v} \subseteq R_{v+w}$. Taking the closure into the lattice of bands gives $B_{v} \subseteq B_{v+w}$. Similarly $B_w \subseteq B_{v+w}$, so $B_v + B_w \subseteq B_{v+w} + B_{v+w} = B_{v+w}$.

Conversely, $B_v+ B_w$ is a projection band by \ref{sumProjectionBands} and it contains $v+w$, so $B_{v+w} \subseteq B_v + B_w$.

The statements about the band projectors are given by \ref{sumProjectionBands}.

(3) This follows from (1) and (2) and the fact that $B_v \cap B_w$ and $B_v + B_w$ are projection bands.

(4) For the equivalences, it is enough to note that $B_v = \{0\}$ iff $v = 0$ iff $P_v = 0$.
\end{proof}

\subsubsection{The projection property}
\begin{definition}
Let $V$ be a Riesz space. We say that
\begin{itemize}
\item $V$ has the \udef{projection property} if every band in $V$ is a projection band;
\item $V$ has the \udef{principal projection property} if element in $V$ is a projection element.
\end{itemize}
\end{definition}

\begin{proposition}
Let $V$ be a Riesz space. Then the following implications hold:

\[ \begin{tikzcd}[column sep=0]
& \text{$V$ is Dedekind complete} \ar[dl, Rightarrow, end anchor={[xshift=2em]north east}] \ar[dr, Rightarrow, end anchor={[xshift = -2em]north west}] & \\
\text{$V$ is Dedekind $\sigma$-complete} \hspace{-7em} \ar[dr, Rightarrow, start anchor={[xshift=2em]south east}] && \hspace{-7em} \text{$V$ has the projection property} \ar[dl, Rightarrow, start anchor={[xshift=-2em]south west}] \\
& \text{$V$ has the principal projection property} \ar[d, Rightarrow] & \\
& \text{$V$ is Archimedean.} &
\end{tikzcd} \]
\end{proposition}
\begin{proof}
(Principal projection) $\Rightarrow$ (Archimedean) Suppose $u,v\in V^+$ are such that $0\leq nv \leq u$ for all $n\in \N$. Thus $u\wedge (nv) = nv$. By \ref{principalProjectionBandCriterion}, the principal projection property implies that $u_1 = \bigvee_{n\in \N} u\wedge (nv) = \bigvee_{n\in \N}nv$ exists.

By \ref{vectorSumOrderHomomorphism}, we have $v+ u_1 = v+\bigvee_{n\in \N}nv = \bigvee_{n\in \N}(1+n)v = u_1$, so $v = 0$.

(Dedekind $\sigma$-completeness) $\Rightarrow$ (Principal projection property) Take $v, u\in V^+$. By \ref{principalProjectionBandCriterion}, we need to show that $\bigvee_{n\in\N}u\wedge (nv)$ exists. Clearly the set $\setbuilder{u\wedge (nv)}{n\in \N}$ has an upper bound $u$ and is countable, thus it has a supremum by Dedekind $\sigma$-completeness.

(Dedekind completeness) $\Rightarrow$ (Projection property) Let $B$ be a band in $V$ and consider $C \defeq B \oplus B^{\perp_r}$. This is a band by \ref{DedekindCompleteDirectSumBands}. Now we can calculate
\[ \{0\} \subseteq C^{\perp_r} = (B\oplus B^{\perp_r})^{\perp_r} \subseteq (B\cup B^{\perp_r})^{\perp_r} \subseteq B^{\perp_r} \cap B^{\perp_r\perp_r} \subseteq \{0\}. \]
Thus $C^{\perp_r} = \{0\}$ and $C^{\perp_r\perp_r} = V$. Now we have already proved that Dedekind completeness implies that $V$ is Archimedean, so \ref{ArchimedeanBandEquivalents} gives that $C= C^{\perp_r\perp_r} = V$ and $B$ is a projection band.
\end{proof}

Principal projection + uniform completeness imply Dedekind completeness, see \url{https://core.ac.uk/download/pdf/82353250.pdf}.

\subsubsection{The principal projection property}

\begin{theorem}[Freudenthal's spectral theorem]
Let $V$ be a Riesz space with the principal projection property, $0 < e \in V$ and $R_e$ be the principal Riesz ideal generated by $e$. Then, for all $x\in R_e$, there exist sequences $\seq{s_n}$ and $\seq{t_n}$ of $e$-step functions such that
\end{theorem}

\subsection{Components}
\begin{definition}
Let $V$ be a Riesz space and $u\in V^+$. Then $p\in V$ is called a \udef{component} of $u$ if $p\wedge (u-p) = 0$. The set of components of $u$ is denoted $C_u$.
\end{definition}

\begin{lemma} \label{RieszComponentsLemma}
Let $V$ be a Riesz space, $u\in V^+$ and $p\in V$. Then
\begin{enumerate}
\item $0$ and $u$ are components of $u$;
\item $p$ is a component of $u$ \textup{if and only if} $u-p$ is a component of $u$;
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Immediate.

(2) We have $p\wedge (u-p) = \big(u - (u-p)\big)\wedge (u-p)$.
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space and $u\in V^+$. Then the set $C_u$ of components of $u$
\begin{enumerate}
\item has top $u$ and bottom $0$;
\item is a sublattice of $V$;
\item has a complement $\overline{p} = u-p$ for all $p\in C_u$.
\end{enumerate}
In particular $C_u$ is a Boolean lattice, since $V$ is distributive.
\end{proposition}
\begin{proof}
(1) Let $p\in C_u$. We have $0 = p\wedge (u-p) \leq p$, so $p\in V^+$.

Since $0 = p\wedge (u-p)$, \ref{lemmaRieszSpaces} gives $0 = (-p)\vee (p-u)$, so $u = u+(-p)\vee (p-u) = (u-p)\vee p \geq p$.

(2) Let $p,q\in V$ be components of $u$. By (1) we have $p\leq u$ and $q\leq u$, so $p\vee q \leq u$. Thus, by \ref{lemmaRieszSpaces},
\[ 0 \leq u - (p\vee q) = u + (-p)\wedge(-q) = (u-p)\wedge (u-q). \]
We then have
\begin{align*}
(p\vee q)\wedge \big(u - p\vee q\big) &= (p\vee q)\wedge \big((u-p)\wedge (u-q)\big) \\
&= \big(p \wedge (u-p)\wedge (u-q)\big)\vee \big(q\wedge (u-p)\wedge (u-q)\big) \\
&= 0\vee 0 = 0,
\end{align*}
since $p \wedge (u-p) = 0$ and $q \wedge (u-q) = 0$. This shows that $p\vee q\in C_u$ and thus that $C_u$ is closed under joins.

We have $u - (p\wedge q) = (u-p)\vee (u-q)$. Since $u-p$ and $u-q$ are components, by \ref{RieszComponentsLemma}, and $C_u$ is closed under joins, we have that $u - (p\wedge q)$ is a component and thus that $p\wedge q$ is a component, by \ref{RieszComponentsLemma}.

(3) We have $p\wedge \overline{p} = p\wedge (u-p) = 0$ by definition. We have $p\vee \overline{p} = p\vee (u-p) = u$ by the calculation in (1).
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space with the principal projection property and $u\in V^+$. Then $p\in C_u$ \textup{if and only if} $p = P_v(u)$ for some $v\in V$.
\end{proposition}
\begin{proof}
First assume $p\in C_u$. Clearly $p\in B_p$. Also $p\perp_r(e-p)$, so $e-p \in \{p\}^{\perp_r} = \{p\}^{\perp_r\perp_r\perp_r} = B_p^{\perp_r}$ by \ref{bandClosureArchimedeanSpaces}. Thus $e = p + (e-p) \in B_p\oplus B_p^{\perp_r}$ and $p = P_p(u)$.

Now assume $p = P_v(u)$ for some $v\in V$. Since $P_v$ is the band projector of a projection band, we have $p = P_v(u) \in B_v$ and $u-p = (\id_V- P_v)(u) \in B_v^{\perp_r}$. Thus $p$ and $u-p$ are disjoint, meaning $0 = |p|\wedge |u-p| = u\wedge (u-p)$.
\end{proof}

\section{Order convergence in ordered vector spaces}

\begin{proposition} \label{continuityOrderConvergenceOrderedVectorSpaces}
Let $\sSet{V, \leq}$ be a partially ordered vector space equipped with the order convergence. Then
\begin{enumerate}
\item vector addition is continuous;
\item for all $\lambda\in \R$, the scalar multiplication $V\to V: v \mapsto \lambda v$ is continuous;
\item if $V$ is Archimedean, then the restriction $\cdot: \R^+ \times V^+ \to V^+$ of scalar multiplication to positive scalars and vectors is continuous.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Suppose $F\overset{\mathfrak{o}}{\longrightarrow} x$ and $G\overset{\mathfrak{o}}{\longrightarrow} y$ in $V$. Then we can find $M_F, N_F, M_G, N_G$ according to \ref{orderConvergenceIntervalBase} such that $\bigvee M_F = x = \bigwedge N_F$ and $\bigvee M_G = y = \bigwedge N_G$.

Now $\bigvee M_F + M_G = \bigvee M_F + \bigvee M_G = x+y$ by \ref{extremaVectorSum}. Similarly $\bigwedge N_F + N_G = \bigwedge N_F + \bigwedge N_G = x+y$.

We want to apply \ref{orderConvergenceIntervalBase} to conclude $F+G \overset{\mathfrak{o}}{\longrightarrow} x + y$. To that end, take $a_F + a_G \in M_F+ M_G$ and $b_F + b_G \in N_F + N_G$.
We have $\interval{a_F, b_F}\in F$ and $\interval{a_G, b_G} \in G$. Since $\interval{a_F, b_F} + \interval{a_G, b_G} \subseteq \interval{a_F + a_G, b_F + b_G}$, by \ref{intervalSumInclusion}, we have $\interval{a_F + a_G, b_F + b_G} \in F+G$.

(2) If $\lambda = 0$, then the scalar multiplication is a constant function and thus constant. If $\lambda > 0$, then the scalar multiplication is an order similarity and thus continuous.

If $\lambda < 0$, then the scalar multiplication is an order similarity to $V^o$ (with the opposite order). The order convergence on $V^o$ is the same as that on $V$.

(3) Assume $V$ is Archimedean. Suppose $F\overset{\R}{\longrightarrow} \lambda$ in $\R^+$ and $G\overset{\mathfrak{o}}{\longrightarrow} x$ in $V^+$. We note that the convergence on $\R^+$ is the order convergence, see \ref{realConvergenceOrderConvergence}. Then we can find $M_F, N_F, M_G, N_G$ according to \ref{orderConvergenceIntervalBase} such that $\bigvee M_F = \lambda = \bigwedge N_F$ and $\bigvee M_G = x = \bigwedge N_G$.

Now $\bigvee M_F \cdot M_G = \bigvee M_F \cdot \bigvee M_G = \lambda x$ by \ref{extremaScalarProduct}. Similarly $\bigwedge N_F \cdot N_G = \bigwedge N_F \cdot \bigwedge N_G = \lambda x$.

We want to apply \ref{orderConvergenceIntervalBase} to conclude $F\cdot G \overset{\mathfrak{o}}{\longrightarrow} \lambda x$. To that end, take $\mu \cdot a \in M_F \cdot M_G$ and $\nu b \in N_F \cdot N_G$.
We have $\interval{\mu, \nu}\in F$ and $\interval{a, b} \in G$. Since $\interval{\mu, \nu} \cdot \interval{a, b} \subseteq \interval{\mu a, \nu b}$, by \ref{intervalScalarMultiplicationInclusion}, we have $\interval{\mu a, \nu b} \in F\cdot G$.
\end{proof}

\begin{lemma} \label{orderConvergencePositiveFilterLemma}
Let $\sSet{V, \leq}$ be a partially ordered vector space equipped with the order convergence. Let $F, G\in\powerfilters(V^+)$ be proper filters of positive vectors. Then
\begin{enumerate}
\item if $G \overset{\mathfrak{o}}{\longrightarrow} 0$ and for all $B\in G$, there exists $A\in F$ such that $A\subseteq \downset B$, then $F \overset{\mathfrak{o}}{\longrightarrow} 0$;
\item $F+G \overset{\mathfrak{o}}{\longrightarrow} 0$ \textup{if and only if} $F \overset{\mathfrak{o}}{\longrightarrow} 0$ and $G \overset{\mathfrak{o}}{\longrightarrow} 0$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) By \ref{orderConvergenceIntervalBase}, there exists $N$ such that $\bigwedge N = 0$ and $\interval{0,b} \in G$ for all $b\in N$.

Suppose $B\in G$ such that $B\subseteq \interval{0,b}$. Then there exists $A$ such that $A\subseteq \downset B$. Since $A\subseteq V^+$, we have $A \subseteq \downset B \cap V^+ \subseteq \interval{0,b}$. We have $F \overset{\mathfrak{o}}{\longrightarrow} 0$ by \ref{orderConvergenceIntervalBase}.

(2) The direction $\Leftarrow$ is given by \ref{continuityOrderConvergenceOrderedVectorSpaces}.

For the direction $\Rightarrow$, we use point (1). Take arbitrary $A+B\in F + G$. Then $A\subseteq \downset (A+B)$ and so $F \overset{\mathfrak{o}}{\longrightarrow} 0$. The argument for $G \overset{\mathfrak{o}}{\longrightarrow} 0$ is similar.
\end{proof}

\subsection{Order convergence in Riesz spaces}

\begin{lemma} \label{RieszSpaceOperationsOrderContinuous}
Let $V$ be a Riesz space equipped with the order convergence. Then
\begin{enumerate}
\item the absolute value function $|-|$ is continuous;
\item the lattice operations $\wedge: V\times V \to V$ and $\vee: V\times V \to V$ are continuous.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) Since $|v| = v^+ + v^- = v\vee 0 + (-v)\vee 0$ for all $v\in V$, the result follows from \ref{continuityOrderConvergenceOrderedVectorSpaces}, \ref{singleArgumentMeetJoinOrderContinuous} and \ref{distributivityRieszSpaces}.

(2) Since, for all $v,w\in V$, we have $v\vee w = \frac{1}{2}(v+w + |v-w|)$ and $v\wedge w = \frac{1}{2}(v+w - |v-w|)$, by \ref{absoluteValueMeetJoin}, the continuity of the lattice operations follows from (1) and \ref{continuityOrderConvergenceOrderedVectorSpaces}.
\end{proof}

\begin{lemma} \label{orderConvergenceInRieszSpaceLemma}
Let $V$ be a Riesz space, $F\in\powerfilters(V)$ and $v\in V$. Then $F\overset{\mathfrak{o}}{\longrightarrow} v$ \textup{if and only if} $|F-v|\overset{\mathfrak{o}}{\longrightarrow} 0$.
\end{lemma}
\begin{proof}
The direction $\Rightarrow$ follows from \ref{RieszSpaceOperationsOrderContinuous}.

Now assume $|F-v|\overset{\mathfrak{o}}{\longrightarrow} 0$. By \ref{orderConvergenceIntervalBase} there exists $N$ such that $\bigwedge N = 0$ (thus $N\subseteq V^+$) and for all $b\in N$, we have $\interval{0,b} \in |F-v|$.

Now, by \ref{scalarMultipleUpperLowerBounds}, we have $\bigvee -N = -\bigwedge N = -0 = 0$. Take arbitrary $a, b\in N$. Then there exist $A, B\in F-v$ such that $|A| \subseteq \interval{0, a}$ and $|B|\subseteq \interval{0,b}$. We have
\[ |A\cap B| \subseteq |A|\cap |B| \subseteq \interval{0,a}\cap \interval{0,b} \subseteq \interval{0,a\wedge b} \]
by \ref{orderPreservingFunctionLatticeOperations} and \ref{unionIntersectionIntervals}. Then $A\cap B \subseteq \interval{-(a\wedge b),a\wedge b} \subseteq \interval{-a, b}$ by \ref{solidLemma}. This implies $\interval{-a,b} \in F-v$. We conclude that $F-v\overset{\mathfrak{o}}{\longrightarrow} 0$ by \ref{orderConvergenceIntervalBase} and thus that $F\overset{\mathfrak{o}}{\longrightarrow} v$ by \ref{continuityOrderConvergenceOrderedVectorSpaces}.
\end{proof}

\begin{proposition}
Let $V$ be an Archimedean Riesz space. Then the order convergence on $V$ is a vector space convergence.
\end{proposition}
\begin{proof}
We have that vector addition is continuous by \ref{continuityOrderConvergenceOrderedVectorSpaces}.

We just need to show that scalar multiplication is continuous. Let $F\to v$ and $H \to \lambda\in \R$. Then, by \ref{orderConvergenceInRieszSpaceLemma}, we just need to show that $|H\cdot F - \lambda v|\overset{\mathfrak{o}}{\longrightarrow} 0$.

By \ref{continuityOrderConvergenceOrderedVectorSpaces}, we have $|H\cdot F - H\cdot v| \supseteq |H|\cdot |F-v| \overset{\mathfrak{o}}{\longrightarrow} 0$, $|H\cdot v - \lambda v| = |H-\lambda|\cdot |v| \overset{\mathfrak{o}}{\longrightarrow} 0$ and thus $|H\cdot F - H\cdot v| + |H\cdot v - \lambda v| \overset{\mathfrak{o}}{\longrightarrow} 0$.

Take arbitrary $|B\cdot A - B\cdot v| + |B\cdot v - \lambda v| \in |H\cdot F - H\cdot v| + |H\cdot v - \lambda v|$ and arbitrary $|ba - \lambda v| \in |B\cdot A - \lambda v|$. Then
\[ |ba - \lambda v| \leq |ba - bv + bv - \lambda v| \leq |ba - bv| + |bv - \lambda v| \in |B\cdot A - B\cdot v| + |B\cdot v - \lambda v| \]
by the triangle inequality \ref{triangleInequalityRieszSpaces}. Thus $|B\cdot A - \lambda v| \subseteq \downset |B\cdot A - B\cdot v| + |B\cdot v - \lambda v|$ and $|H\cdot F - \lambda v| \overset{\mathfrak{o}}{\longrightarrow} 0$ by \ref{orderConvergencePositiveFilterLemma}. This means that $H\cdot F \to \lambda v$.
\end{proof}

\begin{example}
Consider the Riesz space $\R^\R$ of real functions. Then the order convergence is just the pointwise convergence.

TODO: prove
\end{example}

\subsubsection{Uniform Riesz convergence}
\begin{definition}
Let $V$ be a Riesz space and $v\in V^+$. Let $F\in \powerfilters(V)$ and $x\in V$. Then we say $F$ converges \udef{$v$-uniformly} to $x$ if $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq |F - x|$. We write $F\overset{\text{$v$-un}}{\longrightarrow} x$.
\end{definition}

\begin{lemma} \label{uniformRieszConvergenceAbsoluteValueConvergenceLemma}
Let $V$ be a Riesz space and $v\in V^+$. Let $F\in \powerfilters(V)$ and $x\in V$. Then $F\overset{\text{$v$-un}}{\longrightarrow} x$ \textup{if and only if} $|F-x|\overset{\text{$v$-un}}{\longrightarrow} 0$.
\end{lemma}
\begin{proof}
This follows immediately from $|F-x| = \big||F-x|-0\big|$.
\end{proof}

\begin{lemma} \label{filterGreaterThanRieszUniformNeighbourhoodLemma}
Let $V$ be a Riesz space and $F,G\in\powerfilters(V^+)$. Suppose $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq G$ and for all $B\in G$ there exists $A\in F$ such that $A\subseteq \downset B$, then $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq F$.
\end{lemma}
\begin{proof}
Take $\interval{0,\epsilon} \cdot \interval{0,v} \in \neighbourhood_{\R^+}(0)\cdot \interval{0,v}$. Then there exists $B\in G$ such that $B\subseteq \interval{0,\epsilon} \cdot \interval{0,v}$. Then $\downset B \cap V^+ \subseteq \interval{0,\epsilon} \cdot \interval{0,v}$. By assumption, there exists $A\in F$ such that $A\subseteq \downset B$. Since $A\subseteq V^+$, we have $A\subseteq \downset B \cap V^+ \subseteq \interval{0,\epsilon} \cdot \interval{0,v}$. Thus $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq F$.
\end{proof}

\begin{lemma}
Let $V$ be an Archimedean Riesz space and $v\in V^+$. Then $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \overset{\mathfrak{o}}{\longrightarrow} 0$.
\end{lemma}
\begin{proof}
We can set $N \defeq \setbuilder{\epsilon v}{\epsilon > 0}$. Then $\bigwedge N = 0$, since $V$ is Archimedean. We conclude by \ref{orderConvergenceIntervalBase}.
\end{proof}
\begin{corollary}
Let $V$ be an Archimedean Riesz space and $v\in V^+$. Then the $v$-uniform convergence is stronger than the order convergence.
\end{corollary}
\begin{proof}
Let $F\in \powerfilters(V)$ be a filter that converges to $x\in V$. Then $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq |F - x|$. By the lemma, this means that $|F-x|$ order-converges to $0$ and $F$ converges to $x$ by \ref{orderConvergenceInRieszSpaceLemma}.
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space and $v\in V^+$. Equip $V$ with the $v$-uniform convergence. Then
\begin{enumerate}
\item the vector addition $+: V\times V\to V$ is continuous, i.e.\ the $v$-uniform convergence is a group convergence;
\item for all $\lambda \in R$, the scalar multiplication $V\to V: x\mapsto \lambda x$ is continuous;
\item the lattice operations $\wedge: V\times V\to V$ and $\vee: V\times V\to V$ are continuous;
\item the absolute value function $|-|: V\to V$ is continuous.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Since $\sSet{V,+,0}$ is a commutative group, we can use \ref{groupConvergenceConstruction}. Points (1) and (2) are immediate. For point (3), we need to prove that for all $F,G\in \powerfilters(V)$, we have that $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq |F|$ and $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq |G|$ implies $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq |F - G|$. Indeed, take arbitrary $\interval{0,\epsilon}\cdot \interval{0,v}$. We claim
\[ \big|\interval{0,\epsilon/2}\cdot \interval{0,v} - \interval{0,\epsilon/2}\cdot \interval{0,v}\big| \subseteq \interval{0,\epsilon}\cdot \interval{0,v}. \]
To show this, take arbitrary $|\lambda u - \mu w|\in \big|\interval{0,\epsilon/2}\cdot \interval{0,v} - \interval{0,\epsilon/2}\cdot \interval{0,v}\big|$. We have
\begin{align*}
0 \leq |\lambda u - \mu w| &\leq \lambda u + \mu w \\
&\leq (\lambda \vee \mu) (u + w) \\
&\leq (\epsilon / 2) (2v) = \epsilon v.
\end{align*}
This completes the proof.

(2) Take $\lambda\in \R$. If $\lambda = 0$, then the scalar multiplication is a constant function, which is continuous. Now suppose $\lambda \neq 0$. Take $F\overset{\text{$v$-un}}{\longrightarrow} x$. Now we have $\neighbourhood_{\R^+}(0)\cdot \interval{0,v} = |\lambda|\cdot\neighbourhood_{\R^+}(0)\cdot \interval{0,v} \subseteq |\lambda|\cdot|F - x| = |\lambda F - \lambda x|$, so $\lambda F\overset{\text{$v$-un}}{\longrightarrow} \lambda x$.

(3) Take $F\overset{\text{$v$-un}}{\longrightarrow} x$ and $G\overset{\text{$v$-un}}{\longrightarrow} y$. Take arbitrary $A\in F$, $B\in G$, $a\in A$ and $b\in B$. By \ref{BirkhoffInequalities}, we have
\[ |a\vee b - x\vee y| \leq |a-x| + |b-y|, \]
so $|A\vee B - x\vee y| \subseteq \downset \big(|A-x| + |B-y|\big)$. Since $|F-x| \overset{\text{$v$-un}}{\longrightarrow} 0$ and $|G - y| \overset{\text{$v$-un}}{\longrightarrow} 0$, by \ref{uniformRieszConvergenceAbsoluteValueConvergenceLemma}, we have $|F-x| + |G-y|\overset{\text{$v$-un}}{\longrightarrow} 0$ by point (1). By \ref{filterGreaterThanRieszUniformNeighbourhoodLemma}, we have $|F\vee G - x\vee y| \overset{\text{$v$-un}}{\longrightarrow} 0$ and so $F\vee G \overset{\text{$v$-un}}{\longrightarrow} x\vee y$ by \ref{uniformRieszConvergenceAbsoluteValueConvergenceLemma}.

The argument for $\wedge$ is similar.

(4) Since $|v| = v\vee (-v)$, this follows from (2) and (3).
\end{proof}

\subsection{The Daniell integral}
\begin{definition}
Let $V$ be a Riesz space and $E\subseteq V$ a Riesz subspace. A function $f: E\to \R$ is called a \udef{Daniell functional} if it is
\begin{itemize}
\item linear;
\item positive;
\item Riesz-continuous.
\end{itemize}
\end{definition}
TODO: $E$ just vector subspace??

The positivity of $f$ is equivalent to it being order-preserving, see \ref{positiveLinearFunctionIsotone}.

\begin{lemma}
Let $V$ be a Riesz space, $E\subseteq V$ a Riesz subspace and $f: E\to \R$ a positive linear functional. Then the following are equivalent:
\begin{enumerate}
\item $f$ is a Daniell functional;
\item $f$ is Riesz-continuous at $0$;
\item $f$ is order-continuous at $0$;
\item $f$ is order-continuous when restricted to $E^+$;
\item $f$ is order-continuous;
\item $f$ is continuous when $V$ is equipped with the isotonic convergence;
\item $f$ is continuous when $V$ is equipped with the antitonic convergence;
\item $f$ is continuous when $V$ is equipped with the monotonic convergence;
\end{enumerate}
\end{lemma}


\subsection{Riesz space convergence}


\subsubsection{Monotone filters}
\begin{definition}
Let $V$ be a Riesz space and $F$ a filter on $V$ that Riesz-converges to $x$. We say $F$
\begin{enumerate}
\item \udef{converges isotonically} if $F \leq_s\pfilter{x}$;
\item \udef{converges antitonically} if $F \geq_s\pfilter{x}$;
\item \udef{converges monotonically} if $F \leq_s\pfilter{x}$ or $F \geq_s\pfilter{x}$.
\end{enumerate}
\end{definition}

\begin{lemma}
Let $V$ be a Riesz space. Then isotonic, antitonic and monotonic convergence on $V$ are convergences.
\end{lemma}
\begin{proof}
We first note that isotonic convergence is centered, since $\pfilter{v} \leq_s \pfilter{v}$ for all $v\in V$.

Next, suppose $F$ converges isotonically to $v$ and $F\subseteq G$, then there exists $A\in F$ such that $v \in A^\leq$. Since $A\in G$, we have $G\leq_s \pfilter{x}$.

We have a similar argument for antitonic convergence. For monotonic convergence, we use \ref{latticeConvergences}.
\end{proof}

\subsection{Riesz order uniformity}
\begin{definition}
Let $V$ be a Riesz space, $v\in V^+$ and $\epsilon > 0$. Set $V_\epsilon = \setbuilder{(x,y)\in V^2}{|x-y| \leq \epsilon v}$.

The \udef{$v$-order uniformity} is the topological uniformity with entourage filter $\entourage_v \defeq \upset \{V_\epsilon\}_{\epsilon > 0}$.
\end{definition}

\begin{lemma}
Let $V$ be a Riesz space and $v\in V^+$. Then $\entourage_v$ is a topological uniformity.
\end{lemma}
\begin{proof}
TODO
\end{proof}

\begin{proposition}
Let $V$ be an Archimedean Riesz space and $v\in V$. Then $\entourage_\mathfrak{o} \subseteq \entourage_v$.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\subsection{Riesz order uniform convergence}
TODO: a filter converges $v$-uniformly to the origin iff greater than $\neighbourhood_\R(0)\cdot \cball(0,v)$.

\begin{proposition}
Let $V$ be a Riesz space. Then $V$ is Archimedean \textup{if and only if} for all $v\in V^+$, the $v$-uniform convergence is Hausdorff.
\end{proposition}
\begin{proof}
TODO
\end{proof}

\begin{lemma}
Let $V$ be an Archimedean Riesz space, $F\in\powerfilters(V)$, $x,y\in V$ and $u,v,w\in V^+$. Suppose that $F$ converges $v$-uniformly to $x$ and $w$-uniformly to $y$. Then
\begin{enumerate}
\item $x = y$;
\item $F$ converges $v \vee w$-uniformly;
\item $F$ converges $u$-uniformly if $u\leq v$.
\end{enumerate}
\end{lemma}
\begin{proof}

\end{proof}

\subsection{Relative order uniform convergence}
\begin{definition}
Let $V$ be a Riesz space, $x\in V$ and $F\in\powerfilters(V)$. Then $F$ is said to converge \udef{relatively uniformly} to $x$, denoted $F\overset{un}{\longrightarrow} x$ if there exists $v\in V^+$ such that $F$ converges $v$-uniformly to $x$. This $v$ is called a \udef{regulator} of $F$.
\end{definition}


\chapter{Some results and applications}
\section{Rotations}
Rodrigues' rotation formula

eigenvectors and eigenvalues of rotation.
\section{Pauli matrices}

\[ \sigma_x = \begin{pmatrix}
0 & 1 \\ 1 & 0
\end{pmatrix} \qquad \sigma_y = \begin{pmatrix}
0 & -i \\ i & 0
\end{pmatrix} \qquad \sigma_z = \begin{pmatrix}
1 & 0 \\ 0 & -1
\end{pmatrix} \]
All have eigenvalues $\pm 1$. The eigenspaces are spanned by
\[ v_{x+} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ 1
\end{pmatrix}, \quad v_{x-} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ -1
\end{pmatrix}, \quad v_{y+} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ i
\end{pmatrix}, \quad v_{y-} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ -i
\end{pmatrix}, \quad v_{z+} = \begin{pmatrix}
1 \\ 0
\end{pmatrix}, \quad v_{z-} = \begin{pmatrix}
0 \\ 1
\end{pmatrix}, \quad  \]

\[ \Tr[\sigma_i \sigma_j] = \delta_{ij} \]

