\chapter{Vector spaces}

Gauss-Jordan reduction

TODO projective transformations

orientation
\url{https://en.wikipedia.org/wiki/Orientation_(vector_space)}
also for fixed set of $n$ vectors

\url{http://www.physics.rutgers.edu/~gmoore/618Spring2018/GTLect2-LinearAlgebra-2018.pdf}

\section{Formal definition}
A vector space is a collection of vectors, which are objects that have a natural addition and scalar multiplication.
\begin{definition}
A \udef{vector space} over a field $\mathbb{F}$ is a set $V$ together with an \udef{addition}
\[ +: V\times V \to V \]
and a \udef{scalar multiplication}
\[ \cdot: \mathbb{F}\times V \to V \]
such that $(V,+)$ is a commutative group and the following properties hold:
\begin{itemize}[leftmargin=4cm]
\item[\textbf{Distributivity 1}] $\lambda\cdot(v+w) = \lambda v + \lambda w$ for all $\lambda \in \mathbb{F}$ and all $v,w \in V$.
\item[\textbf{Distributivity 2}] $(\lambda_1+\lambda_2)\cdot v = \lambda_1 v + \lambda_2 v$ for all $\lambda_1, \lambda_2 \in \mathbb{F}$ and all $v \in V$.
\item[\textbf{Mixed associativity}] $\lambda_1\cdot(\lambda_2\cdot v) = (\lambda_1 \lambda_2) \cdot v$ for all $\lambda_1, \lambda_2 \in \mathbb{F}$ and all $v \in V$.
\item[\textbf{Multiplicative identity}] $1\cdot v = v$ for all $v \in V$.
\end{itemize}
This vector space can be denoted $\sSet{\mathbb{F}, V, +}$.
\end{definition}
In the definition we have used the following convention: for all $v,w\in V$ and $\lambda\in \mathbb{F}$, we denote $+(v,w)$ as $v+w$ and $\cdot(\lambda, v)$ as $\lambda \cdot v$ or $\lambda v$.

We call the elements of the field \udef{scalars} and the elements of the set $V$ \udef{vectors}. The zero of the group is known as the \udef{zero vector}.

Almost always we will actually be interested in $\mathbb{F} = \R$ or $\mathbb{F} = \C$.
\subsection{Examples}
\begin{enumerate}
\item The $n$-tuples in $\mathbb{F}^n$ with pointwise addition and multiplication. If the entries of the $n$-tuples are written one above the other in a column, it is called a \udef{column vector}.
\item The polynomials in $\mathbb{F}[X]$.
\item The polynomials in $\mathbb{F}[X]_{\leq n}$ of maximally degree $n$.
\item For any set $S$, the functions $(S\to \mathbb{F})$, denoted $\mathbb{F}^S$, with pointwise addition and multiplication.
\item For any topological space $X$, the continuous functions in $(X\to \C)$, denoted $\cont(X)$.
\item The trivial vector space $\{ 0\}$. A vector space can never be empty, because a commutative group always has a neutral element.
\item The set of all possible \textit{displacements} in (Euclidean) space forms a vector space. Once we have chosen an origin, we can view space as a vector space.
\end{enumerate}
\subsection{Some elementary lemmas}
\begin{lemma}
Given the vector space $(\mathbb{F}, V, +)$  and arbitrary $u,v,w\in V$ and $\lambda \in \mathbb{F}$, we have
\begin{enumerate}
\item $0v = 0 = \lambda \cdot 0$;
\item $(-1)v = -v = 1(-v)$;
\item $(-\lambda)v = -(\lambda v) = \lambda(-v)$;
\item $u+v = w+v \implies u = w$.
\end{enumerate}
By $-v$ we mean the additive inverse of $v$.
\end{lemma}
\begin{proof}
\begin{enumerate}
\item First, use distributivity to get
\[ 0v = (0+0)v = 0v + 0v. \]
The apply the previous lemma to $0+0v = 0v = 0v+0v$ to get $0=0v$. The equality $\lambda\cdot 0 = 0$ is proved analogously.
\item To show that $(-1)\cdot v$ is the additive inverse of $v$, i.e.\ $-v$, we simply add $(-1)\cdot v + v$ and observe the result is $0$.
\[ (-1)\cdot v + v = (-1)\cdot v + 1\cdot v = (1+(-1))\cdot v = 0\cdot v = 0. \]
\item Similar to the previous point.
\item The additive inverse $-v$ exists, so we can just add it left and right.
\end{enumerate}
\end{proof}
\subsection{Subspaces}
\begin{definition}
A \textit{subset} $U$ of a vector space $V$ is called a \udef{subspace} of $V$ if $U$ is also a vector space.
\end{definition}
The subset $U$ automatically inherits a lot of the structure of $V$. We only need to verify a couple of conditions.
\begin{proposition}[Subspace criterion] \label{subspaceCriterion}
A subset $U$ of a vector space $V$ is a subspace of $V$ \textup{if and only if} $U$ satisfies the following conditions:
\begin{enumerate}
\item \textbf{Additive identity}: $0 \in U$. Alternatively it is enough to show that $U$ is not empty.
\item \textbf{Closed under addition}: $v,w \in U$ implies $v+w\in U$;
\item \textbf{Closed under scalar multiplication}: $\lambda \in \mathbb{F}$ and $u\in U$ implies $\lambda u \in U$.
\end{enumerate}
\end{proposition}
Alternatively the last two criteria are equivalent to:
\[ v,w\in U; \lambda \in \mathbb{F} \qquad \text{implies} \qquad v+\lambda w \in U. \]

If the question is whether a set is a subspace, this criterion is almost always the answer. An elementary application:
\begin{proposition}
Any arbitrary intersection of subspaces is a subspace.
\end{proposition}
\begin{corollary}
Let $V$ be a vector space. Then the subspaces of $V$ form a complete sublattice of $\sSet{\powerset(V),\subseteq}$.
\end{corollary}

\begin{definition}
The closure operator into the complete lattice of subspaces of $V$ is called the \udef{span}.

If $D$ is a subset of $V$ such that $V = \Span(D)$, then $D$ \udef{spans} $V$.

\begin{itemize}
\item A vector space is called \udef{finite-dimensional} if it is spanned by a finite set of vectors.
\item A vector space is \udef{infinite-dimensional} if it is not finite-dimensional.
\end{itemize}
\end{definition}

\begin{definition}
Let $V$ be a vector space. A \udef{hyperplane} in $V$ is a coatom in the lattice of subspaces of $V$.
\end{definition}

\section{Basis and dimension}
\subsection{Linear combinations and span}
\begin{definition}
A \udef{(finite) linear combination} of vectors $v_1, \ldots, v_n$ is a vector of the form
\[ a_1v_1 + \ldots + a_nv_n \]
where $a_1, \ldots, a_n \in \mathbb{F}$.
\end{definition}

\begin{proposition}
Let $V$ be a vector space over a field $\F$ and $D\subseteq V$ a subset. Then $\Span(D)$ is the set of all finite linear combinations of vectors in $D$ if $D \neq \emptyset$. If $D = \emptyset$, then $\Span(D) = \{0\}$.
\end{proposition}

\subsection{Linear independence}
\begin{definition}
A set of vectors $D$ is \udef{linearly independent} if the only linear combinations in $D$ that equal $0$ are the trivial ones with all scalars zero. i.e.\,
\[ \sum_{i=1}^n a_iv_i = 0 \qquad\implies\qquad a_1=\ldots=a_n = 0 , \]
assuming the $v_i$ are vectors in $D$ and the $a_i$ are scalars.

\udef{Linear dependence} is the opposite of linear independence.
\end{definition}
The empty set $D=\emptyset$ is taken as linearly independent. No non-trivial combinations of vectors in $\emptyset$ are equal to zero, because there are no non-trivial combinations of vectors in $\emptyset$.

\begin{lemma}
Let $D$ be a linearly dependent set of vectors. Then there exists a vector $v\in D$ such that
\begin{enumerate}
\item $v$ is a linear combination of other vectors in $D$;
\item $v\in \Span(D\setminus\{v\})$;
\item $\Span(D) = \Span(D\setminus\{v\})$.
\end{enumerate}
\label{linearDependence}
\end{lemma}
\begin{proof}
Take a linear combination of vectors in $D$ equalling zero,
\[ \sum_i a_iv_i = 0. \]
By linear dependence such a combination can be found such that not all $a_i$ are zero. In particular at least two must be non-zero. Take $a_j\neq 0$. Then
\[ v_j = \sum_{i\neq j}\frac{a_iv_i}{a_j}. \]

To prove the last point, take a $u\in \Span(D)$. Then
\[ u = \sum_i b_iv_i = b_j v_j + \sum_{i\neq j} b_iv_i = b_j\sum_{i\neq j}\frac{a_iv_i}{a_j} + \sum_{i\neq j} b_iv_i = \sum_{i\neq j}\left(\frac{b_ja_i}{a_j}+b_i\right)v_i.  \]
So $u\in \Span(D\setminus\{v\})$. The opposite inclusion is obvious. 
\end{proof}

\subsection{Bases}
\begin{definition}
A \udef{basis} of a vector space $V$ is a set of vectors in $V$ that spans $V$ and is linearly independent.
\end{definition}
\begin{example}
The \udef{standard basis} or \udef{natural basis} of $\mathbb{F}^n$ is given by
\begin{align*}
(1,0,0,&\ldots,0), \\
(0,1,0,&\ldots,0), \\
(0,0,1,&\ldots,0), \\
&\ldots \\
(0,0,0,&\ldots,1).
\end{align*}
We will denote it $\mathcal{E}$ or $\mathcal{E}_n$.
\end{example}
\subsubsection{In finite-dimensional spaces}
\begin{proposition}
A finite set $\{v_1, \ldots, v_n\}$ of vectors in $V$ is a basis of $V$ \textup{if and only if} every $v\in V$ can be written uniquely in the form
\[ v = a_1v_1 + \ldots + a_nv_n, \]
where $a_1, \ldots, a_n \in \mathbb{F}$.
\end{proposition}
\begin{proof}
We prove both directions.
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Suppose $\{v_1, \ldots, v_n\}$ is a basis of $V$. Then any vector $v$ can be written as $a_1v_1 + \ldots + a_nv_n$, because the basis spans the space. We just need to show the decomposition is unique. To that end, assume there was another decomposition $v = b_1v_1 + \ldots + b_nv_n$. Subtracting both decompositions gives
\[ 0 = (a_1-b_1)v_1 + \ldots + (a_n-b_n)v_n. \]
Because $\{v_1, \ldots, v_n\}$ is linearly independent, $a_i = b_i$ for all $i$.
\item[$\boxed{\Leftarrow}$] Now suppose every vector has such a decomposition. Clearly $\{v_1, \ldots, v_n\}$ spans $V$. The unique decomposition of $0$ gives linear independence.
\end{itemize}
\end{proof}

\begin{theorem}[Steinitz exchange lemma] \label{SteinitzExchange}
Let $V$ be a vector space.
If $U = \{u_1, \ldots, u_m\}$ is a linearly independent set of $m$ vectors in $V$, and $W = \{ w_1, \ldots, w_n \}$ spans $V$, then:
\begin{enumerate}
\item $m\leq n$;
\item There is a set $\{u_1, \ldots, u_m, w'_{m+1}, \ldots, w'_n\} \supset U$ that spans $V$ where $w'_{m+1},\ldots, w'_n \in W$.
\end{enumerate}
\end{theorem}
\begin{proof}
We obtain the set $\{u_1, \ldots, u_m, w'_{m+1}, \ldots, w'_n\}$ by starting with the list $B_0 = (w_1, \ldots, w_n)$ and applying the following steps for each element $u_i \in U$, in the process defining sets $B_1, \ldots, B_m$. Each of these sets spans $V$.
\begin{enumerate}
\item Add $u_i$ to $B_{i-1}$. The set is now linearly dependent, because $B_{i-1}$ spans $V$.
\item By lemma \ref{linearDependence}, we can find a vector $v$ that is a linear combination of $B_{i-1}\setminus \{v\}$. Because $u_1,\ldots, u_i$ are linearly independent, we can choose this vector to be an element of $W$. Define $B_i = B_{i-1}\setminus\{v\}$. By lemma \ref{linearDependence}, $B_i$ still spans $V$, as required.
\end{enumerate}
This process only stops when we have had all elements of $U$.
\end{proof}
\begin{corollary}
If a vector space $V$ has a basis with $n$ vectors, then any basis of $V$ has $n$ vectors. \label{nBasis}
\end{corollary}

\begin{theorem}
Suppose $V$ is a finite-dimensional vector space spanned by $D = \{v_1, \ldots, v_n\}$.
\begin{enumerate}
\item We can find a subset of $D$ that is a basis of $V$, i.e.\ $D$ can be reduced to a basis;
\item Each linearly independent set of vectors can be expanded to a basis.
\end{enumerate}
\label{basis}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item Remove $0$ from $D$, if it is an element. If $D$ is not linearly independent, find a vector in $D$ that is a linear combination of other vectors in $D$. Repeat until the set is linearly independent. This process stops due to the finite number of vectors. The set spans $V$ at every step.
\item Follows easily from the Steinitz exchange lemma, taking $W$ to be a basis.
\end{enumerate}
\end{proof}
\begin{corollary}
Every finite-dimensional vector space has a basis. \label{existenceBasis}
\end{corollary}

Thanks to corollaries \ref{nBasis} and \ref{existenceBasis}, the following definition makes sense:
\begin{definition}
The \udef{dimension} of a finite-dimensional vector space is the length of any basis of the vector space.
The dimension of $V$ (if $V$ is finite-dimensional) is denoted by $\dim V$ or $\dim_\mathbb{F}V$.\footnote{The latter notation is particularly useful if when distinguishing between real and complex vector spaces, because every complex vector space can be seen as a real vector space. In this case $\dim_\R V = 2\dim_\C V$, because $v$ and $iv$ are linearly independent over $\R$.}

If $V = \{0\}$, we take $\dim V = 0$.
\end{definition}

\begin{corollary}
Every linearly independent set of vectors in $V$ with length $\dim V$ is a basis of $V$. \label{maxLinearlyIndependent}
\end{corollary}
\begin{corollary}
Every spanning set of vectors in $V$ with length $\dim V$ is a basis of $V$.
\end{corollary}

\begin{proposition}
Let $V$ be a finite-dimensional vector space and $U$ a subspace of $V$. Then
\begin{enumerate}
\item $U$ is finite-dimensional and $\dim U \leq \dim V$;
\item $\dim U = \dim V \iff U=V$.
\end{enumerate}
\label{vectorSpaceEquality}
\end{proposition}
\begin{proof}
We construct a basis for $U$ using the following process:
\begin{enumerate}
\item If U=\{0\}, then we can take the basis $\emptyset$ and we are done. If $U\neq \{0\}$, we choose a nonzero vector $v_1 \in U$.
\item If $U$ is the span of all the vectors we have chosen, we are done. If not choose a vector in $U$, not in the span of the other vectors.
\item Repeat step (2).
\end{enumerate}
By construction, the chosen set of vectors is linearly independent. By the Steinitz exchange lemma this process must stop. In particular it must stop before reaching $\dim V$ vectors.

If the process reaches this upper bound, then by corollary \ref{maxLinearlyIndependent}, the set of vectors in $U$ is also a basis for $V$.
\end{proof}
We now have two tools for proving equalities of finite-dimensional vector spaces: either by proving both inclusions, or by leveraging point (2) of the previous proposition.

\subsubsection{In infinite-dimensional spaces}
Our definition of a basis of a vector spaces still makes sense for infinite-dimensional vector spaces, and many results of the previous section still make sense for infinite-dimensional vector spaces.

For infinite-dimensional vector spaces, there are, however, other notions of basis we might be interested in. In particular, our definition of basis requires all vectors to be constructible as \emph{finite} linear combinations of basis elements. In some contexts we might want to relax this to allow infinite combinations as well. For that, of course, we need some notion of infinite sum. Often we construct infinite sums as the limit of a sequence of finite sums, in which case we need a topology on our vector space that allows us to take limits.\footnote{Although other options exist, such as taking sums over hyperintegers.}  

In order to distinguish our purely algebraic definition of basis from these other notions of basis, a basis in the sense defined above is sometimes known as an \udef{algebraic basis} of \udef{Hamel basis}.

We will be discussing Hamel bases in this section.

\begin{theorem} \label{extensionReductionBasis}
Let $V$ be a vector space.
\begin{enumerate}
\item Any spanning set contains a basis.
\item Any linearly independent subset can be expanded to a basis.
\end{enumerate}
\label{infBasis}
\end{theorem}
\begin{proof}
Requires the axiom of choice. We will use Zorn's lemma twice.
\begin{enumerate}
\item Let $S$ be a spanning subset of $V$. Define
\[ \mathcal{A} = \{ D\subset S \;|\; \text{$D$ is linearly independent}\} \]
ordered by inclusion. It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union which is still linearly independent. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. 
We show that $\Span(R) \supset S$ by contradiction. If $\Span(R) \not\supset S$, we can consider $R\cup \{v\}$ for some $v \in S$ that is not in $\Span(R)$ and we obtain an element of $\mathcal{A}$ which is greater than a maximal element. This is a contradiction. Then from $\Span(R) \supset S$ we conclude
\[ \Span(R) = \Span(\Span(R)) \supset \Span(S) = V \]
from which it follows that $\Span(R) = V$.
\item Let $S$ be a linearly independent subset of $V$. Define
\[ \mathcal{A} = \{ D\subset V \;|\; S \subset D \; \text{and $D$ is linearly independent}\} \]
ordered by inclusion. 
It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. We show that $\Span(R) = V$ by contradiction. If $\Span(R) \neq V$, we can consider $R\cup \{v\}$ for some $v\notin \Span(R)$ and we obtain an element of $\mathcal{A}$ which is greater than a maximal element. This is a contradiction.
\end{enumerate}
\end{proof}
\begin{corollary} \label{existenceHamelBasis}
Every vector space has a Hamel basis
\end{corollary}

\begin{theorem}[Dimension theorem for vector spaces]
Given a vector space $V$, any two bases have the same cardinality.
\end{theorem}
\begin{proof}
The finite-dimensional case has already been proved. Suppose $A$ is a basis of $V$ with $|A| \geq \aleph_0$. Let $B$ be another basis of $V$. Each element $a\in A$ can be written as a finite combination of elements in $B$. Collect all the elements that go into the finite linear combination in a finite set $B_a \subset B$. We claim
\[ B = \bigcup_{a\in A} B_a. \]
Indeed, assume $b\in B \setminus (\cup_{a\in A} B_a)$. Since $A$ spans $V$, so does $\cup_{a\in A} B_a$. Thus $b$ can be written as a non-trivial combination of vectors in $\cup_{a\in A} B_a\subset B$, contradicting the linear independence of $B$. Then we have
\[ |B| = \left| \bigcup_{a\in A}B_a \right| \leq \aleph_0 \cdot |A| = |A| \]
A similar argument gives
\[ |A| \leq \aleph_0 \cdot |B| = |B|. \]
By the Schröder–Bernstein theorem \ref{SchroederBernstein}, we conclude $|A| = |B|$.
\end{proof}
TODO: does this proof work with only the ultrafilter lemma?

Thus the notion of dimension (also known as \udef{Hamel-dimension}) also makes sense for infinite-dimensional vector spaces, except it is a cardinality, not a number.

TODO: do we need a strong cardinality assignment? (Assumed for now)

Many textbooks state results using dimensions only for the finite-dimensional case. As we will see, these results almost always generalise directly to the infinite-dimensional case as well, if we assume the axiom of choice.

\begin{note}
The inverse of this theorem (i.e.\ the infinite-dimensional analogue of proposition \ref{vectorSpaceEquality}) does not hold: infinite-dimensional vector spaces always have proper subspaces with a basis of the same cardinality. This is obvious because dropping one vector in the Hamel basis of an infinite-dimensional vector space will not change the cardinality, but will make it a proper subspace.
\end{note}

 \begin{corollary}
 Let $V$ and $W$ be vector spaces.
 \begin{enumerate}
 \item If $\dim V > \dim W$, then no linear map from $V$ to $W$ is injective.
 \item If $\dim V < \dim W$, then no linear map from $V$ to $W$ is surjective.
 \end{enumerate}
 \end{corollary}

\begin{lemma}
Let $V$ be an infinite-dimensional vector space over a field $\mathbb{F}$. Assume $|\mathbb{F}|\leq \dim_{\mathbb{F}} V$, then $\dim_{\mathbb{F}} V = |V|$. \label{vsCardinality}
\end{lemma}
\begin{proof}
Let $B$ be a basis of $V$. It is supposed infinite. There is a surjection
\[\bigcup_{n\in\N}(\mathbb{F}\times B)^{n} \to V: (a_i,v_i)^{i<n} \mapsto \sum_{i<n}a_iv_i. \]
So we have
\[ |V| \leq \left|\bigcup_{n\in\N}(F\times B)^{n}\right| = \sum_{n\in \N}|F\times B|^n \leq \aleph_0\cdot |\mathbb{F}| \cdot |B| = \max\{\aleph_0, |\mathbb{F}|, |B|\} = |B|. \]
Thus $|V|\leq \dim_{\mathbb{F}} V$. The other inequality is obvious. By the Schröder–Bernstein theorem \ref{SchroederBernstein}, we conclude $\dim_{\mathbb{F}} V = |V|$.
\end{proof}

\section{Constructing vector spaces}
\subsection{Sums of subspaces}
\begin{definition}
Suppose $\{U_i\}_{i\in I}$ a set of subspaces of a vector space $V$. The \udef{sum} of these subspaces, denoted $\sum_{i\in I}U_i$, is the set of all finite linear combinations of elements in $\bigcup_{i\in I}U_i$:
\[ \sum_{i\in I}U_i = \Span\left(\bigcup_{i\in I} U_i\right) = \setbuilder{\sum_{i\in J} u_i}{\text{$J\subset I$ finite}, u_i\in \bigcup_{i\in I}U_i}. \]
\end{definition}
For finite sums this reduces to
\[ U_1+\ldots + U_m = \setbuilder{\sum_{i=1}^m u_i}{u_1\in U_1, \ldots, u_m\in U_m}. \]

\begin{proposition} \label{basisSum}
Let $\{U_i\}_{i\in I}$ be a set of subspaces of a vector space $V$ and $\beta_i$ a basis of $U_i$ for all $i\in I$. Then
\[ \sum_{i\in I}U_i = \Span\left(\bigcup_{i\in I}\beta_i\right). \]
\end{proposition}
\begin{proof}
From $\bigcup_{i\in I}\beta_i \subseteq \bigcup_{i\in I} U_i$, we get $\Span\left(\bigcup_{i\in I}\beta_i\right) \subseteq \Span\left(\bigcup_{i\in I} U_i\right) = \sum_{i\in I}U_i$.

Conversely, take $u\in \sum_{i\in I}U_i$. Then $u = \sum_{j\in J}u_j$ where $J$ is finite subset of $I$ and $u_i\in U_i$. Now each $u_j$ can be written as $\sum_k a_{j,k}v_{j,k}$, where $a_{j,k}$ are scalars and $v_{j,k}$ are vectors in $\beta_j$. So
\[ u = \sum_{j,k}a_{j,k}v_{j,k}, \]
which is a finite linear combination of vectors in $\bigcup_{i\in I}\beta_i$. So $u\in \Span\left(\bigcup_{i\in I}\beta_i\right)$.
\end{proof}

\begin{proposition}
Let $V$ be a vector space and $A,B,C$ subspaces. Then
\begin{enumerate}
\item $A+(B\cap C) \subseteq (A+B)\cap (A+C)$;
\item $(A+B)\cap C \supseteq (A\cap C) + (B\cap C)$. 
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Take $v = v_1+v_2 \in A+(B\cap C)$ where $v_2 \in B$ and $v_2 \in C$, so $v_1+v_2\in A+B$ and $v_1+v_2\in A+C$.

(2) Take $v = v_1+v_2\in (A\cap C) + (B\cap C)$. Then $v_1,v_2\in C$ and thus $v\in (A+B)\cap C$.
\end{proof}

\begin{theorem}[Dimension of a sum]
Let $U_1$ and $U_2$ be subspaces of a finite-dimensional vector space, then
\[ \dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim(U_1\cap U_2). \]
\label{dimOfASum}
\end{theorem}
\begin{proof}
Let $\dim U_1 = r, \dim U_2 = s$ and $\dim(U_1\cap U_2) = t$. Then $t\leq r$ and $t\leq s$.  Take a basis $\{v_1,\ldots, v_t\}$ of $U_1\cap U_2$. This can be expanded to a basis $\beta_{U_1} = \{ v_1, \ldots, v_t, u_{t+1}, \ldots u_{r} \}$ of $U_1$ and also to a basis $\beta_{U_2} = \{ v_1, \ldots, v_t, u'_{t+1}, \ldots u'_{s} \}$ of $U_2$. We will show that $\{ v_1, \ldots, v_t, u_{t+1}, \ldots u_{r}, u'_{t+1}, \ldots, u'_{s} \}$ is a basis of $U_1\cap U_2$. This completes the proof because
\begin{align*}
\dim(U_1 + U_2) &= t + (s-t) + (r-t) = s + r -t\\
&= \dim U_1 + \dim U_2 - \dim(U_1\cap U_2).
\end{align*}
The spanning property is easy. Linear independence is slightly more difficult: Take a linear combination
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j + \sum^s_{k=t+1}\beta'_ku_k' =0. \]
We must show this combination is trivial. Indeed observe that
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j  =-\sum^s_{k=t+1}\beta'_ku_k'. \]
The left-hand side is a vector in $U_1$, the right-hand side is a vector in $U_2$, so it must lie in $U_1\cap U_2$, so we rewrite the left-hand side as
\[ \sum_{i=1}^t\lambda_i v_i =  -\sum^s_{k=t+1}\beta'_ku_k'.\]
Due to $\beta_{U_2}$ being a basis, this linear combination must be trivial and all $\beta'_k$ are zero. This leaves us 
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j =0 \]
from our original linear combination. Due to $\beta_{U_2}$ being a basis this combination must also be trivial. 
\end{proof}
\begin{note}
If $\dim(U_1\cap U_2)<\dim U_1$ and $\dim(U_1\cap U_2)< \dim U_2$, this proof generalises to infinite-dimensional vector spaces.
\end{note}

\subsection{(Internal) direct sum}
\begin{definition}
Suppose $\{U_i\}_{i\in I}$ is a set of subspaces of $V$. The sum $\sum_{i\in I}U_i$ is called a \udef{direct sum} if each element $u$ of the sum can be \emph{uniquely} written as
\[ u = \sum_{i\in I}u_i \qquad (u_i\in U_i) \]
where only finitely many of the $u_i$ are nonzero.

In this case we write $\bigoplus_{i\in I} U_i$, or $U_1 \oplus \ldots \oplus U_m$ if $I = \{1,\ldots, m\}$. 
\end{definition}

\begin{proposition}[Conditions for a direct sum] \label{directSumCriterion}
Let $\{U_i\}_{i\in I}$ be a set of subspaces of a vector space $V$ and $\beta_i$ a basis of $U_i$ for all $i\in I$. Let $U,W\subseteq V$ also be subspaces of $V$.
\begin{enumerate}
\item The sum $\sum_{i\in I}U_i$ is direct \textup{if and only if} $0$ has the unique decomposition as in the definition.
\item The sum $\sum_{i\in I}U_i$ is direct \textup{if and only if} the union $\bigcup_{i\in I}\beta_i$ is disjoint and linearly independent.
\item The sum $U+W$ is direct \textup{if and only if} $U\cap W = \{0\}$.
\end{enumerate}
\end{proposition}
\begin{proof}
TODO
\end{proof}
\begin{corollary}
Let $\{U_i\}_{i\in I}$ be a set of subspaces of a vector space $V$ and $\beta_i$ a basis of $U_i$ for all $i\in I$. Then
\[ \dim\left(\bigoplus_{i \in I}U_i\right) = \sum_{i\in I} \dim U_i \]
\end{corollary}

\begin{definition}
In a vector space $V$, a subspace $W$ is a \udef{complementary subspace} (or a \udef{complement}) of the subspace $U$ if $V = U \oplus W$.
\end{definition}

\begin{proposition}
Let $V$ be a vector space, then each subspace of $V$ has a complement.
\end{proposition}
\begin{proof}
Let $U$ be a subspace of $V$. Then, by \ref{existenceHamelBasis}, we can find a basis $B$ of $U$ and, by \ref{extensionReductionBasis}, we can extend it to a basis $D$ of $V$. Now $V = U \oplus \Span(D\setminus B)$ by \ref{basisSum} and \ref{directSumCriterion}.
\end{proof}
Note this requires the axiom of choice, and is in fact equivalent with it.
\begin{corollary}
Suppose $V$ is finite-dimensional and $U_1,\ldots, U_m$ are subspaces of $V$. Then $U_1+\ldots+ U_m$ is a direct sum \textup{if and only if}
\[ \dim(U_1+\ldots+U_m) = \dim U_1 + \ldots \dim U_m. \]
\label{directSumDimensionCriterion}
\end{corollary}

\subsection{External direct sum}
\begin{definition}
Let $U,W$ be vector spaces over the same field $\mathbb{F}$. We define the vector space  $U\oplus W$, called the \udef{(external) direct sum}, as the set $U\times W$ with the operations
\[ \begin{cases}
(u_1,w_1) + (u_2, w_2) = (u_1 +_U u_2, w_1 +_W w_2) & (u_1,u_2 \in U; w_1, w_2 \in W) \\
r\cdot (u,w) = (ru, rw) & (r\in \mathbb{F}; u\in U; w \in W)
\end{cases} \]
In general we can define a direct sum of an arbitrary collection of vector spaces $\{U_i\}_{i\in I}$, denoted
\[ \bigoplus_{i\in I}U_i \]
as the vector space with as field the subset of the Cartesian product $\prod_{i\in I}U_i$ where all but finitely many of the terms are zero. The operations are defined point-wise.
\end{definition}

\begin{proposition}
Suppose $V_1, \ldots V_m$ are vector spaces over $\mathbb{F}$. Then
\[ \dim(V_1\oplus\ldots \oplus V_m) = \dim V_1 + \ldots + \dim V_m \]
\label{dimDirectSum}
\end{proposition}
\begin{proof}
We construct a basis $\beta$ of $V_1\oplus\ldots \oplus V_m$ from bases $\beta_{V_i}$ of $V_i$:
\[ \beta = (\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \}) \cup (\{0 \} \times \beta_{V_2}\times \{0 \} \times \ldots \times \{0 \}) \cup \ldots \cup (\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m}). \]
All these unions are disjunct, so
\begin{align*}
|\beta| &= |(\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \}) \cup  \ldots \cup (\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m})| \\
&= |(\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \})| + \ldots + |(\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m})| \\
&= | \beta_{V_1}| + \ldots + |\beta_{V_m}| \\
&= \dim V_1 + \ldots + \dim V_m.
\end{align*}
\end{proof}

\begin{proposition}
Let $U,W$ be subspaces of $V$. Then the external direct sum of $U$ and $W$ is isomorphic to the internal direct sum of $U$ and $W$.
\end{proposition}
\begin{proof}
The map $f: U\times W\to V: (u,w) \mapsto u+w$ is an isomorphism.
\end{proof}
For this reason we use the same symbol for both.

\begin{definition}
Let $V,W, X,Y$ be vector spaces over $\mathbb{F}$. Let $S: V\to X$ and $T: W\to Y$ be linear maps. Then the \udef{direct sum} of $S$ and $T$ is a linear map
\[ S\oplus T: V \oplus W \to X\oplus Y: (v,w) \mapsto (S(v), T(v)). \]
\end{definition}

\begin{lemma}
Let $V,W$ be vector spaces over a field $\F$ and $A,C\in\Lin(V)$ and $B,D\in\Lin(W)$. Then
\begin{enumerate}
\item $a(A\oplus B) + b(C\oplus D) = (aA+bC)\oplus (aB + bD)$;
\item $(A\oplus B)(C\oplus D) = AC\oplus BD$;
\item $(A\oplus B)^k = A^k\oplus B^k$.
\end{enumerate}
\end{lemma}

\subsubsection{Matrix representation}
TODO: move
Assume $V$ and $W$ are finite-dimensional vector spaces with resp. bases $\{\vec{e}_i\}_{i=1}^m$ and $\{\vec{f}_j\}_{j=1}^n$.
As in the proof of proposition \ref{dimDirectSum}, we can take the basis $\{\vec{e}_i\}_i\times\{0\} \cup \{0\}\times\{\vec{f}_j\}_j$ of $V\oplus W$.

We can naturally fit the basis into a list of $m+n$ elements:
\[ (\vec{e}_1,0),\ldots (\vec{e}_m, 0), (0, \vec{f}_1), \ldots, (0,\vec{f}_n)  \]
\subsubsection{Linear maps}
TODO: also move
Let $S: V\to X$ and $T:W\to Y$ be linear maps, with matrix representations $A$ and $B$, respectively. The matrix representation of $S\oplus T$ is given by
\[ A\oplus B = \begin{bmatrix}
A & 0 \\
0 & B
\end{bmatrix} \]
with respect to the basis $\{\vec{e}_i\}_i\times\{0\} \cup \{0\}\times\{\vec{f}_j\}_j$.

\section{Linear maps}
\begin{definition}
Let $(\mathbb{R}, V, +)$ and $(\mathbb{R}, W, +)$ be vector spaces over the same field. A \udef{linear map} or \udef{linear transformation} is a function $L:V\to W$ with the following properties:
\begin{itemize}[leftmargin=3cm]
\item[\textbf{Additivity}] $L(u+v) = L(u)+L(v)$ for all $u,v \in V$;
\item[\textbf{Homogeneity}] $L(\lambda v) = \lambda L(v)$ for all $\lambda \in \mathbb{R}$ and all $v\in V$.
\end{itemize}
These conditions are equivalent to the condition that
\[ L(\lambda_1 v_1 + \lambda_2v_2) = \lambda_1L(v_1) + \lambda_2 L(v_2) \qquad \text{for all $\lambda_1,\lambda_2\in \mathbb{F}$ and all $v_1,v_2\in V$.} \]
We denote the set of all linear maps from $V$ to $W$ as $\Lin_\mathbb{F}(V,W)$, or $\Lin(V,W)$. The set of endomorphisms on $V$ is denoted $\Lin(V) \defeq \End(V) = \Lin(V,V)$.
\end{definition}

\begin{lemma} \label{linearMaps}
Let $L\in \Lin(V,W)$.
\begin{enumerate}
\item $L(0) = 0$ and $L(-v) = -L(v)$
\item $L\left(\sum^n_{i=1}\lambda_i v_i\right) = \sum_{i=1}^n\lambda_i L(v_i)$.
\item A linear map is completely determined by the images of a basis of $V$.
\item Let $D$ be a set of vectors. Then $L[D]$ is linearly independent \textup{if and only if} $D$ is linearly independent.
\end{enumerate}
\end{lemma}

\subsection{Examples}
\begin{enumerate}
\item The zero map that maps everything to zero.
\item Identity maps.
\item Differentiation of polynomials.
\item Integration of polynomials.
\item Shifting elements in a list.
\item Projections.
\end{enumerate}

\begin{definition}
A (linear) \udef{operator} between two vector spaces $V$ and $W$ is a linear partial function $T: V \not\to W$ such that the domain $\dom(T)$ is a vector space.

We also say an operator is a function $T: \dom(T)\subseteq V\to W$.
\end{definition}
The requirement that $\dom(T)$ be a subspace of $V$ is necessary for linearity to make sense!

Some authors (e.g\ Axler) use the word ``operator'' to mean a linear endomorphism.

\subsection{Image and kernel}
\begin{definition}
Let $L \in \Lin(V,W)$. The \udef{kernel} or \udef{null space} of $L$ is the set of vectors that $L$ maps to zero:
\[ \ker(L) = \{ v\in V \;|\; L(v) = 0 \}. \]
\end{definition}
\begin{proposition} \label{kernelSubspace}
The kernel of $L\in \Lin(V,W)$ is a subspace of $V$.
\end{proposition}
\begin{definition}
The dimension of the kernel of a linear map is its \udef{nullity}.
\end{definition}
\begin{proposition} \label{injectivityKernelTriviality}
Let $L\in\Lin(V,W)$. Then $L$ is injective if and only if $\ker(L) = 0$.
\end{proposition}
TODO: generalise to groups
\begin{proof}
We show both implications.
\begin{itemize}
\item[\boxed{\Rightarrow}] We know $\{0\}\subset \ker(L)$ by lemma \ref{linearMaps}. Suppose $v\in \ker(L)$, then $L(v) = 0 = L(0)$. So $v=0$ by injectivity and $\{0\}\supset \ker(L)$.
\item[\boxed{\Leftarrow}] Suppose $u,v \in V$ such that $L(u)=L(v)$. Then
\[ 0 = L(u) - L(v) = L(u-v). \]
Thus $u-v\in \ker(L)$, meaning $u-v = 0$ and $u=v$.
\end{itemize}
\end{proof}

\begin{definition}
Let $L \in \Lin(V,W)$. The \udef{image} or \udef{range} of $L$ is the set of vectors that are of the form $L(v)$ for some $v\in V$:
\[ \im(L) = \{ L(v) \;|\; v\in V \}. \]
\end{definition}
\begin{proposition}
The range of $L\in \Lin(V,W)$ is a subspace of $W$.
\end{proposition}
\begin{definition}
The dimension of the image of a linear map is its \udef{rank}.
\end{definition}

\begin{theorem}
Every short exact sequence of vector spaces splits.
\end{theorem}
\begin{proof}
Let
\[ \begin{tikzcd}
0 \rar & U \rar{S} & V \rar{T} & W \rar & 0
\end{tikzcd} \]
be a short exact sequence of vector spaces.
By the splitting lemma TODO ref, it is enough to find a left inverse of $S$. Pick a basis $\beta$ of $U$. Because $S$ is injective, $S[\beta]$ is linearly independent and we can extend it to a basis $\beta'$. We can now define the left inverse by specifying how the basis elements are mapped, by \ref{linearMaps}. To wit: $\beta'\setminus S[\beta]$ is mapped to $0$ and each element $S[\beta]$ has exactly one origin be injectivity and it is to this origin that it is now mapped.
\end{proof}
\begin{corollary} \label{directSumKernelImage}
Let $L \in \Lin(V,W)$. Then
\[ V \cong \ker L \oplus \im L. \]
\end{corollary}
\begin{proof}
Given $L$ we have the short exact sequence
\[ \begin{tikzcd}
0 \rar & \ker L \ar[r, hook] & V \rar{L} & \im L \rar & 0.
\end{tikzcd} \]
The isomorphism then follows from the splitting lemma TODO ref.
\end{proof}
\begin{corollary}[Dimension theorem for linear maps] \label{dimensionLinearMaps}
Let $L \in \Lin(V,W)$. Then
\[ \dim(V) = \dim(\ker L) + \dim(\im L). \]
\end{corollary}
This corollary is also known as the rank-nullity theorem or the fundamental theorem of linear maps.
\begin{proof}
By $\dim(V) = \dim(\ker L \oplus \im L) = \dim(\ker L) + \dim(\im L)$.

Alternatively this can be proven directly as follows:

Take a basis $\beta_0$ of $\ker(L)$. We can expand this to a basis $\beta$ of $V$, by theorem \ref{infBasis}. It is easy to show that $L[\beta\setminus \beta_0]$ is a basis of $\im(L)$. Now $L[\beta\setminus \beta_0] =_c \beta\setminus \beta_0$ and $(\beta\setminus \beta_0) \cap \beta_0 = \emptyset$. Thus $|\beta| = |(\beta\setminus \beta_0) \cup \beta_0| = |\beta\setminus \beta_0| + |\beta_0|$. This proves the assertion.
\end{proof}
\begin{corollary}
Let
\[ \begin{tikzcd}
0 \rar & V_1 \rar & V_2 \rar & \ldots \rar & V_n \rar & 0
\end{tikzcd} \]
be an exact sequence of vector spaces, then
\[ \sum_{i=1}^n (-1)^i\dim(V_i) = 0. \]
\end{corollary}
\begin{proof}
Let $f_i$ be the map $V_i\to V_{i+1}$. By exactness $\im f_i=\ker f_{i+1}$ and $\dim(\im f_i)=\dim(\ker f_{i+1})$. By the previous corollary $\dim(V_i) = \dim(\ker f_i) + \dim(\im f_i)$. Then
\[ \sum_{i=1}^n (-1)^i\dim(V_i) = \sum_{i=1}^n (-1)^i\dim(\ker f_i) + \sum_{i=1}^n (-1)^i\dim(\ker f_{i+1}) = \sum_{i=2}^{n} (-1)^i\dim(\ker f_i) - \sum_{i=2}^{n} (-1)^i\dim(\ker f_{i}) = 0. \]
\end{proof}
\begin{corollary} \label{dimensionImageSmaller}
Let $L \in \Lin(V,W)$. Then
\[ \dim(\im L) \leq \dim(V). \]
\end{corollary}
\begin{proof}
TODO ref cardinal arithmetic.
\end{proof}

\begin{lemma} \label{rankMapComposition}
Let $S,T$ be compatible linear maps. Then
\[ \text{rank of $ST$}\;\leq\;\min\{\text{rank of $S$, rank of $T$}\}. \]
If $T$ is invertible, then the rank of $ST$ equals the rank of $S$. Similarly if $S$ is invertible, then the rank of $ST$ equals the rank of $T$.
\end{lemma}
\begin{proof}
Clearly $\im(ST) \subset \im(S)$, so $\dim\im(ST)\leq \dim\im(S)$.
We also have $ST = S|_{\im T}T$, where $S|_{\im T}$ is $S$ restricted to $\im T$.  Then corollary \ref{dimensionImageSmaller} applied to $S|_{\im T}$ gives $\dim\im(ST)\leq \dim\im T$. Together these inequalities give the result.

To show equality in the invertible case, first assume $T$ invertible:
\[ \dim\im ST \leq \dim\im STT^{-1} = \dim\im S. \]
Together with the first inequality this gives an equality. The case for $S$ invertible is similar.
\end{proof}

\begin{proposition} \label{kernelCompositionLinearMaps}
Let $S,T$ be compatible linear maps. Then
\begin{enumerate}
\item $\ker(ST)\supseteq \ker(T)$;
\item $\dim\ker(ST) = \dim\ker(T) + \dim(\im(T)\cap\ker(S))$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) $x\in\ker(T) \implies (ST)x = S(Tx) = S(0) = 0 \implies x\in\ker(ST)$.
(2) Consider the restriction $T|_{\ker(ST)}$. Applying the dimension theorem gives
\[ \dim\ker(ST) = \dim\ker(T|_{\ker(ST)}) + \dim\im(T|_{\ker(ST)}) = \dim\ker(ST) = \dim\ker(T) + \dim\im(T|_{\ker(ST)}) , \]
so it is enough to show $\im(T|_{\ker(ST)}) = \im(T)\cap\ker(S)$. First take $v\in\im(T|_{\ker(ST)}$, then there exists some $w\in\ker(ST)$ such that $v=Tw$, meaning $v\in\im(T)$. Also $Sv = STw = 0$, meaning $v\in\ker(S)$.

Then take $v\in\im(T)\cap\ker(S)$, so we can find a $w$ such that $v = Tw$. Also $Sv = STw = 0$, so $w\in\ker(ST)$ and $v\in\im(T|_{\ker{ST}})$.
\end{proof}

\subsection{Algebraic operations on linear maps}
\begin{definition}
Suppose $K,L \in \Lin_{\mathbb{F}}(V,W)$ and $\lambda \in \mathbb{F}$.
\begin{itemize}
\item The \udef{sum} $K+L$ is defined by $(K+L)(v) = Kv+Lv$ for all $v\in V$;
\item The \udef{scalar product} is defined by $(\lambda K)(v) = \lambda K(v)$ for all $v\in V$.
\end{itemize}
\end{definition}
\begin{proposition}
\begin{itemize}
\item The sum of linear maps is again a linear maps. Scalar multiples of linear maps are linear maps.
\item With addition and scalar multiplication defined as above, $\Lin_\mathbb{F}(V,W)$ is a vector space.
\end{itemize}
\end{proposition}

\begin{definition}
Let $K\in \Lin_\mathbb{F}(U,V)$ and $L\in \Lin_\mathbb{F}(V,W)$. The \udef{product} $LK$ is defined as the composition
\[ (LK)(u) = L(K(u)) \qquad \text{for all $u\in U$.} \]
If the product of two linear maps $K,L$ makes sense, we call the linear maps \udef{compatible}.
\end{definition}
\begin{proposition}
The product of two (compatible) linear maps is a linear map.
\end{proposition}
\begin{proposition}[Algebraic properties of linear maps]
The product of linear maps has the following properties. 
\begin{itemize}[leftmargin=4.2cm]
\item[\textbf{Associativity}] Let $L_1, L_2, L_3$ be compatible linear maps, then
\[ (L_1L_2)L_3 = L_1(L_2L_3) \]
\item[\textbf{Identity}] Let $L\in \Lin(V,W)$. The identity maps $I_V:V\to V$ and $I_W:W\to W$ are linear and have the property that
\[ LI_V = I_W L = L. \]
\item[\textbf{Distributive properties}]
$ (S_1+S_2)T = S_1T + S_2T \qquad \text{and} \qquad S(T_1 + T_2) = ST_1 + ST_2 $
whenever $T,T_1, T_2 \in \Lin(U,V)$ and $S,S_1, S_2\in \Lin(V,W)$.
\end{itemize}
These properties mean that for any vector space $V$, $\Lin(V)$ forms a unital algebra.
\end{proposition}
Note that multiplication of linear maps is not commutative, not even for maps that are compatible both ways.

\subsection{Invertibility and isomorphisms}
\begin{proposition} \label{inverseLinear}
Let $L$ be a linear map. If $L$ is invertible as a function (i.e.\ bijective), its inverse $L^{-1}$ is linear.
\end{proposition}
\begin{proof}
We calculate for $x,y$ vectors and $a\in\mathbb{F}$
\[ L^{-1}(ax + y) = L^{-1}(aLL^{-1}x + LL^{-1}y) = L^{-1}L(aL^{-1}x + L^{-1}y) = aL^{-1}x + L^{-1}y. \]
\end{proof}

\begin{definition}
\begin{itemize}
\item An invertible linear map is called an \udef{isomorphism}.
\item Two vector spaces $V,W$  are \udef{isomorphic} if there is an isomorphism between them. This is denoted $V\cong W$.
\end{itemize}
\end{definition}

\begin{proposition} \label{isomorphicDimension}
Let $V,W$ be vector spaces over the same field $\mathbb{F}$ and $n\in \N$. Then
\begin{enumerate}
\item $V\cong W \iff \dim V = \dim W$;
\item $V \cong \mathbb{F}^n \iff \dim V = n$;
\item $\F^n \cong \F^m \iff n=m$.
\end{enumerate}
\label{isomorphicCondition}
\end{proposition}
\begin{proof}
We prove the first statement. The second and third follow easily, using $\dim_\mathbb{F} \mathbb{F}^n = n$.
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Let $T:V\to W$ be an isomorphism. Then $\ker T = \{0\}$ and $\im T = W$. Thus
\[ \dim V = \dim \ker T + \dim \im T = 0 + \dim W = \dim W. \]
\item[$\boxed{\Leftarrow}$] Assume $\dim V = \dim W$. Thus there exists an invertible function from a basis of $V$ to a basis of $W$. This can be extended by linearity to a function on $V$, because it is defined on a Hamel basis. It is easy to see this function is linear and bijective.
\end{itemize}
\end{proof}

\begin{proposition} \label{mappingOfBasisByIsomorphism}
Let $L\in\Lin(V,W)$ be an isomorphism. Let $\beta$ be a basis of $V$, then $L[\beta]$ is a basis of $W$.
\end{proposition}

\begin{proposition} \label{invertibleFiniteDim}
Suppose $V$ is a finite-dimensional vector space and $L\in \Lin(V)$ is a linear map on $V$, then
\[ L \;\text{is invertible} \iff L \;\text{is injective} \iff L \;\text{is surjective} \]
\end{proposition}
\begin{proof}
All we need to prove is
\[ L \;\text{is injective} \iff L \;\text{is surjective} \]
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Assume $L$ injective. Then $\ker L = \{0\}$. By the dimension theorem for linear maps, theorem \ref{dimensionLinearMaps}
\[ \dim \im L = \dim V - \dim \ker L = \dim V. \]
Because $\im L \subset V$ and using proposition \ref{vectorSpaceEquality}, we conclude that $\im L = V$ and thus $L$ is surjective.
\item[$\boxed{\Leftarrow}$] Assume $L$ surjective. Then, by the dimension theorem for linear maps,
\[ \dim \ker L = \dim V - \dim \im L = 0, \]
which means $L$ is injective.
\end{itemize}
\end{proof}
Remark that the proof of the first implication uses proposition \ref{vectorSpaceEquality}, and thus cannot be generalised to infinite-dimensional vector spaces. In the proof of the second implication the subtraction of infinite cardinals is only uniquely defined if  $\dim V > \dim \im L$, which is clearly not the case.

\begin{example}
Counterexamples to the previous theorem in the infinite-dimensional case are given by the left shift map on $\mathbb{F}^\N$ (which is injective, but not surjective) and the right shift map on $\mathbb{F}^\N$ (which is surjective, but not injective).
\end{example}


\subsection{Types of linear maps}
\subsubsection{Finite-rank operators}
\begin{definition}
A linear map $T: V\to V$ is said to be a \udef{finite-rank operator} if it has finite rank.
\end{definition}
\subsubsection{Idempotents}

\begin{lemma}
Let $V$ be a vector space and $U\subseteq V$ a subspace. Then for any complement $W$ of $U$ in $V$,
\[ P: U\oplus W = V \to V: u+w \mapsto u \]
is an idempotent such that $\im P = U$.
\end{lemma}

\begin{proposition} \label{directSumKernelImageIdempotent}
Let $V$ be a vector space and $P$ an idempotent linear map. Then
\[ V = \im P \oplus \ker P. \]
\end{proposition}
\begin{proof}
For any $v\in V$, we can write $v= (v-Pv)+Pv$ where $Pv\in \im P$ and $(v-Pv)\in \ker P$ because
\[ P(v-Pv) = Pv- P^2v = Pv - Pv = 0. \]
So we have $V = \im P + \ker P$. To show that the sum is direct, we take $u\in \im P \cap \ker P$. Then $u = Pw$ for some $w\in V$ and applying $P$ gives $0 = Pu = P^2w = Pw = 0$. So the sum is direct by \ref{directSumCriterion}.
\end{proof}

\begin{lemma} \label{idempotentImageEquivalence}
Let $V$ be a vector space, $P$ an idempotent linear map and $v\in V$. Then $v\in \im P$ \textup{if and only if} $v = Pv$.
\end{lemma}
\begin{proof}
We have that $v\in \im P$ iff $\exists u\in V: Pu = v$. Then we have $v = Pu = P^2u = Pv$.
\end{proof}

\begin{lemma}
Let $V$ be a vector space and $P$ an idempotent linear map. Then $P' \defeq \id_V - P$ is an idempotent linear map such that
\[ \im P' = \ker P \qquad\text{and}\qquad \ker P' = \im P. \]
Consequently, $V = \im P\oplus \im P'$.
\end{lemma}
\begin{proof}
Clearly $\id_V - P$ is idempotent: $(\id_V - P)^2 = \id_V - P - P + P^2 = \id_V - 2P + P = \id_V - P$. It is enough to show that $\im P' = \ker P$, because $P = \id_V - P'$.

Assume $v\in \ker P$. Then $P'v = v - Pv = v-0 = v$, so $v\in \im P'$.

Assume $v\in \im P'$. Then, by \ref{idempotentImageEquivalence}, $v = P'v = v - Pv$, so $Pv = 0$.

The last remark follows from \ref{directSumKernelImageIdempotent}
\end{proof}



TODO trace:
\begin{lemma}
Let $V$ be a vector space and $P$ an idempotent linear map. Then
\[ \Tr(P) = \dim(\im(P)). \]
\end{lemma}


\subsubsection{Invariant, reducing and irreducible subspaces}
\begin{definition}
Let $V$ be a vector space, $T$ a linear operator on $V$ and $U\subseteq V$ a subspace. Then $U$ is called
\begin{itemize}
    \item \udef{invariant} under $T$ is $T[U]\subseteq U$;
    \item \udef{reducing} for $T$ if $V = U\oplus W$ and both $U$ and $W$ are invariant under $T$;
    \item \udef{irreducible} w.r.t. $T$ if for all $W\subseteq U$ such that $W$ is reducing for $T$, we have $W = U$ or $W = \emptyset$.
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a vector space, $T$ a linear operator on $V$ and $P$ an idempotent operator on $V$ with image $U = P[V]$. Then 
\begin{enumerate}
\item $U$ is invariant under $T$ \textup{if and only if} $PTP = TP$;
\item the following are equivalent:
\begin{enumerate}
\item $U$ is reducing for $T$;
\item $P[V]$ and $(\id_V - P)[V]$ are invariant under $T$;
\item $PT = PTP = TP$;
\item $PT = TP$.
\end{enumerate} \textup{if and only if} .
\end{enumerate}
\end{lemma}
\begin{proof}
(1) The invariance of $U$ under $T$ can be stated as $TP[V] \subseteq \im P$. By \ref{idempotentImageEquivalence} this can be restated as $TPv = PTPv$ for all $v\in V$.

(2) Points (a) and (b) are equivalent by \ref{idempotentImageEquivalence}.

Points (b) and (c) are equivalent by point (1) and $(\id_V - P)T(\id_V - P) = T - PT - TP + PTP = T(\id_V - P) + PTP - PT$.

Point (d) follows immediately from (c). The converse follows from $P(TP) = P(PT) = PT$ and $(PT)P = (TP)P = TP$.
\end{proof}

\subsubsection{Irreducible operators}
\begin{definition}
Let $V$ be a vector space and $T$ an operator on $V$. Then $T$ is called \udef{irreducible} if $V$ is irreducible w.r.t. $T$.
\end{definition}

\section{Sets of vectors}
\begin{proposition}
Let $V$ be vector space and consider a function $f: \powerset{V} \to \powerset{V}$ and define
\[ \mathcal{X} = \setbuilder{X\subseteq V}{A \subseteq X \implies f(A)\subseteq X}. \]
Then $\mathcal{X}$ is closed under arbitrary intersections and thus a complete sublattice of $\powerset(V)$.

The closure operator into $\mathcal{X}$ is given by the intersection of all supersets in $\mathcal{X}$.
\end{proposition}

Most of the types of sets of vectors in this section are of this form.

\subsection{Star-shaped sets}
\begin{definition}
A subset $S$ of a real or complex vector space $V$ is called
\begin{itemize}
\item \udef{star-shaped} at $a\in V$ if for all $x\in S$ and $0\leq r \leq 1$, $rx + (1-r)a\in C$;
\item \udef{absolutely star-shaped} at $a\in V$ if for all $x\in S$ and $|r| \leq 1$, $rx + (1-r)a\in C$.
\end{itemize}
\end{definition}

\subsection{Affine sets}
\begin{definition}
A subset $A$ of a real or complex vector space $V$ is called \udef{affine} if for all $x,y\in A$ and $\lambda\in\F$, $\lambda x + (1-\lambda)y\in A$.

The closure of a set $X$ into the lattice of affine sets is called the \udef{affine hull} of $X$ and is denoted $\affine(X)$.
\end{definition}

\begin{proposition}
Let $V$ be a vector space and $A\subseteq V$ a subset. Then following are equivalent:
\begin{enumerate}
\item $A$ is affine;
\item for all $x\in A$: the set $A-x$ is a vector subspace;
\item for some $x\in A$: the set $A-x$ is a vector subspace.
\end{enumerate}
\end{proposition}
\begin{proof}
$(1) \Rightarrow (2)$ Assume $A$ affine and take arbitrary $x\in A$. We verify the subspace criterion \ref{subspaceCriterion}: clearly $0\in A-x$ because $x\in A$.

Take $y-x, z-x \in A-x$. Then $y-x + z-x = 2\Big(\frac{1}{2}y + \frac{1}{2}z\Big) - x -x \in A-x$.

Take $y-x\in A-x$ and $\lambda\in \R$. Then $\lambda(y-x) = \lambda y + (1-\lambda)x - x \in A-x$.

$(2) \Rightarrow (3)$ Immediate.

$(3) \Rightarrow (1)$ Assume $A-x$ is a vector subspace for some $x\in A$. Take $y,z\in A$ and $\lambda\in \F$. Then $A-x \ni \lambda(y-x)+(1-\lambda)(z-x) = \lambda y + (1-\lambda)z - x$, so $\lambda y + (1-\lambda)z \in A$. 
\end{proof}


\subsection{Balanced set}
\begin{definition}
A subset $B$ of a vector space $V$ over a field $\F$ with valuation $|\cdot|$ is called \udef{balanced} if for all $|r|\leq 1$, $rC \subseteq C$.

\begin{itemize}
\item The closure of a set $X\subseteq V$ into the lattice of balanced sets is called the \udef{balanced hull} of $X$ and is denoted $\balanced(X)$.
\item The dual closure of a set $X\subseteq V$ into the lattice of balanced sets is called the \udef{balanced core} of $X$ and is denoted $\balancedCore(X)$.
\end{itemize}
\end{definition}
Note that $0$ is an element of any balanced set. The lattice of balanced sets is closed under unions and thus a complete sublattice of $\powerset(X)$.

\begin{lemma}
Let $V$ be a vector space and $B\subseteq V$ a subset. Then
\begin{enumerate}
\item $\balanced(B) = \bigcup_{|r|\leq 1}rB = \cball(0,1)\cdot B$;
\item $\balancedCore(B) = \begin{cases}
\bigcap_{|r|\geq 1}rB & 0\in B\\
\emptyset & 0\notin B
\end{cases}$.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $V$ be a vector space and $B\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $B$ is balanced;
\item $\balanced(B) = \cball(0,1)\cdot B \subseteq B$;
\item $\balanced(B) = \cball(0,1)\cdot B = B$;
\item $B\subseteq \balancedCore(B)$;
\item for all $|r|\geq 1$, $C\subseteq rC$;
\item $B$ is symmetric and star-shaped at $0$.
\end{enumerate}
\end{lemma}

\begin{lemma} \label{balancedLemma}
Let $V$ be a vector space and $B\subseteq V$ a balanced subset. Then
\begin{enumerate}
\item for all $\lambda\in \F$: $\lambda B = |\lambda| B$.
\end{enumerate}
\end{lemma}

\begin{lemma} \label{balancedCoreConvexSet}
The balanced core of a convex set is convex.
\end{lemma}
\begin{proof}
Let $B\subseteq V$ be a convex subset of a vector space $V$. Then
$\balancedCore(B) = \begin{cases}
\bigcap_{|r|\geq 1}rB & 0\in B\\
\emptyset & 0\notin B
\end{cases}$. The empty set is convex. For all $r\in \F$, $rB$ is convex by \ref{translationScalingConvexSet} and arbitrary intersections of convex sets are convex.
\end{proof}

\subsection{Convex sets}
\begin{definition}
A subset $C$ of a real or complex vector space $V$ is called \udef{convex} if for all $x,y\in C$ and $0\leq r \leq 1$, $rx + (1-r)y\in C$.

The closure of a set $X\subseteq V$ into the lattice of convex sets is called the \udef{convex hull} of $X$ and is denoted $\convex(X)$.
\end{definition}
Note that this is a stronger property than metric convexity!
\begin{example}
Let $C$ be the set of all vectors with norm in $\Q\cap [0,1]$. The is metrically convex, but not a convex set of vectors.
\end{example}

\begin{lemma}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then $\convex(X) = \setbuilder{rx + (1-r)y}{0\leq r \leq 1, x,y\in B}$.
\end{lemma}

\begin{lemma}
Let $V$ be a vector space an $X\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $X$ is convex;
\item for all $0\leq r \leq 1$, $rX + (1-r)X \subseteq X$.
\end{enumerate}
\end{lemma}

\begin{lemma} \label{translationScalingConvexSet}
Let $V$ be a vector space over $\F$, $v\in V$, $\lambda\in \F$ and $X\subseteq V$ a convex subset subset. Then $v+\lambda X$ is convex.
\end{lemma}
\begin{proof}
Take $v+\lambda x_1, v+\lambda x_2 \in v+\lambda X$ and $r\in [0,1]$. Then
\[ r(v+\lambda x_1) + (1-r)(v+\lambda x_2) = v + \lambda(rx_1 + (1-r)x_2) \in v+\lambda X. \]
\end{proof}

\subsubsection{Absolutely convex sets}
\begin{definition}
A subset $B$ of a vector space $V$ over a field $\F$ with valuation $|\cdot|$ is called \udef{absolutely convex} or \udef{disked} if for all $x,y\in C$ and $|r| \leq 1$, $rx + (1-r)y\in C$.

The closure of a set $X\subseteq V$ into the lattice of absolutely convex sets is called the \udef{absolute convex hull} or \udef{disked hull} of $X$ and is denoted $\disked(X)$.
\end{definition}

\begin{lemma}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $X$ is absolutely convex;
\item $X$ is convex and balanced;
\item for all $x,y\in X$ and $|r| \leq 1$:  $rx + (1-r)y\in X$;
\item for all $|a|+|b| \leq 1$, $aX +bX \subseteq X$;
\item for all $|a|+|b| \leq |c|$, $aX +bX \subseteq cX$.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then $\disked(X) = \convex(\balanced(X))$.
\end{lemma}
In general $\disked(X) \neq \balanced(\convex(X))$.



\subsection{Cones}
\begin{definition}
A subset $C$ of a real or complex vector space $V$ is called a \udef{cone} if for all real $r>0$, $rC \subseteq C$. A cone is called
\begin{itemize}
\item \udef{pointed} if it contains the origin and \udef{blunt} if not;
\item \udef{flat} if $\exists x\neq 0: x\in C \land -x\in C$, and \udef{salient} if not.
\end{itemize}
The closure of a set $X$ into the lattice of cones is called the \udef{conic hull} of $X$ and is denoted $\conic(X)$.
\end{definition}

\begin{lemma}
Let $V$ be a vector space and $X\subseteq V$ a subset. Then $\conic(X) = \R^{> 0}\cdot X$.
\end{lemma}

The closure of a set $X\subseteq V$ into the lattice of cones is given by $\R\cdot X$.

\begin{lemma} \label{coneEqualityLemma}
A subset $C$ of a vector space $V$ is a cone \textup{if and only if} $rC = C$ for all $r> 0$.
\end{lemma}

\begin{lemma} \label{convexityAdditiveClosure}
A cone $C$ is convex if and only if $C + C \subseteq C$. 
\end{lemma}
\begin{proof}
Assume $C$ convex. Take $v,w\in C$, then $v/2 + w/2\in C$ by convexity and so $v+w = 2(v/2+w/2)\in C$.

Assume $C$ closed under addition. Take $v,w\in C$ and $\lambda\in[0,1]$. Then $(1-\lambda)v$ and $\lambda w$ are elements of $C$ and so the convex combination $(1-\lambda)v + \lambda w$ is too.
\end{proof}


\subsection{Absorbing sets}
\begin{definition}
Let $V$ be a vector space and $A,B\subseteq V$. The $A$ \udef{absorbs} $B$ if there exists a real $r>0$ such that for all $|c| \geq r$: $B\subseteq cA$.

The set $A$ is called \udef{absorbing} if it absorbs $\{v\}$ for all $v\in V$.
\end{definition}

\begin{lemma}
Let $V$ be a vector space and $A\subseteq V$ a subset. Then the following are equivalent:
\begin{enumerate}
\item $A$ is absorbing;
\item for all $v\in V$ there exists an $\epsilon\in \F$ such that $\epsilon v\in A$.
\end{enumerate}
\end{lemma}

\subsection{Translation invariance}
TODO Unique factorisation through $(x,y)\mapsto y-x$. (Universal property)

eg kernel, commutator, metric

\subsubsection{Quotient spaces}
TODO: need closed $U$? For quotient map to be continuous? TODO show quotient topology.

\begin{proposition}
Let $V$ be a vector space. Then $\mathfrak{q}\subset V\times V$ is a congruence \textup{if and only if} the set
\[ U_\mathfrak{q} = \setbuilder{w-v}{(v,w)\in\mathfrak{q}} \]
is a vector space.
\end{proposition}
\begin{proof} Then

- $\mathfrak{q}$ is reflexive iff $0\in U_\mathfrak{q}$;

- $\mathfrak{q}$ is symmetric iff $U_\mathfrak{q}$ is closed under multiplication with $-1$;

- $\mathfrak{q}$ is transitive iff $U_\mathfrak{q}$ is closed under addition;

- $\mathfrak{q}$ is a subalgebra of $V\oplus V$ iff $U_\mathfrak{q}$ is closed under addition and scalar multiplication.

As $U_\mathfrak{q}$ is a subset of $V$, we use the subspace criterion.
\end{proof}
Then the equivalences
\[ [v]_\mathfrak{q}=[w]_\mathfrak{q} \iff (v,w)\in\mathfrak{q} \iff w-v\in U_\mathfrak{q} \iff w+U_\mathfrak{q} = v+U_\mathfrak{q} \]
motivate the following definition:
\begin{definition}
Let $V$ be a vector space.
\begin{itemize}
\item An \udef{affine subset} of $V$ is a subset of $V$ of the form $v+U$ for some $v\in V$ and some subspace $U$ of $V$.
\item An affine subset $v+U$ is \udef{parallel} to $U$.
\end{itemize}
Suppose $U$ subspace of $V$. The \udef{quotient vector space} $V/U$ is the vector space of all affine subsets of $V$ parallel to $U$:
\[ V/U = \{ v+U \;|\; v\in V \}, \]
which is a vector space by virtue of being a quotient algebra.

We call the dimension of $V/U$ the \udef{codimension} of $U$ in $V$:
\[ \codim(U) = \dim(V/U). \]
\end{definition}

\begin{proposition}
Let $U$ be a subspace of a vector space $V$. Then
\[ \dim V = \dim U + \dim V/U = \dim U + \codim U.  \]
\end{proposition}
\begin{proof}
Apply the dimension theorem for linear maps to the quotient map.
\end{proof}

\begin{definition}
Let $f:V\to W$ be a linear map of vector spaces. The \udef{cokernel} of $f$ is the quotient space
\[ \coker(f) = W/\im(f). \]
The dimension of the cokernel is called the \udef{corank}.
\end{definition}
\begin{lemma}
Let $U$ be a subspace of a vector space $V$. The codimension of $U$ is the corank of the inclusion $U\hookrightarrow V$:
\[ \codim(U) = \dim\coker(U\hookrightarrow V). \]
\end{lemma}

\begin{proposition} \label{splittingMap}
Let $T\in \Lin(V,W)$. Then $T$ induces a linear map
\[ \tilde{T}: V/\ker(T) \to W: v +\ker(T) \mapsto Tv \]
with the following properties:
\begin{enumerate}
\item $\tilde{T}$ is injective;
\item $\im\tilde{T} = \im T$;
\item $\tilde{T}$ is an isomorphism from $V/\ker(T)$ to $\im T$.
\end{enumerate}
\end{proposition}

 TODO each short exact sequence of vector spaces splits \url{https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem}


\section{Functionals}
\begin{definition}
Let $V$ be a vector space over a field $\mathbb{F}$.
\begin{enumerate}
\item A \udef{functional} on $V$ is a map $V\to \F$;
\item A \udef{linear functional} on $V$ is a linear map from $V$ to $\mathbb{F}$;
\item A \udef{real functional} on $V$ is a map $V\to \R$.
\end{enumerate}
\end{definition}

\begin{lemma} \label{kernelHyperplane}
Let $V$ be a vector space and $U\subseteq V$ a subspace. Then $U$ is a hyperplane \textup{if and only if} it is the kernel of a functional.
\end{lemma}

\begin{lemma} \label{functionalBoundedNeighbourhood}
Let $f: V\to \F$ be a linear functional and $x\notin \ker(f)$. Let $A\subseteq V$ be a balanced set. Then $(x+A)\perp \ker(f)$ \textup{if and only if} $A \subseteq f^{\preimf}(\ball(0,|f(x)|))$.
\end{lemma}
\begin{proof}
Suppose $A \subseteq f^{\preimf}(\ball(0,|f(x)|))$. Then for all $a\in A$: $f(x+a) = f(x) + f(a) \neq 0$.

Conversely, suppose $A \not\subseteq f^{\preimf}(\ball(0,|f(x)|))$, i.e.\ there exists $a\in A$ such that $|f(a)| \geq |f(x)|$. Then $v= -\frac{f(x)}{f(a)}a\in A$, because $A$ is balanced and so $f(x+ v) = f(x)-\frac{f(x)}{f(a)}f(a) = 0$ and so $(x+A) \mesh \ker(f)$.
\end{proof}

\subsection{Real functionals}
\begin{definition}
Let $V$ be a real or complex vector space. Let $f: V\to \R$ be a real functional. We say
\begin{itemize}
\item $f$ is \udef{subadditive} or satisfies the \udef{triangle inequality} if $\forall x,y\in V: f(x+y) \leq f(x) + f(y)$;
\item $f$ is \udef{convex} if $\forall x,y\in V, \lambda\in[0,1]: f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$;
\item $f$ is \udef{positively homogeneous} if $\forall x\in V,\lambda\geq 0: f(\lambda x) = \lambda f(x)$;
\item $f$ is \udef{absolutely homogeneous} if $\forall x\in V,\lambda\in\F: f(\lambda x) = |\lambda| f(x)$;
\item $f$ \udef{separates points} if $\forall v\in V: f(v) = 0 \implies v = 0$.
\end{itemize}
We call $f$
\begin{itemize}
\item \udef{sublinear} if it is subadditive and positively homogeneous;
\item a \udef{seminorm} if it is subadditive and absolutely homogeneous;
\item a \udef{norm} if it is a point-separating seminorm.
\end{itemize}
\end{definition}
TODO general valued fields.

\begin{lemma}
Let $V$ be a real or complex vector space and $f: V\to \R$ be a real functional. Then
\begin{enumerate}
\item absolute homogeneity $\implies$ positive homogeneity;
\item subadditivity+positive homogeneity $\implies$ convexity $\implies$ subadditivity.
\end{enumerate}
\end{lemma}
Thus norms and seminorms are sublinear.

\begin{lemma}
A subadditive, absolutely homogenous function $f:V\to \R$ is non-negative:
\[ f: V\to \R_{\geq 0}. \]
Thus norms and seminorms are functions $V\to \R_{\geq 0}$.
\end{lemma}
\begin{proof}
For all $v\in V$ we have $0 = f(v-v) \leq f(v)+f(-v) = 2f(v)$, so $f(v) \geq 0$.
\end{proof}

\begin{proposition}[Reverse triangle inequality] \label{reverseTriangleInequality}
Let $V$ be a vector space and $\norm{\cdot}: V\to \R$ a function that satisfies the triangle inequality and has $\norm{-v} = \norm{v}$ for all $v\in V$. Then $\forall v,w\in V$:
\begin{enumerate}
\item $|\norm{v}-\norm{w}|\leq \norm{v-w}$;
\item $|\norm{v}-\norm{w}|\leq \norm{v+w}$.
\end{enumerate}
In particular this holds if $\norm{\cdot}$ is a norm or seminorm.
\end{proposition}
\begin{proof}
We calculate $\norm{v} = \norm{v-w+w} \leq \norm{v-w} + \norm{w}$, so $\norm{v}-\norm{w}\leq \norm{v-w}$. By swapping $v\leftrightarrow w$ we also get $-\norm{v}+\norm{w}\leq \norm{w-v} = \norm{v-w}$ and thus the first inequality is established.

For the second inequality, set $w\to -w$ and use $\norm{-w} = \norm{w}$.
\end{proof}

\subsubsection{Epigraphs}
\begin{definition}
Let $V$ be a vector space and $f: V\to \R$ a real functional on $V$. Then \udef{epigraph} of $f$ is defined as
\[ \epigraph(f) \defeq \setbuilder{(v,r)\in V\times \R}{f(v)\leq r}. \]
\end{definition}

\begin{lemma} \label{epigraphLemma}
Let $V$ be a vector space and $f: V\to \R$ a real functional on $V$. Then for all $v\in V$:
\[ f(v) = \inf\setbuilder{r}{(v,r)\in \epigraph(f)}. \]
\end{lemma}

\begin{proposition}
Let $V$ be a real vector space and $f: V\to \R$ a functional. Then
\begin{enumerate}
\item $f$ is convex \textup{if and only if} $\epigraph(f)$ is a convex subset of $V\oplus \R$;
\item $f$ is positively homogeneous \textup{if and only if} $\epigraph(f)$ is a cone in $V\oplus \R$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) First assume $f$ convex and pick $(v, s), (w,t)\in \epigraph(f)$ and $\lambda\in [0,1]$. Then we need to show that $(\lambda v + (1-\lambda)w, \lambda s + (1-\lambda)t) \in \epigraph(f)$. This is equivalent to saying $f(\lambda v + (1-\lambda)w) \leq \lambda s + (1-\lambda)t$. Indeed we have $f(\lambda v + (1-\lambda)w) \leq \lambda f(v) + (1-\lambda)f(w) \leq \lambda s + (1-\lambda)t$ by the convexity of $f$.

Conversely, assume $\epigraph(f)$ convex. Then $(v, f(v)), (w,f(w))\in \epigraph(f)$, $(\lambda v + (1-\lambda)w, \lambda f(v) + (1-\lambda)f(w)) \in \epigraph(f)$ for all $\lambda\in [0,1]$. This implies $f(\lambda v + (1-\lambda)w) \leq \lambda f(v) + (1-\lambda)f(w)$.

(2) First assume $f$ is positively homogeneous, take $(v,s)\in \epigraph(f)$ and $r>0$. Then we need to show that $r(v,s) = (rv,rs)\in \epigraph(f)$. This follows because of the implications $f(v)\leq s \implies rf(v) \leq rs \implies f(rv) \leq rs$.

Conversely, assume that $\epigraph(f)$ is a cone. Then $\lambda\cdot \epigraph(f) = \epigraph(f)$ for all $\lambda>0$ by \ref{coneEqualityLemma}. We then calculate using \ref{epigraphLemma}:
\begin{align*}
f(\lambda v) &= \inf\setbuilder{r}{(\lambda v,r)\in \epigraph(f)} \\
&= \inf\setbuilder{r}{(\lambda v,r)\in \lambda\cdot\epigraph(f)} \\
&= \inf\setbuilder{r}{\lambda(v,\lambda^{-1}r)\in \lambda\cdot\epigraph(f)} \\
&= \inf\setbuilder{r}{(v,\lambda^{-1}r)\in \epigraph(f)} \\
&= \inf\setbuilder{\lambda r}{(v,r)\in \epigraph(f)} = \lambda f(v).
\end{align*} 
\end{proof}
\begin{corollary}
A functional on a real vector space is sublinear \textup{if and only if} its epigraph is a convex cone.
\end{corollary}

\subsubsection{Convex functionals}

\begin{proposition}
Let $p: V\to\R$ be convex functional. Then
\[ P: V\to\R: x\mapsto \inf_{t>0} t^{-1}p(tx) \]
is sublinear and $P(x)\leq p(x)$.

Also, if $f:V\to \R$ is a linear functional, then $f\leq p \iff f\leq P$.
\end{proposition}
\begin{proof}
For sublinearity: let $x,y\in V$, then for all $s,t>0$
\[ P(x+y) \leq \frac{s+t}{st}p\left(\frac{st}{s+t}(x+y)\right) = \frac{s+t}{st}p\left(\frac{s}{s+t}(tx)+\frac{t}{s+t}(sy)\right) \leq t^{-1}p(tx) + s^{-1}p(sy). \]
This implies that $P(x+y)\leq P(x)+P(y)$.

For positive homogeneity: let $x\in V,\lambda\geq 0$
\[ P(\lambda x) = \inf_{t>0} t^{-1}p(t\lambda x) = \inf_{t\lambda>0} \lambda (t\lambda)^{-1}p(t\lambda x) = \inf_{t>0} \lambda (t)^{-1}p(tx) = \lambda P(x). \]

Finally we prove that $f\leq p \implies f\leq P$ for linear functionals $f$. For all $t>0$ we have $f(tx) \leq p(tx)$, which implies $f(x) = t^{-1}f(tx) \leq t^{-1}p(tx) \leq P(x)$. So $f\leq P$.
\end{proof}

\subsubsection{Seminorms}
\begin{lemma}
The kernel of a seminorm is a vector space.
\end{lemma}
Note this does not follow from \ref{kernelSubspace} because seminorms are not linear.
\begin{proof}
Let $p:V\to \R$ be a seminorm. We verify the subspace criterion \ref{subspaceCriterion}. First $0\in\ker(p)$ because $p(0) = p(0\cdot 0) = |0|p(0) = 0$.

Now take $v,w\in \ker(p)$ and $\lambda\in \F$. Then $0\leq p(v+\lambda w) \leq p(v)+|\lambda|p(w) = 0$, so $v+\lambda w\in\ker(p)$.
\end{proof}

\begin{proposition} \label{gaugeSeminorms}
Let $V$ be a vector space, $p: V\to \R$ a seminorm and $\lambda\in \F$. Then
\[ \setbuilder{v\in V}{p(v) < \lambda} \qquad\text{and}\qquad \setbuilder{v\in V}{p(v) \leq \lambda} \]
are absolutely convex and absorbent.
\end{proposition}
\begin{proof}
Take $|\mu| + |\nu| \leq 1$ and $v,w\in\setbuilder{v\in V}{p(v) \leq \lambda}$. Then $p(|\mu|v + |\nu|w)\leq |\mu|p(v) + |\nu|p(w) \leq (|\mu|+|\nu|)\lambda \leq \lambda$.

For absorbence, take $v\in V$. Then $\frac{\lambda}{2p(v)} v\in \setbuilder{v\in V}{p(v) < \lambda}$.
\end{proof}

\subsubsection{Gauges}
\begin{definition}
Let $V$ be a vector space and $A\subseteq V$ an absorbent subset. The function
\[ p_A: V\to \R^{\geq 0}: v\mapsto \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda A} \]
is called the \udef{gauge} or \udef{Minkowski functional} of $A$.
\end{definition}
The function $p_A$ is well-defined (i.e.\ $p_A(v)$ is finite for all $v\in V$) because $A$ is absorbent.

\begin{lemma} \label{gaugeLemma}
Let $V$ be a vector space and $A\subseteq V$ an absorbent subset and $\lambda\in \R^{> 0}$. Then
\begin{enumerate}
\item $\lambda > p_A(v) \implies \lambda^{-1}v\in A$;
\item $\lambda^{-1}v\in A \implies \lambda \geq p_A(v)$.
\end{enumerate}
In particular, we have
\[ p_A^{\preimf}[\ball(0,1)] = \setbuilder{v\in V}{p_A(v) < 1} \subseteq A \subseteq \setbuilder{v\in V}{p_A(v) \leq 1} = p_A^{\preimf}[\cball(0,1)]. \]
\end{lemma}

\begin{proposition} \label{gaugeClassification}
Let $V$ be a vector space and $f: V\to \R^{\geq 0}$ a function.
Then the following are equivalent:
\begin{enumerate}
\item $f$ is positively homogenous;
\item for all $A\subseteq V$ such that $f^{\preimf}(\ball(0,1)) \subseteq A \subseteq f^{\preimf}(\cball(0,1))$, $A$ is absorbent and $f = p_A$;
\item $f = p_A$ for some absorbent subset $A$.
\end{enumerate}
\end{proposition}
Note that positive homogeneity is equivalent to strictly positive homogeneity.
\begin{proof}
$(1) \Rightarrow (2)$ To show absorbence, take some $v\in V$. Then for any $\epsilon>0$, $f\big((f(v)+\epsilon)^{-1}v\big) = \frac{f(v)}{f(v)+\epsilon} < 1$, so $(f(v)+\epsilon)^{-1}v\in A$.

Now take some $A$ and fix some arbitrary $v\in V$. We have $p_A(v) = \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda A}$, so
\[ \begin{aligned}
p_A(v) &\leq \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda f^{\preimf}(\ball(0,1))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in f^{\preimf}(\ball(0,\lambda))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{f(v) < \lambda} = f(v)
\end{aligned} \quad\text{and}\quad \begin{aligned}
p_A(v) &\geq \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda f^{\preimf}(\cball(0,1))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in f^{\preimf}(\cball(0,\lambda))} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{f(v) \leq \lambda} = f(v).
\end{aligned} \]
We conclude that $f(v) = p_A(v)$.

$(2) \Rightarrow (3)$ Immediate.

$(3) \Rightarrow (1)$ We calculate for $t \geq 0$
\begin{align*}
f(tv) &= \inf\setbuilder{\lambda\in \R^{\geq 0}}{tv\in \lambda A} = \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in t^{-1}\lambda A} \\
&= \inf\setbuilder{t\lambda\in \R^{\geq 0}}{v\in \lambda A} = t\inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \lambda A} = tf(v).
\end{align*}
\end{proof}

\begin{lemma} \label{gaugeZeroLemma}
Let $V$ be a vector space, $A\subseteq V$ an absorbent subset and $a\in A$. If there exists a subspace $U\subseteq A$ such that $a\in U$, then $p_A(a) = 0$.
\end{lemma}
\begin{proof}
For all $\epsilon > 0$, $\epsilon^{-1}a\in A$, so $a\in \epsilon A$.
\end{proof}

\begin{proposition}
Let $V$ be a vector space and $A\subseteq V$ an absorbent subset. Then
\begin{enumerate}
\item $p_A$ is absolutely homogenous if $A$ is balanced;
\item $p_A$ is subadditive if $A$ is convex;
\item $p_A$ is point-separating if $A$ is balanced and contains only the trivial subspace.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) By \ref{balancedLemma} we have $\mu A = |\mu| A$ and thus
\begin{align*}
p_A(\mu\cdot v) &= \inf\setbuilder{\lambda\in \R^{\geq 0}}{\mu\cdot v\in \lambda A} = \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \frac{\lambda}{\mu} A} \\
&= \inf\setbuilder{\lambda\in \R^{\geq 0}}{v\in \frac{\lambda}{|\mu|} A} = \inf\setbuilder{|\mu|\lambda\in \R^{\geq 0}}{v\in \lambda A} = |\mu|\cdot p_A(v).
\end{align*}

(2) Take $v,w\in V$. Now take arbitrary $\epsilon > 0$, so $(p_A(v)+\epsilon)^{-1}v \in A$ and $(p_A(w)+\epsilon)^{-1}w \in A$ by \ref{gaugeLemma}. By convexity of $A$, we have
\[ \frac{v+w}{p_A(v)+p_A(w)+2\epsilon} = \frac{p_A(v)+\epsilon}{p_A(v)+p_A(w)+2\epsilon}(p_A(v)+\epsilon)^{-1}v + \frac{p_A(w)+\epsilon}{p_A(v)+p_A(w)+2\epsilon}(p_A(w)+\epsilon)^{-1}w \in A. \]
By \ref{gaugeLemma} this means $p_A(v)+p_A(w)+2\epsilon \geq p_A(v+w)$ and because $\epsilon$ was arbitrary, we conclude that $p_A(v+w) \leq p_A(v)+p_A(w)$.

(3) Assume $A$ contains only the trivial subspace. Then for all $v\in V$ there exists some $\lambda\in \F$ such that $\lambda\cdot v\notin A$. Now for all $|c|\geq |\lambda|$, $c\cdot v\notin A$ because $A$ is balanced. Then $p_A(2\lambda\cdot v) \neq 0$ and because $p_A$ is absolutely homogeneous we have $p_A(v) = (2\lambda)^{-1}p_A(2\lambda\cdot v) \neq 0$.
\end{proof}
\begin{corollary}
The gauge of an absolutely convex and absorbent subset is a seminorm. If the subset contains only the trivial subspace, then the gauge is a norm.
\end{corollary}

\begin{proposition}
Let $V$ be a vector space, $A,B\subseteq V$ absolutely convex and absorbent subsets and $\mathcal{E}$ a set of absolutely convex and absorbent subsets.
\begin{enumerate}
\item For all $\lambda\in \F\setminus\{0\}$, the gauge of $\lambda A$ is $|\lambda|^{-1}p_A$.
\item The gauge of $\bigcap \mathcal{E}$ is $\sup\setbuilder{p_K}{K\in \mathcal{E}}$.
\item If $A\subseteq B$, then $p_B \leq p_A$.
\end{enumerate}
\end{proposition}

\subsection{Hahn-Banach extension theorems}
\begin{theorem}[Hahn-Banach majorised by convex functionals] \label{convexHahnBanach}
Let $V$ be a real vector space, $U\subset V$ a subspace and $p$ a convex functional on $V$. Let $f:U\to\R$ be a linear functional that is bounded by $p$:
\[ \forall u\in U: \quad f(u) \leq p(u). \]
Then $f$ has an extension $\tilde{f}: V\to \R$ such that $\tilde{f}$ is a linear functional on $V$ bounded by $p$:
\[ \forall v\in V: \tilde{f}(v) \leq p(v) \qquad \text{and} \qquad \forall u\in U: \tilde{f}(u) = f(u). \]
\end{theorem}
\begin{proof}
As a first step, we want to extend $f$ to a functional $g$ on a space that is one dimension larger than $U$. This means $g$ is of the form
\[ g: U\oplus\Span\{v_1\}\to\R: v + \alpha v_1 \mapsto f(v) + \alpha c \]
for some $v_1\in V\setminus U$.

If we want $g$ to be majorised by $p$, then we need to find a $c$ such that
\[ \forall v\in U: \forall \alpha\in\R: \; g(\alpha v_1 + v) = \alpha c + f(v) \leq p(\alpha v_1 + v) \]
this means that we need
\[ \forall v\in U: \forall \alpha\in\R:\; \frac{-p(v - |\alpha|v_1) + f(v)}{|\alpha|} \leq c \leq \frac{p(v + |\alpha|v_1) - f(v)}{|\alpha|} \]
and we can find such a $c$ if and only if
\[ \forall v\in U: \forall \alpha\in\R:\; -p(v - |\alpha|v_1) + f(v) \leq p(v + |\alpha|v_1) - f(v), \]
which is equivalent to $2f(v) \leq p(v+|\alpha|v_1)+p(v-|\alpha|v_1)$. This follows from
\begin{align*}
f(v) \leq p(v) &= p(\tfrac{1}{2}(v+|\alpha|v_1) + \tfrac{1}{2}(v-|\alpha|v_1)) \\
&\leq \tfrac{1}{2}p(v+|\alpha|v_1) + \tfrac{1}{2}p(v-|\alpha|v_1).
\end{align*}
So we can extend the domain of $f$ by one dimension such that it is still majorised by $p$.

An extension by multiple dimensions is determined by a subset of $V\times \R$. Consider the family of all such subsets that determine a majorised extension of $f$. This is a family of finite character. We apply the Teichmüller-Tukey lemma, \ref{ZornEquivalents}, to obtain a maximal element.

This maximal element has domain $V$, because if it did not, it could be extended and was not a maximal element.
\end{proof}
Clearly if $V$ has a well-ordered Hamel basis, we do not need choice as we can just take successive $v$s in the basis and find $c$s constructively.
\begin{corollary}[Hahn-Banach majorised by sublinear functionals] \label{sublinearHahnBanach}
Any majorant $p$ that is sublinear is also convex and can be used in the Hahn-Banach theorem.
\end{corollary}
\begin{corollary}[Hahn-Banach majorised by seminorms] \label{seminormHahnBanach}
Let $(\mathbb{F},V,+)$ be a real or complex vector space, $U\subset V$ a subspace and $p$ a seminorm on $V$. Let $f:U\to\mathbb{F}$ be a linear functional that is bounded by $p$:
\[ \forall u\in U: \quad |f(u)| \leq p(u). \]
Then $f$ has an extension $\tilde{f}: V\to \R$ such that $\tilde{f}$ is a linear functional on $V$ bounded by $p$:
\[ \forall v\in V: |\tilde{f}(v)| \leq p(v) \qquad \text{and} \qquad \forall u\in U: \tilde{f}(u) = f(u). \]
\end{corollary}
\begin{proof}
For \emph{real} vector fields, we notice that every seminorm is a sublinear function, so we can use \ref{sublinearHahnBanach} to find an extension $\tilde{f}$. We then just need to check it satisfies $\forall v\in V: |\tilde{f}(v)| \leq p(v)$.
From \ref{sublinearHahnBanach} we know $\forall v\in V: \tilde{f}(v) \leq p(v)$.
To prove $-\tilde{f}(v) \leq p(v)$, we calculate
\[ -\tilde{f}(v) = \tilde{f}(-v) \leq p(-v) = |-1|p(v) = p(v). \]

For \emph{complex} vector fields, we can write $f= f_1 + if_2$ with $f_1,f_2$ real functionals on $U$, which can also be seen as a real vector space. First take $f_1$. Now $\forall u\in U f_1(u) \leq |f(x)| \leq p(x)$, so we can extend $f_1$ to $\tilde{f}_1$ by \ref{sublinearHahnBanach}.

Now by complex linearity, $if(u) = f(iu)$ so
\[ i[f_1(u) + if_2(u)] = -f_2(u) + if_1(u) = f_1(iu) + if_2(iu) \implies f_2(u) = -if_1(iu). \]
So we set $\tilde{f}(v) = \tilde{f}_1(v)-i\tilde{f}_1(iv)$. It is easy to show $\tilde{f}$ is $\C$-linear. For boundedness, write $\tilde{f}(v) = |\tilde{f}(v)|e^{i\theta}$ then
\[ |\tilde{f}(v)| = e^{-i\theta}\tilde{f}(v) = \tilde{f}(e^{-i\theta}v) = \tilde{f}_1(e^{-i\theta}v) \leq p(e^{-i\theta}v) = |e^{-i\theta}|p(v) = p(v). \]
\end{proof}

\begin{corollary}
Let $X$ be a normed space and $Z\subset X$ a subspace. Any bounded linear functional in $\tdual{Z}$ can be extended to a bounded linear functional in $\tdual{X}$ with the same norm.
\end{corollary}
\begin{proof}
Let $f:Z\to \mathbb{F}$ be such a functional. Extend $f$ by the previous theorem, \ref{seminormHahnBanach}, using $p(x) = \norm{f}_Z\norm{x}$.
\end{proof}
\begin{corollary} \label{existenceBoundedFunctionalOfSameNorm}
Let $X$ be a normed space and $x_0\neq 0$ an element of $X$. Then there exists a bounded linear functional $\omega_{x_0}$ such that
\[ \norm{\omega_{x_0}} = 1 \qquad \text{and} \qquad \omega_{x_0}(x_0)=\norm{x_0}. \]
\end{corollary}
\begin{proof}
Extend the functional $f: \Span\{x_0\}\to \mathbb{F}$ defined by
\[ f(x) = f(ax_0) = a\norm{x_0}. \]
\end{proof}
\begin{corollary}
Let $X$ be a normed space. Then $\forall x\in X:$
\[ \norm{x} = \sup_{\substack{f\in X' \\ f\neq 0}}\frac{|f(x)|}{\norm{f}}. \]
\end{corollary}
\begin{proof}
We calculate
\[ \norm{x} \geq \sup_{\substack{f\in X' \\ f\neq 0}}\frac{|f(x)|}{\norm{f}} \geq \frac{|\omega_{x}(x)|}{\norm{\omega_{x}}} = \frac{\norm{x}}{1} = \norm{x} \]
where the first inequality follows from $|f(x)|\leq \norm{f}\norm{x}$ for all $f\in X', x\in X$.
\end{proof}

\subsubsection{Hahn-Banach separation}

\begin{lemma} \label{gaugeSeparationLemma}
Let $V$ be a real vector space, $A$ an absorbent set and $x_0 \notin A$. Consider the functional $f_{x_0}: \Span\{x_0\}\to \F: tx_0 \mapsto t$. Then $f_{x_0}(x)\leq p_A(x)$ for all $x\in \Span\{x_0\}$.
\end{lemma}
\begin{proof}
Let $x = tx_0$. If $t\leq 0$, then the inequality is immediate. Suppose $t>0$. Because $p_A(x_0) \geq 1$ (by the converse of \ref{gaugeLemma}), we have
\[ f_{x_0}(x) = f_{x_0}(tx_0) = t \leq tp_A(x_0) = p_A(tx_0) = p_A(x)  \]
using positive homogeneity (\ref{gaugeClassification}).
\end{proof}

\begin{proposition}
Let $V$ be a real or complex vector space, $a\in V$ and $A$ a
subset such that $A-a$ is absolutely convex and absorbent. If $U$ is a subspace such that $A\perp U$, then there exists a hyperplane $H \supseteq U$ such that $A\perp H$.
\end{proposition}
\begin{proof}
Consider the set $U+A-a$. This is absolutely convex and absorbent.

Then we have
\[ U\perp A \implies 0\notin U+A \implies -a \notin U+A-a \implies a \notin U+A-a. \]
Consider the functional $f_{a}$ of \ref{gaugeSeparationLemma}, which is majorised by the gauge $p_{U+A-a}$. Then $f_a$ can be extended to all $V$ by the Hahn-Banach extension theorem \ref{seminormHahnBanach}.

We note that $U\subseteq \ker(f_a)$, because $p_{U+A-a}(u) = 0$ by \ref{gaugeZeroLemma}.

In order to conclude with \ref{functionalBoundedNeighbourhood}, we need to show that $A-a \subseteq f_a^{\preimf}(\ball(0,|f_a(a)|)) = f_a^{\preimf}(\ball(0,1))$.
Indeed $A-a \subseteq U+A-a \subseteq p_{U+A-a}^\preimf[\cball(0,1)] \subseteq f_{a}^\preimf[\cball(0,1)]$.
\end{proof}

\begin{proposition}
Let $A$ be a open convex subset of a locally convex TVS and $M$ a vector subspace such that $A\perp M$. Then there exists a closed hyperplane $H\supseteq M$ such that $A\perp H$.
\end{proposition}



\subsubsection{Banach limits}
\begin{proposition}
There exists a linear map $L:l^\infty(\N) \to \C$ satisfying
\begin{enumerate}
\item $\displaystyle L(x) = \lim_{n\to \infty}x_n$ if the limit exists;
\item $L((x_{n+1})_{n\in\N}) = L((x_n)_{n\in\N})$;
\item if $\forall n\in\N:x_n\geq 0$, then $L(x) \geq 0$;
\item $\norm{L} = 1$.
\end{enumerate}
Such a linear map is called a \udef{Banach limit}.
\end{proposition}
\begin{proof}
TODO, after Cesàro means.
\end{proof}





\input{linearAlgebra/representation}

\input{linearAlgebra/normsInnerProducts}

\input{linearAlgebra/multilinear}

\input{linearAlgebra/matrices}











\chapter{Indices and symbols}
\section{Contravariant and covariant vectors and tensors}
When working in finite-dimensional spaces with specified bases, we often get expressions of the form
\[ v = \sum_{i=1}^n a_i \vec{e}_i. \]
We may replace this expression with
\[ v = a^i \vec{e}_i \]
if we take the convention that if an index is repeated once up and once down, then there is a sum over all values of that index. This is the \udef{Einstein summation convention}.

Note that coordinates have their indices up, and basis vectors have their indices down.

Now in the dual space, we have the dual basis $\{\varphi^j\}_j$. In the dual space we take the opposite convention: coordinates have their indices down, and dual basis vectors have their indices up, so
\[ \varphi = b_j \varphi^j. \]
This allows us to write
\[ \varphi(v) = \varphi(a_i \vec{e}_i) = a_i b^j \varphi^j(\vec{e}_i) = a_i b^i \]
where for the last equality we have used that $\varphi^j(\vec{e}_i)$ only does not vanish if $i=j$ and is $1$ in this case.

We call vectors in $V$ \udef{contravariant} vectors and vectors in $V^*$ \udef{covariant} vectors, or covectors.

Per convention we put the coordinates of contravariant vectors in column vectors. This means, by proposition \ref{transpDual}, we must put the coordinates of covariant vectors in row vectors. Indeed, let $v=a^i \vec{e}_i\in V$ and $\varphi = b_j\varphi^j \in V^*$, then
\[ \varphi(v) = a^ib_i = \begin{bmatrix}
a^1 & \hdots & a^n
\end{bmatrix}\begin{bmatrix}
b_1 \\ \vdots  \\ b_n
\end{bmatrix}. \]

We can view covariant vectors as functions that take a contravariant vector and produce a number, and we can view contravariant vectors as functions that take a covariant vector and produce a number. In general we may have linear functions that accept several co- and contravariant vectors and produce a number. By (TODO), such functions are tensor products of various co- and contravariant vectors. They would have multiple up- and down-indices. e.g\
\[ \vec{T} = \tensor{T}{^i_j_k^l^m}(\vec{e}_i\otimes \vec{e}^j\otimes \vec{e}^k\otimes \vec{e}_l \otimes \vec{e}_m) \]

Where $\tensor{T}{^i_j_k^l^m}$ are the coordinates w.r.t. the basis vectors $\vec{e}_i\otimes \vec{e}^j\otimes \vec{e}^k \otimes\vec{e}_l \otimes \vec{e}_m$.

For example, once the basis has been chosen, matrices map contravariant vectors to contravariant vectors. And contravariant vectors map covariant vectors to numbers, so by reverse currying a matrix is maps a contravariant and a covariant vector to a number.

For the indices of matrices we have taken the convention that the first index is for rows and the second for the columns. For a constant row index, the column index spells out a covariant vector, so the column index is down. Conversely, the row index is up. A matrix $A$ with components $(A)_{i,j}$ becomes
\[ \tensor{A}{^i_j}(\vec{e}_i\otimes \vec{e}^j). \]
This is consistent with the observation that the matrix sends a vector to a function on covectors, in other words is a function which accepts vectors in first place and covectors in second place.

In the expressions so far only repeated indices were present. Such repeated indices are called  \udef{bound indices} or \udef{dummy indices}. They may be replaced in the expression by other letters, so long as there is no clash. If an index is not repeated, it is a \udef{free index} and may not just be changed.

\subsection{``Tensors are objects that transform as tensors''}
Variants:
\begin{itemize}
\item ``$N$ arbitrary numbers are not the components of a vector'' (Peres p.65)
\end{itemize}


\section{Covectors}

\subsection{Multi-index notation}
Let $e_1,\ldots, e_n$ be a basis for a real vector space $V$. Let $\alpha^1,\ldots, \alpha^n$ be the dual basis for $V^*$. A \udef{multi-index}
\[ I = (i_1,\ldots,i_k)\]
is a $k$-tuple of numbers $\in (1,\ldots,n)$. We write
\[ \begin{cases}
e_I \defeq e_{i_1}\otimes\ldots\otimes e_{i_k}\\
\alpha^I \defeq \alpha^{i_1}\wedge\ldots \wedge\alpha^{i_k}
\end{cases}. \]
The covector $\alpha^I$ is completely determined by the values in $I$, the order only changes the sign. A multi-index $I = (i_1,\ldots,i_k)$ is \udef{ascending} if
\[ 1\leq i_1<\ldots<i_k\leq n. \]
\begin{proposition}
Let $I,J$ be ascending multi-indices of length $k$, then
\[ \alpha^I(e_J) = \begin{cases}
1 & I=J \\ 0& I\neq J
\end{cases}. \]
\end{proposition}
\begin{proposition}
The covectors $\alpha^I$, with $I$ an ascending multi-index of length $k$, form a basis of $A_k(V)$.
\end{proposition}
\begin{corollary}
If $\dim V=n$, then
\[ \dim A_k(V) = \begin{pmatrix}
n\\k
\end{pmatrix}. \]
\end{corollary}

\section{Symmetrisation and anti-symmetrisation of indices}

\[ T_{\{a_1\dots a_n\}} = \frac{1}{n!} \sum_{\sigma\in S_n} T_{a_{\sigma(1)} \dots a_{\sigma(n)}} \]

\[ T_{[a_1\dots a_n]} = \frac{1}{n!} \sum_{\sigma\in S_n} (\sgn \sigma)T_{a_{\sigma(1)} \dots a_{\sigma(n)}} \]
\section{Symbols}
\subsection{Kronecker delta}
\begin{definition}
The \udef{Kronecker delta} is defined by
\[ \delta_{ij} = \delta^i_j = \begin{cases}
1 & (i=j) \\
0 & (i \neq j)
\end{cases}.\]
\end{definition}
\subsection{Levi-Civita symbol}
\begin{definition}
The \udef{Levi-Civita symbol} is defined by
\[ \varepsilon_{a_{1}\ldots a_{n}} = \begin{cases}
+1 & \text{$(a_{1},\ldots, a_{n})$ is an even permutation of $(1,\ldots, n)$} \\
-1 & \text{$(a_{1},\ldots, a_{n})$ is an odd permutation of $(1,\ldots, n)$} \\
0 & \text{otherwise}
\end{cases}.\]
The indices may be placed up or down.
\end{definition}

\begin{lemma} \label{LeviCivitaProduct}
The Levi-Civita symbol is given by the explicit expression
\[ \varepsilon_{a_{1}\ldots a_{n}} = \prod_{1\leq i<j\leq n}\sgn(a_j-a_i).\]
\end{lemma}

\begin{proposition}
Working in $n$ dimensions, when all $i_1,\ldots i_n;j_1,\ldots, j_n$ take values in $\{ 1,\ldots, n \}$:
\begin{enumerate}
\item $\displaystyle \varepsilon_{i_1\ldots i_n}\varepsilon^{j_1\ldots j_n} = n!\delta^{j_1}_{[i_1}\ldots \delta^{j_n}_{i_n]} = \sum_{\sigma\in S_n} (-1)^{{\sgn}(\sigma)} \delta^{j_1}_{i_{\sigma(1)}} \dots \delta^{j_n}_{i_{\sigma(n)}}$
\item $\displaystyle \varepsilon _{i_{1}\dots i_{n}}\varepsilon ^{i_{1}\dots i_{n}}=n!$
\item $\displaystyle \varepsilon _{i_{1}\dots i_{k}~i_{k+1}\dots i_{n}}\varepsilon ^{i_{1}\dots i_{k}~j_{k+1}\dots j_{n}}=k!(n-k)!~\delta _{[i_{k+1}}^{j_{k+1}}\dots \delta _{i_{n}]}^{j_{n}}$.
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item Both sides of the equation are a sum over the same indices. We consider each term in the sum separately and show that the sums are equal term-by-term. We split the terms into two categories.
\begin{enumerate}
\item First consider the case that $j_1\ldots j_n$ is not a permutation of $(1,\ldots, n)$, i.e.\ a number is repeated. Then $\varepsilon_{i_1\ldots i_n}\varepsilon_{j_1\ldots j_n}$ is automatically zero. The right-hand side is definitely zero if the $i$s do not take the same values as the $j$s. If they do take the same values, there is a number that is repeated at least twice. For every term in the sum over permutations, there is another term with the repeated $i$s swapped, which also adds a minus due to the change of sign of the permutation. Hence the sum over permutations is zero.
\item Now assume that $j_1\ldots j_n$ is a permutation of $(1,\ldots, n)$. Then either the $i$s are also a permutation, or $\delta^{j_1}_{i_{\sigma(1)}} \dots \delta^{j_n}_{i_{\sigma(n)}}$ is always zero. The only possible non-zero term is with a $\sigma\in S_n$ such that $j_k = i_{\sigma(k)}$ for all $k$. If $\sgn(i_1,\ldots, i_n) = \sgn(j_1,\ldots, j_n)$, then $\sgn(\sigma)=1$ and both sides match. If $\sgn(i_1,\ldots, i_n) = -\sgn(j_1,\ldots, j_n)$, then $\sgn(\sigma)=-1$ and both sides again match. 
\end{enumerate}
So, in fact, we have shown something slightly stronger, namely 
\[ \varepsilon_{i_1\ldots i_n}\varepsilon_{j_1\ldots j_n} = n!\delta^{j_1}_{[i_1}\ldots \delta^{j_n}_{i_n]} \]
where there is no sum over indices.
\item The number of permutations of any $n$-element set number is exactly $n!$. Every permutation is either even or odd and $(+1)^2 = (-1)^2 = 1$. Non-permutations do not contribute to the sum.
\item The sum on the left only has terms where the $i$s and $j$s are permutations of $(1,\ldots, n)$. In each such term we can bring the indices with values $1-k$ to the first $k$ spots, each by a transposition. Because both Levi-Civita symbols have the same first $k$ indices, each will need the same number of transpositions and thus the sign does not change. Then by considering lemma \ref{LeviCivitaProduct} we see that we have obtained a product of cases 1. and 2. This yields the answer.

\end{enumerate}
\end{proof}
\begin{corollary}
In two dimensions, where all $i,j,m,n$ each take values in $\{1,2\}$,
\begin{enumerate}
\item $\varepsilon _{ij}\varepsilon ^{mn}={\delta _{i}}^{m}{\delta _{j}}^{n}-{\delta _{i}}^{n}{\delta _{j}}^{m}$
\item $\varepsilon _{ij}\varepsilon ^{in}={\delta _{j}}^{n}$
\item $\varepsilon _{ij}\varepsilon ^{ij}=2.$
\end{enumerate}
\end{corollary}
\begin{corollary}
In three dimensions, where all $i,j,k,m,n$ each take values in $\{1,2,3\}$,
\begin{enumerate}
\item $\varepsilon _{ijk}\varepsilon ^{imn}={\delta _{j}}^{m}{\delta _{k}}^{n}-{\delta _{j}}^{n}{\delta _{k}}^{m}$
\item $\varepsilon _{jmn}\varepsilon ^{imn}={\delta _{j}}^{i}$
\item $\varepsilon _{ijk}\varepsilon ^{ijk}=6.$
\end{enumerate}
\end{corollary}
\begin{proposition}
Working in 3 dimensions,
\begin{align*}
\varepsilon _{ijk}\varepsilon _{lmn}&={\begin{vmatrix}\delta _{il}&\delta _{im}&\delta _{in}\\\delta _{jl}&\delta _{jm}&\delta _{jn}\\\delta _{kl}&\delta _{km}&\delta _{kn}\\\end{vmatrix}}\\[6pt]&=\delta _{il}\left(\delta _{jm}\delta _{kn}-\delta _{jn}\delta _{km}\right)-\delta _{im}\left(\delta _{jl}\delta _{kn}-\delta _{jn}\delta _{kl}\right)+\delta _{in}\left(\delta _{jl}\delta _{km}-\delta _{jm}\delta _{kl}\right).
\end{align*}
This can directly be generalised to $n$ dimensions.
\end{proposition}

\section{Writing matrix operations using using tensor notation}
A matrix $A$ with components $(A)_{i,j}$ becomes
\[ \tensor{A}{^i_j}(\vec{e}_i\otimes \vec{e}^j). \]
\subsection{Trace}
The trace of $\tensor{A}{^i_j}$ is $\tensor{A}{^i_i}$.
\subsection{Matrix multiplication}
\[ \tensor{(AB)}{^{i}_{k}}=\tensor{A}{^{i}_{j}}\tensor{B}{^{j}_{k}} \]
which in particular for matrix-vector multiplication becomes
\[ (Av)^i = \tensor{A}{^i_j} v^j. \]
\subsection{Transpose}
The transpose of $\tensor{A}{^i_j}$ is $\tensor{(A^\transp)}{^j_i}$.

Or: $(A^\transp)_{ab} = A_{ba}$ and $\tensor{(A^\transp)}{_i^j} = \tensor{A}{^j_i}$?
\subsection{Determinant}
\begin{align*}
\det(A) &= \varepsilon^{j_1\ldots j_n}\tensor{A}{^{1}_{j_1}}\ldots \tensor{A}{^{n}_{j_n}} \\
&= \frac{1}{n!}\varepsilon_{i_1\ldots i_n}\varepsilon^{j_1\ldots j_n}\tensor{A}{^{i_1}_{j_1}}\ldots \tensor{A}{^{i_n}_{j_n}}
\end{align*}

\chapter{Ordered vector spaces}
TODO link ordered groups.
\begin{definition}
Let $\sSet{\R, V, +}$ be a real vector space and $\precsim$ a preorder on the set $V$. Then $\precsim$ is a \udef{vector preorder} if it is compatible with the vector space structure as follows: $\forall x,y,z\in V, \lambda\in\R$
\begin{enumerate}
\item $x\precsim y$ implies $x+z \precsim y+z$;
\item if $\lambda\geq 0$, then $x \precsim y$ implies $\lambda x \precsim \lambda y$.
\end{enumerate}
We call $(\R, V, +, \precsim)$ a \udef{preordered vector space}.

\begin{itemize}
\item If $\precsim$ is a partial order, we call $(\R, V, +, \precsim)$ a \udef{partially ordered vector space} or simply a \udef{ordered vector space}.
\item If $\sSet{V, \precsim}$ is a lattice, we call $(\R, V, +, \precsim)$ a \udef{vector lattice} or a \udef{Riesz space}.
\end{itemize}
\end{definition}

\begin{lemma} \label{positiveConeOrderCharacterisation}
Let $\sSet{\R, V, +}$ be a real vector space and $\precsim$ a preorder on the set $V$. The compatibility of the order can equivalently be expressed by:
$\forall x,y,z\in V, \lambda\in\R$
\[ \begin{cases}
\text{$x \precsim y$ implies $x+z \precsim y+z$;} \\
\text{if $\lambda\geq 0$ and $0 \precsim x$, then $0 \precsim \lambda x$.}
\end{cases} \]
\end{lemma}

\begin{lemma} \label{elementaryVectorPreorderManipulations}
Let $V$ be a preordered vector space. For all $v,w \in V$ we have
\[ v \precsim w \;\iff\; 0 \precsim  w - v \;\iff\; -w \precsim -v.  \]
\end{lemma}
\begin{proof}
We get the implications
\[ v \precsim w \implies 0 \precsim  w - v \implies -w \precsim -v \implies v-w \precsim 0 \implies v\precsim w \]
by subsequently adding $-v, -w, v,w$ to both sides by compatibility of the order.
\end{proof}
\begin{corollary}
Let $V$ be a preordered vector space and $\alpha \in \R\setminus\{0\}$. Then for all $v,w\in V$
\[ v \precsim w \quad \iff \quad \begin{cases}
\alpha v \precsim \alpha w & (0 < \alpha) \\
\alpha v \succsim \alpha w & (\alpha > 0)
\end{cases}. \] 
\end{corollary}

\begin{lemma} \label{additionVectorInequalities}
Let $V$ be a preordered vector space. For all $v,w, x, y \in V$ we have
\[ \begin{cases}
v \precsim w \\ x \precsim y
\end{cases} \implies v+ x \precsim w+y. \]
\end{lemma}
\begin{proof}
We calculate $v + x \leq w + x \leq w+y$.
\end{proof}

\begin{example}
\begin{itemize}
\item The finite-dimensional vector spaces $\R^n$ with coordinate-wise addition, scalar multiplication and order are Riesz spaces.
\item The finite-dimensional vector spaces $\R^n$ with coordinate-wise addition, scalar multiplication and lexicographical order are Riesz spaces.
\item Let $X$ be a set. The set $(X\to \R)$ is a real vector space with point-wise addition and scalar multiplication. If the order is also defined point-wise, i.e.\ $f \leq g$ iff $\forall x\in X: f(x) \leq g(x)$, then $(X\to \R)$ is a Riesz space.
\end{itemize}
\end{example}

\begin{lemma}
Let $X$ be a topological space. The spaces
\begin{enumerate}
\item $\cont(X,\R)$;
\item $\cont_0(X,\R)$;
\item $\cont_c(X,\R)$; and
\item $\cont_b(X,\R)$
\end{enumerate}
with point-wise operations are Riesz spaces.
\end{lemma}
\begin{proof}
In all these cases the join and meet of $f,g$ are given by
\begin{align*}
f \vee g &= \frac{1}{2}(f+g)+ \frac{1}{2}|f-g| \\
f \wedge g &= \frac{1}{2}(f+g) - \frac{1}{2}|f-g|.
\end{align*}
So the join and meet are still continuous and have the same properties as $f,g$.
\end{proof}

\section{Upsets and downsets}
\begin{lemma} \label{sumMultipleUpDownsets}
Let $V$ be a preordered vector space, $S\subseteq V$ a subset, $v\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha > 0$, then $(\alpha S)^u = \alpha S^u$ and $(\alpha S)^l = \alpha S^l$;
\item if $\alpha < 0$, then $(\alpha S)^u = \alpha S^l$ and $(\alpha S)^l = \alpha S^u$;
\item $(S+v)^u = S^u + v$ and $(S+v)^l = S^l + v$.
\end{enumerate}
\end{lemma}
\begin{corollary}
Let $V$ be a preordered vector space, $S\subseteq V$ a subset, $v\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item $\sup(S+v) = \sup(S)+v$ and $\inf(S+v) = \inf(S)+v$;
\item if $\alpha > 0$, then $\sup(\alpha S) = \alpha \sup(S)$ and $\inf(\alpha S) = \alpha \inf(S)$;
\item if $\alpha < 0$, then $\sup(\alpha S) = \alpha \inf(S)$ and $\inf(\alpha S) = \alpha \sup(S)$.
\end{enumerate}
\end{corollary}

\section{The positive cone}
\begin{definition}
Let $V$ be a preordered vector space. The subset
\[ V^+ \defeq \setbuilder{v\in V}{0 \precsim v} \]
is called the \udef{positive cone} of $V$. The elements of the positive cone $V^+$ are called the \udef{positive elements} of $V$.
\end{definition}
That the positive cone is in fact a cone follows from \ref{positiveConeOrderCharacterisation}
\begin{proposition} \label{positiveCone}
Let $V$ be a vector space.
\begin{enumerate}
\item A vector preorder on $V$ is uniquely determined by its positive cone:
\[ x \precsim y \quad\iff\quad y-x \in V^+. \]
\item The positive cone of a vector preorder is pointed and convex.
\item Any pointed convex cone in $V$ determines a (unique) vector preorder.
\item A vector preorder is a partial order \textup{if and only if} the positive cone is salient.
\end{enumerate}
\end{proposition}
Convexity is equivalent to closure under addition (see \ref{convexityAdditiveClosure})
\begin{proof}
(1) This is just \ref{elementaryVectorPreorderManipulations}.

(2) $V^+$  is pointed by reflexivity: $0\precsim 0$. It is closed under addition by \ref{additionVectorInequalities}.

(3) Compatibility with addition is immediate from the definition of the order. Compatibility with scalar multiplication is due to it being a cone (see \ref{positiveConeOrderCharacterisation}). Reflexivity is equivalent with pointedness. Finally transitivity follows from closure under addition:
\begin{align*}
 \begin{cases}
x\precsim y \\ y\precsim x
\end{cases} &\iff \quad \begin{cases}
0 \precsim y -x \\ 0 \precsim z-y
\end{cases} \iff\quad \begin{cases}
y-x \in V^+ \\ z-y \in V^+
\end{cases} \\
&\implies (z-y)+(y-x) = z-x \in V^+ \iff x \precsim z. 
\end{align*}

(4) We have $x\precsim y$ and $y\precsim x$ iff $(y-x) \in V^+$ and $-(y-x) \in V^+$. Thus both salience and anti-symmetry are equivalent to this situation implying $x-y = 0$.
\end{proof}


\begin{lemma} \label{scalarMultiplicationInequalities}
Let $V$ be a preordered vector space, $v\in V^+$ and $\alpha\in \R$.
\begin{enumerate}
\item If $\alpha \geq 1$, then $\alpha v \succsim v$.
\item If $\alpha \leq 1$, then $\alpha v \precsim v$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $v\succsim 0$ and $(\alpha-1) \geq 0$, so $(\alpha-1)v \succsim 0$ and $\alpha v \succsim v$.

(2) We have $v\succsim 0$ and $(\alpha-1) \leq 0$, so $(\alpha-1)v \precsim 0$ and $\alpha v \precsim v$.
\end{proof}

\section{Riesz spaces}

\begin{lemma} \label{lemmaRieszSpaces}
Let $V$ be a Riesz space, $u,v,w\in V$ and $\alpha\in \R$, then
\begin{enumerate}
\item $-(v \wedge w) = (-v)\vee (-w)$ and $-(v \vee w) = (-v)\wedge (-w)$;
\item if $\alpha \geq 0$, then $\alpha(v \wedge w) = (\alpha v)\wedge (\alpha w)$ and $\alpha(v \vee w) = (\alpha v)\vee (\alpha w)$;
\item if $\alpha \leq 0$, then $\alpha(v \wedge w) = (\alpha v)\vee (\alpha w)$ and $\alpha(v \vee w) = (\alpha v)\wedge (\alpha w)$;
\item $u+(v \wedge w) = (u+v)\wedge (u+w)$ and $u+(v \vee w) = (u+v)\vee (u+w)$.
\end{enumerate}
\end{lemma}
\begin{proof}
We apply \ref{imagePolars} to

(1) the reverse order-embedding $v\mapsto -v$;

(2) the order-embedding $v\mapsto \alpha v$ for $\alpha > 0$; (if $\alpha = 0$ the result is trivial);

(3) the reverse order-embedding $v\mapsto \alpha v$ for $\alpha > 0$; (if $\alpha = 0$ the result is trivial);

(4) the order-embedding $v\mapsto u+v$.
\end{proof}

\begin{proposition}[Riesz decomposition]
Let $V$ be a Riesz space and $v,w_1,w_2\in V^+$ such that $v \leq w_1 + w_2$. Then $\exists v_1, v_2\in V^+$ such that $v = v_1 + v_2$ and $v_1 \leq w_2, v_2 \leq w_2$.
\end{proposition}
\begin{proof}
Set $v_1 = v\wedge w_1$ and $v_2 = v - v_1$. These satisfy all the properties. We verify the inequality $v_2 \leq w_2$: from $w_2 \geq v - w_1$ we get
\[ w_2 = 0\vee w_2 \geq 0\vee (v - w_1) = v + (-v)\vee(-w_1) = v - v\wedge w_1 = v-v_1 = v_2. \]
\end{proof}

\begin{proposition} \label{sumAsMeetJoin}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ (v \vee w) + (v \wedge w) = v+w. \]
\end{proposition}
\begin{proof}
We calculate
\[ (v \vee w) + (v \wedge w) = v + 0 \vee (w-v) + w + (v-w)\wedge 0 = (v + w) + 0 \vee (w-v) - 0 \vee (w-v) = v + w. \]
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space, $u,v,w\in V$ and $x,y,z\in V^+$. Then
\begin{enumerate}
\item $(u+v)\vee (2w) \leq u\vee w + v\vee w$;
\item $(u+v)\vee z \leq u\vee z + v\vee z$;
\item $(x+y)\wedge z \leq x\wedge z + y\wedge z$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) From $u\leq u\vee w$ and $v\leq v\vee w$, we get $u+v \leq u\vee w + v\vee w$. Similarly from $w\leq u\vee w$ and $w\leq v\vee w$, we get $2w \leq u\vee w + v\vee w$. Together this gives (1).

(2) We have $2z \geq z$ by \ref{scalarMultiplicationInequalities}.

(3) TODO (use Birkhoff inequality??)
\end{proof}

\begin{proposition}[Infinite distributivity in Riesz spaces]
Let $V$ be a Riesz space, $v\in V$ and $S\subseteq V$ a subset. Then
\begin{enumerate}
\item if $\bigvee S$ exists, then $\left(\bigvee S\right) \wedge v = \bigvee (S\wedge v)$;
\item if $\bigwedge S$ exists, then $\left(\bigwedge S\right) \vee v = \bigwedge (S\vee v)$;
\end{enumerate}
\end{proposition}
\begin{proof}
We already have the inequality $\left(\bigvee S\right) \wedge v \geq \bigvee (S\wedge v)$ from \ref{infiniteDistributiveInequalities}. To show the other inequality, it is enough to show that for 
\end{proof}
\begin{corollary}
Riesz spaces are distributive lattices.
\end{corollary}

\subsection{Positive elements}
\subsubsection{Positive and negative parts}
\begin{definition}
Let $V$ be a Riesz space and $v\in V$. Then we define
\begin{align*}
v^+ &\defeq v \vee 0 \\
v^- &\defeq (-v) \vee 0 = - (v \wedge 0).
\end{align*}
We call $v^+$ the \udef{positive part} of $v$ and $v^-$ the \udef{negative part} of $v$.
\end{definition}

\begin{lemma} \label{MeetJoinAsPositiveNegative}
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $v\vee w = (v-w)^+ + w = (v-w)^- + v$;
\item $v\wedge w = v - (v-w)^+ = w - (v-w)^-$.
\end{enumerate}
\end{lemma}
\begin{proof}
We calculate $v\vee w = (v-w)\vee 0 + w = (v-w)^+ + w$. The other equalities are similar.
\end{proof}

\begin{proposition} \label{PositiveNegativeElements} \label{minimalPositiveDecomposition} 
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $v^+, v^- \in V^+$;
\item $v= v^+ - v^-$;
\item $v^+\perp v^-$ (i.e.\ $v^+ \wedge v^- = 0$).
\end{enumerate}
Furthermore,
\begin{enumerate} \setcounter{enumi}{3}
\item if $p,q\in V$ satisfy 1 and 2, i.e.\ $p,q\in V^+$ and $v = p-q$, then $p \geq v^+$ and $q \geq v^-$; we may say $v=v^+-v^-$ is the minimal such decomposition; 
\item the elements $v^+, v^-$ are uniquely determined by properties 2 and 3. 
\end{enumerate}
Also
\begin{enumerate} \setcounter{enumi}{5}
\item $(-v)^- = v^+$ and $(-v)^+ = v^-$;
\item if $\alpha \geq 0$, then $(\alpha v)^+ = \alpha v^+$ and $(\alpha v)^- = \alpha v^-$;
\item $-v^- \leq v \leq v^+$;
\item $v\leq w$ \textup{if and only if} $v^+ \leq w^+$ and $v^- \geq w^-$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Evident from definitions.

(2) We calculate $v^+ - v = (v \vee 0) - v = (v-v) \vee (0-v) = 0\vee (-v) = v^-$.

(3) We calculate $0 = v^- - v^- = v^-  + (v\wedge 0) = (v^- + v)\wedge (0 + v^-) = v^+ \wedge v^-$.

(4) From $v\leq p$ and $0\leq p$, we get $v^+ = v \vee 0 \leq p$. Then we also have $v^- = v^+ - v \leq p - v = q$.

(5) Assume $p,q\in V$ satisfy (2) and (3), then (1) automatically follows from (3). Using \ref{MeetJoinAsPositiveNegative}, we calculate
\[ 0 = p\wedge q = p - (p-q)^+ = p - v^+. \]
So $p = v^+$ and $q = p - v = v^+ - v = v^-$.

(6) It is evident that $(-v)^- = (--v)\vee 0 = v\vee 0$.

(7) We calculate $\alpha v^+ = \alpha (v \vee 0) = (\alpha v) \vee 0 = (\alpha v)^+$; the calculation for $\alpha v^-$ is similar.

(8) This is clear from $-v^- = v\wedge 0 \;\leq\; v \;\leq\; v \vee 0 = v^+$.

(9) $v\leq w$ implies $v^+ = v\vee 0 \leq w\vee 0 = w^+$ and $-v^- = v\wedge 0 \leq w\wedge 0 = - w^-$.

Conversely, we have $v = v^+ - v^- \leq w^+ - w^- = w$.
\end{proof}


\begin{proposition} \label{triangleInequalityPositiveNegativeElements}
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $(v+w)^+ \leq v^+ + w^+$;
\item $(v+w)^- \leq v^- + w^-$.
\end{enumerate}
\end{proposition}
\begin{proof}
From \ref{PositiveNegativeElements} we get $v \leq v^+$ and $w\leq w^+$, so $v+w \leq v^+ + w^+$. Also $0 \leq v^+ + w^+$. So
\[ (v+w)^+ = (v+w)\vee 0 \leq v^+ + w^+. \]
Then we also have
\[ (v+w)^- = (-v-w)^+ \leq (-v)^+ + (-w)^+ = v^- + w^-. \]
\end{proof}

\subsubsection{Absolute value}
\begin{definition}
Let $V$ be a Riesz space and $v\in V$. Then the \udef{absolute value} of $v$ is
\[ |v| \defeq v\vee (-v) = -(v\wedge (-v)). \]
\end{definition}

If the Riesz space is a real function space with pointwise order, then $|f| = |\cdot|\circ f$ as usual, where $|\cdot|: \R\to \R$ is the usual absolute value function.

\begin{lemma} \label{absoluteValue}
Let $V$ be a Riesz space, $v,w\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item $|v| = v^+ + v^-$;
\item $|v| \in V^+$;
\item if $v\in V^+$, then $|v| = v$;
\item $|v| = |-v|$;
\item $|\alpha v| = |\alpha|\cdot |v|$;
\item $\big||v|\big| = |v|$;
\item $0 \leq v^+ \leq |v|$ and $0 \leq v^- \leq |v|$;
\item $|v| = 0$ \textup{if and only if} $v = 0$.
\end{enumerate}
\end{lemma}
\begin{proof}
We prove (1):
\[ |v| = v\vee (-v) = (2v)\vee 0 - v = 2v^+ - v = 2v^+ - (v^+ - v^-) = v^+ - v^- \]
and (6), using the absorption law:
\[ \big||v|\big| = |v|\vee (-|v|) = v \vee (-v) \vee \big( v\wedge (-v) \big) = v \vee (-v) = |v|. \]
The rest are immediate consequences, using the results of \ref{PositiveNegativeElements}.
\end{proof}

\begin{lemma}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $(v+w)\vee (v-w) = v + |w|$;
\item $(v+w)\wedge (v-w) = v - |w|$;
\end{enumerate}
or, equivalently,
\begin{enumerate} \setcounter{enumi}{2}
\item $v \vee w = \frac{1}{2}\big(v+w + |v - w|\big)$;
\item $v \wedge w = \frac{1}{2}\big(v+w - |v - w|\big)$.
\end{enumerate}
\end{lemma}
\begin{proof}
We calculate, using \ref{lemmaRieszSpaces}
\[ (v+w)\vee (v-w) = v+ w\vee(-w) = v+ |w| \quad\text{and}\quad (v+w)\wedge (v-w) = v + w\wedge(-w) = v-|w|. \]
The next two equalities follow by the substitutions $v+w \leftrightarrow v$ and $v-w \leftrightarrow w$.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |v - w| = (v \vee w) - (v \wedge w). \]
\end{corollary}
\begin{corollary} \label{meetJoinAbsoluteValues}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $|v| \vee |w| = \frac{1}{2}\Big(|v|+|w| + \big||v| - |w|\big|\Big)$;
\item $|v| \wedge |w| = \frac{1}{2}\Big(|v|+|w| - \big||v| - |w|\big|\Big)$.
\end{enumerate}
\end{corollary}
\begin{proof}
Substitute $v\to |v|$ and $w\to |w|$.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $u,v,w\in V$, then
\begin{enumerate}
\item $|u\vee v - u\vee w| + |u\wedge v - u\wedge w| = |v-w|$;
\item $|v^+-w^+|\leq |v-w|$ and $|v^- - w^-|\leq |v-w|$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Using the proposition, we get
\[ |u\vee v - u\vee w| + |u\wedge v - u\wedge w| = (u\vee v)\vee(u\vee w) - (u\vee v)\wedge (u\vee w) + (u\wedge v)\vee(u\wedge w) - (u\wedge v)\wedge(u\wedge w). \]
Using distributivity this simplifies to $(v\vee w) - (v\wedge w) = |v-w|$.

(2) Using (1) we have
\[ |v-w| = |0\vee v - 0\vee w| + |0\wedge v - 0\wedge w| = |v^+ -w^+| + |v^- - w^-| \geq \begin{cases}
|v^+ -w^+| \\
|v^- - w^-|.
\end{cases}  \]
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $|v|\vee |w| = \frac{1}{2}\Big( |v+w| + |v - w| \Big)$;
\item $|v|\wedge |w| = \frac{1}{2}\Big| |v+w| - |v - w| \Big|$;
\end{enumerate}
or, equivalently,
\begin{enumerate} \setcounter{enumi}{2}
\item $|v+w|\vee |v-w| = |v| + |w|$;
\item $|v+w|\wedge |v-w| = \big| |v| - |w| \big|$.
\end{enumerate}
Also
\begin{enumerate} \setcounter{enumi}{4}
\item $|v|+|w| = |v+w| + |v-w| - \big||v|-|w|\big|$;
\item $|v+w|+|v-w| = 2|v| + 2|w| - \big||v+w|-|v-w|\big|$;
\end{enumerate}
and
\begin{enumerate} \setcounter{enumi}{6}
\item $|v|+|w| = \big||v|-|w|\big| + \big||v+w|-|v-w|\big|$.
\end{enumerate}
\end{proposition}
The only real trick is in the proof of (1). All the other results follow from elementary substitutions.
\begin{proof}
(3,4,6) Are equivalent to (1,2,5) by the replacements $v \leftrightarrow v+w$ and $w \leftrightarrow v-w$.

(1) We calculate
\begin{align*}
|v|\vee |w| &= v\vee (-v)\vee w \vee (-w) = \big(v\vee(-w)\big)\vee \big((-v)\vee w\big) \\
&= \frac{1}{2}\Big((v-w) + |v + w|\Big)\vee \frac{1}{2}\Big( (-v+w) + |- v - w| \Big) \\
&= \frac{1}{2}|v+ w| + \frac{1}{2}\big((v-w)\vee (-v+w)\big) = \frac{1}{2}\Big( |v+w| + |v - w| \Big).
\end{align*}

(5) Using \ref{sumAsMeetJoin}, (1) and \ref{meetJoinAbsoluteValues} we get
\begin{align*}
|v|+|w| &= |v|\vee|w| + |v|\wedge |w| \\
&= \frac{1}{2}\Big( |v+w| + |v - w| \Big) + \frac{1}{2}\Big(|v|+|w| - \big||v| - |w|\big|\Big).
\end{align*}
This simplifies to the required equation.

(7) Follows from substituting (6) into (5).

(2) Follows from (7) and \ref{meetJoinAbsoluteValues}.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $v,w\in V$, then the following are equivalent:
\begin{enumerate}
\item $|v|\wedge |w| = 0$;
\item $|v+w| = |v-w|$;
\item $|v|\vee |w| = |v+w|$.
\end{enumerate}
\end{corollary}

The absolute value also satisfies the triangle inequality.
\begin{proposition}[Triangle and reverse triangle inequality in Riesz spaces]
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |v| + |w| \geq \big|v+w\big| \geq \big||v|-|w|\big|. \]
\end{proposition}
\begin{proof}
The first inequality is the triangle inequality. It follows straight from \ref{triangleInequalityPositiveNegativeElements}.

The second inequality is the reverse triangle inequality and follows from the triangle inequality as in \ref{reverseTriangleInequality}.
\end{proof}

\subsection{Subsets}
\begin{definition}
Let $V$ be a Riesz space. A subset $E$ is called
\begin{itemize}
\item a \udef{Riesz subspace} if it is both a subspace and a sublattice;
\item \udef{solid} if for all $v\in E$ the interval $[-|v|,|v|]$ is a subset of $E$;
\item a \udef{band} if for all subsets $S\subseteq E$ we have $\sup(S) \subset E$. 
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |w|\leq |v| \iff -|v| \leq w \leq |v|. \]
\end{lemma}
\begin{proof}
We have $w \leq |w|$ and $|w| \leq |v|$, so $w\leq |v|$. Also $-w \leq |-w| = |w|$, so $-|v| \leq -|w| \leq w$.

Conversely, $-|v| \leq w$ implies $-w\leq |v|$. So $|w| = w\vee (-w) \leq |v|$.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $E\subseteq V$ a subset. Then $E$ is solid \textup{if and only if}
\[ \forall v\in E: \forall w\in V: \; |w|\leq |v| \implies w\in E. \]
\end{corollary}

\begin{lemma}
Let $V$ be a Riesz space and $E\subseteq V$ a subset. Then $E$ is an (order) ideal \textup{if and only if} it is a solid Riesz subspace.
\end{lemma}
\begin{proof}
TODO
\end{proof}


\subsection{Disjointness}

\subsection{Archimedean}

\chapter{Some results and applications}
\section{Rotations}
Rodrigues' rotation formula

eigenvectors and eigenvalues of rotation.
\section{Pauli matrices}

\[ \sigma_x = \begin{pmatrix}
0 & 1 \\ 1 & 0
\end{pmatrix} \qquad \sigma_y = \begin{pmatrix}
0 & -i \\ i & 0
\end{pmatrix} \qquad \sigma_z = \begin{pmatrix}
1 & 0 \\ 0 & -1
\end{pmatrix} \]
All have eigenvalues $\pm 1$. The eigenspaces are spanned by
\[ v_{x+} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ 1
\end{pmatrix}, \quad v_{x-} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ -1
\end{pmatrix}, \quad v_{y+} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ i
\end{pmatrix}, \quad v_{y-} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ -i
\end{pmatrix}, \quad v_{z+} = \begin{pmatrix}
1 \\ 0
\end{pmatrix}, \quad v_{z-} = \begin{pmatrix}
0 \\ 1
\end{pmatrix}, \quad  \]

\[ \Tr[\sigma_i \sigma_j] = \delta_{ij} \]

