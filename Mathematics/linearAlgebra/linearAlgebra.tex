\chapter{Vector spaces}

Gauss-Jordan reduction

TODO projective transformations

orientation
\url{https://en.wikipedia.org/wiki/Orientation_(vector_space)}
also for fixed set of $n$ vectors

\url{http://www.physics.rutgers.edu/~gmoore/618Spring2018/GTLect2-LinearAlgebra-2018.pdf}

\section{Formal definition}
A vector space is a collection of vectors, which are objects that have a natural addition and scalar multiplication.
\begin{definition}
A \udef{vector space} over a field $\mathbb{F}$ is a set $V$ together with an \udef{addition}
\[ +: V\times V \to V \]
and a \udef{scalar multiplication}
\[ \cdot: \mathbb{F}\times V \to V \]
such that $(V,+)$ is a commutative group and the following properties hold:
\begin{itemize}[leftmargin=4cm]
\item[\textbf{Distributivity 1}] $\lambda\cdot(v+w) = \lambda v + \lambda w$ for all $\lambda \in \mathbb{F}$ and all $v,w \in V$.
\item[\textbf{Distributivity 2}] $(\lambda_1+\lambda_2)\cdot v = \lambda_1 v + \lambda_2 v$ for all $\lambda_1, \lambda_2 \in \mathbb{F}$ and all $v \in V$.
\item[\textbf{Mixed associativity}] $\lambda_1\cdot(\lambda_2\cdot v) = (\lambda_1 \lambda_2) \cdot v$ for all $\lambda_1, \lambda_2 \in \mathbb{F}$ and all $v \in V$.
\item[\textbf{Multiplicative identity}] $1\cdot v = v$ for all $v \in V$.
\end{itemize}
This vector space can be denoted $\sSet{\mathbb{F}, V, +}$.
\end{definition}
In the definition we have used the following convention: for all $v,w\in V$ and $\lambda\in \mathbb{F}$, we denote $+(v,w)$ as $v+w$ and $\cdot(\lambda, v)$ as $\lambda \cdot v$ or $\lambda v$.

We call the elements of the field \udef{scalars} and the elements of the set $V$ \udef{vectors}. The zero of the group is known as the \udef{zero vector}.

Almost always we will actually be interested in $\mathbb{F} = \R$ or $\mathbb{F} = \C$.
\subsection{Examples}
\begin{enumerate}
\item The $n$-tuples in $\mathbb{F}^n$ with pointwise addition and multiplication. If the entries of the $n$-tuples are written one above the other in a column, it is called a \udef{column vector}.
\item The polynomials in $\mathbb{F}[X]$.
\item The polynomials in $\mathbb{F}[X]_{\leq n}$ of maximally degree $n$.
\item For any set $S$, the functions $(S\to \mathbb{F})$, denoted $\mathbb{F}^S$, with pointwise addition and multiplication.
\item For any topological space $X$, the continuous functions in $(X\to \C)$, denoted $\cont(X)$.
\item The trivial vector space $\{ 0\}$. A vector space can never be empty, because a commutative group always has a neutral element.
\item The set of all possible \textit{displacements} in (Euclidean) space forms a vector space. Once we have chosen an origin, we can view space as a vector space.
\end{enumerate}
\subsection{Some elementary lemmas}
\begin{lemma}
Given the vector space $(\mathbb{F}, V, +)$  and arbitrary $u,v,w\in V$ and $\lambda \in \mathbb{F}$, we have
\begin{enumerate}
\item $0v = 0 = \lambda \cdot 0$;
\item $(-1)v = -v = 1(-v)$;
\item $(-\lambda)v = -(\lambda v) = \lambda(-v)$;
\item $u+v = w+v \implies u = w$.
\end{enumerate}
By $-v$ we mean the additive inverse of $v$.
\end{lemma}
\begin{proof}
\begin{enumerate}
\item First, use distributivity to get
\[ 0v = (0+0)v = 0v + 0v. \]
The apply the previous lemma to $0+0v = 0v = 0v+0v$ to get $0=0v$. The equality $\lambda\cdot 0 = 0$ is proved analogously.
\item To show that $(-1)\cdot v$ is the additive inverse of $v$, i.e.\ $-v$, we simply add $(-1)\cdot v + v$ and observe the result is $0$.
\[ (-1)\cdot v + v = (-1)\cdot v + 1\cdot v = (1+(-1))\cdot v = 0\cdot v = 0. \]
\item Similar to the previous point.
\item The additive inverse $-v$ exists, so we can just add it left and right.
\end{enumerate}
\end{proof}
\subsection{Subspaces}
\begin{definition}
A \textit{subset} $U$ of a vector space $V$ is called a \udef{subspace} of $V$ if $U$ is also a vector space.
\end{definition}
The subset $U$ automatically inherits a lot of the structure of $V$. We only need to verify a couple of conditions.
\begin{proposition}[Subspace criterion] \label{subspaceCriterion}
A subset $U$ of a vector space $V$ is a subspace of $V$ \textup{if and only if} $U$ satisfies the following conditions:
\begin{enumerate}
\item \textbf{Additive identity}: $0 \in U$. Alternatively it is enough to show that $U$ is not empty.
\item \textbf{Closed under addition}: $v,w \in U$ implies $v+w\in U$;
\item \textbf{Closed under scalar multiplication}: $\lambda \in \mathbb{F}$ and $u\in U$ implies $\lambda u \in U$.
\end{enumerate}
\end{proposition}
Alternatively the last two criteria are equivalent to:
\[ v,w\in U; \lambda \in \mathbb{F} \qquad \text{implies} \qquad v+\lambda w \in U. \]

If the question is whether a set is a subspace, this criterion is almost always the answer. An elementary application:
\begin{proposition}
Any arbitrary intersection of subspaces is a subspace.
\end{proposition}

\section{Basis and dimension}
\subsection{Linear combinations and span}
\begin{definition}
A \udef{linear combination} of vectors $v_1, \ldots, v_n$ is a vector of the form
\[ a_1v_1 + \ldots + a_nv_n \]
where $a_1, \ldots, a_n \in \mathbb{F}$.
\end{definition}
\begin{definition}
The set of all linear combinations of a set of vectors $D$ in $V$ is called called the \udef{span} of $D$.
\[ \Span_{\mathbb{F}}(D) = \left\{ \sum_i a_iv_i \;|\; v_i\in D, a_i \in \mathbb{F} \right\} \]
If $D=\emptyset$, we conventionally say that $\Span(D) = \{0\}$.

$\Span(D)$ is also written $<D>$.
\end{definition}
Note that $D$ may be an infinite set, but the linear combinations are always finite sums. Often the set $D$ is simply a finite set $v_1,\ldots, v_n$

\begin{proposition}
The span of a set of vectors in $V$ is the smallest subspace of $V$ containing all the vectors in the set.
\end{proposition}

\begin{definition}
If $V = \Span(D)$, then the set $D$ \udef{spans} $V$.
\end{definition}

\begin{lemma}
Let $D,E$ be subsets of a vector space $V$.
\begin{enumerate}
\item $\Span(D) = \Span(\Span(D))$;
\item $D\subset E \implies \Span(D) \subset \Span(E)$.
\end{enumerate}
\label{span}
\end{lemma}

\begin{definition}
A vector space is \udef{finite-dimensional} if it is spanned by a finite set of vectors.

A vector space is \udef{infinite-dimensional} if it is not finite-dimensional.
\end{definition}
\subsection{Linear independence}
\begin{definition}
A set of vectors $D$ is \udef{linearly independent} if the only linear combinations in $D$ that equal $0$ are the trivial ones with all scalars zero. i.e.\,
\[ \text{If} \qquad \sum_{i=1}^n a_iv_i = 0 \qquad \text{then} \qquad a_1=\ldots=a_n = 0  \]
assuming the $v_i$ are vectors and the $a_i$ are scalars.

\udef{Linear dependence} is the opposite of linear independence.
\end{definition}
The empty set $D=\emptyset$ is taken as linearly independent. No non-trivial combinations of vectors in $\emptyset$ are equal to zero, because there are no non-trivial combinations of vectors in $\emptyset$.

\begin{lemma}
Let $D$ be a linearly dependent set of vectors. Then there exists a vector $v\in D$ such that
\begin{enumerate}
\item $v$ is a linear combination of other vectors in $D$;
\item $v\in \Span(D\setminus\{v\})$;
\item $\Span(D) = \Span(D\setminus\{v\})$.
\end{enumerate}
\label{linearDependence}
\end{lemma}
\begin{proof}
Take a linear combination of vectors in $D$ equalling zero,
\[ \sum_i a_iv_i = 0. \]
By linear dependence such a combination can be found such that not all $a_i$ are zero. In particular at least two must be non-zero. Take $a_j\neq 0$. Then
\[ v_j = \sum_{i\neq j}\frac{a_iv_i}{a_j}. \]

To prove the last point, take a $u\in \Span(D)$. Then
\[ u = \sum_i b_iv_i = b_j v_j + \sum_{i\neq j} b_iv_i = b_j\sum_{i\neq j}\frac{a_iv_i}{a_j} + \sum_{i\neq j} b_iv_i = \sum_{i\neq j}\left(\frac{b_ja_i}{a_j}+b_i\right)v_i.  \]
So $u\in \Span(D\setminus\{v\})$. The opposite inclusion is obvious. 
\end{proof}

\subsection{Bases}
\begin{definition}
A \udef{basis} of a vector space $V$ is a set of vectors in $V$ that spans $V$ and is linearly independent.
\end{definition}
\begin{example}
The \udef{standard basis} or \udef{natural basis} of $\mathbb{F}^n$ is given by
\begin{align*}
(1,0,0,&\ldots,0), \\
(0,1,0,&\ldots,0), \\
(0,0,1,&\ldots,0), \\
&\ldots \\
(0,0,0,&\ldots,1).
\end{align*}
We will denote it $\mathcal{E}$ or $\mathcal{E}_n$.
\end{example}
\subsubsection{In finite-dimensional spaces}
\begin{proposition}
A finite set $\{v_1, \ldots, v_n\}$ of vectors in $V$ is a basis of $V$ \textup{if and only if} every $v\in V$ can be written uniquely in the form
\[ v = a_1v_1 + \ldots + a_nv_n, \]
where $a_1, \ldots, a_n \in \mathbb{F}$.
\end{proposition}
\begin{proof}
We prove both directions.
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Suppose $\{v_1, \ldots, v_n\}$ is a basis of $V$. Then any vector $v$ can be written as $a_1v_1 + \ldots + a_nv_n$, because the basis spans the space. We just need to show the decomposition is unique. To that end, assume there was another decomposition $v = b_1v_1 + \ldots + b_nv_n$. Subtracting both decompositions gives
\[ 0 = (a_1-b_1)v_1 + \ldots + (a_n-b_n)v_n. \]
Because $\{v_1, \ldots, v_n\}$ is linearly independent, $a_i = b_i$ for all $i$.
\item[$\boxed{\Leftarrow}$] Now suppose every vector has such a decomposition. Clearly $\{v_1, \ldots, v_n\}$ spans $V$. The unique decomposition of $0$ gives linear independence.
\end{itemize}
\end{proof}

\begin{theorem}[Steinitz exchange lemma] \label{SteinitzExchange}
Let $V$ be a vector space.
If $U = \{u_1, \ldots, u_m\}$ is a linearly independent set of $m$ vectors in $V$, and $W = \{ w_1, \ldots, w_n \}$ spans $V$, then:
\begin{enumerate}
\item $m\leq n$;
\item There is a set $\{u_1, \ldots, u_m, w'_{m+1}, \ldots, w'_n\} \supset U$ that spans $V$ where $w'_{m+1},\ldots, w'_n \in W$.
\end{enumerate}
\end{theorem}
\begin{proof}
We obtain the set $\{u_1, \ldots, u_m, w'_{m+1}, \ldots, w'_n\}$ by starting with the list $B_0 = (w_1, \ldots, w_n)$ and applying the following steps for each element $u_i \in U$, in the process defining sets $B_1, \ldots, B_m$. Each of these sets spans $V$.
\begin{enumerate}
\item Add $u_i$ to $B_{i-1}$. The set is now linearly dependent, because $B_{i-1}$ spans $V$.
\item By lemma \ref{linearDependence}, we can find a vector $v$ that is a linear combination of $B_{i-1}\setminus \{v\}$. Because $u_1,\ldots, u_i$ are linearly independent, we can choose this vector to be an element of $W$. Define $B_i = B_{i-1}\setminus\{v\}$. By lemma \ref{linearDependence}, $B_i$ still spans $V$, as required.
\end{enumerate}
This process only stops when we have had all elements of $U$.
\end{proof}
\begin{corollary}
If a vector space $V$ has a basis with $n$ vectors, then any basis of $V$ has $n$ vectors. \label{nBasis}
\end{corollary}

\begin{theorem}
Suppose $V$ is a finite-dimensional vector space spanned by $D = \{v_1, \ldots, v_n\}$.
\begin{enumerate}
\item We can find a subset of $D$ that is a basis of $V$, i.e.\ $D$ can be reduced to a basis;
\item Each linearly independent set of vectors can be expanded to a basis.
\end{enumerate}
\label{basis}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item Remove $0$ from $D$, if it is an element. If $D$ is not linearly independent, find a vector in $D$ that is a linear combination of other vectors in $D$. Repeat until the set is linearly independent. This process stops due to the finite number of vectors. The set spans $V$ at every step.
\item Follows easily from the Steinitz exchange lemma, taking $W$ to be a basis.
\end{enumerate}
\end{proof}
\begin{corollary}
Every finite-dimensional vector space has a basis. \label{existenceBasis}
\end{corollary}

Thanks to corollaries \ref{nBasis} and \ref{existenceBasis}, the following definition makes sense:
\begin{definition}
The \udef{dimension} of a finite-dimensional vector space is the length of any basis of the vector space.
The dimension of $V$ (if $V$ is finite-dimensional) is denoted by $\dim V$ or $\dim_\mathbb{F}V$.\footnote{The latter notation is particularly useful if when distinguishing between real and complex vector spaces, because every complex vector space can be seen as a real vector space. In this case $\dim_\R V = 2\dim_\C V$, because $v$ and $iv$ are linearly independent over $\R$.}

If $V = \{0\}$, we take $\dim V = 0$.
\end{definition}

\begin{corollary}
Every linearly independent set of vectors in $V$ with length $\dim V$ is a basis of $V$. \label{maxLinearlyIndependent}
\end{corollary}
\begin{corollary}
Every spanning set of vectors in $V$ with length $\dim V$ is a basis of $V$.
\end{corollary}

\begin{proposition}
Let $V$ be a finite-dimensional vector space and $U$ a subspace of $V$. Then
\begin{enumerate}
\item $U$ is finite-dimensional and $\dim U \leq \dim V$;
\item $\dim U = \dim V \iff U=V$.
\end{enumerate}
\label{vectorSpaceEquality}
\end{proposition}
\begin{proof}
We construct a basis for $U$ using the following process:
\begin{enumerate}
\item If U=\{0\}, then we can take the basis $\emptyset$ and we are done. If $U\neq \{0\}$, we choose a nonzero vector $v_1 \in U$.
\item If $U$ is the span of all the vectors we have chosen, we are done. If not choose a vector in $U$, not in the span of the other vectors.
\item Repeat step (2).
\end{enumerate}
By construction, the chosen set of vectors is linearly independent. By the Steinitz exchange lemma this process must stop. In particular it must stop before reaching $\dim V$ vectors.

If the process reaches this upper bound, then by corollary \ref{maxLinearlyIndependent}, the set of vectors in $U$ is also a basis for $V$.
\end{proof}
We now have two tools for proving equalities of finite-dimensional vector spaces: either by proving both inclusions, or by leveraging point (2) of the previous proposition.

\subsubsection{In infinite-dimensional spaces}
Our definition of a basis of a vector spaces still makes sense for infinite-dimensional vector spaces, and many results of the previous section still make sense for infinite-dimensional vector spaces.

For infinite-dimensional vector spaces, there are, however, other notions of basis we might be interested in. In particular, our definition of basis requires all vectors to be constructible as \emph{finite} linear combinations of basis elements. In some contexts we might want to relax this to allow infinite combinations as well. For that, of course, we need some notion of infinite sum. Often we construct infinite sums as the limit of a sequence of finite sums, in which case we need a topology on our vector space that allows us to take limits.\footnote{Although other options exist, such as taking sums over hyperintegers.}  

In order to distinguish our purely algebraic definition of basis from these other notions of basis, a basis in the sense defined above is sometimes known as an \udef{algebraic basis} of \udef{Hamel basis}.

We will be discussing Hamel bases in this section.

\begin{theorem}
Let $V$ be a vector space.
\begin{enumerate}
\item Any spanning set contains a basis.
\item Any linearly independent subset can be expanded to a basis.
\end{enumerate}
\label{infBasis}
\end{theorem}
\begin{proof}
Requires the axiom of choice. We will use Zorn's lemma twice.
\begin{enumerate}
\item Let $S$ be a spanning subset of $V$. Define
\[ \mathcal{A} = \{ D\subset S \;|\; \text{$D$ is linearly independent}\} \]
ordered by inclusion. It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union which is still linearly independent. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. 
We show that $\Span(R) \supset S$ by contradiction. If $\Span(R) \not\supset S$, we can consider $R\cup \{v\}$ for some $v \in S$ that is not in $\Span(R)$ and we obtain an element of $\mathcal{A}$ which is greater than a maximal element. This is a contradiction. Then from $\Span(R) \supset S$ we conclude, using lemma \ref{span}
\[ \Span(R) = \Span(\Span(R)) \supset \Span(S) = V \]
from which it follows that $\Span(R) = V$.
\item Let $S$ be a linearly independent subset of $V$. Define
\[ \mathcal{A} = \{ D\subset V \;|\; S \subset D \; \text{and $D$ is linearly independent}\} \]
ordered by inclusion. 
It is easy to see that any chain on $\mathcal{A}$ has an upper bound on $\mathcal{A}$, by just taking the union. It follows from Zorn's lemma that $\mathcal{A}$ has a maximal element $R$. We show that $\Span(R) = V$ by contradiction. If $\Span(R) \neq V$, we can consider $R\cup \{v\}$ for some $v\notin \Span(R)$ and we obtain an element of $\mathcal{A}$ which is greater than a maximal element. This is a contradiction.
\end{enumerate}
\end{proof}
\begin{corollary}
Every vector space has a Hamel basis
\end{corollary}

\begin{theorem}[Dimension theorem for vector spaces]
Given a vector space $V$, any two bases have the same cardinality.
\end{theorem}
\begin{proof}
The finite-dimensional case has already been proved. Suppose $A$ is a basis of $V$ with $|A| \geq \aleph_0$. Let $B$ be another basis of $V$. Each element $a\in A$ can be written as a finite combination of elements in $B$. Collect all the elements that go into the finite linear combination in a finite set $B_a \subset B$. We claim
\[ B = \bigcup_{a\in A} B_a. \]
Indeed, assume $b\in B \setminus (\cup_{a\in A} B_a)$. Since $A$ spans $V$, so does $\cup_{a\in A} B_a$. Thus $b$ can be written as a non-trivial combination of vectors in $\cup_{a\in A} B_a\subset B$, contradicting the linear independence of $B$. Then we have
\[ |B| = \left| \bigcup_{a\in A}B_a \right| \leq \aleph_0 \cdot |A| = |A| \]
A similar argument gives
\[ |A| \leq \aleph_0 \cdot |B| = |B|. \]
By the Schröder–Bernstein theorem \ref{SchroederBernstein}, we conclude $|A| = |B|$.
\end{proof}
TODO: does this proof work with only the ultrafilter lemma?

Thus the notion of dimension (also known as \udef{Hamel-dimension}) also makes sense for infinite-dimensional vector spaces, except it is a cardinality, not a number.

TODO: do we need a strong cardinality assignment? (Assumed for now)

Many textbooks state results using dimensions only for the finite-dimensional case. As we will see, these results almost always generalise directly to the infinite-dimensional case as well, if we assume the axiom of choice.

\begin{note}
The inverse of this theorem (i.e.\ the infinite-dimensional analogue of proposition \ref{vectorSpaceEquality}) does not hold: infinite-dimensional vector spaces always have proper subspaces with a basis of the same cardinality. This is obvious because dropping one vector in the Hamel basis of an infinite-dimensional vector space will not change the cardinality, but will make it a proper subspace.
\end{note}

 \begin{corollary}
 Let $V$ and $W$ be vector spaces.
 \begin{enumerate}
 \item If $\dim V > \dim W$, then no linear map from $V$ to $W$ is injective.
 \item If $\dim V < \dim W$, then no linear map from $V$ to $W$ is surjective.
 \end{enumerate}
 \end{corollary}

\begin{lemma}
Let $V$ be an infinite-dimensional vector space over a field $\mathbb{F}$. Assume $|\mathbb{F}|\leq \dim_{\mathbb{F}} V$, then $\dim_{\mathbb{F}} V = |V|$. \label{vsCardinality}
\end{lemma}
\begin{proof}
Let $B$ be a basis of $V$. It is supposed infinite. There is a surjection
\[\bigcup_{n\in\N}(\mathbb{F}\times B)^{n} \to V: (a_i,v_i)^{i<n} \mapsto \sum_{i<n}a_iv_i. \]
So we have
\[ |V| \leq \left|\bigcup_{n\in\N}(F\times B)^{n}\right| = \sum_{n\in \N}|F\times B|^n \leq \aleph_0\cdot |\mathbb{F}| \cdot |B| = \max\{\aleph_0, |\mathbb{F}|, |B|\} = |B|. \]
Thus $|V|\leq \dim_{\mathbb{F}} V$. The other inequality is obvious. By the Schröder–Bernstein theorem \ref{SchroederBernstein}, we conclude $\dim_{\mathbb{F}} V = |V|$.
\end{proof}

\section{Constructing vector spaces}
\subsection{Sums of subspaces}
\begin{definition}
Suppose $\{U_i\}_{i\in I}$ a set of subspaces of a vector space $V$. The \udef{sum} of these subspaces, denoted $\sum_{i\in I}U_i$, is the set of all finite linear combinations of elements in $\bigcup_{i\in I}U_i$:
\[ \sum_{i\in I}U_i =  \setbuilder{\sum_{i\in J} u_i}{\text{$J\subset I$ finite}, u_i\in \bigcup_{i\in I}U_i}. \]
\end{definition}
For finite sums this reduces to
\[ U_1+\ldots + U_m = \setbuilder{\sum_{i=1}^m u_i}{u_1\in U_1, \ldots, u_m\in U_m}. \]

\begin{proposition}
The sum $\sum_{i\in I}U_i$ is the smallest subspace of $V$ containing all $\bigcup_{i\in I}U_i$.
\end{proposition}

\begin{proposition}
Let $V$ be a vector space and $A,B,C$ subspaces. Then
\begin{enumerate}
\item $A+(B\cap C) \subseteq (A+B)\cap (A+C)$;
\item $(A+B)\cap C \supseteq (A\cap C) + (B\cap C)$. 
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Take $v = v_1+v_2 \in A+(B\cap C)$ where $v_2 \in B$ and $v_2 \in C$, so $v_1+v_2\in A+B$ and $v_1+v_2\in A+C$.

(2) Take $v = v_1+v_2\in (A\cap C) + (B\cap C)$. Then $v_1,v_2\in C$ and thus $v\in (A+B)\cap C$.
\end{proof}

\begin{theorem}[Dimension of a sum]
Let $U_1$ and $U_2$ be subspaces of a finite-dimensional vector space, then
\[ \dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim(U_1\cap U_2). \]
\label{dimOfASum}
\end{theorem}
\begin{proof}
Let $\dim U_1 = r, \dim U_2 = s$ and $\dim(U_1\cap U_2) = t$. Then $t\leq r$ and $t\leq s$.  Take a basis $\{v_1,\ldots, v_t\}$ of $U_1\cap U_2$. This can be expanded to a basis $\beta_{U_1} = \{ v_1, \ldots, v_t, u_{t+1}, \ldots u_{r} \}$ of $U_1$ and also to a basis $\beta_{U_2} = \{ v_1, \ldots, v_t, u'_{t+1}, \ldots u'_{s} \}$ of $U_2$. We will show that $\{ v_1, \ldots, v_t, u_{t+1}, \ldots u_{r}, u'_{t+1}, \ldots, u'_{s} \}$ is a basis of $U_1\cap U_2$. This completes the proof because
\begin{align*}
\dim(U_1 + U_2) &= t + (s-t) + (r-t) = s + r -t\\
&= \dim U_1 + \dim U_2 - \dim(U_1\cap U_2).
\end{align*}
The spanning property is easy. Linear independence is slightly more difficult: Take a linear combination
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j + \sum^s_{k=t+1}\beta'_ku_k' =0. \]
We must show this combination is trivial. Indeed observe that
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j  =-\sum^s_{k=t+1}\beta'_ku_k'. \]
The left-hand side is a vector in $U_1$, the right-hand side is a vector in $U_2$, so it must lie in $U_1\cap U_2$, so we rewrite the left-hand side as
\[ \sum_{i=1}^t\lambda_i v_i =  -\sum^s_{k=t+1}\beta'_ku_k'.\]
Due to $\beta_{U_2}$ being a basis, this linear combination must be trivial and all $\beta'_k$ are zero. This leaves us 
\[ \sum_{i=1}^t\alpha_i v_i + \sum^r_{j=t+1}\beta_ju_j =0 \]
from our original linear combination. Due to $\beta_{U_2}$ being a basis this combination must also be trivial. 
\end{proof}
\begin{note}
If $\dim(U_1\cap U_2)<\dim U_1$ and $\dim(U_1\cap U_2)< \dim U_2$, this proof generalises to infinite-dimensional vector spaces.
\end{note}

\subsection{(Internal) direct sum}
\begin{definition}
Suppose $\{U_i\}_{i\in I}$ is a set of subspaces of $V$. The sum $\sum_{i\in I}U_i$ is called a \udef{direct sum} if each element $u$ of the sum can be \emph{uniquely} written as
\[ u = \sum_{i\in I}u_i \qquad (u_i\in U_i) \]
where only finitely many of the $u_i$ are nonzero.

In this case we write $\bigoplus_{i\in I} U_i$, or $U_1 \oplus \ldots \oplus U_m$ if $I = \{1,\ldots, m\}$. 
\end{definition}

\begin{proposition}[Conditions for a direct sum] \label{directSumCriterion}
Suppose $\sum_{i\in I}U_i$ a sum of subspaces.
\begin{itemize}
\item The sum is direct \textup{if and only if} $0$ has the unique decomposition as in the definition.
\item The sum is direct if the $U_i$ are pairwise disjoint, i.e.\ for all $i\neq j$
\[ U_i \cap U_j = \{0\}. \]
\end{itemize}
\end{proposition}

\begin{proposition}
Let $V$ be a vector space and $\bigoplus_{i=1}^m U_i$ a direct sum of subspaces of $V$.
If for all $i\in I$, $\beta_i$ is a basis of $U_i$, then $\cup_{i\in I}\beta_i$ is a basis of $V$. Consequently,
\[ \dim V = \sum_{i\in I} \dim U_i. \]
\end{proposition}
\begin{proof}
The finite-dimensional case is an easy corollary of proposition \ref{dimOfASum}. In general it is not difficult to show that $\cup_{i\in I}\beta_i$ is spanning and linearly independent. The formula for the dimension follows from cardinal arithmetic, noting that the $\beta_i$ must be pairwise disjoint.
\end{proof}

\begin{definition}
In a vector space $V$, a subspace $W$ is a \udef{complementary subspace} (or a \udef{complement}) of the subspace $U$ if $V = U \oplus W$.
\end{definition}

\begin{proposition}
If $V$ is a finite-dimensional vector space, the each subspace of $V$ has a complement.
\end{proposition}
\begin{corollary}
Suppose $V$ is finite-dimensional and $U_1,\ldots, U_m$ are subspaces of $V$. Then $U_1+\ldots+ U_m$ is a direct sum \textup{if and only if}
\[ \dim(U_1+\ldots+U_m) = \dim U_1 + \ldots \dim U_m. \]
\label{directSumDimensionCriterion}
\end{corollary}

\subsection{External direct sum}
\begin{definition}
Let $U,W$ be vector spaces over the same field $\mathbb{F}$. We define the vector space  $U\oplus W$, called the \udef{(external) direct sum}, as the set $U\times W$ with the operations
\[ \begin{cases}
(u_1,w_1) + (u_2, w_2) = (u_1 +_U u_2, w_1 +_W w_2) & (u_1,u_2 \in U; w_1, w_2 \in W) \\
r\cdot (u,w) = (ru, rw) & (r\in \mathbb{F}; u\in U; w \in W)
\end{cases} \]
In general we can define a direct sum of an arbitrary collection of vector spaces $\{U_i\}_{i\in I}$, denoted
\[ \bigoplus_{i\in I}U_i \]
as the vector space with as field the subset of the Cartesian product $\prod_{i\in I}U_i$ where all but finitely many of the terms are zero. The operations are defined point-wise.
\end{definition}

\begin{proposition}
Suppose $V_1, \ldots V_m$ are vector spaces over $\mathbb{F}$. Then
\[ \dim(V_1\oplus\ldots \oplus V_m) = \dim V_1 + \ldots + \dim V_m \]
\label{dimDirectSum}
\end{proposition}
\begin{proof}
We construct a basis $\beta$ of $V_1\oplus\ldots \oplus V_m$ from bases $\beta_{V_i}$ of $V_i$:
\[ \beta = (\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \}) \cup (\{0 \} \times \beta_{V_2}\times \{0 \} \times \ldots \times \{0 \}) \cup \ldots \cup (\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m}). \]
All these unions are disjunct, so
\begin{align*}
|\beta| &= |(\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \}) \cup  \ldots \cup (\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m})| \\
&= |(\beta_{V_1} \times \{0 \} \times \ldots \times \{0 \})| + \ldots + |(\{0 \}\times \ldots \times \{0 \} \times \beta_{V_m})| \\
&= | \beta_{V_1}| + \ldots + |\beta_{V_m}| \\
&= \dim V_1 + \ldots + \dim V_m.
\end{align*}
\end{proof}

\begin{proposition}
Let $U,W$ be subspaces of $V$. Then the external direct sum of $U$ and $W$ is isomorphic to the internal direct sum of $U$ and $W$.
\end{proposition}
\begin{proof}
The map $f: U\times W\to V: (u,w) \mapsto u+w$ is an isomorphism.
\end{proof}
For this reason we use the same symbol for both.

\begin{definition}
Let $V,W, X,Y$ be vector spaces over $\mathbb{F}$. Let $S: V\to X$ and $T: W\to Y$ be linear maps. Then the \udef{direct sum} of $S$ and $T$ is a linear map
\[ S\oplus T: V \oplus W \to X\oplus Y: (v,w) \mapsto (S(v), T(v)). \]
\end{definition}

\begin{lemma}
Let $V,W$ be vector spaces over a field $\F$ and $A,C\in\Lin(V)$ and $B,D\in\Lin(W)$. Then
\begin{enumerate}
\item $a(A\oplus B) + b(C\oplus D) = (aA+bC)\oplus (aB + bD)$;
\item $(A\oplus B)(C\oplus D) = AC\oplus BD$;
\item $(A\oplus B)^k = A^k\oplus B^k$.
\end{enumerate}
\end{lemma}

\subsubsection{Matrix representation}
TODO: move
Assume $V$ and $W$ are finite-dimensional vector spaces with resp. bases $\{\vec{e}_i\}_{i=1}^m$ and $\{\vec{f}_j\}_{j=1}^n$.
As in the proof of proposition \ref{dimDirectSum}, we can take the basis $\{\vec{e}_i\}_i\times\{0\} \cup \{0\}\times\{\vec{f}_j\}_j$ of $V\oplus W$.

We can naturally fit the basis into a list of $m+n$ elements:
\[ (\vec{e}_1,0),\ldots (\vec{e}_m, 0), (0, \vec{f}_1), \ldots, (0,\vec{f}_n)  \]
\subsubsection{Linear maps}
TODO: also move
Let $S: V\to X$ and $T:W\to Y$ be linear maps, with matrix representations $A$ and $B$, respectively. The matrix representation of $S\oplus T$ is given by
\[ A\oplus B = \begin{bmatrix}
A & 0 \\
0 & B
\end{bmatrix} \]
with respect to the basis $\{\vec{e}_i\}_i\times\{0\} \cup \{0\}\times\{\vec{f}_j\}_j$.


\section{Linear maps}
\begin{definition}
Let $(\mathbb{R}, V, +)$ and $(\mathbb{R}, W, +)$ be vector spaces over the same field. A \udef{linear map} or \udef{linear transformation} is a function $L:V\to W$ with the following properties:
\begin{itemize}[leftmargin=3cm]
\item[\textbf{Additivity}] $L(u+v) = L(u)+L(v)$ for all $u,v \in V$;
\item[\textbf{Homogeneity}] $L(\lambda v) = \lambda L(v)$ for all $\lambda \in \mathbb{R}$ and all $v\in V$.
\end{itemize}
These conditions are equivalent to the condition that
\[ L(\lambda_1 v_1 + \lambda_2v_2) = \lambda_1L(v_1) + \lambda_2 L(v_2) \qquad \text{for all $\lambda_1,\lambda_2\in \mathbb{F}$ and all $v_1,v_2\in V$.} \]
We denote the set of all linear maps from $V$ to $W$ as $\Lin_\mathbb{F}(V,W)$, or $\Lin(V,W)$. The set of endomorphisms on $V$ is denoted $\Lin(V) \defeq \End(V) = \Lin(V,V)$.
\end{definition}

\begin{lemma} \label{linearMaps}
Let $L\in \Lin(V,W)$.
\begin{enumerate}
\item $L(0) = 0$ and $L(-v) = -L(v)$
\item $L\left(\sum^n_{i=1}\lambda_i v_i\right) = \sum_{i=1}^n\lambda_i L(v_i)$.
\item A linear map is completely determined by the images of a basis of $V$.
\item Let $D$ be a set of vectors. Then $L[D]$ is linearly independent \textup{if and only if} $D$ is linearly independent.
\end{enumerate}
\end{lemma}

\subsection{Examples}
\begin{enumerate}
\item The zero map that maps everything to zero.
\item Identity maps.
\item Differentiation of polynomials.
\item Integration of polynomials.
\item Shifting elements in a list.
\item Projections.
\end{enumerate}

\begin{definition}
A (linear) \udef{operator} between two vector spaces $V$ and $W$ is a linear partial function $T: V \not\to W$ such that the domain $\dom(T)$ is a vector space.

We also say an operator is a function $T: \dom(T)\subseteq V\to W$.
\end{definition}
The requirement that $\dom(T)$ be a subspace of $V$ is necessary for linearity to make sense!

Some authors (e.g\ Axler) use the word ``operator'' to mean a linear endomorphism.

\subsection{Image and kernel}
\begin{definition}
Let $L \in \Lin(V,W)$. The \udef{kernel} or \udef{null space} of $L$ is the set of vectors that $L$ maps to zero:
\[ \ker(L) = \{ v\in V \;|\; L(v) = 0 \}. \]
\end{definition}
\begin{proposition}
The kernel of $L\in \Lin(V,W)$ is a subspace of $V$.
\end{proposition}
\begin{definition}
The dimension of the kernel of a linear map is its \udef{nullity}.
\end{definition}
\begin{proposition} \label{injectivityKernelTriviality}
Let $L\in\Lin(V,W)$. Then $L$ is injective if and only if $\ker(L) = 0$.
\end{proposition}
TODO: generalise to groups
\begin{proof}
We show both implications.
\begin{itemize}
\item[\boxed{\Rightarrow}] We know $\{0\}\subset \ker(L)$ by lemma \ref{linearMaps}. Suppose $v\in \ker(L)$, then $L(v) = 0 = L(0)$. So $v=0$ by injectivity and $\{0\}\supset \ker(L)$.
\item[\boxed{\Leftarrow}] Suppose $u,v \in V$ such that $L(u)=L(v)$. Then
\[ 0 = L(u) - L(v) = L(u-v). \]
Thus $u-v\in \ker(L)$, meaning $u-v = 0$ and $u=v$.
\end{itemize}
\end{proof}

\begin{definition}
Let $L \in \Lin(V,W)$. The \udef{image} or \udef{range} of $L$ is the set of vectors that are of the form $L(v)$ for some $v\in V$:
\[ \im(L) = \{ L(v) \;|\; v\in V \}. \]
\end{definition}
\begin{proposition}
The range of $L\in \Lin(V,W)$ is a subspace of $W$.
\end{proposition}
\begin{definition}
The dimension of the image of a linear map is its \udef{rank}.
\end{definition}

\begin{theorem}
Every short exact sequence of vector spaces splits.
\end{theorem}
\begin{proof}
Let
\[ \begin{tikzcd}
0 \rar & U \rar{S} & V \rar{T} & W \rar & 0
\end{tikzcd} \]
be a short exact sequence of vector spaces.
By the splitting lemma TODO ref, it is enough to find a left inverse of $S$. Pick a basis $\beta$ of $U$. Because $S$ is injective, $S[\beta]$ is linearly independent and we can extend it to a basis $\beta'$. We can now define the left inverse by specifying how the basis elements are mapped, by \ref{linearMaps}. To wit: $\beta'\setminus S[\beta]$ is mapped to $0$ and each element $S[\beta]$ has exactly one origin be injectivity and it is to this origin that it is now mapped.
\end{proof}
\begin{corollary} \label{directSumKernelImage}
Let $L \in \Lin(V,W)$. Then
\[ V \cong \ker L \oplus \im L. \]
\end{corollary}
\begin{proof}
Given $L$ we have the short exact sequence
\[ \begin{tikzcd}
0 \rar & \ker L \ar[r, hook] & V \rar{L} & \im L \rar & 0.
\end{tikzcd} \]
The isomorphism then follows from the splitting lemma TODO ref.
\end{proof}
\begin{corollary}[Dimension theorem for linear maps] \label{dimensionLinearMaps}
Let $L \in \Lin(V,W)$. Then
\[ \dim(V) = \dim(\ker L) + \dim(\im L). \]
\end{corollary}
This corollary is also known as the rank-nullity theorem or the fundamental theorem of linear maps.
\begin{proof}
By $\dim(V) = \dim(\ker L \oplus \im L) = \dim(\ker L) + \dim(\im L)$.

Alternatively this can be proven directly as follows:

Take a basis $\beta_0$ of $\ker(L)$. We can expand this to a basis $\beta$ of $V$, by theorem \ref{infBasis}. It is easy to show that $L[\beta\setminus \beta_0]$ is a basis of $\im(L)$. Now $L[\beta\setminus \beta_0] =_c \beta\setminus \beta_0$ and $(\beta\setminus \beta_0) \cap \beta_0 = \emptyset$. Thus $|\beta| = |(\beta\setminus \beta_0) \cup \beta_0| = |\beta\setminus \beta_0| + |\beta_0|$. This proves the assertion.
\end{proof}
\begin{corollary}
Let
\[ \begin{tikzcd}
0 \rar & V_1 \rar & V_2 \rar & \ldots \rar & V_n \rar & 0
\end{tikzcd} \]
be an exact sequence of vector spaces, then
\[ \sum_{i=1}^n (-1)^i\dim(V_i) = 0. \]
\end{corollary}
\begin{proof}
Let $f_i$ be the map $V_i\to V_{i+1}$. By exactness $\im f_i=\ker f_{i+1}$ and $\dim(\im f_i)=\dim(\ker f_{i+1})$. By the previous corollary $\dim(V_i) = \dim(\ker f_i) + \dim(\im f_i)$. Then
\[ \sum_{i=1}^n (-1)^i\dim(V_i) = \sum_{i=1}^n (-1)^i\dim(\ker f_i) + \sum_{i=1}^n (-1)^i\dim(\ker f_{i+1}) = \sum_{i=2}^{n} (-1)^i\dim(\ker f_i) - \sum_{i=2}^{n} (-1)^i\dim(\ker f_{i}) = 0. \]
\end{proof}
\begin{corollary} \label{dimensionImageSmaller}
Let $L \in \Lin(V,W)$. Then
\[ \dim(\im L) \leq \dim(V). \]
\end{corollary}
\begin{proof}
TODO ref cardinal arithmetic.
\end{proof}

\begin{lemma} \label{rankMapComposition}
Let $S,T$ be compatible linear maps. Then
\[ \text{rank of $ST$}\;\leq\;\min\{\text{rank of $S$, rank of $T$}\}. \]
If $T$ is invertible, then the rank of $ST$ equals the rank of $S$. Similarly if $S$ is invertible, then the rank of $ST$ equals the rank of $T$.
\end{lemma}
\begin{proof}
Clearly $\im(ST) \subset \im(S)$, so $\dim\im(ST)\leq \dim\im(S)$.
We also have $ST = S|_{\im T}T$, where $S|_{\im T}$ is $S$ restricted to $\im T$.  Then corollary \ref{dimensionImageSmaller} applied to $S|_{\im T}$ gives $\dim\im(ST)\leq \dim\im T$. Together these inequalities give the result.

To show equality in the invertible case, first assume $T$ invertible:
\[ \dim\im ST \leq \dim\im STT^{-1} = \dim\im S. \]
Together with the first inequality this gives an equality. The case for $S$ invertible is similar.
\end{proof}

\begin{proposition} \label{kernelCompositionLinearMaps}
Let $S,T$ be compatible linear maps. Then
\begin{enumerate}
\item $\ker(ST)\supseteq \ker(T)$;
\item $\dim\ker(ST) = \dim\ker(T) + \dim(\im(T)\cap\ker(S))$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) $x\in\ker(T) \implies (ST)x = S(Tx) = S(0) = 0 \implies x\in\ker(ST)$.
(2) Consider the restriction $T|_{\ker(ST)}$. Applying the dimension theorem gives
\[ \dim\ker(ST) = \dim\ker(T|_{\ker(ST)}) + \dim\im(T|_{\ker(ST)}) = \dim\ker(ST) = \dim\ker(T) + \dim\im(T|_{\ker(ST)}) , \]
so it is enough to show $\im(T|_{\ker(ST)}) = \im(T)\cap\ker(S)$. First take $v\in\im(T|_{\ker(ST)}$, then there exists some $w\in\ker(ST)$ such that $v=Tw$, meaning $v\in\im(T)$. Also $Sv = STw = 0$, meaning $v\in\ker(S)$.

Then take $v\in\im(T)\cap\ker(S)$, so we can find a $w$ such that $v = Tw$. Also $Sv = STw = 0$, so $w\in\ker(ST)$ and $v\in\im(T|_{\ker{ST}})$.
\end{proof}

\subsubsection{Algebraic operations on linear maps}
\begin{definition}
Suppose $K,L \in \Lin_{\mathbb{F}}(V,W)$ and $\lambda \in \mathbb{F}$.
\begin{itemize}
\item The \udef{sum} $K+L$ is defined by $(K+L)(v) = Kv+Lv$ for all $v\in V$;
\item The \udef{scalar product} is defined by $(\lambda K)(v) = \lambda K(v)$ for all $v\in V$.
\end{itemize}
\end{definition}
\begin{proposition}
\begin{itemize}
\item The sum of linear maps is again a linear maps. Scalar multiples of linear maps are linear maps.
\item With addition and scalar multiplication defined as above, $\Lin_\mathbb{F}(V,W)$ is a vector space.
\end{itemize}
\end{proposition}

\begin{definition}
Let $K\in \Lin_\mathbb{F}(U,V)$ and $L\in \Lin_\mathbb{F}(V,W)$. The \udef{product} $LK$ is defined as the composition
\[ (LK)(u) = L(K(u)) \qquad \text{for all $u\in U$.} \]
If the product of two linear maps $K,L$ makes sense, we call the linear maps \udef{compatible}.
\end{definition}
\begin{proposition}
The product of two (compatible) linear maps is a linear map.
\end{proposition}
\begin{proposition}[Algebraic properties of linear maps]
The product of linear maps has the following properties. 
\begin{itemize}[leftmargin=4.2cm]
\item[\textbf{Associativity}] Let $L_1, L_2, L_3$ be compatible linear maps, then
\[ (L_1L_2)L_3 = L_1(L_2L_3) \]
\item[\textbf{Identity}] Let $L\in \Lin(V,W)$. The identity maps $I_V:V\to V$ and $I_W:W\to W$ are linear and have the property that
\[ LI_V = I_W L = L. \]
\item[\textbf{Distributive properties}]
$ (S_1+S_2)T = S_1T + S_2T \qquad \text{and} \qquad S(T_1 + T_2) = ST_1 + ST_2 $
whenever $T,T_1, T_2 \in \Lin(U,V)$ and $S,S_1, S_2\in \Lin(V,W)$.
\end{itemize}
These properties mean that for any vector space $V$, $\Lin(V)$ forms a unital algebra.
\end{proposition}
Note that multiplication of linear maps is not commutative, not even for maps that are compatible both ways.

\subsection{Invertibility and isomorphisms}
\begin{proposition} \label{inverseLinear}
Let $L$ be a linear map. If $L$ is invertible as a function (i.e.\ bijective), its inverse $L^{-1}$ is linear.
\end{proposition}
\begin{proof}
We calculate for $x,y$ vectors and $a\in\mathbb{F}$
\[ L^{-1}(ax + y) = L^{-1}(aLL^{-1}x + LL^{-1}y) = L^{-1}L(aL^{-1}x + L^{-1}y) = aL^{-1}x + L^{-1}y. \]
\end{proof}

\begin{definition}
\begin{itemize}
\item An invertible linear map is called an \udef{isomorphism}.
\item Two vector spaces $V,W$  are \udef{isomorphic} if there is an isomorphism between them. This is denoted $V\cong W$.
\end{itemize}
\end{definition}

\begin{proposition} \label{isomorphicDimension}
Let $V,W$ be vector spaces over the same field $\mathbb{F}$ and $n\in \N$. Then
\begin{enumerate}
\item $V\cong W \iff \dim V = \dim W$;
\item $V \cong \mathbb{F}^n \iff \dim V = n$;
\item $\F^n \cong \F^m \iff n=m$.
\end{enumerate}
\label{isomorphicCondition}
\end{proposition}
\begin{proof}
We prove the first statement. The second and third follow easily, using $\dim_\mathbb{F} \mathbb{F}^n = n$.
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Let $T:V\to W$ be an isomorphism. Then $\ker T = \{0\}$ and $\im T = W$. Thus
\[ \dim V = \dim \ker T + \dim \im T = 0 + \dim W = \dim W. \]
\item[$\boxed{\Leftarrow}$] Assume $\dim V = \dim W$. Thus there exists an invertible function from a basis of $V$ to a basis of $W$. This can be extended by linearity to a function on $V$, because it is defined on a Hamel basis. It is easy to see this function is linear and bijective.
\end{itemize}
\end{proof}

\begin{proposition} \label{mappingOfBasisByIsomorphism}
Let $L\in\Lin(V,W)$ be an isomorphism. Let $\beta$ be a basis of $V$, then $L[\beta]$ is a basis of $W$.
\end{proposition}

\begin{proposition} \label{invertibleFiniteDim}
Suppose $V$ is a finite-dimensional vector space and $L\in \Lin(V)$ is a linear map on $V$, then
\[ L \;\text{is invertible} \iff L \;\text{is injective} \iff L \;\text{is surjective} \]
\end{proposition}
\begin{proof}
All we need to prove is
\[ L \;\text{is injective} \iff L \;\text{is surjective} \]
\begin{itemize}
\item[$\boxed{\Rightarrow}$] Assume $L$ injective. Then $\ker L = \{0\}$. By the dimension theorem for linear maps, theorem \ref{dimensionLinearMaps}
\[ \dim \im L = \dim V - \dim \ker L = \dim V. \]
Because $\im L \subset V$ and using proposition \ref{vectorSpaceEquality}, we conclude that $\im L = V$ and thus $L$ is surjective.
\item[$\boxed{\Leftarrow}$] Assume $L$ surjective. Then, by the dimension theorem for linear maps,
\[ \dim \ker L = \dim V - \dim \im L = 0, \]
which means $L$ is injective.
\end{itemize}
\end{proof}
Remark that the proof of the first implication uses proposition \ref{vectorSpaceEquality}, and thus cannot be generalised to infinite-dimensional vector spaces. In the proof of the second implication the subtraction of infinite cardinals is only uniquely defined if  $\dim V > \dim \im L$, which is clearly not the case.

\begin{example}
Counterexamples to the previous theorem in the infinite-dimensional case are given by the left shift map on $\mathbb{F}^\N$ (which is injective, but not surjective) and the right shift map on $\mathbb{F}^\N$ (which is surjective, but not injective).
\end{example}


\subsection{Special types of linear maps}
\subsubsection{Finite-rank operators}
\begin{definition}
A linear map $T: V\to V$ is said to be a \udef{finite-rank operator} if it has finite rank.
\end{definition}
\subsubsection{Idempotents}
\begin{proposition} \label{directSumKernelImageIdempotent}
Let $V$ be a vector space and $P$ an idempotent linear map. Then
\[ V = \im P \oplus \ker P. \]
\end{proposition}
\begin{proof}
For any $v\in V$, we can write $v= (v-Pv)+Pv$ where $Pv\in \im P$ and $(v-Pv)\in \ker P$ because
\[ P(v-Pv) = Pv- P^2v = Pv - Pv = 0. \]
So we have $V = \im P + \ker P$. To show that the sum is direct, we take $u\in \im P \cap \ker P$. Then $u = Pw$ for some $w\in V$ and applying $P$ gives $0 = Pu = P^2w = Pw = 0$. So the sum is direct by \ref{directSumCriterion}.
\end{proof}
\subsubsection{Nilpotents}

\section{Sets of vectors}
\subsection{Cones}
\begin{definition}
A subset $C$ of a real or complex vector space $V$ is called a \udef{cone} if for all real $r>0$, $rC \subseteq C$. A cone is called \udef{pointed} if it contains the origin.
\end{definition}

\begin{lemma} \label{convexityAdditiveClosure}
A cone $C$ is convex if and only if $C + C \subseteq C$. 
\end{lemma}
\begin{proof}
Assume $C$ convex. Take $v,w\in C$, then $v/2 + w/2\in C$ by convexity and so $v+w = 2(v/2+w/2)\in C$.

Assume $C$ closed under addition. Take $v,w\in C$ and $\lambda\in[0,1]$. Then $(1-\lambda)v$ and $\lambda w$ are elements of $C$ and so the convex combination $(1-\lambda)v + \lambda w$ is too.
\end{proof}


\subsection{Translation invariance}
TODO Unique factorisation through $(x,y)\mapsto y-x$. (Universal property)

eg kernel, commutator, metric

\subsubsection{Quotient spaces}
TODO: need closed $U$? For quotient map to be continuous? TODO show quotient topology.

\begin{proposition}
Let $V$ be a vector space. Then $\mathfrak{q}\subset V\times V$ is a congruence \textup{if and only if} the set
\[ U_\mathfrak{q} = \setbuilder{w-v}{(v,w)\in\mathfrak{q}} \]
is a vector space.
\end{proposition}
\begin{proof} Then

- $\mathfrak{q}$ is reflexive iff $0\in U_\mathfrak{q}$;

- $\mathfrak{q}$ is symmetric iff $U_\mathfrak{q}$ is closed under multiplication with $-1$;

- $\mathfrak{q}$ is transitive iff $U_\mathfrak{q}$ is closed under addition;

- $\mathfrak{q}$ is a subalgebra of $V\oplus V$ iff $U_\mathfrak{q}$ is closed under addition and scalar multiplication.

As $U_\mathfrak{q}$ is a subset of $V$, we use the subspace criterion.
\end{proof}
Then the equivalences
\[ [v]_\mathfrak{q}=[w]_\mathfrak{q} \iff (v,w)\in\mathfrak{q} \iff w-v\in U_\mathfrak{q} \iff w+U_\mathfrak{q} = v+U_\mathfrak{q} \]
motivate the following definition:
\begin{definition}
Let $V$ be a vector space.
\begin{itemize}
\item An \udef{affine subset} of $V$ is a subset of $V$ of the form $v+U$ for some $v\in V$ and some subspace $U$ of $V$.
\item An affine subset $v+U$ is \udef{parallel} to $U$.
\end{itemize}
Suppose $U$ subspace of $V$. The \udef{quotient vector space} $V/U$ is the vector space of all affine subsets of $V$ parallel to $U$:
\[ V/U = \{ v+U \;|\; v\in V \}, \]
which is a vector space by virtue of being a quotient algebra.

We call the dimension of $V/U$ the \udef{codimension} of $U$ in $V$:
\[ \codim(U) = \dim(V/U). \]
\end{definition}

\begin{proposition}
Let $U$ be a subspace of a vector space $V$. Then
\[ \dim V = \dim U + \dim V/U = \dim U + \codim U.  \]
\end{proposition}
\begin{proof}
Apply the dimension theorem for linear maps to the quotient map.
\end{proof}

\begin{definition}
Let $f:V\to W$ be a linear map of vector spaces. The \udef{cokernel} of $f$ is the quotient space
\[ \coker(f) = W/\im(f). \]
The dimension of the cokernel is called the \udef{corank}.
\end{definition}
\begin{lemma}
Let $U$ be a subspace of a vector space $V$. The codimension of $U$ is the corank of the inclusion $U\hookrightarrow V$:
\[ \codim(U) = \dim\coker(U\hookrightarrow V). \]
\end{lemma}

\begin{proposition} \label{splittingMap}
Let $T\in \Lin(V,W)$. Then $T$ induces a linear map
\[ \tilde{T}: V/\ker(T) \to W: v +\ker(T) \mapsto Tv \]
with the following properties:
\begin{enumerate}
\item $\tilde{T}$ is injective;
\item $\im\tilde{T} = \im T$;
\item $\tilde{T}$ is an isomorphism from $V/\ker(T)$ to $\im T$.
\end{enumerate}
\end{proposition}

 TODO each short exact sequence of vector spaces splits \url{https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem}






\input{linearAlgebra/representation}

\input{linearAlgebra/multilinear}

\input{linearAlgebra/normsInnerProducts}

\input{linearAlgebra/matrices}











\chapter{Indices and symbols}
\section{Contravariant and covariant vectors and tensors}
When working in finite-dimensional spaces with specified bases, we often get expressions of the form
\[ v = \sum_{i=1}^n a_i \vec{e}_i. \]
We may replace this expression with
\[ v = a^i \vec{e}_i \]
if we take the convention that if an index is repeated once up and once down, then there is a sum over all values of that index. This is the \udef{Einstein summation convention}.

Note that coordinates have their indices up, and basis vectors have their indices down.

Now in the dual space, we have the dual basis $\{\varphi^j\}_j$. In the dual space we take the opposite convention: coordinates have their indices down, and dual basis vectors have their indices up, so
\[ \varphi = b_j \varphi^j. \]
This allows us to write
\[ \varphi(v) = \varphi(a_i \vec{e}_i) = a_i b^j \varphi^j(\vec{e}_i) = a_i b^i \]
where for the last equality we have used that $\varphi^j(\vec{e}_i)$ only does not vanish if $i=j$ and is $1$ in this case.

We call vectors in $V$ \udef{contravariant} vectors and vectors in $V^*$ \udef{covariant} vectors, or covectors.

Per convention we put the coordinates of contravariant vectors in column vectors. This means, by proposition \ref{transpDual}, we must put the coordinates of covariant vectors in row vectors. Indeed, let $v=a^i \vec{e}_i\in V$ and $\varphi = b_j\varphi^j \in V^*$, then
\[ \varphi(v) = a^ib_i = \begin{bmatrix}
a^1 & \hdots & a^n
\end{bmatrix}\begin{bmatrix}
b_1 \\ \vdots  \\ b_n
\end{bmatrix}. \]

We can view covariant vectors as functions that take a contravariant vector and produce a number, and we can view contravariant vectors as functions that take a covariant vector and produce a number. In general we may have linear functions that accept several co- and contravariant vectors and produce a number. By (TODO), such functions are tensor products of various co- and contravariant vectors. They would have multiple up- and down-indices. e.g\
\[ \vec{T} = \tensor{T}{^i_j_k^l^m}(\vec{e}_i\otimes \vec{e}^j\otimes \vec{e}^k\otimes \vec{e}_l \otimes \vec{e}_m) \]

Where $\tensor{T}{^i_j_k^l^m}$ are the coordinates w.r.t. the basis vectors $\vec{e}_i\otimes \vec{e}^j\otimes \vec{e}^k \otimes\vec{e}_l \otimes \vec{e}_m$.

For example, once the basis has been chosen, matrices map contravariant vectors to contravariant vectors. And contravariant vectors map covariant vectors to numbers, so by reverse currying a matrix is maps a contravariant and a covariant vector to a number.

For the indices of matrices we have taken the convention that the first index is for rows and the second for the columns. For a constant row index, the column index spells out a covariant vector, so the column index is down. Conversely, the row index is up. A matrix $A$ with components $(A)_{i,j}$ becomes
\[ \tensor{A}{^i_j}(\vec{e}_i\otimes \vec{e}^j). \]
This is consistent with the observation that the matrix sends a vector to a function on covectors, in other words is a function which accepts vectors in first place and covectors in second place.

In the expressions so far only repeated indices were present. Such repeated indices are called  \udef{bound indices} or \udef{dummy indices}. They may be replaced in the expression by other letters, so long as there is no clash. If an index is not repeated, it is a \udef{free index} and may not just be changed.

\subsection{``Tensors are objects that transform as tensors''}
Variants:
\begin{itemize}
\item ``$N$ arbitrary numbers are not the components of a vector'' (Peres p.65)
\end{itemize}


\section{Covectors}

\subsection{Multi-index notation}
Let $e_1,\ldots, e_n$ be a basis for a real vector space $V$. Let $\alpha^1,\ldots, \alpha^n$ be the dual basis for $V^*$. A \udef{multi-index}
\[ I = (i_1,\ldots,i_k)\]
is a $k$-tuple of numbers $\in (1,\ldots,n)$. We write
\[ \begin{cases}
e_I \defeq e_{i_1}\otimes\ldots\otimes e_{i_k}\\
\alpha^I \defeq \alpha^{i_1}\wedge\ldots \wedge\alpha^{i_k}
\end{cases}. \]
The covector $\alpha^I$ is completely determined by the values in $I$, the order only changes the sign. A multi-index $I = (i_1,\ldots,i_k)$ is \udef{ascending} if
\[ 1\leq i_1<\ldots<i_k\leq n. \]
\begin{proposition}
Let $I,J$ be ascending multi-indices of length $k$, then
\[ \alpha^I(e_J) = \begin{cases}
1 & I=J \\ 0& I\neq J
\end{cases}. \]
\end{proposition}
\begin{proposition}
The covectors $\alpha^I$, with $I$ an ascending multi-index of length $k$, form a basis of $A_k(V)$.
\end{proposition}
\begin{corollary}
If $\dim V=n$, then
\[ \dim A_k(V) = \begin{pmatrix}
n\\k
\end{pmatrix}. \]
\end{corollary}

\section{Symmetrisation and anti-symmetrisation of indices}

\[ T_{\{a_1\dots a_n\}} = \frac{1}{n!} \sum_{\sigma\in S_n} T_{a_{\sigma(1)} \dots a_{\sigma(n)}} \]

\[ T_{[a_1\dots a_n]} = \frac{1}{n!} \sum_{\sigma\in S_n} (\sgn \sigma)T_{a_{\sigma(1)} \dots a_{\sigma(n)}} \]
\section{Symbols}
\subsection{Kronecker delta}
\begin{definition}
The \udef{Kronecker delta} is defined by
\[ \delta_{ij} = \delta^i_j = \begin{cases}
1 & (i=j) \\
0 & (i \neq j)
\end{cases}.\]
\end{definition}
\subsection{Levi-Civita symbol}
\begin{definition}
The \udef{Levi-Civita symbol} is defined by
\[ \varepsilon_{a_{1}\ldots a_{n}} = \begin{cases}
+1 & \text{$(a_{1},\ldots, a_{n})$ is an even permutation of $(1,\ldots, n)$} \\
-1 & \text{$(a_{1},\ldots, a_{n})$ is an odd permutation of $(1,\ldots, n)$} \\
0 & \text{otherwise}
\end{cases}.\]
The indices may be placed up or down.
\end{definition}

\begin{lemma} \label{LeviCivitaProduct}
The Levi-Civita symbol is given by the explicit expression
\[ \varepsilon_{a_{1}\ldots a_{n}} = \prod_{1\leq i<j\leq n}\sgn(a_j-a_i).\]
\end{lemma}

\begin{proposition}
Working in $n$ dimensions, when all $i_1,\ldots i_n;j_1,\ldots, j_n$ take values in $\{ 1,\ldots, n \}$:
\begin{enumerate}
\item $\displaystyle \varepsilon_{i_1\ldots i_n}\varepsilon^{j_1\ldots j_n} = n!\delta^{j_1}_{[i_1}\ldots \delta^{j_n}_{i_n]} = \sum_{\sigma\in S_n} (-1)^{{\sgn}(\sigma)} \delta^{j_1}_{i_{\sigma(1)}} \dots \delta^{j_n}_{i_{\sigma(n)}}$
\item $\displaystyle \varepsilon _{i_{1}\dots i_{n}}\varepsilon ^{i_{1}\dots i_{n}}=n!$
\item $\displaystyle \varepsilon _{i_{1}\dots i_{k}~i_{k+1}\dots i_{n}}\varepsilon ^{i_{1}\dots i_{k}~j_{k+1}\dots j_{n}}=k!(n-k)!~\delta _{[i_{k+1}}^{j_{k+1}}\dots \delta _{i_{n}]}^{j_{n}}$.
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item Both sides of the equation are a sum over the same indices. We consider each term in the sum separately and show that the sums are equal term-by-term. We split the terms into two categories.
\begin{enumerate}
\item First consider the case that $j_1\ldots j_n$ is not a permutation of $(1,\ldots, n)$, i.e.\ a number is repeated. Then $\varepsilon_{i_1\ldots i_n}\varepsilon_{j_1\ldots j_n}$ is automatically zero. The right-hand side is definitely zero if the $i$s do not take the same values as the $j$s. If they do take the same values, there is a number that is repeated at least twice. For every term in the sum over permutations, there is another term with the repeated $i$s swapped, which also adds a minus due to the change of sign of the permutation. Hence the sum over permutations is zero.
\item Now assume that $j_1\ldots j_n$ is a permutation of $(1,\ldots, n)$. Then either the $i$s are also a permutation, or $\delta^{j_1}_{i_{\sigma(1)}} \dots \delta^{j_n}_{i_{\sigma(n)}}$ is always zero. The only possible non-zero term is with a $\sigma\in S_n$ such that $j_k = i_{\sigma(k)}$ for all $k$. If $\sgn(i_1,\ldots, i_n) = \sgn(j_1,\ldots, j_n)$, then $\sgn(\sigma)=1$ and both sides match. If $\sgn(i_1,\ldots, i_n) = -\sgn(j_1,\ldots, j_n)$, then $\sgn(\sigma)=-1$ and both sides again match. 
\end{enumerate}
So, in fact, we have shown something slightly stronger, namely 
\[ \varepsilon_{i_1\ldots i_n}\varepsilon_{j_1\ldots j_n} = n!\delta^{j_1}_{[i_1}\ldots \delta^{j_n}_{i_n]} \]
where there is no sum over indices.
\item The number of permutations of any $n$-element set number is exactly $n!$. Every permutation is either even or odd and $(+1)^2 = (-1)^2 = 1$. Non-permutations do not contribute to the sum.
\item The sum on the left only has terms where the $i$s and $j$s are permutations of $(1,\ldots, n)$. In each such term we can bring the indices with values $1-k$ to the first $k$ spots, each by a transposition. Because both Levi-Civita symbols have the same first $k$ indices, each will need the same number of transpositions and thus the sign does not change. Then by considering lemma \ref{LeviCivitaProduct} we see that we have obtained a product of cases 1. and 2. This yields the answer.

\end{enumerate}
\end{proof}
\begin{corollary}
In two dimensions, where all $i,j,m,n$ each take values in $\{1,2\}$,
\begin{enumerate}
\item $\varepsilon _{ij}\varepsilon ^{mn}={\delta _{i}}^{m}{\delta _{j}}^{n}-{\delta _{i}}^{n}{\delta _{j}}^{m}$
\item $\varepsilon _{ij}\varepsilon ^{in}={\delta _{j}}^{n}$
\item $\varepsilon _{ij}\varepsilon ^{ij}=2.$
\end{enumerate}
\end{corollary}
\begin{corollary}
In three dimensions, where all $i,j,k,m,n$ each take values in $\{1,2,3\}$,
\begin{enumerate}
\item $\varepsilon _{ijk}\varepsilon ^{imn}={\delta _{j}}^{m}{\delta _{k}}^{n}-{\delta _{j}}^{n}{\delta _{k}}^{m}$
\item $\varepsilon _{jmn}\varepsilon ^{imn}={\delta _{j}}^{i}$
\item $\varepsilon _{ijk}\varepsilon ^{ijk}=6.$
\end{enumerate}
\end{corollary}
\begin{proposition}
Working in 3 dimensions,
\begin{align*}
\varepsilon _{ijk}\varepsilon _{lmn}&={\begin{vmatrix}\delta _{il}&\delta _{im}&\delta _{in}\\\delta _{jl}&\delta _{jm}&\delta _{jn}\\\delta _{kl}&\delta _{km}&\delta _{kn}\\\end{vmatrix}}\\[6pt]&=\delta _{il}\left(\delta _{jm}\delta _{kn}-\delta _{jn}\delta _{km}\right)-\delta _{im}\left(\delta _{jl}\delta _{kn}-\delta _{jn}\delta _{kl}\right)+\delta _{in}\left(\delta _{jl}\delta _{km}-\delta _{jm}\delta _{kl}\right).
\end{align*}
This can directly be generalised to $n$ dimensions.
\end{proposition}

\section{Writing matrix operations using using tensor notation}
A matrix $A$ with components $(A)_{i,j}$ becomes
\[ \tensor{A}{^i_j}(\vec{e}_i\otimes \vec{e}^j). \]
\subsection{Trace}
The trace of $\tensor{A}{^i_j}$ is $\tensor{A}{^i_i}$.
\subsection{Matrix multiplication}
\[ \tensor{(AB)}{^{i}_{k}}=\tensor{A}{^{i}_{j}}\tensor{B}{^{j}_{k}} \]
which in particular for matrix-vector multiplication becomes
\[ (Av)^i = \tensor{A}{^i_j} v^j. \]
\subsection{Transpose}
The transpose of $\tensor{A}{^i_j}$ is $\tensor{(A^\transp)}{^j_i}$.

Or: $(A^\transp)_{ab} = A_{ba}$ and $\tensor{(A^\transp)}{_i^j} = \tensor{A}{^j_i}$?
\subsection{Determinant}
\begin{align*}
\det(A) &= \varepsilon^{j_1\ldots j_n}\tensor{A}{^{1}_{j_1}}\ldots \tensor{A}{^{n}_{j_n}} \\
&= \frac{1}{n!}\varepsilon_{i_1\ldots i_n}\varepsilon^{j_1\ldots j_n}\tensor{A}{^{i_1}_{j_1}}\ldots \tensor{A}{^{i_n}_{j_n}}
\end{align*}

\chapter{Ordered vector spaces}
TODO link ordered groups.
\begin{definition}
Let $\sSet{\R, V, +}$ be a real vector space and $\precsim$ a preorder on the set $V$. Then $\precsim$ is a \udef{vector preorder} if it is compatible with the vector space structure as follows: $\forall x,y,z\in V, \lambda\in\R$
\begin{enumerate}
\item $x\precsim y$ implies $x+z \precsim y+z$;
\item if $\lambda\geq 0$, then $x \precsim y$ implies $\lambda x \precsim \lambda y$.
\end{enumerate}
We call $(\R, V, +, \precsim)$ a \udef{preordered vector space}.

\begin{itemize}
\item If $\precsim$ is a partial order, we call $(\R, V, +, \precsim)$ a \udef{partially ordered vector space} or simply a \udef{ordered vector space}.
\item If $\sSet{V, \precsim}$ is a lattice, we call $(\R, V, +, \precsim)$ a \udef{vector lattice} or a \udef{Riesz space}.
\end{itemize}
\end{definition}

\begin{lemma}
Let $\sSet{\R, V, +}$ be a real vector space and $\precsim$ a preorder on the set $V$. The compatibility of the order can equivalently be expressed by:
$\forall x,y,z\in V, \lambda\in\R$
\begin{enumerate}
\item $x \precsim y$ implies $x+z \precsim y+z$;
\item if $\lambda\geq 0$ and $0 \precsim x$, then $0 \precsim \lambda x$.
\end{enumerate}
\end{lemma}

\begin{lemma}
Let $V$ be a preordered vector space. For all $v,w \in V$ we have
\[ v \precsim w \;\iff\; 0 \precsim  w - v \;\iff\; -w \precsim -v.  \]
\end{lemma}
\begin{proof}
We get the implications
\[ v \precsim w \implies 0 \precsim  w - v \implies -w \precsim -v \implies v-w \precsim 0 \implies v\precsim w \]
by subsequently adding $-v, -w, v,w$ to both sides by compatibility of the order.
\end{proof}
\begin{corollary}
Let $V$ be a preordered vector space and $\alpha \in \R\setminus\{0\}$. Then for all $v,w\in V$
\[ v \precsim w \quad \iff \quad \begin{cases}
\alpha v \precsim \alpha w & (0 < \alpha) \\
\alpha v \succsim \alpha w & (\alpha > 0)
\end{cases}. \] 
\end{corollary}

\begin{lemma}
Let $V$ be a preordered vector space. For all $v,w, x, y \in V$ we have
\[ \begin{cases}
v \precsim w \\ x \precsim y
\end{cases} \implies v+ x \precsim w+y. \]
\end{lemma}
\begin{proof}
We calculate $v + x \leq w + x \leq w+y$.
\end{proof}

\begin{example}
\begin{itemize}
\item The finite-dimensional vector spaces $\R^n$ with coordinate-wise addition, scalar multiplication and order are Riesz spaces.
\item The finite-dimensional vector spaces $\R^n$ with coordinate-wise addition, scalar multiplication and lexicographical order are Riesz spaces.
\item Let $X$ be a set. The set $(X\to \R)$ is a real vector space with point-wise addition and scalar multiplication. If the order is also defined point-wise, i.e.\ $f \leq g$ iff $\forall x\in X: f(x) \leq g(x)$, then $(X\to \R)$ is a Riesz space.
\end{itemize}
\end{example}

\begin{lemma}
Let $X$ be a topological space. The spaces
\begin{enumerate}
\item $\cont(X,\R)$;
\item $\cont_0(X,\R)$;
\item $\cont_c(X,\R)$; and
\item $\cont_b(X,\R)$
\end{enumerate}
with point-wise operations are Riesz spaces.
\end{lemma}
\begin{proof}
In all these cases the join and meet of $f,g$ are given by
\begin{align*}
f \vee g &= \frac{1}{2}(f+g)+ \frac{1}{2}|f-g| \\
f \wedge g &= \frac{1}{2}(f+g) - \frac{1}{2}|f-g|.
\end{align*}
So the join and meet are still continuous and have the same properties as $f,g$.
\end{proof}

\section{Upsets and downsets}
\begin{lemma} \label{sumMultipleUpDownsets}
Let $V$ be a preordered vector space, $S\subseteq V$ a subset, $v\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item if $\alpha > 0$, then $(\alpha S)^u = \alpha S^u$ and $(\alpha S)^l = \alpha S^l$;
\item if $\alpha < 0$, then $(\alpha S)^u = \alpha S^l$ and $(\alpha S)^l = \alpha S^u$;
\item $(S+v)^u = S^u + v$ and $(S+v)^l = S^l + v$.
\end{enumerate}
\end{lemma}
\begin{corollary}
Let $V$ be a preordered vector space, $S\subseteq V$ a subset, $v\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item $\sup(S+v) = \sup(S)+v$ and $\inf(S+v) = \inf(S)+v$;
\item if $\alpha > 0$, then $\sup(\alpha S) = \alpha \sup(S)$ and $\inf(\alpha S) = \alpha \inf(S)$;
\item if $\alpha < 0$, then $\sup(\alpha S) = \alpha \inf(S)$ and $\inf(\alpha S) = \alpha \sup(S)$.
\end{enumerate}
\end{corollary}

\section{The positive cone}
\begin{definition}
Let $V$ be a preordered vector space. The subset
\[ V^+ \defeq \setbuilder{v\in V}{0 \precsim v} \]
is called the \udef{positive cone} of $V$. The elements of the positive cone $V^+$ are called the \udef{positive elements} of $V$.
\end{definition}

\begin{lemma}
Let $V$ be a preordered vector space. Then
\begin{enumerate}
\item $V^+$ is a pointed cone;
\item $V^+$ is convex;
\item $V^+$ is closed under addition.
\end{enumerate}
If the order on $V$ is a partial order, then
\begin{enumerate} \setcounter{enumi}{3}
\item if $v\in V^+$ and $-v\in V^+$, then $v = 0$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) $V^+$ is clearly a cone by compatibility of the order. It is pointed by reflexivity: $0\precsim 0$.

(2) and (3) are equivalent by \ref{convexityAdditiveClosure} and (3) is also an immediate consequence of the compatibility of the order.

(4) If $v\in V^+$ and $-v\in V^+$, then $v\precsim 0$ and $0\precsim v$, so $v=0$ by anti-symmetry.
\end{proof}

\begin{lemma} \label{scalarMultiplicationInequalities}
Let $V$ be a preordered vector space, $v\in V^+$ and $\alpha\in \R$.
\begin{enumerate}
\item If $\alpha \geq 1$, then $\alpha v \succsim v$.
\item If $\alpha \leq 1$, then $\alpha v \precsim v$.
\end{enumerate}
\end{lemma}
\begin{proof}
(1) We have $v\succsim 0$ and $(\alpha-1) \geq 0$, so $(\alpha-1)v \succsim 0$ and $\alpha v \succsim v$.

(2) We have $v\succsim 0$ and $(\alpha-1) \leq 0$, so $(\alpha-1)v \precsim 0$ and $\alpha v \precsim v$.
\end{proof}

\section{Riesz spaces}

\begin{lemma} \label{lemmaRieszSpaces}
Let $V$ be a Riesz space, $u,v,w\in V$ and $\alpha\in \R$, then
\begin{enumerate}
\item $-(v \wedge w) = (-v)\vee (-w)$ and $-(v \vee w) = (-v)\wedge (-w)$;
\item if $\alpha \geq 0$, then $\alpha(v \wedge w) = (\alpha v)\wedge (\alpha w)$ and $\alpha(v \vee w) = (\alpha v)\vee (\alpha w)$;
\item if $\alpha \leq 0$, then $\alpha(v \wedge w) = (\alpha v)\vee (\alpha w)$ and $\alpha(v \vee w) = (\alpha v)\wedge (\alpha w)$;
\item $u+(v \wedge w) = (u+v)\wedge (u+w)$ and $u+(v \vee w) = (u+v)\vee (u+w)$.
\end{enumerate}
\end{lemma}
\begin{proof}
We apply \ref{imagePolars} to

(1) the reverse order-embedding $v\mapsto -v$;

(2) the order-embedding $v\mapsto \alpha v$ for $\alpha > 0$; (if $\alpha = 0$ the result is trivial);

(3) the reverse order-embedding $v\mapsto \alpha v$ for $\alpha > 0$; (if $\alpha = 0$ the result is trivial);

(4) the order-embedding $v\mapsto u+v$.
\end{proof}

\begin{proposition}[Riesz decomposition]
Let $V$ be a Riesz space and $v,w_1,w_2\in V^+$ such that $v \leq w_1 + w_2$. Then $\exists v_1, v_2\in V^+$ such that $v = v_1 + v_2$ and $v_1 \leq w_2, v_2 \leq w_2$.
\end{proposition}
\begin{proof}
Set $v_1 = v\wedge w_1$ and $v_2 = v - v_1$. These satisfy all the properties. We verify the inequality $v_2 \leq w_2$: from $w_2 \geq v - w_1$ we get
\[ w_2 = 0\vee w_2 \geq 0\vee (v - w_1) = v + (-v)\vee(-w_1) = v - v\wedge w_1 = v-v_1 = v_2. \]
\end{proof}

\begin{proposition} \label{sumAsMeetJoin}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ (v \vee w) + (v \wedge w) = v+w. \]
\end{proposition}
\begin{proof}
We calculate
\[ (v \vee w) + (v \wedge w) = v + 0 \vee (w-v) + w + (v-w)\wedge 0 = (v + w) + 0 \vee (w-v) - 0 \vee (w-v) = v + w. \]
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space, $u,v,w\in V$ and $x,y,z\in V^+$. Then
\begin{enumerate}
\item $(u+v)\vee (2w) \leq u\vee w + v\vee w$;
\item $(u+v)\vee z \leq u\vee z + v\vee z$;
\item $(x+y)\wedge z \leq x\wedge z + y\wedge z$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) From $u\leq u\vee w$ and $v\leq v\vee w$, we get $u+v \leq u\vee w + v\vee w$. Similarly from $w\leq u\vee w$ and $w\leq v\vee w$, we get $2w \leq u\vee w + v\vee w$. Together this gives (1).

(2) We have $2z \geq z$ by \ref{scalarMultiplicationInequalities}.

(3) TODO (use Birkhoff inequality??)
\end{proof}

\begin{proposition}[Infinite distributivity in Riesz spaces]
Let $V$ be a Riesz space, $v\in V$ and $S\subseteq V$ a subset. Then
\begin{enumerate}
\item if $\bigvee S$ exists, then $\left(\bigvee S\right) \wedge v = \bigvee (S\wedge v)$;
\item if $\bigwedge S$ exists, then $\left(\bigwedge S\right) \vee v = \bigwedge (S\vee v)$;
\end{enumerate}
\end{proposition}
\begin{proof}
We already have the inequality $\left(\bigvee S\right) \wedge v \geq \bigvee (S\wedge v)$ from \ref{infiniteDistributiveInequalities}. To show the other inequality, it is enough to show that for 
\end{proof}
\begin{corollary}
Riesz spaces are distributive lattices.
\end{corollary}

\subsection{Positive elements}
\subsubsection{Positive and negative parts}
\begin{definition}
Let $V$ be a Riesz space and $v\in V$. Then we define
\begin{align*}
v^+ &\defeq v \vee 0 \\
v^- &\defeq (-v) \vee 0 = - (v \wedge 0).
\end{align*}
We call $v^+$ the \udef{positive part} of $v$ and $v^-$ the \udef{negative part} of $v$.
\end{definition}

\begin{lemma} \label{MeetJoinAsPositiveNegative}
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $v\vee w = (v-w)^+ + w = (v-w)^- + v$;
\item $v\wedge w = v - (v-w)^+ = w - (v-w)^-$.
\end{enumerate}
\end{lemma}
\begin{proof}
We calculate $v\vee w = (v-w)\vee 0 + w = (v-w)^+ + w$. The other equalities are similar.
\end{proof}

\begin{proposition} \label{PositiveNegativeElements} \label{minimalPositiveDecomposition} 
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $v^+, v^- \in V^+$;
\item $v= v^+ - v^-$;
\item $v^+\perp v^-$ (i.e.\ $v^+ \wedge v^- = 0$).
\end{enumerate}
Furthermore,
\begin{enumerate} \setcounter{enumi}{3}
\item if $p,q\in V$ satisfy 1 and 2, i.e.\ $p,q\in V^+$ and $v = p-q$, then $p \geq v^+$ and $q \geq v^-$; we may say $v=v^+-v^-$ is the minimal such decomposition; 
\item the elements $v^+, v^-$ are uniquely determined by properties 2 and 3. 
\end{enumerate}
Also
\begin{enumerate} \setcounter{enumi}{5}
\item $(-v)^- = v^+$ and $(-v)^+ = v^-$;
\item if $\alpha \geq 0$, then $(\alpha v)^+ = \alpha v^+$ and $(\alpha v)^- = \alpha v^-$;
\item $-v^- \leq v \leq v^+$;
\item $v\leq w$ \textup{if and only if} $v^+ \leq w^+$ and $v^- \geq w^-$.
\end{enumerate}
\end{proposition}
\begin{proof}
(1) Evident from definitions.

(2) We calculate $v^+ - v = (v \vee 0) - v = (v-v) \vee (0-v) = 0\vee (-v) = v^-$.

(3) We calculate $0 = v^- - v^- = v^-  + (v\wedge 0) = (v^- + v)\wedge (0 + v^-) = v^+ \wedge v^-$.

(4) From $v\leq p$ and $0\leq p$, we get $v^+ = v \vee 0 \leq p$. Then we also have $v^- = v^+ - v \leq p - v = q$.

(5) Assume $p,q\in V$ satisfy (2) and (3), then (1) automatically follows from (3). Using \ref{MeetJoinAsPositiveNegative}, we calculate
\[ 0 = p\wedge q = p - (p-q)^+ = p - v^+. \]
So $p = v^+$ and $q = p - v = v^+ - v = v^-$.

(6) It is evident that $(-v)^- = (--v)\vee 0 = v\vee 0$.

(7) We calculate $\alpha v^+ = \alpha (v \vee 0) = (\alpha v) \vee 0 = (\alpha v)^+$; the calculation for $\alpha v^-$ is similar.

(8) This is clear from $-v^- = v\wedge 0 \;\leq\; v \;\leq\; v \vee 0 = v^+$.

(9) $v\leq w$ implies $v^+ = v\vee 0 \leq w\vee 0 = w^+$ and $-v^- = v\wedge 0 \leq w\wedge 0 = - w^-$.

Conversely, we have $v = v^+ - v^- \leq w^+ - w^- = w$.
\end{proof}


\begin{proposition} \label{triangleInequalityPositiveNegativeElements}
Let $V$ be a Riesz space and $v,w\in V$. Then
\begin{enumerate}
\item $(v+w)^+ \leq v^+ + w^+$;
\item $(v+w)^- \leq v^- + w^-$.
\end{enumerate}
\end{proposition}
\begin{proof}
From \ref{PositiveNegativeElements} we get $v \leq v^+$ and $w\leq w^+$, so $v+w \leq v^+ + w^+$. Also $0 \leq v^+ + w^+$. So
\[ (v+w)^+ = (v+w)\vee 0 \leq v^+ + w^+. \]
Then we also have
\[ (v+w)^- = (-v-w)^+ \leq (-v)^+ + (-w)^+ = v^- + w^-. \]
\end{proof}

\subsubsection{Absolute value}
\begin{definition}
Let $V$ be a Riesz space and $v\in V$. Then the \udef{absolute value} of $v$ is
\[ |v| \defeq v\vee (-v) = -(v\wedge (-v)). \]
\end{definition}

If the Riesz space is a real function space with pointwise order, then $|f| = |\cdot|\circ f$ as usual, where $|\cdot|: \R\to \R$ is the usual absolute value function.

\begin{lemma} \label{absoluteValue}
Let $V$ be a Riesz space, $v,w\in V$ and $\alpha\in \R$. Then
\begin{enumerate}
\item $|v| = v^+ + v^-$;
\item $|v| \in V^+$;
\item if $v\in V^+$, then $|v| = v$;
\item $|v| = |-v|$;
\item $|\alpha v| = |\alpha|\cdot |v|$;
\item $\big||v|\big| = |v|$;
\item $0 \leq v^+ \leq |v|$ and $0 \leq v^- \leq |v|$;
\item $|v| = 0$ \textup{if and only if} $v = 0$.
\end{enumerate}
\end{lemma}
\begin{proof}
We prove (1):
\[ |v| = v\vee (-v) = (2v)\vee 0 - v = 2v^+ - v = 2v^+ - (v^+ - v^-) = v^+ - v^- \]
and (6), using the absorption law:
\[ \big||v|\big| = |v|\vee (-|v|) = v \vee (-v) \vee \big( v\wedge (-v) \big) = v \vee (-v) = |v|. \]
The rest are immediate consequences, using the results of \ref{PositiveNegativeElements}.
\end{proof}

\begin{lemma}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $(v+w)\vee (v-w) = v + |w|$;
\item $(v+w)\wedge (v-w) = v - |w|$;
\end{enumerate}
or, equivalently,
\begin{enumerate} \setcounter{enumi}{2}
\item $v \vee w = \frac{1}{2}\big(v+w + |v - w|\big)$;
\item $v \wedge w = \frac{1}{2}\big(v+w - |v - w|\big)$.
\end{enumerate}
\end{lemma}
\begin{proof}
We calculate, using \ref{lemmaRieszSpaces}
\[ (v+w)\vee (v-w) = v+ w\vee(-w) = v+ |w| \quad\text{and}\quad (v+w)\wedge (v-w) = v + w\wedge(-w) = v-|w|. \]
The next two equalities follow by the substitutions $v+w \leftrightarrow v$ and $v-w \leftrightarrow w$.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |v - w| = (v \vee w) - (v \wedge w). \]
\end{corollary}
\begin{corollary} \label{meetJoinAbsoluteValues}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $|v| \vee |w| = \frac{1}{2}\Big(|v|+|w| + \big||v| - |w|\big|\Big)$;
\item $|v| \wedge |w| = \frac{1}{2}\Big(|v|+|w| - \big||v| - |w|\big|\Big)$.
\end{enumerate}
\end{corollary}
\begin{proof}
Substitute $v\to |v|$ and $w\to |w|$.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $u,v,w\in V$, then
\begin{enumerate}
\item $|u\vee v - u\vee w| + |u\wedge v - u\wedge w| = |v-w|$;
\item $|v^+-w^+|\leq |v-w|$ and $|v^- - w^-|\leq |v-w|$.
\end{enumerate}
\end{corollary}
\begin{proof}
(1) Using the proposition, we get
\[ |u\vee v - u\vee w| + |u\wedge v - u\wedge w| = (u\vee v)\vee(u\vee w) - (u\vee v)\wedge (u\vee w) + (u\wedge v)\vee(u\wedge w) - (u\wedge v)\wedge(u\wedge w). \]
Using distributivity this simplifies to $(v\vee w) - (v\wedge w) = |v-w|$.

(2) Using (1) we have
\[ |v-w| = |0\vee v - 0\vee w| + |0\wedge v - 0\wedge w| = |v^+ -w^+| + |v^- - w^-| \geq \begin{cases}
|v^+ -w^+| \\
|v^- - w^-|.
\end{cases}  \]
\end{proof}

\begin{proposition}
Let $V$ be a Riesz space and $v,w\in V$, then
\begin{enumerate}
\item $|v|\vee |w| = \frac{1}{2}\Big( |v+w| + |v - w| \Big)$;
\item $|v|\wedge |w| = \frac{1}{2}\Big| |v+w| - |v - w| \Big|$;
\end{enumerate}
or, equivalently,
\begin{enumerate} \setcounter{enumi}{2}
\item $|v+w|\vee |v-w| = |v| + |w|$;
\item $|v+w|\wedge |v-w| = \big| |v| - |w| \big|$.
\end{enumerate}
Also
\begin{enumerate} \setcounter{enumi}{4}
\item $|v|+|w| = |v+w| + |v-w| - \big||v|-|w|\big|$;
\item $|v+w|+|v-w| = 2|v| + 2|w| - \big||v+w|-|v-w|\big|$;
\end{enumerate}
and
\begin{enumerate} \setcounter{enumi}{6}
\item $|v|+|w| = \big||v|-|w|\big| + \big||v+w|-|v-w|\big|$.
\end{enumerate}
\end{proposition}
The only real trick is in the proof of (1). All the other results follow from elementary substitutions.
\begin{proof}
(3,4,6) Are equivalent to (1,2,5) by the replacements $v \leftrightarrow v+w$ and $w \leftrightarrow v-w$.

(1) We calculate
\begin{align*}
|v|\vee |w| &= v\vee (-v)\vee w \vee (-w) = \big(v\vee(-w)\big)\vee \big((-v)\vee w\big) \\
&= \frac{1}{2}\Big((v-w) + |v + w|\Big)\vee \frac{1}{2}\Big( (-v+w) + |- v - w| \Big) \\
&= \frac{1}{2}|v+ w| + \frac{1}{2}\big((v-w)\vee (-v+w)\big) = \frac{1}{2}\Big( |v+w| + |v - w| \Big).
\end{align*}

(5) Using \ref{sumAsMeetJoin}, (1) and \ref{meetJoinAbsoluteValues} we get
\begin{align*}
|v|+|w| &= |v|\vee|w| + |v|\wedge |w| \\
&= \frac{1}{2}\Big( |v+w| + |v - w| \Big) + \frac{1}{2}\Big(|v|+|w| - \big||v| - |w|\big|\Big).
\end{align*}
This simplifies to the required equation.

(7) Follows from substituting (6) into (5).

(2) Follows from (7) and \ref{meetJoinAbsoluteValues}.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $v,w\in V$, then the following are equivalent:
\begin{enumerate}
\item $|v|\wedge |w| = 0$;
\item $|v+w| = |v-w|$;
\item $|v|\vee |w| = |v+w|$.
\end{enumerate}
\end{corollary}

The absolute value also satisfies the triangle inequality.
\begin{proposition}[Triangle and reverse triangle inequality in Riesz spaces]
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |v| + |w| \geq \big|v+w\big| \geq \big||v|-|w|\big|. \]
\end{proposition}
\begin{proof}
The first inequality is the triangle inequality. It follows straight from \ref{triangleInequalityPositiveNegativeElements}.

The second inequality is the reverse triangle inequality and follows from the triangle inequality as in \ref{reverseTriangleInequality}.
\end{proof}

\subsection{Subsets}
\begin{definition}
Let $V$ be a Riesz space. A subset $E$ is called
\begin{itemize}
\item a \udef{Riesz subspace} if it is both a subspace and a sublattice;
\item \udef{solid} if for all $v\in E$ the interval $[-|v|,|v|]$ is a subset of $E$;
\item a \udef{band} if for all subsets $S\subseteq E$ we have $\sup(S) \subset E$. 
\end{itemize}
\end{definition}

\begin{lemma}
Let $V$ be a Riesz space and $v,w\in V$, then
\[ |w|\leq |v| \iff -|v| \leq w \leq |v|. \]
\end{lemma}
\begin{proof}
We have $w \leq |w|$ and $|w| \leq |v|$, so $w\leq |v|$. Also $-w \leq |-w| = |w|$, so $-|v| \leq -|w| \leq w$.

Conversely, $-|v| \leq w$ implies $-w\leq |v|$. So $|w| = w\vee (-w) \leq |v|$.
\end{proof}
\begin{corollary}
Let $V$ be a Riesz space and $E\subseteq V$ a subset. Then $E$ is solid \textup{if and only if}
\[ \forall v\in E: \forall w\in V: \; |w|\leq |v| \implies w\in E. \]
\end{corollary}

\begin{lemma}
Let $V$ be a Riesz space and $E\subseteq V$ a subset. Then $E$ is an (order) ideal \textup{if and only if} it is a solid Riesz subspace.
\end{lemma}
\begin{proof}
TODO
\end{proof}


\subsection{Disjointness}

\subsection{Archimedean}

\chapter{Some results and applications}
\section{Rotations}
Rodrigues' rotation formula

eigenvectors and eigenvalues of rotation.
\section{Pauli matrices}

\[ \sigma_x = \begin{pmatrix}
0 & 1 \\ 1 & 0
\end{pmatrix} \qquad \sigma_y = \begin{pmatrix}
0 & -i \\ i & 0
\end{pmatrix} \qquad \sigma_z = \begin{pmatrix}
1 & 0 \\ 0 & -1
\end{pmatrix} \]
All have eigenvalues $\pm 1$. The eigenspaces are spanned by
\[ v_{x+} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ 1
\end{pmatrix}, \quad v_{x-} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ -1
\end{pmatrix}, \quad v_{y+} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ i
\end{pmatrix}, \quad v_{y-} = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ -i
\end{pmatrix}, \quad v_{z+} = \begin{pmatrix}
1 \\ 0
\end{pmatrix}, \quad v_{z-} = \begin{pmatrix}
0 \\ 1
\end{pmatrix}, \quad  \]

\[ \Tr[\sigma_i \sigma_j] = \delta_{ij} \]

